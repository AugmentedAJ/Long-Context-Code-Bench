{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 134945,
  "base_commit": "6b20d65a53e13e7b0b6060caba0da8c96e52ca30",
  "runner": "factory",
  "model": "mcp:glm-4.6",
  "timeout_s": 1800,
  "status": "success",
  "elapsed_ms": 417529,
  "patch_unified": "diff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/services/SenderService.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/services/SenderService.java\nindex f483eaac..57bb636f 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/services/SenderService.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/services/SenderService.java\n@@ -75,31 +75,29 @@ public abstract class SenderService implements InferenceService {\n     ) {\n         timeout = ServiceUtils.resolveInferenceTimeout(timeout, inputType, clusterService);\n         init();\n-        var chunkInferenceInput = input.stream().map(i -> new ChunkInferenceInput(i, null)).toList();\n-        var inferenceInput = createInput(this, model, chunkInferenceInput, inputType, query, returnDocuments, topN, stream);\n+        var inferenceInput = createInput(this, model, input, inputType, query, returnDocuments, topN, stream);\n         doInfer(model, inferenceInput, taskSettings, timeout, listener);\n     }\n \n     private static InferenceInputs createInput(\n         SenderService service,\n         Model model,\n-        List<ChunkInferenceInput> input,\n+        List<String> input,\n         InputType inputType,\n         @Nullable String query,\n         @Nullable Boolean returnDocuments,\n         @Nullable Integer topN,\n         boolean stream\n     ) {\n-        List<String> textInput = ChunkInferenceInput.inputs(input);\n         return switch (model.getTaskType()) {\n-            case COMPLETION, CHAT_COMPLETION -> new ChatCompletionInput(textInput, stream);\n+            case COMPLETION, CHAT_COMPLETION -> new ChatCompletionInput(input, stream);\n             case RERANK -> {\n                 ValidationException validationException = new ValidationException();\n                 service.validateRerankParameters(returnDocuments, topN, validationException);\n                 if (validationException.validationErrors().isEmpty() == false) {\n                     throw validationException;\n                 }\n-                yield new QueryAndDocsInputs(query, textInput, returnDocuments, topN, stream);\n+                yield new QueryAndDocsInputs(query, input, returnDocuments, topN, stream);\n             }\n             case TEXT_EMBEDDING, SPARSE_EMBEDDING -> {\n                 ValidationException validationException = new ValidationException();\n@@ -107,7 +105,9 @@ public abstract class SenderService implements InferenceService {\n                 if (validationException.validationErrors().isEmpty() == false) {\n                     throw validationException;\n                 }\n-                yield new EmbeddingsInput(input, inputType, stream);\n+                // Convert to ChunkInferenceInput only for embedding tasks\n+                var chunkInferenceInput = input.stream().map(i -> new ChunkInferenceInput(i, null)).toList();\n+                yield new EmbeddingsInput(chunkInferenceInput, inputType, stream);\n             }\n             default -> throw new ElasticsearchStatusException(\n                 Strings.format(\"Invalid task type received when determining input type: [%s]\", model.getTaskType().toString()),",
  "logs_path": "factory/mcp:glm-4.6/6bf04592/elastic_elasticsearch_pr134945/logs.jsonl",
  "errors": [],
  "edit_run_id": "6bf04592",
  "test_label": "v1-mcp"
}