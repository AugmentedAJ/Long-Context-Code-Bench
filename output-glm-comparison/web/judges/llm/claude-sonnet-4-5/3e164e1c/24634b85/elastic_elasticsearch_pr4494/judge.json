{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 4494,
  "base_commit": "40ec7116d8bf5341f00dcc561d57788eb66e48ca",
  "head_commit": "a8ca8497c5a40586e4a48c754016074ac2a7137b",
  "judge_mode": "llm",
  "judge_model": "claude-sonnet-4-5",
  "scores": {
    "correctness": 0.0,
    "completeness": 0.0,
    "code_reuse": 0.0,
    "best_practices": 0.0,
    "unsolicited_docs": 0.0
  },
  "aggregate": 0.0,
  "rationale": "LLM judge failed: [Errno 7] Argument list too long: 'claude'",
  "edit_run_id": "24634b85",
  "judge_run_id": "3e164e1c",
  "ground_truth_patch": "diff --git a/.gitmodules b/.gitmodules\ndeleted file mode 100644\nindex 6072fee3e1a..00000000000\n--- a/.gitmodules\n+++ /dev/null\n@@ -1,3 +0,0 @@\n-[submodule \"rest-spec\"]\n-\tpath = rest-spec\n-\turl = git@github.com:elasticsearch/elasticsearch-rest-api-spec.git\ndiff --git a/TESTING.asciidoc b/TESTING.asciidoc\nindex d07a8c7506f..4079b99c43c 100644\n--- a/TESTING.asciidoc\n+++ b/TESTING.asciidoc\n@@ -166,45 +166,3 @@ even if tests are passing.\n ------------------------------\n mvn test -Dtests.output=always\n ------------------------------\n-\n-== Testing the REST layer\n-\n-The available integration tests make use of the java API to communicate with\n-the elasticsearch nodes, using the internal binary transport (port 9300 by\n-default).\n-The REST layer is tested through specific tests that are shared between all\n-the elasticsearch official clients and can be found on the\n-https://github.com/elasticsearch/elasticsearch-rest-api-spec[elasticsearch-rest-api-spec project].\n-They consist of\n-https://github.com/elasticsearch/elasticsearch-rest-api-spec/tree/master/test[YAML files]\n-that describe the operations to be executed and the obtained results that\n-need to be tested.\n-\n-`ElasticsearchRestTests` is the executable test class that runs all the\n-yaml suites available through a git submodule within the `rest-spec` folder.\n-The submodule gets automatically initialized through maven right before\n-running tests (generate-test-resources phase).\n-The REST tests cannot be run without the files pulled from the submodule,\n-thus if the `rest-spec` folder is empty on your working copy, it means\n-that it needs to be initialized with the following command:\n-\n-------------------------------\n-git submodule update --init\n-------------------------------\n-\n-The following are the options supported by the REST tests runner:\n-\n-* `tests.rest[true|false|host:port]`: determines whether the REST tests need\n-to be run and if so whether to rely on an external cluster (providing host\n-and port) or fire a test cluster (default)\n-* `tests.rest.suite`: comma separated paths of the test suites to be run\n-(by default loaded from /rest-spec/test). It is possible to run only a subset\n-of the tests providing a sub-folder or even a single yaml file (the default\n-/rest-spec/test prefix is optional when files are loaded from classpath)\n-e.g. -Dtests.rest.suite=index,get,create/10_with_id\n-* `tests.rest.spec`: REST spec path (default /rest-spec/api)\n-* `tests.iters`: runs multiple iterations\n-* `tests.seed`: seed to base the random behaviours on\n-* `tests.appendseed[true|false]`: enables adding the seed to each test\n-section's description (default false)\n-* `tests.cluster_seed`: seed used to create the test cluster (if enabled)\n\\ No newline at end of file\ndiff --git a/bin/elasticsearch b/bin/elasticsearch\nindex 794cd1b0895..9d1eeac6c90 100755\n--- a/bin/elasticsearch\n+++ b/bin/elasticsearch\n@@ -1,7 +1,7 @@\n #!/bin/sh\n \n # OPTIONS:\n-#   -d: daemonize, start in the background\n+#   -f: start in the foreground\n #   -p <filename>: log the pid to a file (useful to kill it later)\n \n # CONTROLLING STARTUP:\n@@ -46,20 +46,6 @@\n # Be aware that you will be entirely responsible for populating the needed\n # environment variables.\n \n-\n-# Maven will replace the project.name with elasticsearch below. If that\n-# hasn't been done, we assume that this is not a packaged version and the\n-# user has forgotten to run Maven to create a package.\n-IS_PACKAGED_VERSION='${project.name}'\n-if [ \"$IS_PACKAGED_VERSION\" != \"elasticsearch\" ]; then\n-    cat >&2 << EOF\n-Error: You must build the project with Maven or download a pre-built package\n-before you can run Elasticsearch. See 'Building from Source' in README.textile\n-or visit http://www.elasticsearch.org/download to get a pre-built package.\n-EOF\n-    exit 1\n-fi\n-\n CDPATH=\"\"\n SCRIPT=\"$0\"\n \n@@ -127,7 +113,7 @@ esac\n launch_service()\n {\n     pidpath=$1\n-    daemonized=$2\n+    foreground=$2\n     props=$3\n     es_parms=\"-Delasticsearch\"\n \n@@ -135,8 +121,8 @@ launch_service()\n         es_parms=\"$es_parms -Des.pidfile=$pidpath\"\n     fi\n \n-    # The es-foreground option will tell ElasticSearch not to close stdout/stderr, but it's up to us not to daemonize.\n-    if [ \"x$daemonized\" == \"x\" ]; then\n+    # The es-foreground option will tell ElasticSearch not to close stdout/stderr, but it's up to us not to background.\n+    if [ \"x$foreground\" != \"x\" ]; then\n         es_parms=\"$es_parms -Des.foreground=yes\"\n         exec \"$JAVA\" $JAVA_OPTS $ES_JAVA_OPTS $es_parms -Des.path.home=\"$ES_HOME\" -cp \"$ES_CLASSPATH\" $props \\\n                 org.elasticsearch.bootstrap.ElasticSearch\n@@ -150,24 +136,8 @@ launch_service()\n     fi\n }\n \n-# Parse any long getopt options and put them into properties before calling getopt below\n-# Be dash compatible to make sure running under ubuntu works\n-ARGV=\"\"\n-while [ $# -gt 0 ]\n-do\n-    case $1 in\n-      --*=*) properties=\"$properties -Des.${1#--}\"\n-           shift 1\n-           ;;\n-      --*) properties=\"$properties -Des.${1#--}=$2\"\n-           shift 2\n-           ;;\n-      *) ARGV=\"$ARGV $1\" ; shift\n-    esac\n-done\n-\n # Parse any command line options.\n-args=`getopt vfhp:D:X: $ARGV`\n+args=`getopt vfhp:D:X: \"$@\"`\n eval set -- \"$args\"\n \n while true; do\n@@ -181,12 +151,12 @@ while true; do\n             pidfile=\"$2\"\n             shift 2\n         ;;\n-        -d)\n-            daemonized=\"yes\"\n+        -f)\n+            foreground=\"yes\"\n             shift\n         ;;\n         -h)\n-            echo \"Usage: $0 [-d] [-h] [-p pidfile]\"\n+            echo \"Usage: $0 [-f] [-h] [-p pidfile]\"\n             exit 0\n         ;;\n         -D)\n@@ -209,6 +179,6 @@ while true; do\n done\n \n # Start up the service\n-launch_service \"$pidfile\" \"$daemonized\" \"$properties\"\n+launch_service \"$pidfile\" \"$foreground\" \"$properties\"\n \n exit $?\ndiff --git a/config/elasticsearch.yml b/config/elasticsearch.yml\nindex 9a5e9f182d7..1baec237fa3 100644\n--- a/config/elasticsearch.yml\n+++ b/config/elasticsearch.yml\n@@ -296,8 +296,8 @@\n # and master node is elected. Multicast discovery is the default.\n \n # Set to ensure a node sees N other master eligible nodes to be considered\n-# operational within the cluster. Its recommended to set it to a higher value\n-# than 1 when running more than 2 nodes in the cluster.\n+# operational within the cluster. Set this option to a higher value (2-4)\n+# for large clusters (>3 nodes):\n #\n # discovery.zen.minimum_master_nodes: 1\n \ndiff --git a/docs/community/clients.asciidoc b/docs/community/clients.asciidoc\nindex af37153ee6c..96b670e439c 100644\n--- a/docs/community/clients.asciidoc\n+++ b/docs/community/clients.asciidoc\n@@ -69,7 +69,9 @@ See the http://www.elasticsearch.org/guide/en/elasticsearch/client/ruby-api/curr\n \n \n [[community-javascript]]\n-=== Javascript\n+=== JavaScript\n+\n+See the http://www.elasticsearch.org/guide/en/elasticsearch/client/elasticsearch/index.html[official Elasticsearch JavaScript client].\n \n * https://github.com/fullscale/elastic.js[Elastic.js]:\n   A JavaScript implementation of the ElasticSearch Query DSL and Core API.\ndiff --git a/docs/javascript/index.asciidoc b/docs/javascript/index.asciidoc\nnew file mode 100644\nindex 00000000000..2364cc51493\n--- /dev/null\n+++ b/docs/javascript/index.asciidoc\n@@ -0,0 +1,138 @@\n+= elasticsearch-js\n+\n+== Overview\n+\n+Official low-level client for Elasticsearch. Its goal is to provide common\n+ground for all Elasticsearch-related code in JavaScript; because of this it tries\n+to be opinion-free and very extendable.\n+\n+The full documentation is available at http://elasticsearch.github.io/elasticsearch-js\n+\n+\n+=== Getting the Node.js module\n+\n+To install the module into an existing Node.js project use npm:\n+\n+[source,sh]\n+------------------------------------\n+npm install elasticsearch\n+------------------------------------\n+\n+=== Getting the browser client\n+\n+For a browser-based projects, builds for modern browsers are available http://elasticsearch.github.io/elasticsearch-js#browser-builds[here]. Download one of the archives and extract it, inside you'll find three files, pick the one that best matches your environment:\n+\n+ * elasticsearch.jquery.js - for projects that already use jQuery\n+ * elasticsearch.angular.js - for Angular projects\n+ * elasticsearch.js - generic build for all other projects\n+\n+Each of the library specific builds tie into the AJAX and Promise creation facilities provided by their respective libraries. This is an example of how Elasticsearch.js can be extended to provide a more opinionated approach when appropriate.\n+\n+=== Setting up the client\n+\n+Now you are ready to get busy! First thing you'll need to do is create an instance of `elasticsearch.Client`. Here are several examples of configuration parameters you can use when creating that instance. For a full list of configuration options see http://elasticsearch.github.io/elasticsearch-js/index.html#configuration[the configuration docs].\n+\n+[source,javascript]\n+------------------------------------\n+var elasticsearch = require('elasticsearch');\n+\n+// Connect to localhost:9200 and use the default settings\n+var client = new elasticsearch.Client();\n+\n+// Connect the client to two nodes, requests will be\n+// load-balanced between them using round-robin\n+var client = elasticsearch.Client({\n+  hosts: [\n+    'elasticsearch1:9200',\n+    'elasticsearch2:9200'\n+  ]\n+});\n+\n+// Connect to the this host's cluster, sniff\n+// for the rest of the cluster right away, and\n+// again every 5 minutes\n+var client = elasticsearch.Client({\n+  host: 'elasticsearch1:9200',\n+  sniffOnStart: true,\n+  sniffInterval: 300000\n+});\n+\n+// Connect to this host using https, basic auth,\n+// a path prefix, and static query string values\n+var client = new elasticsearch.Client({\n+  host: 'https://user:password@elasticsearch1/search?app=blog'\n+});\n+------------------------------------\n+\n+\n+=== Setting up the client in the browser\n+\n+The params accepted by the `Client` constructor are the same in the browser versions of the client, but how you access the Client constructor is different based on the build you are using. Below is an example of instantiating a client in each build.\n+\n+[source,javascript]\n+------------------------------------\n+// elasticsearch.js adds the elasticsearch namespace to the window\n+var client = elasticsearch.Client({ ... });\n+\n+// elasticsearch.jquery.js adds the es namespace to the jQuery object\n+var client = jQuery.es.Client({ ... });\n+\n+// elasticsearch.angular.js creates an elasticsearch\n+// module, which provides an esFactory\n+var app = angular.module('app', ['elasticsearch']);\n+app.service('es', function (esFactory) {\n+  return esFactory({ ... });\n+});\n+------------------------------------\n+\n+=== Using the client instance to make API calls.\n+\n+Once you create the client, making API calls is simple.\n+\n+[source,javascript]\n+------------------------------------\n+// get the current status of the entire cluster.\n+// Note: params are always optional, you can just send a callback\n+client.cluster.health(function (err, resp) {\n+  if (err) {\n+    console.error(err.message);\n+  } else {\n+    console.dir(resp);\n+  }\n+});\n+\n+// index a document\n+client.index({\n+  index: 'blog',\n+  type: 'post',\n+  id: 1,\n+  body: {\n+    title: 'JavaScript Everywhere!',\n+    content: 'It all started when...',\n+    date: '2013-12-17'\n+  }\n+}, function (err, resp) {\n+  // ...\n+});\n+\n+// search for documents (and also promises!!)\n+client.search({\n+  index: 'users',\n+  size: 50,\n+  body: {\n+    query: {\n+      match: {\n+        profile: 'elasticsearch'\n+      }\n+    }\n+  }\n+}).then(function (resp) {\n+  var hits = resp.body.hits;\n+});\n+------------------------------------\n+\n+== Copyright and License\n+\n+This software is Copyright (c) 2013 by Elasticsearch BV.\n+\n+This is free software, licensed under The Apache License Version 2.0.\n\\ No newline at end of file\ndiff --git a/docs/reference/cluster.asciidoc b/docs/reference/cluster.asciidoc\nindex 25576760723..6c7e4d7eb28 100644\n--- a/docs/reference/cluster.asciidoc\n+++ b/docs/reference/cluster.asciidoc\n@@ -33,8 +33,6 @@ include::cluster/health.asciidoc[]\n \n include::cluster/state.asciidoc[]\n \n-include::cluster/stats.asciidoc[]\n-\n include::cluster/pending.asciidoc[]\n \n include::cluster/reroute.asciidoc[]\ndiff --git a/docs/reference/cluster/stats.asciidoc b/docs/reference/cluster/stats.asciidoc\ndeleted file mode 100644\nindex 7ead927a62a..00000000000\n--- a/docs/reference/cluster/stats.asciidoc\n+++ /dev/null\n@@ -1,165 +0,0 @@\n-[[cluster-stats]]\n-== Cluster Stats\n-\n-added[0.90.8]\n-\n-The Cluster Stats API allows to retrieve statistics from a cluster wide perspective.\n-The API returns basic index metrics (shard numbers, store size, memory usage) and\n-information about the current nodes that form the cluster (number, roles, os, jvm\n-versions, memory usage, cpu and installed plugins).\n-\n-[source,js]\n---------------------------------------------------\n-curl -XGET 'http://localhost:9200/_cluster/stats?human'\n---------------------------------------------------\n-\n-Will return, for example:\n-[source,js]\n---------------------------------------------------\n-{\n-   \"cluster_name\": \"elasticsearch\",\n-   \"indices\": {\n-      \"count\": 3,\n-      \"shards\": {\n-         \"total\": 35,\n-         \"primaries\": 15,\n-         \"replication\": 1.333333333333333,\n-         \"index\": {\n-            \"shards\": {\n-               \"min\": 10,\n-               \"max\": 15,\n-               \"avg\": 11.66666666666666\n-            },\n-            \"primaries\": {\n-               \"min\": 5,\n-               \"max\": 5,\n-               \"avg\": 5\n-            },\n-            \"replication\": {\n-               \"min\": 1,\n-               \"max\": 2,\n-               \"avg\": 1.3333333333333333\n-            }\n-         }\n-      },\n-      \"docs\": {\n-         \"count\": 2,\n-         \"deleted\": 0\n-      },\n-      \"store\": {\n-         \"size\": \"5.6kb\",\n-         \"size_in_bytes\": 5770,\n-         \"throttle_time\": \"0s\",\n-         \"throttle_time_in_millis\": 0\n-      },\n-      \"fielddata\": {\n-         \"memory_size\": \"0b\",\n-         \"memory_size_in_bytes\": 0,\n-         \"evictions\": 0\n-      },\n-      \"filter_cache\": {\n-         \"memory_size\": \"0b\",\n-         \"memory_size_in_bytes\": 0,\n-         \"evictions\": 0\n-      },\n-      \"id_cache\": {\n-         \"memory_size\": \"0b\",\n-         \"memory_size_in_bytes\": 0\n-      },\n-      \"completion\": {\n-         \"size\": \"0b\",\n-         \"size_in_bytes\": 0\n-      },\n-      \"segments\": {\n-         \"count\": 2\n-      }\n-   },\n-   \"nodes\": {\n-      \"count\": {\n-         \"total\": 2,\n-         \"master_only\": 0,\n-         \"data_only\": 0,\n-         \"master_data\": 2,\n-         \"client\": 0\n-      },\n-      \"versions\": [\n-         \"0.90.8\"\n-      ],\n-      \"os\": {\n-         \"available_processors\": 4,\n-         \"mem\": {\n-            \"total\": \"8gb\",\n-            \"total_in_bytes\": 8589934592\n-         },\n-         \"cpu\": [\n-            {\n-               \"vendor\": \"Intel\",\n-               \"model\": \"MacBookAir5,2\",\n-               \"mhz\": 2000,\n-               \"total_cores\": 4,\n-               \"total_sockets\": 4,\n-               \"cores_per_socket\": 16,\n-               \"cache_size\": \"256b\",\n-               \"cache_size_in_bytes\": 256,\n-               \"count\": 1\n-            }\n-         ]\n-      },\n-      \"process\": {\n-         \"cpu\": {\n-            \"percent\": 3\n-         },\n-         \"avg_open_file_descriptors\": 218\n-      },\n-      \"jvm\": {\n-         \"max_uptime\": \"24s\",\n-         \"max_uptime_in_millis\": 24054,\n-         \"version\": [\n-            {\n-               \"version\": \"1.6.0_45\",\n-               \"vm_name\": \"Java HotSpot(TM) 64-Bit Server VM\",\n-               \"vm_version\": \"20.45-b01-451\",\n-               \"vm_vendor\": \"Apple Inc.\",\n-               \"count\": 2\n-            }\n-         ],\n-         \"mem\": {\n-            \"heap_used\": \"38.3mb\",\n-            \"heap_used_in_bytes\": 40237120,\n-            \"heap_max\": \"1.9gb\",\n-            \"heap_max_in_bytes\": 2130051072\n-         },\n-         \"threads\": 89\n-      },\n-      \"fs\":\n-         {\n-            \"total\": \"232.9gb\",\n-            \"total_in_bytes\": 250140434432,\n-            \"free\": \"31.3gb\",\n-            \"free_in_bytes\": 33705881600,\n-            \"available\": \"31.1gb\",\n-            \"available_in_bytes\": 33443737600,\n-            \"disk_reads\": 21202753,\n-            \"disk_writes\": 27028840,\n-            \"disk_io_op\": 48231593,\n-            \"disk_read_size\": \"528gb\",\n-            \"disk_read_size_in_bytes\": 566980806656,\n-            \"disk_write_size\": \"617.9gb\",\n-            \"disk_write_size_in_bytes\": 663525366784,\n-            \"disk_io_size\": \"1145.9gb\",\n-            \"disk_io_size_in_bytes\": 1230506173440\n-       },\n-      \"plugins\": [\n-         // all plugins installed on nodes\n-         {\n-            \"name\": \"inquisitor\",\n-            \"description\": \"\",\n-            \"url\": \"/_plugin/inquisitor/\",\n-            \"jvm\": false,\n-            \"site\": true\n-         }\n-      ]\n-   }\n-}\n---------------------------------------------------\n-\ndiff --git a/docs/reference/index-modules/fielddata.asciidoc b/docs/reference/index-modules/fielddata.asciidoc\nindex 01f094b86ac..d74d648700f 100644\n--- a/docs/reference/index-modules/fielddata.asciidoc\n+++ b/docs/reference/index-modules/fielddata.asciidoc\n@@ -26,8 +26,6 @@ example, can be set to `5m` for a 5 minute expiry.\n \n === Field data formats\n \n-The field data format controls how field data should be stored.\n-\n Depending on the field type, there might be several field data types\n available. In particular, string and numeric types support the `doc_values`\n format which allows for computing the field data data-structures at indexing\n@@ -35,9 +33,6 @@ time and storing them on disk. Although it will make the index larger and may\n be slightly slower, this implementation will be more near-realtime-friendly\n and will require much less memory from the JVM than other implementations.\n \n-Here is an example of how to configure the `tag` field to use the `fst` field\n-data format.\n-\n [source,js]\n --------------------------------------------------\n {\n@@ -50,25 +45,6 @@ data format.\n }\n --------------------------------------------------\n \n-It is possible to change the field data format (and the field data settings\n-in general) on a live index by using the update mapping API. When doing so,\n-field data which had already been loaded for existing segments will remain\n-alive while new segments will use the new field data configuration. Thanks to\n-the background merging process, all segments will eventually use the new\n-field data format.\n-\n-[float]\n-==== Disallowing field data loading\n-\n-Field data can take a lot of RAM so it makes sense to disable field data\n-loading on the fields that don't need field data, for example those that are\n-used for full-text search only. In order to disable field data loading, just\n-change the field data type to `disabled`. Request that will try to load field\n-data on any field which is configured with this format will then return an\n-error.\n-\n-The `disabled` format is supported by all field types.\n-\n [float]\n ==== String field data types\n \ndiff --git a/docs/reference/mapping/types/geo-point-type.asciidoc b/docs/reference/mapping/types/geo-point-type.asciidoc\nindex 19b38e5f124..2d843ae5c8b 100644\n--- a/docs/reference/mapping/types/geo-point-type.asciidoc\n+++ b/docs/reference/mapping/types/geo-point-type.asciidoc\n@@ -156,47 +156,6 @@ is `true`)\n |`normalize_lon` |Set to `true` to normalize longitude\n |=======================================================================\n \n-[float]\n-==== Field data\n-\n-By default, geo points use the `array` format which loads geo points into two\n-parallel double arrays, making sure there is no precision loss. However, this\n-can require a non-negligible amount of memory (16 bytes per document) which is\n-why Elasticsearch also provides a field data implementation with lossy\n-compression called `compressed`:\n-\n-[source,js]\n---------------------------------------------------\n-{\n-    \"pin\" : {\n-        \"properties\" : {\n-            \"location\" : {\n-                \"type\" : \"geo_point\",\n-                \"fielddata\" : {\n-                    \"format\" : \"compressed\",\n-                    \"precision\" : \"1cm\"\n-                }\n-            }\n-        }\n-    }\n-}\n---------------------------------------------------\n-\n-This field data format comes with a `precision` option which allows to\n-configure how much precision can be traded for memory. The default value is\n-`1cm`. The following table presents values of the memory savings given various\n-precisions:\n-\n-|=============================================\n-| Precision | Bytes per point | Size reduction\n-|       1km |               4 |            75%\n-|        3m |               6 |          62.5%\n-|       1cm |               8 |            50%\n-|       1mm |              10 |          37.5%\n-|=============================================\n-\n-Precision can be changed on a live index by using the update mapping API.\n-\n [float]\n ==== Usage in Scripts\n \ndiff --git a/docs/reference/query-dsl/filters/nested-filter.asciidoc b/docs/reference/query-dsl/filters/nested-filter.asciidoc\nindex 6235be215db..9c200b3474c 100644\n--- a/docs/reference/query-dsl/filters/nested-filter.asciidoc\n+++ b/docs/reference/query-dsl/filters/nested-filter.asciidoc\n@@ -1,8 +1,8 @@\n [[query-dsl-nested-filter]]\n === Nested Filter\n \n-A `nested` filter works in a similar fashion to the\n-<<query-dsl-nested-query,nested>> query, except it's\n+A `nested` filter, works in a similar fashion to the\n+<<query-dsl-nested-query,nested>> query, except\n used as a filter. It follows exactly the same structure, but also allows\n to cache the results (set `_cache` to `true`), and have it named (set\n the `_name` value). For example:\n@@ -37,8 +37,8 @@ the `_name` value). For example:\n [float]\n ==== Join option\n \n-The nested filter also supports a `join` option which controls whether to perform the block join or not.\n-By default, it's enabled. But when it's disabled, it emits the hidden nested documents as hits instead of the joined root document.\n+The nested filter also supports an `join` option. Which controls whether to perform the block join.\n+By default this enabled, but when disabled it emits the hidden nested documents as hits instead of the joined root document.\n \n This is useful when a `nested` filter is used in a facet where nested is enabled, like you can see in the example below:\n \ndiff --git a/docs/reference/query-dsl/queries.asciidoc b/docs/reference/query-dsl/queries.asciidoc\nindex 6572de6258d..e4d09b4da8b 100644\n--- a/docs/reference/query-dsl/queries.asciidoc\n+++ b/docs/reference/query-dsl/queries.asciidoc\n@@ -26,6 +26,8 @@ include::queries/constant-score-query.asciidoc[]\n \n include::queries/dis-max-query.asciidoc[]\n \n+include::queries/field-query.asciidoc[]\n+\n include::queries/filtered-query.asciidoc[]\n \n include::queries/flt-query.asciidoc[]\ndiff --git a/docs/reference/query-dsl/queries/field-query.asciidoc b/docs/reference/query-dsl/queries/field-query.asciidoc\nnew file mode 100644\nindex 00000000000..7affdb21b33\n--- /dev/null\n+++ b/docs/reference/query-dsl/queries/field-query.asciidoc\n@@ -0,0 +1,33 @@\n+[[query-dsl-field-query]]\n+=== Field Query\n+\n+A query that executes a query string against a specific field. It is a\n+simplified version of\n+<<query-dsl-query-string-query,query_string>>\n+query (by setting the `default_field` to the field this query executed\n+against). In its simplest form:\n+\n+[source,js]\n+--------------------------------------------------\n+{\n+    \"field\" : {\n+        \"name.first\" : \"+something -else\"\n+    }\n+}\n+--------------------------------------------------\n+\n+Most of the `query_string` parameters are allowed with the `field` query\n+as well, in such a case, the query should be formatted as follows:\n+\n+[source,js]\n+--------------------------------------------------\n+{\n+    \"field\" : {\n+        \"name.first\" : {\n+            \"query\" : \"+something -else\",\n+            \"boost\" : 2.0,\n+            \"enable_position_increments\": false\n+        }\n+    }\n+}\n+--------------------------------------------------\ndiff --git a/docs/reference/query-dsl/queries/query-string-syntax.asciidoc b/docs/reference/query-dsl/queries/query-string-syntax.asciidoc\nindex 2eaa66a9076..8fdc5ef59ca 100644\n--- a/docs/reference/query-dsl/queries/query-string-syntax.asciidoc\n+++ b/docs/reference/query-dsl/queries/query-string-syntax.asciidoc\n@@ -3,7 +3,7 @@\n ==== Query string syntax\n \n The query string ``mini-language'' is used by the\n-<<query-dsl-query-string-query>> and by the\n+<<query-dsl-query-string-query>> and <<query-dsl-field-query>>, by the\n `q` query string parameter in the <<search-search,`search` API>>.\n \n The query string is parsed into a series of _terms_ and _operators_. A\ndiff --git a/docs/reference/search/aggregations/bucket/terms-aggregation.asciidoc b/docs/reference/search/aggregations/bucket/terms-aggregation.asciidoc\nindex 35cfeb52d42..5427d6c95c9 100644\n--- a/docs/reference/search/aggregations/bucket/terms-aggregation.asciidoc\n+++ b/docs/reference/search/aggregations/bucket/terms-aggregation.asciidoc\n@@ -236,31 +236,4 @@ http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#LITERAL[`L\n http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#MULTILINE[`MULTILINE`],\n http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#UNICODE_CASE[`UNICODE_CASE`],\n http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#UNICODE_CHARACTER_CLASS[`UNICODE_CHARACTER_CLASS`] and\n-http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#UNIX_LINES[`UNIX_LINES`]\n-\n-==== Execution hint\n-\n-There are two mechanisms by which terms aggregations can be executed: either by using field values directly in order to aggregate\n-data per-bucket (`map`), or by using ordinals of the field values instead of the values themselves (`ordinals`). Although the\n-latter execution mode can be expected to be slightly faster, it is only available for use when the underlying data source exposes\n-those terms ordinals. Moreover, it may actually be slower if most field values are unique. Elasticsearch tries to have sensible\n-defaults when it comes to the execution mode that should be used, but in case you know that an execution mode may perform better\n-than the other one, you have the ability to provide Elasticsearch with a hint:\n-\n-[source,js]\n---------------------------------------------------\n-{\n-    \"aggs\" : {\n-        \"tags\" : {\n-             \"terms\" : {\n-                 \"field\" : \"tags\",\n-                 \"execution_hint\": \"map\" <1>\n-             }\n-         }\n-    }\n-}\n---------------------------------------------------\n-\n-<1> the possible values are `map` and `ordinals`\n-\n-Please note that Elasticsearch will ignore this execution hint if it is not applicable.\n+http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#UNIX_LINES[`UNIX_LINES`]\n\\ No newline at end of file\ndiff --git a/docs/reference/search/request-body.asciidoc b/docs/reference/search/request-body.asciidoc\nindex 7704bad5a32..0e93eeef303 100644\n--- a/docs/reference/search/request-body.asciidoc\n+++ b/docs/reference/search/request-body.asciidoc\n@@ -87,7 +87,7 @@ include::request/fields.asciidoc[]\n \n include::request/script-fields.asciidoc[]\n \n-include::request/post-filter.asciidoc[]\n+include::request/filter.asciidoc[]\n \n include::request/highlighting.asciidoc[]\n \ndiff --git a/docs/reference/search/request/post-filter.asciidoc b/docs/reference/search/request/filter.asciidoc\nsimilarity index 59%\nrename from docs/reference/search/request/post-filter.asciidoc\nrename to docs/reference/search/request/filter.asciidoc\nindex e9340021f0e..4b4ff317250 100644\n--- a/docs/reference/search/request/post-filter.asciidoc\n+++ b/docs/reference/search/request/filter.asciidoc\n@@ -1,21 +1,10 @@\n-[[search-request-post-filter]]\n-=== Post filter\n+[[search-request-filter]]\n+=== Filter\n \n-The `post_filter` allows any filter that it holds to be executed as last filter, because\n-of this the `post_filter` only has affect on the search hits and not facets.\n-\n-There are several reasons why to specify filters as `post_filter`. One reason is to force\n-expensive filters to be executed as last filter, so that these filters only operate on the\n-docs that match with the rest of the query. An example of for what filter a post_filter\n-should be used for this reason is the `geo_distance` filter. The `geo_distance` filter is\n-in general an expensive filter to execute and to reduce the execution time for this filter,\n-one can choose to specify it as `post_filter`, so it runs on documents that are very likely\n-to be end up as matching documents.\n-\n-Another important reason is when doing things like facet navigation,\n-sometimes only the hits are needed to be filtered by the chosen facet,\n-and all the facets should continue to be calculated based on the original query.\n-The `post_filter` element within the search request can be used to accomplish it.\n+When doing things like facet navigation, sometimes only the hits are\n+needed to be filtered by the chosen facet, and all the facets should\n+continue to be calculated based on the original query. The `filter`\n+element within the search request can be used to accomplish it.\n \n Note, this is different compared to creating a `filtered` query with the\n filter, since this will cause the facets to only process the filtered\n@@ -71,7 +60,7 @@ curl -XPOST 'localhost:9200/twitter/_search?pretty=true' -d '\n     \"query\" : {\n         \"term\" : { \"message\" : \"something\" }\n     },\n-    \"post_filter\" : {\n+    \"filter\" : {\n         \"term\" : { \"tag\" : \"green\" }\n     },\n     \"facets\" : {\n@@ -87,5 +76,3 @@ And now, we get only 1 hit back, but the facets remain the same.\n \n Note, if additional filters are required on specific facets, they can be\n added as a `facet_filter` to the relevant facets.\n-\n-deprecated[0.90.8, The top level `filter` has been renamed to `post_filter`]\n\\ No newline at end of file\ndiff --git a/docs/reference/search/request/highlighting.asciidoc b/docs/reference/search/request/highlighting.asciidoc\nindex fb6ccbf0a20..300b66d8f9d 100644\n--- a/docs/reference/search/request/highlighting.asciidoc\n+++ b/docs/reference/search/request/highlighting.asciidoc\n@@ -110,24 +110,6 @@ The following is an example that forces the use of the plain highlighter:\n }\n --------------------------------------------------\n \n-==== Force highlighting on source\n-\n-added[1.0.0.RC1]\n-\n-Forces the highlighting to highlight fields based on the source even if fields are\n-stored separately. Defaults to `false`.\n-\n-[source,js]\n---------------------------------------------------\n-{\n-    \"query\" : {...},\n-    \"highlight\" : {\n-        \"fields\" : {\n-            \"content\" : {\"force_source\" : true}\n-        }\n-    }\n-}\n---------------------------------------------------\n \n [[tags]]\n ==== Highlighting Tags\ndiff --git a/docs/reference/search/request/rescore.asciidoc b/docs/reference/search/request/rescore.asciidoc\nindex 563da4c0143..62d1d29ab44 100644\n--- a/docs/reference/search/request/rescore.asciidoc\n+++ b/docs/reference/search/request/rescore.asciidoc\n@@ -4,7 +4,7 @@\n Rescoring can help to improve precision by reordering just the top (eg\n 100 - 500) documents returned by the\n <<search-request-query,`query`>> and\n-<<search-request-post-filter,`post_filter`>> phases, using a\n+<<search-request-filter,`filter`>> phases, using a\n secondary (usually more costly) algorithm, instead of applying the\n costly algorithm to all documents in the index.\n \n@@ -23,7 +23,7 @@ to `scan` or `count`.\n \n The query rescorer executes a second query only on the Top-K results\n returned by the <<search-request-query,`query`>> and\n-<<search-request-post-filter,`post_filter`>> phases. The\n+<<search-request-filter,`filter`>> phases. The\n number of docs which will be examined on each shard can be controlled by\n the `window_size` parameter, which defaults to\n <<search-request-from-size,`from` and `size`>>.\ndiff --git a/docs/reference/search/validate.asciidoc b/docs/reference/search/validate.asciidoc\nindex 1dc0d5ad420..371c99ecd87 100644\n--- a/docs/reference/search/validate.asciidoc\n+++ b/docs/reference/search/validate.asciidoc\n@@ -32,7 +32,7 @@ curl -XGET 'http://localhost:9200/twitter/tweet/_validate/query' -d '{\n         \"query\" : \"*:*\"\n       }\n     },\n-    \"post_filter\" : {\n+    \"filter\" : {\n       \"term\" : { \"user\" : \"kimchy\" }\n     }\n   }\ndiff --git a/docs/reference/setup.asciidoc b/docs/reference/setup.asciidoc\nindex 799dd47f279..e5d31848739 100644\n--- a/docs/reference/setup.asciidoc\n+++ b/docs/reference/setup.asciidoc\n@@ -19,12 +19,12 @@ After link:/download[downloading] the latest release and extracting it,\n $ bin/elasticsearch\n --------------------------------------------------\n \n-Under *nix system, the command will start the process in the foreground.\n-To run it in the background, add the `-d` switch to it:\n+Under *nix system, the command will start the process in the background.\n+To run it in the foreground, add the `-f` switch to it:\n \n [source,sh]\n --------------------------------------------------\n-$ bin/elasticsearch -d\n+$ bin/elasticsearch -f\n --------------------------------------------------\n \n ElasticSearch is built using Java, and requires at least\n@@ -38,13 +38,13 @@ There are added features when using the `elasticsearch` shell script.\n The first, which was explained earlier, is the ability to easily run the\n process either in the foreground or the background.\n \n-Another feature is the ability to pass `-X` and `-D` or getopt long style\n-configuration parameters directly to the script. When set, all override\n-anything set using either `JAVA_OPTS` or `ES_JAVA_OPTS`. For example:\n+Another feature is the ability to pass `-X` and `-D` directly to the\n+script. When set, both override anything set using either `JAVA_OPTS` or\n+`ES_JAVA_OPTS`. For example:\n \n [source,sh]\n --------------------------------------------------\n-$ bin/elasticsearch -f -Xmx2g -Xms2g -Des.index.store.type=memory --node.name=my-node\n+$ bin/elasticsearch -f -Xmx2g -Xms2g -Des.index.store.type=memory\n --------------------------------------------------\n *************************************************************************\n --\ndiff --git a/docs/reference/setup/configuration.asciidoc b/docs/reference/setup/configuration.asciidoc\nindex 6d6a2d10902..8aca7313983 100644\n--- a/docs/reference/setup/configuration.asciidoc\n+++ b/docs/reference/setup/configuration.asciidoc\n@@ -157,7 +157,7 @@ command, for example:\n \n [source,sh]\n --------------------------------------------------\n-$ elasticsearch -Des.network.host=10.0.0.4\n+$ elasticsearch -f -Des.network.host=10.0.0.4\n --------------------------------------------------\n \n Another option is to set `es.default.` prefix instead of `es.` prefix,\n@@ -181,7 +181,7 @@ system property:\n \n [source,sh]\n --------------------------------------------------\n-$ elasticsearch -Des.config=/path/to/config/file\n+$ elasticsearch -f -Des.config=/path/to/config/file\n --------------------------------------------------\n \n [float]\n@@ -221,7 +221,7 @@ above can also be set as a \"collapsed\" setting, for example:\n \n [source,sh]\n --------------------------------------------------\n-$ elasticsearch -Des.index.store.type=memory\n+$ elasticsearch -f -Des.index.store.type=memory\n --------------------------------------------------\n \n All of the index level configuration can be found within each\ndiff --git a/docs/reference/setup/installation.asciidoc b/docs/reference/setup/installation.asciidoc\nnew file mode 100644\nindex 00000000000..20d14f62a09\n--- /dev/null\n+++ b/docs/reference/setup/installation.asciidoc\n@@ -0,0 +1,39 @@\n+[[setup-installation]]\n+== Installation\n+\n+After link:/download[downloading] the latest release and extracting it,\n+*elasticsearch* can be started using:\n+\n+[source,sh]\n+--------------------------------------------------\n+$ bin/elasticsearch\n+--------------------------------------------------\n+\n+Under *nix system, the command will start the process in the background.\n+To run it in the foreground, add the `-f` switch to it:\n+\n+[source,sh]\n+--------------------------------------------------\n+$ bin/elasticsearch -f\n+--------------------------------------------------\n+\n+ElasticSearch is built using Java, and requires at least\n+http://java.sun.com/javase/downloads/index.jsp[Java 6] in order to run.\n+The version of Java that will be used can be set by setting the\n+`JAVA_HOME` environment variable.\n+\n+.*NIX\n+*************************************************************************\n+There are added features when using the `elasticsearch` shell script.\n+The first, which was explained earlier, is the ability to easily run the\n+process either in the foreground or the background.\n+\n+Another feature is the ability to pass `-X` and `-D` directly to the\n+script. When set, both override anything set using either `JAVA_OPTS` or\n+`ES_JAVA_OPTS`. For example:\n+\n+[source,sh]\n+--------------------------------------------------\n+$ bin/elasticsearch -f -Xmx2g -Xms2g -Des.index.store.type=memory\n+--------------------------------------------------\n+*************************************************************************\ndiff --git a/pom.xml b/pom.xml\nindex 82063e9111c..9efebb749a7 100644\n--- a/pom.xml\n+++ b/pom.xml\n@@ -35,8 +35,6 @@\n         <tests.shuffle>true</tests.shuffle>\n         <tests.output>onerror</tests.output>\n         <tests.client.ratio></tests.client.ratio>\n-        <rest.pull.skip>false</rest.pull.skip>\n-        <rest.init.skip>false</rest.init.skip>\n         <es.logger.level>INFO</es.logger.level>\n     </properties>\n \n@@ -67,12 +65,6 @@\n             <version>${lucene.version}</version>\n             <scope>test</scope>\n         </dependency>\n-        <dependency>\n-            <groupId>org.apache.httpcomponents</groupId>\n-            <artifactId>httpclient</artifactId>\n-            <version>4.3.1</version>\n-            <scope>test</scope>\n-        </dependency>\n \n         <dependency>\n             <groupId>org.apache.lucene</groupId>\n@@ -209,21 +201,21 @@\n         <dependency>\n             <groupId>com.fasterxml.jackson.core</groupId>\n             <artifactId>jackson-core</artifactId>\n-            <version>2.3.0</version>\n+            <version>2.2.3</version>\n             <scope>compile</scope>\n         </dependency>\n \n         <dependency>\n             <groupId>com.fasterxml.jackson.dataformat</groupId>\n             <artifactId>jackson-dataformat-smile</artifactId>\n-            <version>2.3.0</version>\n+            <version>2.2.3</version>\n             <scope>compile</scope>\n         </dependency>\n \n         <dependency>\n             <groupId>com.fasterxml.jackson.dataformat</groupId>\n             <artifactId>jackson-dataformat-yaml</artifactId>\n-            <version>2.3.0</version>\n+            <version>2.2.3</version>\n             <scope>compile</scope>\n         </dependency>\n \n@@ -321,14 +313,6 @@\n                     <include>**/*.*</include>\n                 </includes>\n             </testResource>\n-            <testResource>\n-                <directory>${basedir}/rest-spec</directory>\n-                <targetPath>rest-spec</targetPath>\n-                <includes>\n-                    <include>api/*.json</include>\n-                    <include>test/**/*.yaml</include>\n-                </includes>\n-            </testResource>\n         </testResources>\n \n         <plugins>\n@@ -340,13 +324,6 @@\n                     <source>1.6</source>\n                     <target>1.6</target>\n                     <fork>true</fork>\n-                    <!-- REMOVE WHEN UPGRADE:\n-                         see https://jira.codehaus.org/browse/MCOMPILER-209 it's a bug where\n-                         incremental compilation doesn't work unless it's set to false causeing\n-                         recompilation of the entire codebase each time without any changes. Should\n-                         be fixed in version > 3.1\n-                     -->\n-                    <useIncrementalCompilation>false</useIncrementalCompilation>\n                 </configuration>\n             </plugin>\n             <plugin>\n@@ -412,7 +389,6 @@\n                             <systemProperties>\n                                 <!-- RandomizedTesting library system properties -->\n                                 <tests.jvm.argline>${tests.jvm.argline}</tests.jvm.argline>\n-                                <tests.appendseed>${tests.appendseed}</tests.appendseed>\n                                 <tests.iters>${tests.iters}</tests.iters>\n                                 <tests.maxfailures>${tests.maxfailures}</tests.maxfailures>\n                                 <tests.failfast>${tests.failfast}</tests.failfast>\n@@ -429,9 +405,6 @@\n                                 <tests.integration>${tests.integration}</tests.integration>\n                                 <tests.cluster_seed>${tests.cluster_seed}</tests.cluster_seed>\n                                 <tests.client.ratio>${tests.client.ratio}</tests.client.ratio>\n-                                <tests.rest>${tests.rest}</tests.rest>\n-                                <tests.rest.suite>${tests.rest.suite}</tests.rest.suite>\n-                                <tests.rest.spec>${tests.rest.spec}</tests.rest.spec>\n                                 <es.node.local>${env.ES_TEST_LOCAL}</es.node.local>\n                                 <es.node.mode>${es.node.mode}</es.node.mode>\n                                 <es.logger.level>${es.logger.level}</es.logger.level>\n@@ -1022,50 +995,14 @@\n                         <goals>\n                             <goal>exec</goal>\n                         </goals>\n-                        <configuration>\n-                            <executable>java</executable>\n-                            <arguments>\n-                                <argument>-version</argument>\n-                            </arguments>\n-                        </configuration>\n-                    </execution>\n-                    <execution>\n-                        <id>Init Rest Spec</id>\n-                        <phase>generate-test-resources</phase>\n-                        <goals>\n-                            <goal>exec</goal>\n-                        </goals>\n-                        <configuration>\n-                            <skip>${rest.init.skip}</skip>\n-                            <executable>git</executable>\n-                            <arguments>\n-                                <argument>submodule</argument>\n-                                <argument>update</argument>\n-                                <argument>--init</argument>\n-                            </arguments>\n-                        </configuration>\n-                    </execution>\n-                    <execution>\n-                        <id>Pull Rest Spec</id>\n-                        <phase>generate-test-resources</phase>\n-                        <goals>\n-                            <goal>exec</goal>\n-                        </goals>\n-                        <configuration>\n-                            <skip>${rest.pull.skip}</skip>\n-                            <executable>git</executable>\n-                            <arguments>\n-                                <argument>submodule</argument>\n-                                <argument>foreach</argument>\n-                                <argument>git</argument>\n-                                <argument>pull</argument>\n-                                <argument>origin</argument>\n-                                <argument>master</argument>\n-                            </arguments>\n-                        </configuration>\n                     </execution>\n                 </executions>\n-\n+                <configuration>\n+                    <executable>java</executable>\n+                    <arguments>\n+                        <argument>-version</argument>\n+                    </arguments>\n+                </configuration>\n             </plugin>\n             <plugin>\n                 <groupId>org.apache.maven.plugins</groupId>\n@@ -1082,14 +1019,7 @@\n                                 <include>org/elasticsearch/test/**/*</include>\n                                 <include>org/apache/lucene/util/AbstractRandomizedTest.class</include>\n                                 <include>org/apache/lucene/util/AbstractRandomizedTest$*.class</include> <!-- inner classes -->\n-                                <include>com/carrotsearch/randomizedtesting/StandaloneRandomizedContext.class</include>\n                             </includes>\n-                            <excludes>\n-                                <!-- we only distribute the REST tests runner, not the executable test  -->\n-                                <exclude>org/elasticsearch/test/rest/ElasticsearchRestTests.class</exclude>\n-                                <!-- unit tests for yaml suite parser & rest spec parser need to be excluded -->\n-                                <exclude>org/elasticsearch/test/rest/test/**/*</exclude>\n-                            </excludes>\n                         </configuration>\n                     </execution>\n                 </executions>\ndiff --git a/rest-spec b/rest-spec\ndeleted file mode 160000\nindex 2f5f78f24d8..00000000000\n--- a/rest-spec\n+++ /dev/null\n@@ -1 +0,0 @@\n-Subproject commit 2f5f78f24d8fbacf69c83ab7545654c83965e846\ndiff --git a/src/deb/init.d/elasticsearch b/src/deb/init.d/elasticsearch\nindex 93255a6fd0f..2008276f505 100755\n--- a/src/deb/init.d/elasticsearch\n+++ b/src/deb/init.d/elasticsearch\n@@ -103,7 +103,7 @@ fi\n # Define other required variables\n PID_FILE=/var/run/$NAME.pid\n DAEMON=$ES_HOME/bin/elasticsearch\n-DAEMON_OPTS=\"-d -p $PID_FILE -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\"\n+DAEMON_OPTS=\"-p $PID_FILE -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\"\n \n export ES_HEAP_SIZE\n export ES_HEAP_NEWSIZE\ndiff --git a/src/main/java/jsr166e/CompletableFuture.java b/src/main/java/jsr166e/CompletableFuture.java\nindex 2a9e0bb0492..d8a944f0040 100644\n--- a/src/main/java/jsr166e/CompletableFuture.java\n+++ b/src/main/java/jsr166e/CompletableFuture.java\n@@ -1392,7 +1392,6 @@ public class CompletableFuture<T> implements Future<T> {\n      *\n      * @param supplier a function returning the value to be used\n      * to complete the returned CompletableFuture\n-     * @param <U> the function's return type\n      * @return the new CompletableFuture\n      */\n     public static <U> CompletableFuture<U> supplyAsync(Generator<U> supplier) {\n@@ -1411,7 +1410,6 @@ public class CompletableFuture<T> implements Future<T> {\n      * @param supplier a function returning the value to be used\n      * to complete the returned CompletableFuture\n      * @param executor the executor to use for asynchronous execution\n-     * @param <U> the function's return type\n      * @return the new CompletableFuture\n      */\n     public static <U> CompletableFuture<U> supplyAsync(Generator<U> supplier,\n@@ -1464,7 +1462,6 @@ public class CompletableFuture<T> implements Future<T> {\n      * the given value.\n      *\n      * @param value the value\n-     * @param <U> the type of the value\n      * @return the completed CompletableFuture\n      */\n     public static <U> CompletableFuture<U> completedFuture(U value) {\n@@ -2832,7 +2829,7 @@ public class CompletableFuture<T> implements Future<T> {\n             }\n             if (dst == null)\n                 dst = new CompletableFuture<U>();\n-            if (ex != null)\n+            if (e == null || ex != null)\n                 dst.internalComplete(null, ex);\n         }\n         helpPostComplete();\ndiff --git a/src/main/java/jsr166e/ConcurrentHashMapV8.java b/src/main/java/jsr166e/ConcurrentHashMapV8.java\nindex b4d041086d2..5706cd04d76 100644\n--- a/src/main/java/jsr166e/ConcurrentHashMapV8.java\n+++ b/src/main/java/jsr166e/ConcurrentHashMapV8.java\n@@ -12,7 +12,6 @@ import java.io.ObjectStreamField;\n import java.io.Serializable;\n import java.lang.reflect.ParameterizedType;\n import java.lang.reflect.Type;\n-import java.util.AbstractMap;\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Comparator;\n@@ -219,7 +218,7 @@ import java.util.concurrent.locks.ReentrantLock;\n  * @param <K> the type of keys maintained by this map\n  * @param <V> the type of mapped values\n  */\n-public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n+public class ConcurrentHashMapV8<K,V>\n     implements ConcurrentMap<K,V>, Serializable {\n     private static final long serialVersionUID = 7249069246763182397L;\n \n@@ -276,7 +275,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n     /** Interface describing a function mapping two ints to an int */\n     public interface IntByIntToInt { int apply(int a, int b); }\n \n-\n     /*\n      * Overview:\n      *\n@@ -382,15 +380,14 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      * The table is resized when occupancy exceeds a percentage\n      * threshold (nominally, 0.75, but see below).  Any thread\n      * noticing an overfull bin may assist in resizing after the\n-     * initiating thread allocates and sets up the replacement array.\n-     * However, rather than stalling, these other threads may proceed\n-     * with insertions etc.  The use of TreeBins shields us from the\n-     * worst case effects of overfilling while resizes are in\n+     * initiating thread allocates and sets up the replacement\n+     * array. However, rather than stalling, these other threads may\n+     * proceed with insertions etc.  The use of TreeBins shields us\n+     * from the worst case effects of overfilling while resizes are in\n      * progress.  Resizing proceeds by transferring bins, one by one,\n-     * from the table to the next table. However, threads claim small\n-     * blocks of indices to transfer (via field transferIndex) before\n-     * doing so, reducing contention.  A generation stamp in field\n-     * sizeCtl ensures that resizings do not overlap. Because we are\n+     * from the table to the next table. To enable concurrency, the\n+     * next table must be (incrementally) prefilled with place-holders\n+     * serving as reverse forwarders to the old table.  Because we are\n      * using power-of-two expansion, the elements from each bin must\n      * either stay at same index, or move with a power of two\n      * offset. We eliminate unnecessary node creation by catching\n@@ -411,19 +408,13 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      * locks, average aggregate waits become shorter as resizing\n      * progresses.  The transfer operation must also ensure that all\n      * accessible bins in both the old and new table are usable by any\n-     * traversal.  This is arranged in part by proceeding from the\n-     * last bin (table.length - 1) up towards the first.  Upon seeing\n-     * a forwarding node, traversals (see class Traverser) arrange to\n-     * move to the new table without revisiting nodes.  To ensure that\n-     * no intervening nodes are skipped even when moved out of order,\n-     * a stack (see class TableStack) is created on first encounter of\n-     * a forwarding node during a traversal, to maintain its place if\n-     * later processing the current table. The need for these\n-     * save/restore mechanics is relatively rare, but when one\n-     * forwarding node is encountered, typically many more will be.\n-     * So Traversers use a simple caching scheme to avoid creating so\n-     * many new TableStack nodes. (Thanks to Peter Levart for\n-     * suggesting use of a stack here.)\n+     * traversal.  This is arranged by proceeding from the last bin\n+     * (table.length - 1) up towards the first.  Upon seeing a\n+     * forwarding node, traversals (see class Traverser) arrange to\n+     * move to the new table without revisiting nodes.  However, to\n+     * ensure that no intervening nodes are skipped, bin splitting can\n+     * only begin after the associated reverse-forwarders are in\n+     * place.\n      *\n      * The traversal scheme also applies to partial traversals of\n      * ranges of bins (via an alternate Traverser constructor)\n@@ -455,18 +446,16 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      * related operations (which is the main reason we cannot use\n      * existing collections such as TreeMaps). TreeBins contain\n      * Comparable elements, but may contain others, as well as\n-     * elements that are Comparable but not necessarily Comparable for\n-     * the same T, so we cannot invoke compareTo among them. To handle\n-     * this, the tree is ordered primarily by hash value, then by\n-     * Comparable.compareTo order if applicable.  On lookup at a node,\n-     * if elements are not comparable or compare as 0 then both left\n-     * and right children may need to be searched in the case of tied\n-     * hash values. (This corresponds to the full list search that\n-     * would be necessary if all elements were non-Comparable and had\n-     * tied hashes.) On insertion, to keep a total ordering (or as\n-     * close as is required here) across rebalancings, we compare\n-     * classes and identityHashCodes as tie-breakers. The red-black\n-     * balancing code is updated from pre-jdk-collections\n+     * elements that are Comparable but not necessarily Comparable\n+     * for the same T, so we cannot invoke compareTo among them. To\n+     * handle this, the tree is ordered primarily by hash value, then\n+     * by Comparable.compareTo order if applicable.  On lookup at a\n+     * node, if elements are not comparable or compare as 0 then both\n+     * left and right children may need to be searched in the case of\n+     * tied hash values. (This corresponds to the full list search\n+     * that would be necessary if all elements were non-Comparable and\n+     * had tied hashes.)  The red-black balancing code is updated from\n+     * pre-jdk-collections\n      * (http://gee.cs.oswego.edu/dl/classes/collections/RBCell.java)\n      * based in turn on Cormen, Leiserson, and Rivest \"Introduction to\n      * Algorithms\" (CLR).\n@@ -496,10 +485,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      * unused \"Segment\" class that is instantiated in minimal form\n      * only when serializing.\n      *\n-     * Also, solely for compatibility with previous versions of this\n-     * class, it extends AbstractMap, even though all of its methods\n-     * are overridden, so it is just useless baggage.\n-     *\n      * This file is organized to make things a little easier to follow\n      * while reading than they might otherwise: First the main static\n      * declarations and utilities, then fields, then main public\n@@ -580,23 +565,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      */\n     private static final int MIN_TRANSFER_STRIDE = 16;\n \n-    /**\n-     * The number of bits used for generation stamp in sizeCtl.\n-     * Must be at least 6 for 32bit arrays.\n-     */\n-    private static int RESIZE_STAMP_BITS = 16;\n-\n-    /**\n-     * The maximum number of threads that can help resize.\n-     * Must fit in 32 - RESIZE_STAMP_BITS bits.\n-     */\n-    private static final int MAX_RESIZERS = (1 << (32 - RESIZE_STAMP_BITS)) - 1;\n-\n-    /**\n-     * The bit shift for recording size stamp in sizeCtl.\n-     */\n-    private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;\n-\n     /*\n      * Encodings for Node hash fields. See above for explanation.\n      */\n@@ -754,7 +722,7 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      * errors by users, these checks must operate on local variables,\n      * which accounts for some odd-looking inline assignments below.\n      * Note that calls to setTabAt always occur within locked regions,\n-     * and so in principle require only release ordering, not\n+     * and so in principle require only release ordering, not need\n      * full volatile semantics, but are currently coded as volatile\n      * writes to be conservative.\n      */\n@@ -808,6 +776,11 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      */\n     private transient volatile int transferIndex;\n \n+    /**\n+     * The least available table index to split while resizing.\n+     */\n+    private transient volatile int transferOrigin;\n+\n     /**\n      * Spinlock (locked via CAS) used when resizing and/or creating CounterCells.\n      */\n@@ -1386,7 +1359,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      * Saves the state of the {@code ConcurrentHashMapV8} instance to a\n      * stream (i.e., serializes it).\n      * @param s the stream\n-     * @throws java.io.IOException if an I/O error occurs\n      * @serialData\n      * the key (Object) and value (Object)\n      * for each key-value mapping, followed by a null pair.\n@@ -1429,9 +1401,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n     /**\n      * Reconstitutes the instance from a stream (that is, deserializes it).\n      * @param s the stream\n-     * @throws ClassNotFoundException if the class of a serialized object\n-     *         could not be found\n-     * @throws java.io.IOException if an I/O error occurs\n      */\n     private void readObject(java.io.ObjectInputStream s)\n         throws java.io.IOException, ClassNotFoundException {\n@@ -1466,8 +1435,8 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                 int sz = (int)size;\n                 n = tableSizeFor(sz + (sz >>> 1) + 1);\n             }\n-            @SuppressWarnings(\"unchecked\")\n-                Node<K,V>[] tab = (Node<K,V>[])new Node<?,?>[n];\n+            @SuppressWarnings({\"rawtypes\",\"unchecked\"})\n+                Node<K,V>[] tab = (Node<K,V>[])new Node[n];\n             int mask = n - 1;\n             long added = 0L;\n             while (p != null) {\n@@ -2132,9 +2101,9 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      *\n      * @param initialCapacity The implementation performs internal\n      * sizing to accommodate this many elements.\n-     * @return the new set\n      * @throws IllegalArgumentException if the initial capacity of\n      * elements is negative\n+     * @return the new set\n      * @since 1.8\n      */\n     public static <K> KeySetView<K,Boolean> newKeySet(int initialCapacity) {\n@@ -2213,14 +2182,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n \n     /* ---------------- Table Initialization and Resizing -------------- */\n \n-    /**\n-     * Returns the stamp bits for resizing a table of size n.\n-     * Must be negative when shifted left by RESIZE_STAMP_SHIFT.\n-     */\n-    static final int resizeStamp(int n) {\n-        return Integer.numberOfLeadingZeros(n) | (1 << (RESIZE_STAMP_BITS - 1));\n-    }\n-\n     /**\n      * Initializes table, using the size recorded in sizeCtl.\n      */\n@@ -2233,8 +2194,8 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                 try {\n                     if ((tab = table) == null || tab.length == 0) {\n                         int n = (sc > 0) ? sc : DEFAULT_CAPACITY;\n-                        @SuppressWarnings(\"unchecked\")\n-                        Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];\n+                        @SuppressWarnings({\"rawtypes\",\"unchecked\"})\n+                            Node<K,V>[] nt = (Node<K,V>[])new Node[n];\n                         table = tab = nt;\n                         sc = n - (n >>> 2);\n                     }\n@@ -2276,20 +2237,17 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n             s = sumCount();\n         }\n         if (check >= 0) {\n-            Node<K,V>[] tab, nt; int n, sc;\n+            Node<K,V>[] tab, nt; int sc;\n             while (s >= (long)(sc = sizeCtl) && (tab = table) != null &&\n-                   (n = tab.length) < MAXIMUM_CAPACITY) {\n-                int rs = resizeStamp(n);\n+                   tab.length < MAXIMUM_CAPACITY) {\n                 if (sc < 0) {\n-                    if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||\n-                        sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||\n-                        transferIndex <= 0)\n+                    if (sc == -1 || transferIndex <= transferOrigin ||\n+                        (nt = nextTable) == null)\n                         break;\n-                    if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))\n+                    if (U.compareAndSwapInt(this, SIZECTL, sc, sc - 1))\n                         transfer(tab, nt);\n                 }\n-                else if (U.compareAndSwapInt(this, SIZECTL, sc,\n-                                             (rs << RESIZE_STAMP_SHIFT) + 2))\n+                else if (U.compareAndSwapInt(this, SIZECTL, sc, -2))\n                     transfer(tab, null);\n                 s = sumCount();\n             }\n@@ -2301,19 +2259,12 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n      */\n     final Node<K,V>[] helpTransfer(Node<K,V>[] tab, Node<K,V> f) {\n         Node<K,V>[] nextTab; int sc;\n-        if (tab != null && (f instanceof ForwardingNode) &&\n+        if ((f instanceof ForwardingNode) &&\n             (nextTab = ((ForwardingNode<K,V>)f).nextTable) != null) {\n-            int rs = resizeStamp(tab.length);\n-            while (nextTab == nextTable && table == tab &&\n-                   (sc = sizeCtl) < 0) {\n-                if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||\n-                    sc == rs + MAX_RESIZERS || transferIndex <= 0)\n-                    break;\n-                if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) {\n-                    transfer(tab, nextTab);\n-                    break;\n-                }\n-            }\n+            if (nextTab == nextTable && tab == table &&\n+                transferIndex > transferOrigin && (sc = sizeCtl) < -1 &&\n+                U.compareAndSwapInt(this, SIZECTL, sc, sc - 1))\n+                transfer(tab, nextTab);\n             return nextTab;\n         }\n         return table;\n@@ -2335,8 +2286,8 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                 if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {\n                     try {\n                         if (table == tab) {\n-                            @SuppressWarnings(\"unchecked\")\n-                            Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];\n+                            @SuppressWarnings({\"rawtypes\",\"unchecked\"})\n+                                Node<K,V>[] nt = (Node<K,V>[])new Node[n];\n                             table = nt;\n                             sc = n - (n >>> 2);\n                         }\n@@ -2347,21 +2298,9 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n             }\n             else if (c <= sc || n >= MAXIMUM_CAPACITY)\n                 break;\n-            else if (tab == table) {\n-                int rs = resizeStamp(n);\n-                if (sc < 0) {\n-                    Node<K,V>[] nt;\n-                    if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||\n-                        sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||\n-                        transferIndex <= 0)\n-                        break;\n-                    if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))\n-                        transfer(tab, nt);\n-                }\n-                else if (U.compareAndSwapInt(this, SIZECTL, sc,\n-                                             (rs << RESIZE_STAMP_SHIFT) + 2))\n-                    transfer(tab, null);\n-            }\n+            else if (tab == table &&\n+                     U.compareAndSwapInt(this, SIZECTL, sc, -2))\n+                transfer(tab, null);\n         }\n     }\n \n@@ -2375,27 +2314,36 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n             stride = MIN_TRANSFER_STRIDE; // subdivide range\n         if (nextTab == null) {            // initiating\n             try {\n-                @SuppressWarnings(\"unchecked\")\n-                Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n << 1];\n+                @SuppressWarnings({\"rawtypes\",\"unchecked\"})\n+                    Node<K,V>[] nt = (Node<K,V>[])new Node[n << 1];\n                 nextTab = nt;\n             } catch (Throwable ex) {      // try to cope with OOME\n                 sizeCtl = Integer.MAX_VALUE;\n                 return;\n             }\n             nextTable = nextTab;\n+            transferOrigin = n;\n             transferIndex = n;\n+            ForwardingNode<K,V> rev = new ForwardingNode<K,V>(tab);\n+            for (int k = n; k > 0;) {    // progressively reveal ready slots\n+                int nextk = (k > stride) ? k - stride : 0;\n+                for (int m = nextk; m < k; ++m)\n+                    nextTab[m] = rev;\n+                for (int m = n + nextk; m < n + k; ++m)\n+                    nextTab[m] = rev;\n+                U.putOrderedInt(this, TRANSFERORIGIN, k = nextk);\n+            }\n         }\n         int nextn = nextTab.length;\n         ForwardingNode<K,V> fwd = new ForwardingNode<K,V>(nextTab);\n         boolean advance = true;\n         boolean finishing = false; // to ensure sweep before committing nextTab\n         for (int i = 0, bound = 0;;) {\n-            Node<K,V> f; int fh;\n+            int nextIndex, nextBound, fh; Node<K,V> f;\n             while (advance) {\n-                int nextIndex, nextBound;\n                 if (--i >= bound || finishing)\n                     advance = false;\n-                else if ((nextIndex = transferIndex) <= 0) {\n+                else if ((nextIndex = transferIndex) <= transferOrigin) {\n                     i = -1;\n                     advance = false;\n                 }\n@@ -2409,22 +2357,29 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                 }\n             }\n             if (i < 0 || i >= n || i + n >= nextn) {\n-                int sc;\n                 if (finishing) {\n                     nextTable = null;\n                     table = nextTab;\n                     sizeCtl = (n << 1) - (n >>> 1);\n                     return;\n                 }\n-                if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) {\n-                    if ((sc - 2) != resizeStamp(n) << RESIZE_STAMP_SHIFT)\n-                        return;\n-                    finishing = advance = true;\n-                    i = n; // recheck before commit\n+                for (int sc;;) {\n+                    if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, ++sc)) {\n+                        if (sc != -1)\n+                            return;\n+                        finishing = advance = true;\n+                        i = n; // recheck before commit\n+                        break;\n+                    }\n+                }\n+            }\n+            else if ((f = tabAt(tab, i)) == null) {\n+                if (casTabAt(tab, i, null, fwd)) {\n+                    setTabAt(nextTab, i, null);\n+                    setTabAt(nextTab, i + n, null);\n+                    advance = true;\n                 }\n             }\n-            else if ((f = tabAt(tab, i)) == null)\n-                advance = casTabAt(tab, i, null, fwd);\n             else if ((fh = f.hash) == MOVED)\n                 advance = true; // already processed\n             else {\n@@ -2511,8 +2466,11 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n     private final void treeifyBin(Node<K,V>[] tab, int index) {\n         Node<K,V> b; int n, sc;\n         if (tab != null) {\n-            if ((n = tab.length) < MIN_TREEIFY_CAPACITY)\n-                tryPresize(n << 1);\n+            if ((n = tab.length) < MIN_TREEIFY_CAPACITY) {\n+                if (tab == table && (sc = sizeCtl) >= 0 &&\n+                    U.compareAndSwapInt(this, SIZECTL, sc, -2))\n+                    transfer(tab, null);\n+            }\n             else if ((b = tabAt(tab, index)) != null && b.hash >= 0) {\n                 synchronized (b) {\n                     if (tabAt(tab, index) == b) {\n@@ -2588,18 +2546,19 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                         p = pr;\n                     else if ((pk = p.key) == k || (pk != null && k.equals(pk)))\n                         return p;\n-                    else if (pl == null)\n-                        p = pr;\n-                    else if (pr == null)\n-                        p = pl;\n+                    else if (pl == null && pr == null)\n+                        break;\n                     else if ((kc != null ||\n                               (kc = comparableClassFor(k)) != null) &&\n                              (dir = compareComparables(kc, k, pk)) != 0)\n                         p = (dir < 0) ? pl : pr;\n-                    else if ((q = pr.findTreeNode(h, k, kc)) != null)\n-                        return q;\n-                    else\n+                    else if (pl == null)\n+                        p = pr;\n+                    else if (pr == null ||\n+                             (q = pr.findTreeNode(h, k, kc)) == null)\n                         p = pl;\n+                    else\n+                        return q;\n                 } while (p != null);\n             }\n             return null;\n@@ -2625,23 +2584,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n         static final int WAITER = 2; // set when waiting for write lock\n         static final int READER = 4; // increment value for setting read lock\n \n-        /**\n-         * Tie-breaking utility for ordering insertions when equal\n-         * hashCodes and non-comparable. We don't require a total\n-         * order, just a consistent insertion rule to maintain\n-         * equivalence across rebalancings. Tie-breaking further than\n-         * necessary simplifies testing a bit.\n-         */\n-        static int tieBreakOrder(Object a, Object b) {\n-            int d;\n-            if (a == null || b == null ||\n-                (d = a.getClass().getName().\n-                 compareTo(b.getClass().getName())) == 0)\n-                d = (System.identityHashCode(a) <= System.identityHashCode(b) ?\n-                     -1 : 1);\n-            return d;\n-        }\n-\n         /**\n          * Creates bin with initial set of nodes headed by b.\n          */\n@@ -2658,21 +2600,21 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                     r = x;\n                 }\n                 else {\n-                    K k = x.key;\n-                    int h = x.hash;\n+                    Object key = x.key;\n+                    int hash = x.hash;\n                     Class<?> kc = null;\n                     for (TreeNode<K,V> p = r;;) {\n                         int dir, ph;\n-                        K pk = p.key;\n-                        if ((ph = p.hash) > h)\n+                        if ((ph = p.hash) > hash)\n                             dir = -1;\n-                        else if (ph < h)\n+                        else if (ph < hash)\n                             dir = 1;\n-                        else if ((kc == null &&\n-                                  (kc = comparableClassFor(k)) == null) ||\n-                                 (dir = compareComparables(kc, k, pk)) == 0)\n-                            dir = tieBreakOrder(k, pk);\n-                            TreeNode<K,V> xp = p;\n+                        else if ((kc != null ||\n+                                  (kc = comparableClassFor(key)) != null))\n+                            dir = compareComparables(kc, key, p.key);\n+                        else\n+                            dir = 0;\n+                        TreeNode<K,V> xp = p;\n                         if ((p = (dir <= 0) ? p.left : p.right) == null) {\n                             x.parent = xp;\n                             if (dir <= 0)\n@@ -2686,7 +2628,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                 }\n             }\n             this.root = r;\n-            assert checkInvariants(root);\n         }\n \n         /**\n@@ -2710,14 +2651,14 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n         private final void contendedLock() {\n             boolean waiting = false;\n             for (int s;;) {\n-                if (((s = lockState) & ~WAITER) == 0) {\n+                if (((s = lockState) & WRITER) == 0) {\n                     if (U.compareAndSwapInt(this, LOCKSTATE, s, WRITER)) {\n                         if (waiting)\n                             waiter = null;\n                         return;\n                     }\n                 }\n-                else if ((s & WAITER) == 0) {\n+                else if ((s | WAITER) == 0) {\n                     if (U.compareAndSwapInt(this, LOCKSTATE, s, s | WAITER)) {\n                         waiting = true;\n                         waiter = Thread.currentThread();\n@@ -2735,13 +2676,12 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n          */\n         final Node<K,V> find(int h, Object k) {\n             if (k != null) {\n-                for (Node<K,V> e = first; e != null; ) {\n+                for (Node<K,V> e = first; e != null; e = e.next) {\n                     int s; K ek;\n                     if (((s = lockState) & (WAITER|WRITER)) != 0) {\n                         if (e.hash == h &&\n                             ((ek = e.key) == k || (ek != null && k.equals(ek))))\n                             return e;\n-                        e = e.next;\n                     }\n                     else if (U.compareAndSwapInt(this, LOCKSTATE, s,\n                                                  s + READER)) {\n@@ -2771,9 +2711,8 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n          */\n         final TreeNode<K,V> putTreeVal(int h, K k, V v) {\n             Class<?> kc = null;\n-            boolean searched = false;\n             for (TreeNode<K,V> p = root;;) {\n-                int dir, ph; K pk;\n+                int dir, ph; K pk; TreeNode<K,V> q, pr;\n                 if (p == null) {\n                     first = root = new TreeNode<K,V>(h, k, v, null, null);\n                     break;\n@@ -2787,25 +2726,21 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                 else if ((kc == null &&\n                           (kc = comparableClassFor(k)) == null) ||\n                          (dir = compareComparables(kc, k, pk)) == 0) {\n-                    if (!searched) {\n-                        TreeNode<K,V> q, ch;\n-                        searched = true;\n-                        if (((ch = p.left) != null &&\n-                             (q = ch.findTreeNode(h, k, kc)) != null) ||\n-                            ((ch = p.right) != null &&\n-                             (q = ch.findTreeNode(h, k, kc)) != null))\n-                            return q;\n-                    }\n-                    dir = tieBreakOrder(k, pk);\n+                    if (p.left == null)\n+                        dir = 1;\n+                    else if ((pr = p.right) == null ||\n+                             (q = pr.findTreeNode(h, k, kc)) == null)\n+                        dir = -1;\n+                    else\n+                        return q;\n                 }\n-\n                 TreeNode<K,V> xp = p;\n-                if ((p = (dir <= 0) ? p.left : p.right) == null) {\n+                if ((p = (dir < 0) ? p.left : p.right) == null) {\n                     TreeNode<K,V> x, f = first;\n                     first = x = new TreeNode<K,V>(h, k, v, f, xp);\n                     if (f != null)\n                         f.prev = x;\n-                    if (dir <= 0)\n+                    if (dir < 0)\n                         xp.left = x;\n                     else\n                         xp.right = x;\n@@ -3159,18 +3094,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n \n     /* ----------------Table Traversal -------------- */\n \n-    /**\n-     * Records the table, its length, and current traversal index for a\n-     * traverser that must process a region of a forwarded table before\n-     * proceeding with current table.\n-     */\n-    static final class TableStack<K,V> {\n-        int length;\n-        int index;\n-        Node<K,V>[] tab;\n-        TableStack<K,V> next;\n-    }\n-\n     /**\n      * Encapsulates traversal for methods such as containsValue; also\n      * serves as a base class for other iterators and spliterators.\n@@ -3195,7 +3118,6 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n     static class Traverser<K,V> {\n         Node<K,V>[] tab;        // current table; updated if resized\n         Node<K,V> next;         // the next entry to use\n-        TableStack<K,V> stack, spare; // to save/restore on ForwardingNodes\n         int index;              // index of bin to use next\n         int baseIndex;          // current index of initial table\n         int baseLimit;          // index bound for initial table\n@@ -3217,17 +3139,16 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n             if ((e = next) != null)\n                 e = e.next;\n             for (;;) {\n-                Node<K,V>[] t; int i, n;  // must use locals in checks\n+                Node<K,V>[] t; int i, n; K ek;  // must use locals in checks\n                 if (e != null)\n                     return next = e;\n                 if (baseIndex >= baseLimit || (t = tab) == null ||\n                     (n = t.length) <= (i = index) || i < 0)\n                     return next = null;\n-                if ((e = tabAt(t, i)) != null && e.hash < 0) {\n+                if ((e = tabAt(t, index)) != null && e.hash < 0) {\n                     if (e instanceof ForwardingNode) {\n                         tab = ((ForwardingNode<K,V>)e).nextTable;\n                         e = null;\n-                        pushState(t, i, n);\n                         continue;\n                     }\n                     else if (e instanceof TreeBin)\n@@ -3235,48 +3156,9 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                     else\n                         e = null;\n                 }\n-                if (stack != null)\n-                    recoverState(n);\n-                else if ((index = i + baseSize) >= n)\n-                    index = ++baseIndex; // visit upper slots if present\n-            }\n-        }\n-\n-        /**\n-         * Saves traversal state upon encountering a forwarding node.\n-         */\n-        private void pushState(Node<K,V>[] t, int i, int n) {\n-            TableStack<K,V> s = spare;  // reuse if possible\n-            if (s != null)\n-                spare = s.next;\n-            else\n-                s = new TableStack<K,V>();\n-            s.tab = t;\n-            s.length = n;\n-            s.index = i;\n-            s.next = stack;\n-            stack = s;\n-        }\n-\n-        /**\n-         * Possibly pops traversal state.\n-         *\n-         * @param n length of current table\n-         */\n-        private void recoverState(int n) {\n-            TableStack<K,V> s; int len;\n-            while ((s = stack) != null && (index += (len = s.length)) >= n) {\n-                n = len;\n-                index = s.index;\n-                tab = s.tab;\n-                s.tab = null;\n-                TableStack<K,V> next = s.next;\n-                s.next = spare; // save for reuse\n-                stack = next;\n-                spare = s;\n+                if ((index += baseSize) >= n)\n+                    index = ++baseIndex;    // visit upper slots if present\n             }\n-            if (s == null && (index += baseSize) >= n)\n-                index = ++baseIndex;\n         }\n     }\n \n@@ -6244,6 +6126,7 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n     private static final sun.misc.Unsafe U;\n     private static final long SIZECTL;\n     private static final long TRANSFERINDEX;\n+    private static final long TRANSFERORIGIN;\n     private static final long BASECOUNT;\n     private static final long CELLSBUSY;\n     private static final long CELLVALUE;\n@@ -6258,6 +6141,8 @@ public class ConcurrentHashMapV8<K,V> extends AbstractMap<K,V>\n                 (k.getDeclaredField(\"sizeCtl\"));\n             TRANSFERINDEX = U.objectFieldOffset\n                 (k.getDeclaredField(\"transferIndex\"));\n+            TRANSFERORIGIN = U.objectFieldOffset\n+                (k.getDeclaredField(\"transferOrigin\"));\n             BASECOUNT = U.objectFieldOffset\n                 (k.getDeclaredField(\"baseCount\"));\n             CELLSBUSY = U.objectFieldOffset\ndiff --git a/src/main/java/jsr166e/CountedCompleter.java b/src/main/java/jsr166e/CountedCompleter.java\nindex c7717ee9032..d9d1d48132e 100644\n--- a/src/main/java/jsr166e/CountedCompleter.java\n+++ b/src/main/java/jsr166e/CountedCompleter.java\n@@ -686,7 +686,7 @@ public abstract class CountedCompleter<T> extends ForkJoinTask<T> {\n     }\n \n     /**\n-     * Returns the result of the computation.  By default,\n+     * Returns the result of the computation. By default\n      * returns {@code null}, which is appropriate for {@code Void}\n      * actions, but in other cases should be overridden, almost\n      * always to return a field or function of a field that\ndiff --git a/src/main/java/jsr166e/ForkJoinPool.java b/src/main/java/jsr166e/ForkJoinPool.java\nindex 8a0497371c2..e814f23d529 100644\n--- a/src/main/java/jsr166e/ForkJoinPool.java\n+++ b/src/main/java/jsr166e/ForkJoinPool.java\n@@ -532,8 +532,8 @@ public class ForkJoinPool extends AbstractExecutorService {\n          * Returns a new worker thread operating in the given pool.\n          *\n          * @param pool the pool this thread works in\n-         * @return the new worker thread\n          * @throws NullPointerException if the pool is null\n+         * @return the new worker thread\n          */\n         public ForkJoinWorkerThread newThread(ForkJoinPool pool);\n     }\n@@ -2098,7 +2098,7 @@ public class ForkJoinPool extends AbstractExecutorService {\n                     w.currentSteal = ps;\n                 }\n             }\n-            else if (active) {      // decrement active count without queuing\n+            else if (active) {       // decrement active count without queuing\n                 long nc = ((c = ctl) & ~AC_MASK) | ((c & AC_MASK) - AC_UNIT);\n                 if ((int)(nc >> AC_SHIFT) + parallelism == 0)\n                     break;          // bypass decrement-then-increment\n@@ -2497,7 +2497,6 @@ public class ForkJoinPool extends AbstractExecutorService {\n      * minimally only the latter.\n      *\n      * @param task the task\n-     * @param <T> the type of the task's result\n      * @return the task's result\n      * @throws NullPointerException if the task is null\n      * @throws RejectedExecutionException if the task cannot be\n@@ -2546,7 +2545,6 @@ public class ForkJoinPool extends AbstractExecutorService {\n      * Submits a ForkJoinTask for execution.\n      *\n      * @param task the task to submit\n-     * @param <T> the type of the task's result\n      * @return the task\n      * @throws NullPointerException if the task is null\n      * @throws RejectedExecutionException if the task cannot be\ndiff --git a/src/main/java/jsr166e/ForkJoinTask.java b/src/main/java/jsr166e/ForkJoinTask.java\nindex 99ac374ec79..e23dfbe3f79 100644\n--- a/src/main/java/jsr166e/ForkJoinTask.java\n+++ b/src/main/java/jsr166e/ForkJoinTask.java\n@@ -134,7 +134,7 @@ import java.lang.reflect.Constructor;\n  * (DAG). Otherwise, executions may encounter a form of deadlock as\n  * tasks cyclically wait for each other.  However, this framework\n  * supports other methods and techniques (for example the use of\n- * {@link java.util.concurrent.Phaser Phaser}, {@link #helpQuiesce}, and {@link #complete}) that\n+ * {@link Phaser}, {@link #helpQuiesce}, and {@link #complete}) that\n  * may be of use in constructing custom subclasses for problems that\n  * are not statically structured as DAGs. To support such usages, a\n  * ForkJoinTask may be atomically <em>tagged</em> with a {@code short}\n@@ -411,13 +411,11 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n         final Throwable ex;\n         ExceptionNode next;\n         final long thrower;  // use id not ref to avoid weak cycles\n-        final int hashCode;  // store task hashCode before weak ref disappears\n         ExceptionNode(ForkJoinTask<?> task, Throwable ex, ExceptionNode next) {\n             super(task, exceptionTableRefQueue);\n             this.ex = ex;\n             this.next = next;\n             this.thrower = Thread.currentThread().getId();\n-            this.hashCode = System.identityHashCode(task);\n         }\n     }\n \n@@ -573,18 +571,15 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n         return ex;\n     }\n \n-    /**\n-     * Poll stale refs and remove them. Call only while holding lock.\n-     */\n     /**\n      * Poll stale refs and remove them. Call only while holding lock.\n      */\n     private static void expungeStaleExceptions() {\n         for (Object x; (x = exceptionTableRefQueue.poll()) != null;) {\n             if (x instanceof ExceptionNode) {\n-                int hashCode = ((ExceptionNode)x).hashCode;\n+                ForkJoinTask<?> key = ((ExceptionNode)x).get();\n                 ExceptionNode[] t = exceptionTable;\n-                int i = hashCode & (t.length - 1);\n+                int i = System.identityHashCode(key) & (t.length - 1);\n                 ExceptionNode e = t[i];\n                 ExceptionNode pred = null;\n                 while (e != null) {\n@@ -787,7 +782,6 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n      * unprocessed.\n      *\n      * @param tasks the collection of tasks\n-     * @param <T> the type of the values returned from the tasks\n      * @return the tasks argument, to simplify usage\n      * @throws NullPointerException if tasks or any element are null\n      */\n@@ -1450,7 +1444,6 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n      *\n      * @param runnable the runnable action\n      * @param result the result upon completion\n-     * @param <T> the type of the result\n      * @return the task\n      */\n     public static <T> ForkJoinTask<T> adapt(Runnable runnable, T result) {\n@@ -1464,7 +1457,6 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n      * encountered into {@code RuntimeException}.\n      *\n      * @param callable the callable action\n-     * @param <T> the type of the callable's result\n      * @return the task\n      */\n     public static <T> ForkJoinTask<T> adapt(Callable<? extends T> callable) {\n@@ -1478,8 +1470,6 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n     /**\n      * Saves this task to a stream (that is, serializes it).\n      *\n-     * @param s the stream\n-     * @throws java.io.IOException if an I/O error occurs\n      * @serialData the current run status and the exception thrown\n      * during execution, or {@code null} if none\n      */\n@@ -1491,10 +1481,6 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n \n     /**\n      * Reconstitutes this task from a stream (that is, deserializes it).\n-     * @param s the stream\n-     * @throws ClassNotFoundException if the class of a serialized object\n-     *         could not be found\n-     * @throws java.io.IOException if an I/O error occurs\n      */\n     private void readObject(java.io.ObjectInputStream s)\n         throws java.io.IOException, ClassNotFoundException {\ndiff --git a/src/main/java/jsr166e/RecursiveTask.java b/src/main/java/jsr166e/RecursiveTask.java\nindex 263364af1a2..a31bd0cfff8 100644\n--- a/src/main/java/jsr166e/RecursiveTask.java\n+++ b/src/main/java/jsr166e/RecursiveTask.java\n@@ -15,7 +15,7 @@ package jsr166e;\n  * class Fibonacci extends RecursiveTask<Integer> {\n  *   final int n;\n  *   Fibonacci(int n) { this.n = n; }\n- *   protected Integer compute() {\n+ *   Integer compute() {\n  *     if (n <= 1)\n  *       return n;\n  *     Fibonacci f1 = new Fibonacci(n - 1);\n@@ -46,7 +46,6 @@ public abstract class RecursiveTask<V> extends ForkJoinTask<V> {\n \n     /**\n      * The main computation performed by this task.\n-     * @return the result of the computation\n      */\n     protected abstract V compute();\n \ndiff --git a/src/main/java/jsr166e/StampedLock.java b/src/main/java/jsr166e/StampedLock.java\nindex 45d873a1c0a..d8fe6f8af79 100644\n--- a/src/main/java/jsr166e/StampedLock.java\n+++ b/src/main/java/jsr166e/StampedLock.java\n@@ -198,11 +198,7 @@ public class StampedLock implements java.io.Serializable {\n      * incoming reader arrives while read lock is held but there is a\n      * queued writer, this incoming reader is queued.  (This rule is\n      * responsible for some of the complexity of method acquireRead,\n-     * but without it, the lock becomes highly unfair.) Method release\n-     * does not (and sometimes cannot) itself wake up cowaiters. This\n-     * is done by the primary thread, but helped by any other threads\n-     * with nothing better to do in methods acquireRead and\n-     * acquireWrite.\n+     * but without it, the lock becomes highly unfair.)\n      *\n      * These rules apply to threads actually queued. All tryLock forms\n      * opportunistically try to acquire locks regardless of preference\n@@ -246,14 +242,11 @@ public class StampedLock implements java.io.Serializable {\n     /** Number of processors, for spin control */\n     private static final int NCPU = Runtime.getRuntime().availableProcessors();\n \n-    /** Maximum number of retries before enqueuing on acquisition */\n+    /** Maximum number of retries before blocking on acquisition */\n     private static final int SPINS = (NCPU > 1) ? 1 << 6 : 0;\n \n-    /** Maximum number of retries before blocking at head on acquisition */\n-    private static final int HEAD_SPINS = (NCPU > 1) ? 1 << 10 : 0;\n-\n     /** Maximum number of retries before re-blocking */\n-    private static final int MAX_HEAD_SPINS = (NCPU > 1) ? 1 << 16 : 0;\n+    private static final int MAX_HEAD_SPINS = (NCPU > 1) ? 1 << 12 : 0;\n \n     /** The period for yielding when waiting for overflow spinlock */\n     private static final int OVERFLOW_YIELD_RATE = 7; // must be power 2 - 1\n@@ -397,8 +390,8 @@ public class StampedLock implements java.io.Serializable {\n      * @return a stamp that can be used to unlock or convert mode\n      */\n     public long readLock() {\n-        long s = state, next;  // bypass acquireRead on common uncontended case\n-        return ((whead == wtail && (s & ABITS) < RFULL &&\n+        long s, next;  // bypass acquireRead on fully unlocked case only\n+        return ((((s = state) & ABITS) == 0L &&\n                  U.compareAndSwapLong(this, STATE, s, next = s + RUNIT)) ?\n                 next : acquireRead(false, 0L));\n     }\n@@ -675,7 +668,7 @@ public class StampedLock implements java.io.Serializable {\n         long a = stamp & ABITS, m, s, next; WNode h;\n         for (;;) {\n             s = U.getLongVolatile(this, STATE); // see above\n-            if (((s = state) & SBITS) != (stamp & SBITS))\n+            if ((s & SBITS) != (stamp & SBITS))\n                 break;\n             if ((m = s & ABITS) == 0L) {\n                 if (a != 0L)\n@@ -994,8 +987,17 @@ public class StampedLock implements java.io.Serializable {\n                     if (t.status <= 0)\n                         q = t;\n             }\n-            if (q != null && (w = q.thread) != null)\n-                U.unpark(w);\n+            if (q != null) {\n+                for (WNode r = q;;) {  // release co-waiters too\n+                    if ((w = r.thread) != null) {\n+                        r.thread = null;\n+                        U.unpark(w);\n+                    }\n+                    if ((r = q.cowait) == null)\n+                        break;\n+                    U.compareAndSwapObject(q, WCOWAIT, r, r.cowait);\n+                }\n+            }\n         }\n     }\n \n@@ -1011,22 +1013,22 @@ public class StampedLock implements java.io.Serializable {\n     private long acquireWrite(boolean interruptible, long deadline) {\n         WNode node = null, p;\n         for (int spins = -1;;) { // spin while enqueuing\n-            long m, s, ns;\n-            if ((m = (s = state) & ABITS) == 0L) {\n+            long s, ns;\n+            if (((s = state) & ABITS) == 0L) {\n                 if (U.compareAndSwapLong(this, STATE, s, ns = s + WBIT))\n                     return ns;\n             }\n-            else if (spins < 0)\n-                spins = (m == WBIT && wtail == whead) ? SPINS : 0;\n             else if (spins > 0) {\n                 if (ThreadLocalRandom.current().nextInt() >= 0)\n                     --spins;\n             }\n             else if ((p = wtail) == null) { // initialize queue\n-                WNode hd = new WNode(WMODE, null);\n-                if (U.compareAndSwapObject(this, WHEAD, null, hd))\n-                    wtail = hd;\n+                WNode h = new WNode(WMODE, null);\n+                if (U.compareAndSwapObject(this, WHEAD, null, h))\n+                    wtail = h;\n             }\n+            else if (spins < 0)\n+                spins = (p == whead) ? SPINS : 0;\n             else if (node == null)\n                 node = new WNode(WMODE, p);\n             else if (node.prev != p)\n@@ -1037,18 +1039,14 @@ public class StampedLock implements java.io.Serializable {\n             }\n         }\n \n-        for (int spins = -1;;) {\n-            WNode h, np, pp; int ps;\n-            if ((h = whead) == p) {\n-                if (spins < 0)\n-                    spins = HEAD_SPINS;\n-                else if (spins < MAX_HEAD_SPINS)\n-                    spins <<= 1;\n+        for (int spins = SPINS;;) {\n+            WNode np, pp; int ps; long s, ns; Thread w;\n+            while ((np = node.prev) != p && np != null)\n+                (p = np).next = node;   // stale\n+            if (whead == p) {\n                 for (int k = spins;;) { // spin at head\n-                    long s, ns;\n                     if (((s = state) & ABITS) == 0L) {\n-                        if (U.compareAndSwapLong(this, STATE, s,\n-                                                 ns = s + WBIT)) {\n+                        if (U.compareAndSwapLong(this, STATE, s, ns = s+WBIT)) {\n                             whead = node;\n                             node.prev = null;\n                             return ns;\n@@ -1058,45 +1056,33 @@ public class StampedLock implements java.io.Serializable {\n                              --k <= 0)\n                         break;\n                 }\n+                if (spins < MAX_HEAD_SPINS)\n+                    spins <<= 1;\n             }\n-            else if (h != null) { // help release stale waiters\n-                WNode c; Thread w;\n-                while ((c = h.cowait) != null) {\n-                    if (U.compareAndSwapObject(h, WCOWAIT, c, c.cowait) &&\n-                        (w = c.thread) != null)\n-                        U.unpark(w);\n+            if ((ps = p.status) == 0)\n+                U.compareAndSwapInt(p, WSTATUS, 0, WAITING);\n+            else if (ps == CANCELLED) {\n+                if ((pp = p.prev) != null) {\n+                    node.prev = pp;\n+                    pp.next = node;\n                 }\n             }\n-            if (whead == h) {\n-                if ((np = node.prev) != p) {\n-                    if (np != null)\n-                        (p = np).next = node;   // stale\n-                }\n-                else if ((ps = p.status) == 0)\n-                    U.compareAndSwapInt(p, WSTATUS, 0, WAITING);\n-                else if (ps == CANCELLED) {\n-                    if ((pp = p.prev) != null) {\n-                        node.prev = pp;\n-                        pp.next = node;\n-                    }\n-                }\n-                else {\n-                    long time; // 0 argument to park means no timeout\n-                    if (deadline == 0L)\n-                        time = 0L;\n-                    else if ((time = deadline - System.nanoTime()) <= 0L)\n-                        return cancelWaiter(node, node, false);\n-                    Thread wt = Thread.currentThread();\n-                    U.putObject(wt, PARKBLOCKER, this);\n-                    node.thread = wt;\n-                    if (p.status < 0 && (p != h || (state & ABITS) != 0L) &&\n-                        whead == h && node.prev == p)\n-                        U.park(false, time);  // emulate LockSupport.park\n-                    node.thread = null;\n-                    U.putObject(wt, PARKBLOCKER, null);\n-                    if (interruptible && Thread.interrupted())\n-                        return cancelWaiter(node, node, true);\n-                }\n+            else {\n+                long time; // 0 argument to park means no timeout\n+                if (deadline == 0L)\n+                    time = 0L;\n+                else if ((time = deadline - System.nanoTime()) <= 0L)\n+                    return cancelWaiter(node, node, false);\n+                Thread wt = Thread.currentThread();\n+                U.putObject(wt, PARKBLOCKER, this); // emulate LockSupport.park\n+                node.thread = wt;\n+                if (node.prev == p && p.status == WAITING && // recheck\n+                    (p != whead || (state & ABITS) != 0L))\n+                    U.park(false, time);\n+                node.thread = null;\n+                U.putObject(wt, PARKBLOCKER, null);\n+                if (interruptible && Thread.interrupted())\n+                    return cancelWaiter(node, node, true);\n             }\n         }\n     }\n@@ -1111,159 +1097,138 @@ public class StampedLock implements java.io.Serializable {\n      * @return next state, or INTERRUPTED\n      */\n     private long acquireRead(boolean interruptible, long deadline) {\n-        WNode node = null, p;\n+        WNode node = null, group = null, p;\n         for (int spins = -1;;) {\n-            WNode h;\n-            if ((h = whead) == (p = wtail)) {\n-                for (long m, s, ns;;) {\n-                    if ((m = (s = state) & ABITS) < RFULL ?\n-                        U.compareAndSwapLong(this, STATE, s, ns = s + RUNIT) :\n-                        (m < WBIT && (ns = tryIncReaderOverflow(s)) != 0L))\n-                        return ns;\n-                    else if (m >= WBIT) {\n-                        if (spins > 0) {\n-                            if (ThreadLocalRandom.current().nextInt() >= 0)\n-                                --spins;\n-                        }\n-                        else {\n-                            if (spins == 0) {\n-                                WNode nh = whead, np = wtail;\n-                                if ((nh == h && np == p) || (h = nh) != (p = np))\n-                                    break;\n+            for (;;) {\n+                long s, m, ns; WNode h, q; Thread w; // anti-barging guard\n+                if (group == null && (h = whead) != null &&\n+                    (q = h.next) != null && q.mode != RMODE)\n+                    break;\n+                if ((m = (s = state) & ABITS) < RFULL ?\n+                    U.compareAndSwapLong(this, STATE, s, ns = s + RUNIT) :\n+                    (m < WBIT && (ns = tryIncReaderOverflow(s)) != 0L)) {\n+                    if (group != null) {  // help release others\n+                        for (WNode r = group;;) {\n+                            if ((w = r.thread) != null) {\n+                                r.thread = null;\n+                                U.unpark(w);\n                             }\n-                            spins = SPINS;\n+                            if ((r = group.cowait) == null)\n+                                break;\n+                            U.compareAndSwapObject(group, WCOWAIT, r, r.cowait);\n                         }\n                     }\n+                    return ns;\n                 }\n+                if (m >= WBIT)\n+                    break;\n             }\n-            if (p == null) { // initialize queue\n-                WNode hd = new WNode(WMODE, null);\n-                if (U.compareAndSwapObject(this, WHEAD, null, hd))\n-                    wtail = hd;\n+            if (spins > 0) {\n+                if (ThreadLocalRandom.current().nextInt() >= 0)\n+                    --spins;\n             }\n-            else if (node == null)\n-                node = new WNode(RMODE, p);\n-            else if (h == p || p.mode != RMODE) {\n-                if (node.prev != p)\n-                    node.prev = p;\n-                else if (U.compareAndSwapObject(this, WTAIL, p, node)) {\n-                    p.next = node;\n-                    break;\n-                }\n+            else if ((p = wtail) == null) {\n+                WNode h = new WNode(WMODE, null);\n+                if (U.compareAndSwapObject(this, WHEAD, null, h))\n+                    wtail = h;\n             }\n-            else if (!U.compareAndSwapObject(p, WCOWAIT,\n-                                             node.cowait = p.cowait, node))\n-                node.cowait = null;\n-            else {\n-                for (;;) {\n-                    WNode pp, c; Thread w;\n-                    if ((h = whead) != null && (c = h.cowait) != null &&\n-                        U.compareAndSwapObject(h, WCOWAIT, c, c.cowait) &&\n-                        (w = c.thread) != null) // help release\n-                        U.unpark(w);\n-                    if (h == (pp = p.prev) || h == p || pp == null) {\n-                        long m, s, ns;\n-                        do {\n-                            if ((m = (s = state) & ABITS) < RFULL ?\n-                                U.compareAndSwapLong(this, STATE, s,\n-                                                     ns = s + RUNIT) :\n-                                (m < WBIT &&\n-                                 (ns = tryIncReaderOverflow(s)) != 0L))\n-                                return ns;\n-                        } while (m < WBIT);\n-                    }\n-                    if (whead == h && p.prev == pp) {\n-                        long time;\n-                        if (pp == null || h == p || p.status > 0) {\n-                            node = null; // throw away\n-                            break;\n-                        }\n+            else if (spins < 0)\n+                spins = (p == whead) ? SPINS : 0;\n+            else if (node == null)\n+                node = new WNode(WMODE, p);\n+            else if (node.prev != p)\n+                node.prev = p;\n+            else if (p.mode == RMODE && p != whead) {\n+                WNode pp = p.prev;  // become co-waiter with group p\n+                if (pp != null && p == wtail &&\n+                    U.compareAndSwapObject(p, WCOWAIT,\n+                                           node.cowait = p.cowait, node)) {\n+                    node.thread = Thread.currentThread();\n+                    for (long time;;) {\n                         if (deadline == 0L)\n                             time = 0L;\n                         else if ((time = deadline - System.nanoTime()) <= 0L)\n                             return cancelWaiter(node, p, false);\n+                        if (node.thread == null)\n+                            break;\n+                        if (p.prev != pp || p.status == CANCELLED ||\n+                            p == whead || p.prev != pp) {\n+                            node.thread = null;\n+                            break;\n+                        }\n                         Thread wt = Thread.currentThread();\n                         U.putObject(wt, PARKBLOCKER, this);\n-                        node.thread = wt;\n-                        if ((h != pp || (state & ABITS) == WBIT) &&\n-                            whead == h && p.prev == pp)\n-                            U.park(false, time);\n-                        node.thread = null;\n+                        if (node.thread == null) // must recheck\n+                            break;\n+                        U.park(false, time);\n                         U.putObject(wt, PARKBLOCKER, null);\n                         if (interruptible && Thread.interrupted())\n                             return cancelWaiter(node, p, true);\n                     }\n+                    group = p;\n                 }\n+                node = null; // throw away\n+            }\n+            else if (U.compareAndSwapObject(this, WTAIL, p, node)) {\n+                p.next = node;\n+                break;\n             }\n         }\n \n-        for (int spins = -1;;) {\n-            WNode h, np, pp; int ps;\n-            if ((h = whead) == p) {\n-                if (spins < 0)\n-                    spins = HEAD_SPINS;\n-                else if (spins < MAX_HEAD_SPINS)\n-                    spins <<= 1;\n-                for (int k = spins;;) { // spin at head\n-                    long m, s, ns;\n-                    if ((m = (s = state) & ABITS) < RFULL ?\n-                        U.compareAndSwapLong(this, STATE, s, ns = s + RUNIT) :\n-                        (m < WBIT && (ns = tryIncReaderOverflow(s)) != 0L)) {\n-                        WNode c; Thread w;\n-                        whead = node;\n-                        node.prev = null;\n-                        while ((c = node.cowait) != null) {\n-                            if (U.compareAndSwapObject(node, WCOWAIT,\n-                                                       c, c.cowait) &&\n-                                (w = c.thread) != null)\n-                                U.unpark(w);\n+        for (int spins = SPINS;;) {\n+            WNode np, pp, r; int ps; long m, s, ns; Thread w;\n+            while ((np = node.prev) != p && np != null)\n+                (p = np).next = node;\n+            if (whead == p) {\n+                for (int k = spins;;) {\n+                    if ((m = (s = state) & ABITS) != WBIT) {\n+                        if (m < RFULL ?\n+                            U.compareAndSwapLong(this, STATE, s, ns = s + RUNIT):\n+                            (ns = tryIncReaderOverflow(s)) != 0L) {\n+                            whead = node;\n+                            node.prev = null;\n+                            while ((r = node.cowait) != null) {\n+                                if (U.compareAndSwapObject(node, WCOWAIT,\n+                                                           r, r.cowait) &&\n+                                    (w = r.thread) != null) {\n+                                    r.thread = null;\n+                                    U.unpark(w); // release co-waiter\n+                                }\n+                            }\n+                            return ns;\n                         }\n-                        return ns;\n                     }\n-                    else if (m >= WBIT &&\n-                             ThreadLocalRandom.current().nextInt() >= 0 && --k <= 0)\n+                    else if (ThreadLocalRandom.current().nextInt() >= 0 &&\n+                             --k <= 0)\n                         break;\n                 }\n+                if (spins < MAX_HEAD_SPINS)\n+                    spins <<= 1;\n             }\n-            else if (h != null) {\n-                WNode c; Thread w;\n-                while ((c = h.cowait) != null) {\n-                    if (U.compareAndSwapObject(h, WCOWAIT, c, c.cowait) &&\n-                        (w = c.thread) != null)\n-                        U.unpark(w);\n+            if ((ps = p.status) == 0)\n+                U.compareAndSwapInt(p, WSTATUS, 0, WAITING);\n+            else if (ps == CANCELLED) {\n+                if ((pp = p.prev) != null) {\n+                    node.prev = pp;\n+                    pp.next = node;\n                 }\n             }\n-            if (whead == h) {\n-                if ((np = node.prev) != p) {\n-                    if (np != null)\n-                        (p = np).next = node;   // stale\n-                }\n-                else if ((ps = p.status) == 0)\n-                    U.compareAndSwapInt(p, WSTATUS, 0, WAITING);\n-                else if (ps == CANCELLED) {\n-                    if ((pp = p.prev) != null) {\n-                        node.prev = pp;\n-                        pp.next = node;\n-                    }\n-                }\n-                else {\n-                    long time;\n-                    if (deadline == 0L)\n-                        time = 0L;\n-                    else if ((time = deadline - System.nanoTime()) <= 0L)\n-                        return cancelWaiter(node, node, false);\n-                    Thread wt = Thread.currentThread();\n-                    U.putObject(wt, PARKBLOCKER, this);\n-                    node.thread = wt;\n-                    if (p.status < 0 &&\n-                        (p != h || (state & ABITS) == WBIT) &&\n-                        whead == h && node.prev == p)\n-                        U.park(false, time);\n-                    node.thread = null;\n-                    U.putObject(wt, PARKBLOCKER, null);\n-                    if (interruptible && Thread.interrupted())\n-                        return cancelWaiter(node, node, true);\n-                }\n+            else {\n+                long time;\n+                if (deadline == 0L)\n+                    time = 0L;\n+                else if ((time = deadline - System.nanoTime()) <= 0L)\n+                    return cancelWaiter(node, node, false);\n+                Thread wt = Thread.currentThread();\n+                U.putObject(wt, PARKBLOCKER, this);\n+                node.thread = wt;\n+                if (node.prev == p && p.status == WAITING &&\n+                    (p != whead || (state & ABITS) != WBIT))\n+                    U.park(false, time);\n+                node.thread = null;\n+                U.putObject(wt, PARKBLOCKER, null);\n+                if (interruptible && Thread.interrupted())\n+                    return cancelWaiter(node, node, true);\n             }\n         }\n     }\n@@ -1288,19 +1253,22 @@ public class StampedLock implements java.io.Serializable {\n         if (node != null && group != null) {\n             Thread w;\n             node.status = CANCELLED;\n+            node.thread = null;\n             // unsplice cancelled nodes from group\n             for (WNode p = group, q; (q = p.cowait) != null;) {\n-                if (q.status == CANCELLED) {\n-                    U.compareAndSwapObject(p, WCOWAIT, q, q.cowait);\n-                    p = group; // restart\n-                }\n+                if (q.status == CANCELLED)\n+                    U.compareAndSwapObject(p, WNEXT, q, q.next);\n                 else\n                     p = q;\n             }\n             if (group == node) {\n-                for (WNode r = group.cowait; r != null; r = r.cowait) {\n-                    if ((w = r.thread) != null)\n-                        U.unpark(w);       // wake up uncancelled co-waiters\n+                WNode r; // detach and wake up uncancelled co-waiters\n+                while ((r = node.cowait) != null) {\n+                    if (U.compareAndSwapObject(node, WCOWAIT, r, r.cowait) &&\n+                        (w = r.thread) != null) {\n+                        r.thread = null;\n+                        U.unpark(w);\n+                    }\n                 }\n                 for (WNode pred = node.prev; pred != null; ) { // unsplice\n                     WNode succ, pp;        // find valid successor\ndiff --git a/src/main/java/jsr166e/extra/AtomicDouble.java b/src/main/java/jsr166e/extra/AtomicDouble.java\nindex b3976589ff9..4a176135671 100644\n--- a/src/main/java/jsr166e/extra/AtomicDouble.java\n+++ b/src/main/java/jsr166e/extra/AtomicDouble.java\n@@ -212,8 +212,6 @@ public class AtomicDouble extends Number implements java.io.Serializable {\n     /**\n      * Saves the state to a stream (that is, serializes it).\n      *\n-     * @param s the stream\n-     * @throws java.io.IOException if an I/O error occurs\n      * @serialData The current value is emitted (a {@code double}).\n      */\n     private void writeObject(java.io.ObjectOutputStream s)\n@@ -225,10 +223,6 @@ public class AtomicDouble extends Number implements java.io.Serializable {\n \n     /**\n      * Reconstitutes the instance from a stream (that is, deserializes it).\n-     * @param s the stream\n-     * @throws ClassNotFoundException if the class of a serialized object\n-     *         could not be found\n-     * @throws java.io.IOException if an I/O error occurs\n      */\n     private void readObject(java.io.ObjectInputStream s)\n         throws java.io.IOException, ClassNotFoundException {\ndiff --git a/src/main/java/jsr166e/extra/AtomicDoubleArray.java b/src/main/java/jsr166e/extra/AtomicDoubleArray.java\nindex 33b10f82565..8d56c51f031 100644\n--- a/src/main/java/jsr166e/extra/AtomicDoubleArray.java\n+++ b/src/main/java/jsr166e/extra/AtomicDoubleArray.java\n@@ -237,8 +237,6 @@ public class AtomicDoubleArray implements java.io.Serializable {\n     /**\n      * Saves the state to a stream (that is, serializes it).\n      *\n-     * @param s the stream\n-     * @throws java.io.IOException if an I/O error occurs\n      * @serialData The length of the array is emitted (int), followed by all\n      *             of its elements (each a {@code double}) in the proper order.\n      */\n@@ -257,10 +255,6 @@ public class AtomicDoubleArray implements java.io.Serializable {\n \n     /**\n      * Reconstitutes the instance from a stream (that is, deserializes it).\n-     * @param s the stream\n-     * @throws ClassNotFoundException if the class of a serialized object\n-     *         could not be found\n-     * @throws java.io.IOException if an I/O error occurs\n      */\n     private void readObject(java.io.ObjectInputStream s)\n         throws java.io.IOException, ClassNotFoundException {\ndiff --git a/src/main/java/jsr166e/extra/SequenceLock.java b/src/main/java/jsr166e/extra/SequenceLock.java\nindex ed2a8c80a4a..cc997b6d9d7 100644\n--- a/src/main/java/jsr166e/extra/SequenceLock.java\n+++ b/src/main/java/jsr166e/extra/SequenceLock.java\n@@ -257,10 +257,8 @@ public class SequenceLock implements Lock, java.io.Serializable {\n \n     /**\n      * Creates an instance of {@code SequenceLock} that will retry\n-     * attempts to acquire the lock at least the given number of times\n+     * attempts to acquire the lock at least the given number times\n      * before blocking.\n-     *\n-     * @param spins the number of times before blocking\n      */\n     public SequenceLock(int spins) { sync = new Sync(spins); }\n \n@@ -510,7 +508,7 @@ public class SequenceLock implements Lock, java.io.Serializable {\n      * Throws UnsupportedOperationException. SequenceLocks\n      * do not support Condition objects.\n      *\n-     * @throws UnsupportedOperationException always\n+     * @throws UnsupportedOperationException\n      */\n     public Condition newCondition() {\n         throw new UnsupportedOperationException();\ndiff --git a/src/main/java/jsr166y/CountedCompleter.java b/src/main/java/jsr166y/CountedCompleter.java\nindex 7e742409a36..b23db5311dd 100644\n--- a/src/main/java/jsr166y/CountedCompleter.java\n+++ b/src/main/java/jsr166y/CountedCompleter.java\n@@ -680,7 +680,7 @@ public abstract class CountedCompleter<T> extends ForkJoinTask<T> {\n     }\n \n     /**\n-     * Returns the result of the computation.  By default,\n+     * Returns the result of the computation. By default\n      * returns {@code null}, which is appropriate for {@code Void}\n      * actions, but in other cases should be overridden, almost\n      * always to return a field or function of a field that\ndiff --git a/src/main/java/jsr166y/ForkJoinPool.java b/src/main/java/jsr166y/ForkJoinPool.java\nindex 211722d4548..5ebd0276185 100644\n--- a/src/main/java/jsr166y/ForkJoinPool.java\n+++ b/src/main/java/jsr166y/ForkJoinPool.java\n@@ -2127,10 +2127,10 @@ public class ForkJoinPool extends AbstractExecutorService {\n                     w.runSubtask(t);\n                 }\n             }\n-            else if (active) {      // decrement active count without queuing\n+            else if (active) {       // decrement active count without queuing\n                 long nc = (c = ctl) - AC_UNIT;\n                 if ((int)(nc >> AC_SHIFT) + (config & SMASK) == 0)\n-                    return;         // bypass decrement-then-increment\n+                    return;          // bypass decrement-then-increment\n                 if (U.compareAndSwapLong(this, CTL, c, nc))\n                     active = false;\n             }\ndiff --git a/src/main/java/jsr166y/ForkJoinTask.java b/src/main/java/jsr166y/ForkJoinTask.java\nindex ab56eca152e..df5739ee12a 100644\n--- a/src/main/java/jsr166y/ForkJoinTask.java\n+++ b/src/main/java/jsr166y/ForkJoinTask.java\n@@ -395,13 +395,11 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n         final Throwable ex;\n         ExceptionNode next;\n         final long thrower;  // use id not ref to avoid weak cycles\n-        final int hashCode;  // store task hashCode before weak ref disappears\n         ExceptionNode(ForkJoinTask<?> task, Throwable ex, ExceptionNode next) {\n             super(task, exceptionTableRefQueue);\n             this.ex = ex;\n             this.next = next;\n             this.thrower = Thread.currentThread().getId();\n-            this.hashCode = System.identityHashCode(task);\n         }\n     }\n \n@@ -563,9 +561,9 @@ public abstract class ForkJoinTask<V> implements Future<V>, Serializable {\n     private static void expungeStaleExceptions() {\n         for (Object x; (x = exceptionTableRefQueue.poll()) != null;) {\n             if (x instanceof ExceptionNode) {\n-                int hashCode = ((ExceptionNode)x).hashCode;\n+                ForkJoinTask<?> key = ((ExceptionNode)x).get();\n                 ExceptionNode[] t = exceptionTable;\n-                int i = hashCode & (t.length - 1);\n+                int i = System.identityHashCode(key) & (t.length - 1);\n                 ExceptionNode e = t[i];\n                 ExceptionNode pred = null;\n                 while (e != null) {\ndiff --git a/src/main/java/jsr166y/RecursiveTask.java b/src/main/java/jsr166y/RecursiveTask.java\nindex 01929664189..eba46631dc5 100644\n--- a/src/main/java/jsr166y/RecursiveTask.java\n+++ b/src/main/java/jsr166y/RecursiveTask.java\n@@ -15,7 +15,7 @@ package jsr166y;\n  * class Fibonacci extends RecursiveTask<Integer> {\n  *   final int n;\n  *   Fibonacci(int n) { this.n = n; }\n- *   protected Integer compute() {\n+ *   Integer compute() {\n  *     if (n <= 1)\n  *       return n;\n  *     Fibonacci f1 = new Fibonacci(n - 1);\ndiff --git a/src/main/java/jsr166y/ThreadLocalRandom.java b/src/main/java/jsr166y/ThreadLocalRandom.java\nindex fe6fc8ed654..032f77a04bc 100644\n--- a/src/main/java/jsr166y/ThreadLocalRandom.java\n+++ b/src/main/java/jsr166y/ThreadLocalRandom.java\n@@ -107,9 +107,9 @@ public class ThreadLocalRandom extends Random {\n      *\n      * @param least the least value returned\n      * @param bound the upper bound (exclusive)\n-     * @return the next value\n      * @throws IllegalArgumentException if least greater than or equal\n      * to bound\n+     * @return the next value\n      */\n     public int nextInt(int least, int bound) {\n         if (least >= bound)\ndiff --git a/src/main/java/jsr166y/package-info.java b/src/main/java/jsr166y/package-info.java\nindex 980280390a3..b6e19b97965 100644\n--- a/src/main/java/jsr166y/package-info.java\n+++ b/src/main/java/jsr166y/package-info.java\n@@ -25,4 +25,6 @@\n  * complete the tasks.  In general, the most efficient ForkJoinTasks\n  * are those that directly implement this algorithmic design pattern.\n  */\n+\n+// Built on 2013-07-10\n package jsr166y;\ndiff --git a/src/main/java/org/elasticsearch/action/ActionModule.java b/src/main/java/org/elasticsearch/action/ActionModule.java\nindex 0e7b58e1830..4ec9131783f 100644\n--- a/src/main/java/org/elasticsearch/action/ActionModule.java\n+++ b/src/main/java/org/elasticsearch/action/ActionModule.java\n@@ -54,8 +54,6 @@ import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotA\n import org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction;\n import org.elasticsearch.action.admin.cluster.state.ClusterStateAction;\n import org.elasticsearch.action.admin.cluster.state.TransportClusterStateAction;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsAction;\n-import org.elasticsearch.action.admin.cluster.stats.TransportClusterStatsAction;\n import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksAction;\n import org.elasticsearch.action.admin.cluster.tasks.TransportPendingClusterTasksAction;\n import org.elasticsearch.action.admin.indices.alias.IndicesAliasesAction;\n@@ -201,7 +199,6 @@ public class ActionModule extends AbstractModule {\n         registerAction(NodesRestartAction.INSTANCE, TransportNodesRestartAction.class);\n         registerAction(NodesHotThreadsAction.INSTANCE, TransportNodesHotThreadsAction.class);\n \n-        registerAction(ClusterStatsAction.INSTANCE, TransportClusterStatsAction.class);\n         registerAction(ClusterStateAction.INSTANCE, TransportClusterStateAction.class);\n         registerAction(ClusterHealthAction.INSTANCE, TransportClusterHealthAction.class);\n         registerAction(ClusterUpdateSettingsAction.INSTANCE, TransportClusterUpdateSettingsAction.class);\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java b/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java\nindex 7a66cedf559..6b3e4e87a5b 100644\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java\n+++ b/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java\n@@ -21,10 +21,6 @@ package org.elasticsearch.action.admin.cluster.health;\n \n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Maps;\n-import org.elasticsearch.cluster.metadata.IndexMetaData;\n-import org.elasticsearch.cluster.routing.IndexRoutingTable;\n-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n-import org.elasticsearch.cluster.routing.ShardRouting;\n import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n import org.elasticsearch.common.io.stream.Streamable;\n@@ -73,68 +69,6 @@ public class ClusterIndexHealth implements Iterable<ClusterShardHealth>, Streama\n         this.validationFailures = validationFailures;\n     }\n \n-    public ClusterIndexHealth(IndexMetaData indexMetaData, IndexRoutingTable indexRoutingTable) {\n-        this.index = indexMetaData.index();\n-        this.numberOfShards = indexMetaData.getNumberOfShards();\n-        this.numberOfReplicas = indexMetaData.getNumberOfReplicas();\n-        this.validationFailures = indexRoutingTable.validate(indexMetaData);\n-\n-        for (IndexShardRoutingTable shardRoutingTable : indexRoutingTable) {\n-            ClusterShardHealth shardHealth = new ClusterShardHealth(shardRoutingTable.shardId().id());\n-            for (ShardRouting shardRouting : shardRoutingTable) {\n-                if (shardRouting.active()) {\n-                    shardHealth.activeShards++;\n-                    if (shardRouting.relocating()) {\n-                        // the shard is relocating, the one he is relocating to will be in initializing state, so we don't count it\n-                        shardHealth.relocatingShards++;\n-                    }\n-                    if (shardRouting.primary()) {\n-                        shardHealth.primaryActive = true;\n-                    }\n-                } else if (shardRouting.initializing()) {\n-                    shardHealth.initializingShards++;\n-                } else if (shardRouting.unassigned()) {\n-                    shardHealth.unassignedShards++;\n-                }\n-            }\n-            if (shardHealth.primaryActive) {\n-                if (shardHealth.activeShards == shardRoutingTable.size()) {\n-                    shardHealth.status = ClusterHealthStatus.GREEN;\n-                } else {\n-                    shardHealth.status = ClusterHealthStatus.YELLOW;\n-                }\n-            } else {\n-                shardHealth.status = ClusterHealthStatus.RED;\n-            }\n-            shards.put(shardHealth.getId(), shardHealth);\n-        }\n-\n-        // update the index status\n-        status = ClusterHealthStatus.GREEN;\n-\n-        for (ClusterShardHealth shardHealth : shards.values()) {\n-            if (shardHealth.isPrimaryActive()) {\n-                activePrimaryShards++;\n-            }\n-            activeShards += shardHealth.activeShards;\n-            relocatingShards += shardHealth.relocatingShards;\n-            initializingShards += shardHealth.initializingShards;\n-            unassignedShards += shardHealth.unassignedShards;\n-\n-            if (shardHealth.getStatus() == ClusterHealthStatus.RED) {\n-                status = ClusterHealthStatus.RED;\n-            } else if (shardHealth.getStatus() == ClusterHealthStatus.YELLOW && status != ClusterHealthStatus.RED) {\n-                // do not override an existing red\n-                status = ClusterHealthStatus.YELLOW;\n-            }\n-        }\n-        if (!validationFailures.isEmpty()) {\n-            status = ClusterHealthStatus.RED;\n-        } else if (shards.isEmpty()) { // might be since none has been created yet (two phase index creation)\n-            status = ClusterHealthStatus.RED;\n-        }\n-    }\n-\n     public String getIndex() {\n         return index;\n     }\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java b/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java\nindex db81c46ddd1..9c68938091f 100644\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java\n+++ b/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java\n@@ -28,7 +28,9 @@ import org.elasticsearch.cluster.ClusterState;\n import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;\n import org.elasticsearch.cluster.metadata.IndexMetaData;\n import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n import org.elasticsearch.cluster.routing.RoutingTableValidation;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.indices.IndexMissingException;\n@@ -221,7 +223,6 @@ public class TransportClusterHealthAction extends TransportMasterNodeOperationAc\n         }\n     }\n \n-\n     private ClusterHealthResponse clusterHealth(ClusterHealthRequest request, ClusterState clusterState) {\n         if (logger.isTraceEnabled()) {\n             logger.trace(\"Calculating health based on state version [{}]\", clusterState.version());\n@@ -243,8 +244,64 @@ public class TransportClusterHealthAction extends TransportMasterNodeOperationAc\n             if (indexRoutingTable == null) {\n                 continue;\n             }\n+            ClusterIndexHealth indexHealth = new ClusterIndexHealth(index, indexMetaData.numberOfShards(), indexMetaData.numberOfReplicas(), validation.indexFailures(indexMetaData.index()));\n+\n+            for (IndexShardRoutingTable shardRoutingTable : indexRoutingTable) {\n+                ClusterShardHealth shardHealth = new ClusterShardHealth(shardRoutingTable.shardId().id());\n+                for (ShardRouting shardRouting : shardRoutingTable) {\n+                    if (shardRouting.active()) {\n+                        shardHealth.activeShards++;\n+                        if (shardRouting.relocating()) {\n+                            // the shard is relocating, the one he is relocating to will be in initializing state, so we don't count it\n+                            shardHealth.relocatingShards++;\n+                        }\n+                        if (shardRouting.primary()) {\n+                            shardHealth.primaryActive = true;\n+                        }\n+                    } else if (shardRouting.initializing()) {\n+                        shardHealth.initializingShards++;\n+                    } else if (shardRouting.unassigned()) {\n+                        shardHealth.unassignedShards++;\n+                    }\n+                }\n+                if (shardHealth.primaryActive) {\n+                    if (shardHealth.activeShards == shardRoutingTable.size()) {\n+                        shardHealth.status = ClusterHealthStatus.GREEN;\n+                    } else {\n+                        shardHealth.status = ClusterHealthStatus.YELLOW;\n+                    }\n+                } else {\n+                    shardHealth.status = ClusterHealthStatus.RED;\n+                }\n+                indexHealth.shards.put(shardHealth.getId(), shardHealth);\n+            }\n \n-            ClusterIndexHealth indexHealth = new ClusterIndexHealth(indexMetaData, indexRoutingTable);\n+            for (ClusterShardHealth shardHealth : indexHealth) {\n+                if (shardHealth.isPrimaryActive()) {\n+                    indexHealth.activePrimaryShards++;\n+                }\n+                indexHealth.activeShards += shardHealth.activeShards;\n+                indexHealth.relocatingShards += shardHealth.relocatingShards;\n+                indexHealth.initializingShards += shardHealth.initializingShards;\n+                indexHealth.unassignedShards += shardHealth.unassignedShards;\n+            }\n+            // update the index status\n+            indexHealth.status = ClusterHealthStatus.GREEN;\n+            if (!indexHealth.getValidationFailures().isEmpty()) {\n+                indexHealth.status = ClusterHealthStatus.RED;\n+            } else if (indexHealth.getShards().isEmpty()) { // might be since none has been created yet (two phase index creation)\n+                indexHealth.status = ClusterHealthStatus.RED;\n+            } else {\n+                for (ClusterShardHealth shardHealth : indexHealth) {\n+                    if (shardHealth.getStatus() == ClusterHealthStatus.RED) {\n+                        indexHealth.status = ClusterHealthStatus.RED;\n+                        break;\n+                    }\n+                    if (shardHealth.getStatus() == ClusterHealthStatus.YELLOW) {\n+                        indexHealth.status = ClusterHealthStatus.YELLOW;\n+                    }\n+                }\n+            }\n \n             response.indices.put(indexHealth.getIndex(), indexHealth);\n         }\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginInfo.java b/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginInfo.java\nindex 5b705b68ef5..d86fc97d00f 100644\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginInfo.java\n+++ b/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginInfo.java\n@@ -29,11 +29,10 @@ public class PluginInfo implements Streamable, Serializable, ToXContent {\n \n     /**\n      * Information about plugins\n-     *\n-     * @param name        Its name\n+     * @param name Its name\n      * @param description Its description\n-     * @param site        true if it's a site plugin\n-     * @param jvm         true if it's a jvm plugin\n+     * @param site true if it's a site plugin\n+     * @param jvm true if it's a jvm plugin\n      */\n     public PluginInfo(String name, String description, boolean site, boolean jvm) {\n         this.name = name;\n@@ -72,7 +71,6 @@ public class PluginInfo implements Streamable, Serializable, ToXContent {\n \n     /**\n      * We compute the URL for sites: \"/_plugin/\" + name + \"/\"\n-     *\n      * @return\n      */\n     public String getUrl() {\n@@ -119,24 +117,4 @@ public class PluginInfo implements Streamable, Serializable, ToXContent {\n \n         return builder;\n     }\n-\n-    @Override\n-    public boolean equals(Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-\n-        PluginInfo p = (PluginInfo) o;\n-\n-        return name.equals(p.getName());\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return name.hashCode();\n-    }\n-\n }\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsAction.java b/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsAction.java\ndeleted file mode 100644\nindex be27cd54101..00000000000\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsAction.java\n+++ /dev/null\n@@ -1,45 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.action.admin.cluster.stats;\n-\n-import org.elasticsearch.action.admin.cluster.ClusterAction;\n-import org.elasticsearch.client.ClusterAdminClient;\n-\n-/**\n- */\n-public class ClusterStatsAction extends ClusterAction<ClusterStatsRequest, ClusterStatsResponse, ClusterStatsRequestBuilder> {\n-\n-    public static final ClusterStatsAction INSTANCE = new ClusterStatsAction();\n-    public static final String NAME = \"cluster/stats\";\n-\n-    private ClusterStatsAction() {\n-        super(NAME);\n-    }\n-\n-    @Override\n-    public ClusterStatsResponse newResponse() {\n-        return new ClusterStatsResponse();\n-    }\n-\n-    @Override\n-    public ClusterStatsRequestBuilder newRequestBuilder(ClusterAdminClient client) {\n-        return new ClusterStatsRequestBuilder(client);\n-    }\n-}\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java b/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java\ndeleted file mode 100644\nindex 1d12e5036bc..00000000000\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java\n+++ /dev/null\n@@ -1,423 +0,0 @@\n-package org.elasticsearch.action.admin.cluster.stats;\n-/*\n- * Licensed to ElasticSearch under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-\n-import com.carrotsearch.hppc.ObjectObjectOpenHashMap;\n-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;\n-import org.elasticsearch.Version;\n-import org.elasticsearch.action.admin.indices.stats.CommonStats;\n-import org.elasticsearch.common.io.stream.StreamInput;\n-import org.elasticsearch.common.io.stream.StreamOutput;\n-import org.elasticsearch.common.io.stream.Streamable;\n-import org.elasticsearch.common.xcontent.ToXContent;\n-import org.elasticsearch.common.xcontent.XContentBuilder;\n-import org.elasticsearch.common.xcontent.XContentBuilderString;\n-import org.elasticsearch.index.cache.filter.FilterCacheStats;\n-import org.elasticsearch.index.cache.id.IdCacheStats;\n-import org.elasticsearch.index.engine.SegmentsStats;\n-import org.elasticsearch.index.fielddata.FieldDataStats;\n-import org.elasticsearch.index.percolator.stats.PercolateStats;\n-import org.elasticsearch.index.shard.DocsStats;\n-import org.elasticsearch.index.store.StoreStats;\n-import org.elasticsearch.search.suggest.completion.CompletionStats;\n-\n-import java.io.IOException;\n-\n-public class ClusterStatsIndices implements ToXContent, Streamable {\n-\n-    private int indexCount;\n-    private ShardStats shards;\n-    private DocsStats docs;\n-    private StoreStats store;\n-    private FieldDataStats fieldData;\n-    private FilterCacheStats filterCache;\n-    private IdCacheStats idCache;\n-    private CompletionStats completion;\n-    private SegmentsStats segments;\n-    private PercolateStats peroclate;\n-\n-    private ClusterStatsIndices() {\n-    }\n-\n-    public ClusterStatsIndices(ClusterStatsNodeResponse[] nodeResponses) {\n-        ObjectObjectOpenHashMap<String, ShardStats> countsPerIndex = new ObjectObjectOpenHashMap<String, ShardStats>();\n-\n-        this.docs = new DocsStats();\n-        this.store = new StoreStats();\n-        this.fieldData = new FieldDataStats();\n-        this.filterCache = new FilterCacheStats();\n-        this.idCache = new IdCacheStats();\n-        this.completion = new CompletionStats();\n-        this.segments = new SegmentsStats();\n-        this.peroclate = new PercolateStats();\n-\n-        for (ClusterStatsNodeResponse r : nodeResponses) {\n-            for (org.elasticsearch.action.admin.indices.stats.ShardStats shardStats : r.shardsStats()) {\n-                ShardStats indexShardStats = countsPerIndex.get(shardStats.getIndex());\n-                if (indexShardStats == null) {\n-                    indexShardStats = new ShardStats();\n-                    countsPerIndex.put(shardStats.getIndex(), indexShardStats);\n-                }\n-\n-                indexShardStats.total++;\n-\n-                CommonStats shardCommonStats = shardStats.getStats();\n-\n-                if (shardStats.getShardRouting().primary()) {\n-                    indexShardStats.primaries++;\n-                    docs.add(shardCommonStats.docs);\n-                }\n-                store.add(shardCommonStats.store);\n-                fieldData.add(shardCommonStats.fieldData);\n-                filterCache.add(shardCommonStats.filterCache);\n-                idCache.add(shardCommonStats.idCache);\n-                completion.add(shardCommonStats.completion);\n-                segments.add(shardCommonStats.segments);\n-                peroclate.add(shardCommonStats.percolate);\n-            }\n-        }\n-\n-        shards = new ShardStats();\n-        indexCount = countsPerIndex.size();\n-        for (ObjectObjectCursor<String, ShardStats> indexCountsCursor : countsPerIndex) {\n-            shards.addIndexShardCount(indexCountsCursor.value);\n-        }\n-    }\n-\n-    public int getIndexCount() {\n-        return indexCount;\n-    }\n-\n-    public ShardStats getShards() {\n-        return this.shards;\n-    }\n-\n-    public DocsStats getDocs() {\n-        return docs;\n-    }\n-\n-    public StoreStats getStore() {\n-        return store;\n-    }\n-\n-    public FieldDataStats getFieldData() {\n-        return fieldData;\n-    }\n-\n-    public FilterCacheStats getFilterCache() {\n-        return filterCache;\n-    }\n-\n-    public IdCacheStats getIdCache() {\n-        return idCache;\n-    }\n-\n-    public CompletionStats getCompletion() {\n-        return completion;\n-    }\n-\n-    public SegmentsStats getSegments() {\n-        return segments;\n-    }\n-\n-    public PercolateStats getPercolate() {\n-        return peroclate;\n-    }\n-\n-    @Override\n-    public void readFrom(StreamInput in) throws IOException {\n-        indexCount = in.readVInt();\n-        shards = ShardStats.readShardStats(in);\n-        docs = DocsStats.readDocStats(in);\n-        store = StoreStats.readStoreStats(in);\n-        fieldData = FieldDataStats.readFieldDataStats(in);\n-        filterCache = FilterCacheStats.readFilterCacheStats(in);\n-        idCache = IdCacheStats.readIdCacheStats(in);\n-        completion = CompletionStats.readCompletionStats(in);\n-        segments = SegmentsStats.readSegmentsStats(in);\n-        if (in.getVersion().after(Version.V_1_0_0_RC1)) {\n-            peroclate = PercolateStats.readPercolateStats(in);\n-        } else {\n-            peroclate = new PercolateStats();\n-        }\n-    }\n-\n-    @Override\n-    public void writeTo(StreamOutput out) throws IOException {\n-        out.writeVInt(indexCount);\n-        shards.writeTo(out);\n-        docs.writeTo(out);\n-        store.writeTo(out);\n-        fieldData.writeTo(out);\n-        filterCache.writeTo(out);\n-        idCache.writeTo(out);\n-        completion.writeTo(out);\n-        segments.writeTo(out);\n-        if (out.getVersion().onOrAfter(Version.V_1_0_0_RC1)) {\n-            peroclate.writeTo(out);\n-        }\n-    }\n-\n-    public static ClusterStatsIndices readIndicesStats(StreamInput in) throws IOException {\n-        ClusterStatsIndices indicesStats = new ClusterStatsIndices();\n-        indicesStats.readFrom(in);\n-        return indicesStats;\n-    }\n-\n-    static final class Fields {\n-        static final XContentBuilderString COUNT = new XContentBuilderString(\"count\");\n-    }\n-\n-    @Override\n-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-        builder.field(Fields.COUNT, indexCount);\n-        shards.toXContent(builder, params);\n-        docs.toXContent(builder, params);\n-        store.toXContent(builder, params);\n-        fieldData.toXContent(builder, params);\n-        filterCache.toXContent(builder, params);\n-        idCache.toXContent(builder, params);\n-        completion.toXContent(builder, params);\n-        segments.toXContent(builder, params);\n-        peroclate.toXContent(builder, params);\n-        return builder;\n-    }\n-\n-    public static class ShardStats implements ToXContent, Streamable {\n-\n-        int indices;\n-        int total;\n-        int primaries;\n-\n-        // min/max\n-        int minIndexShards = -1;\n-        int maxIndexShards = -1;\n-        int minIndexPrimaryShards = -1;\n-        int maxIndexPrimaryShards = -1;\n-        double minIndexReplication = -1;\n-        double totalIndexReplication = 0;\n-        double maxIndexReplication = -1;\n-\n-        public ShardStats() {\n-        }\n-\n-        /**\n-         * number of indices in the cluster\n-         */\n-        public int getIndices() {\n-            return this.indices;\n-        }\n-\n-        /**\n-         * total number of shards in the cluster\n-         */\n-        public int getTotal() {\n-            return this.total;\n-        }\n-\n-        /**\n-         * total number of primary shards in the cluster\n-         */\n-        public int getPrimaries() {\n-            return this.primaries;\n-        }\n-\n-        /**\n-         * returns how many *redundant* copies of the data the cluster holds - running with no replicas will return 0\n-         */\n-        public double getReplication() {\n-            if (primaries == 0) {\n-                return 0;\n-            }\n-            return (((double) (total - primaries)) / primaries);\n-        }\n-\n-        /** the maximum number of shards (primary+replicas) an index has */\n-        public int getMaxIndexShards() {\n-            return this.maxIndexShards;\n-        }\n-\n-        /** the minimum number of shards (primary+replicas) an index has */\n-        public int getMinIndexShards() {\n-            return this.minIndexShards;\n-        }\n-\n-        /** average number of shards (primary+replicas) across the indices */\n-        public double getAvgIndexShards() {\n-            if (this.indices == 0) {\n-                return -1;\n-            }\n-            return ((double) this.total) / this.indices;\n-        }\n-\n-        /**\n-         * the maximum number of primary shards an index has\n-         */\n-        public int getMaxIndexPrimaryShards() {\n-            return this.maxIndexPrimaryShards;\n-        }\n-\n-        /** the minimum number of primary shards an index has */\n-        public int getMinIndexPrimaryShards() {\n-            return this.minIndexPrimaryShards;\n-        }\n-\n-        /** the average number primary shards across the indices */\n-        public double getAvgIndexPrimaryShards() {\n-            if (this.indices == 0) {\n-                return -1;\n-            }\n-            return ((double) this.primaries) / this.indices;\n-        }\n-\n-        /**\n-         * minimum replication factor across the indices. See {@link #getReplication}\n-         */\n-        public double getMinIndexReplication() {\n-            return this.minIndexReplication;\n-        }\n-\n-        /**\n-         * average replication factor across the indices. See {@link #getReplication}\n-         */\n-        public double getAvgIndexReplication() {\n-            if (indices == 0) {\n-                return -1;\n-            }\n-            return this.totalIndexReplication / this.indices;\n-        }\n-\n-        /**\n-         * maximum replication factor across the indices. See {@link #getReplication\n-         */\n-        public double getMaxIndexReplication() {\n-            return this.maxIndexReplication;\n-        }\n-\n-        public void addIndexShardCount(ShardStats indexShardCount) {\n-            this.indices++;\n-            this.primaries += indexShardCount.primaries;\n-            this.total += indexShardCount.total;\n-            this.totalIndexReplication += indexShardCount.getReplication();\n-            if (this.indices == 1) {\n-                // first index, uninitialized.\n-                minIndexPrimaryShards = indexShardCount.primaries;\n-                maxIndexPrimaryShards = indexShardCount.primaries;\n-                minIndexShards = indexShardCount.total;\n-                maxIndexShards = indexShardCount.total;\n-                minIndexReplication = indexShardCount.getReplication();\n-                maxIndexReplication = minIndexReplication;\n-            } else {\n-                minIndexShards = Math.min(minIndexShards, indexShardCount.total);\n-                minIndexPrimaryShards = Math.min(minIndexPrimaryShards, indexShardCount.primaries);\n-                minIndexReplication = Math.min(minIndexReplication, indexShardCount.getReplication());\n-\n-                maxIndexShards = Math.max(maxIndexShards, indexShardCount.total);\n-                maxIndexPrimaryShards = Math.max(maxIndexPrimaryShards, indexShardCount.primaries);\n-                maxIndexReplication = Math.max(maxIndexReplication, indexShardCount.getReplication());\n-            }\n-        }\n-\n-        public static ShardStats readShardStats(StreamInput in) throws IOException {\n-            ShardStats c = new ShardStats();\n-            c.readFrom(in);\n-            return c;\n-        }\n-\n-        @Override\n-        public void readFrom(StreamInput in) throws IOException {\n-            indices = in.readVInt();\n-            total = in.readVInt();\n-            primaries = in.readVInt();\n-            minIndexShards = in.readVInt();\n-            maxIndexShards = in.readVInt();\n-            minIndexPrimaryShards = in.readVInt();\n-            maxIndexPrimaryShards = in.readVInt();\n-            minIndexReplication = in.readDouble();\n-            totalIndexReplication = in.readDouble();\n-            maxIndexReplication = in.readDouble();\n-        }\n-\n-        @Override\n-        public void writeTo(StreamOutput out) throws IOException {\n-            out.writeVInt(indices);\n-            out.writeVInt(total);\n-            out.writeVInt(primaries);\n-            out.writeVInt(minIndexShards);\n-            out.writeVInt(maxIndexShards);\n-            out.writeVInt(minIndexPrimaryShards);\n-            out.writeVInt(maxIndexPrimaryShards);\n-            out.writeDouble(minIndexReplication);\n-            out.writeDouble(totalIndexReplication);\n-            out.writeDouble(maxIndexReplication);\n-        }\n-\n-        static final class Fields {\n-            static final XContentBuilderString SHARDS = new XContentBuilderString(\"shards\");\n-            static final XContentBuilderString TOTAL = new XContentBuilderString(\"total\");\n-            static final XContentBuilderString PRIMARIES = new XContentBuilderString(\"primaries\");\n-            static final XContentBuilderString REPLICATION = new XContentBuilderString(\"replication\");\n-            static final XContentBuilderString MIN = new XContentBuilderString(\"min\");\n-            static final XContentBuilderString MAX = new XContentBuilderString(\"max\");\n-            static final XContentBuilderString AVG = new XContentBuilderString(\"avg\");\n-            static final XContentBuilderString INDEX = new XContentBuilderString(\"index\");\n-        }\n-\n-        private void addIntMinMax(XContentBuilderString field, int min, int max, double avg, XContentBuilder builder) throws IOException {\n-            builder.startObject(field);\n-            builder.field(Fields.MIN, min);\n-            builder.field(Fields.MAX, max);\n-            builder.field(Fields.AVG, avg);\n-            builder.endObject();\n-        }\n-\n-        private void addDoubleMinMax(XContentBuilderString field, double min, double max, double avg, XContentBuilder builder) throws IOException {\n-            builder.startObject(field);\n-            builder.field(Fields.MIN, min);\n-            builder.field(Fields.MAX, max);\n-            builder.field(Fields.AVG, avg);\n-            builder.endObject();\n-        }\n-\n-        @Override\n-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-            builder.startObject(Fields.SHARDS);\n-            if (indices > 0) {\n-\n-                builder.field(Fields.TOTAL, total);\n-                builder.field(Fields.PRIMARIES, primaries);\n-                builder.field(Fields.REPLICATION, getReplication());\n-\n-                builder.startObject(Fields.INDEX);\n-                addIntMinMax(Fields.SHARDS, minIndexShards, maxIndexShards, getAvgIndexShards(), builder);\n-                addIntMinMax(Fields.PRIMARIES, minIndexPrimaryShards, maxIndexPrimaryShards, getAvgIndexPrimaryShards(), builder);\n-                addDoubleMinMax(Fields.REPLICATION, minIndexReplication, maxIndexReplication, getAvgIndexReplication(), builder);\n-                builder.endObject();\n-            }\n-            builder.endObject();\n-            return builder;\n-        }\n-\n-        @Override\n-        public String toString() {\n-            return \"total [\" + total + \"] primaries [\" + primaries + \"]\";\n-        }\n-    }\n-}\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java b/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java\ndeleted file mode 100644\nindex 597edbe986e..00000000000\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java\n+++ /dev/null\n@@ -1,88 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.action.admin.cluster.stats;\n-\n-import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;\n-import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;\n-import org.elasticsearch.action.admin.indices.stats.ShardStats;\n-import org.elasticsearch.action.support.nodes.NodeOperationResponse;\n-import org.elasticsearch.cluster.node.DiscoveryNode;\n-import org.elasticsearch.common.io.stream.StreamInput;\n-import org.elasticsearch.common.io.stream.StreamOutput;\n-\n-import java.io.IOException;\n-\n-public class ClusterStatsNodeResponse extends NodeOperationResponse {\n-\n-    private NodeInfo nodeInfo;\n-    private NodeStats nodeStats;\n-    private ShardStats[] shardsStats;\n-\n-    ClusterStatsNodeResponse() {\n-    }\n-\n-    public ClusterStatsNodeResponse(DiscoveryNode node, NodeInfo nodeInfo, NodeStats nodeStats, ShardStats[] shardsStats) {\n-        super(node);\n-        this.nodeInfo = nodeInfo;\n-        this.nodeStats = nodeStats;\n-        this.shardsStats = shardsStats;\n-    }\n-\n-    public NodeInfo nodeInfo() {\n-        return this.nodeInfo;\n-    }\n-\n-    public NodeStats nodeStats() {\n-        return this.nodeStats;\n-    }\n-\n-    public ShardStats[] shardsStats() {\n-        return this.shardsStats;\n-    }\n-\n-    public static ClusterStatsNodeResponse readNodeResponse(StreamInput in) throws IOException {\n-        ClusterStatsNodeResponse nodeResponse = new ClusterStatsNodeResponse();\n-        nodeResponse.readFrom(in);\n-        return nodeResponse;\n-    }\n-\n-    @Override\n-    public void readFrom(StreamInput in) throws IOException {\n-        super.readFrom(in);\n-        this.nodeInfo = NodeInfo.readNodeInfo(in);\n-        this.nodeStats = NodeStats.readNodeStats(in);\n-        int size = in.readVInt();\n-        shardsStats = new ShardStats[size];\n-        for (size--; size >= 0; size--) {\n-            shardsStats[size] = ShardStats.readShardStats(in);\n-        }\n-    }\n-\n-    @Override\n-    public void writeTo(StreamOutput out) throws IOException {\n-        super.writeTo(out);\n-        nodeInfo.writeTo(out);\n-        nodeStats.writeTo(out);\n-        out.writeVInt(shardsStats.length);\n-        for (ShardStats ss : shardsStats) {\n-            ss.writeTo(out);\n-        }\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java b/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java\ndeleted file mode 100644\nindex 74d3ebc5453..00000000000\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java\n+++ /dev/null\n@@ -1,658 +0,0 @@\n-package org.elasticsearch.action.admin.cluster.stats;\n-/*\n- * Licensed to ElasticSearch under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-\n-import com.carrotsearch.hppc.ObjectIntOpenHashMap;\n-import com.carrotsearch.hppc.cursors.ObjectIntCursor;\n-import org.elasticsearch.Version;\n-import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;\n-import org.elasticsearch.action.admin.cluster.node.info.PluginInfo;\n-import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;\n-import org.elasticsearch.cluster.node.DiscoveryNode;\n-import org.elasticsearch.common.io.stream.StreamInput;\n-import org.elasticsearch.common.io.stream.StreamOutput;\n-import org.elasticsearch.common.io.stream.Streamable;\n-import org.elasticsearch.common.transport.InetSocketTransportAddress;\n-import org.elasticsearch.common.transport.TransportAddress;\n-import org.elasticsearch.common.unit.ByteSizeValue;\n-import org.elasticsearch.common.unit.TimeValue;\n-import org.elasticsearch.common.xcontent.ToXContent;\n-import org.elasticsearch.common.xcontent.XContentBuilder;\n-import org.elasticsearch.common.xcontent.XContentBuilderString;\n-import org.elasticsearch.monitor.fs.FsStats;\n-import org.elasticsearch.monitor.jvm.JvmInfo;\n-import org.elasticsearch.monitor.os.OsInfo;\n-\n-import java.io.IOException;\n-import java.net.InetAddress;\n-import java.util.HashSet;\n-import java.util.Set;\n-\n-public class ClusterStatsNodes implements ToXContent, Streamable {\n-\n-    private Counts counts;\n-    private Set<Version> versions;\n-    private OsStats os;\n-    private ProcessStats process;\n-    private JvmStats jvm;\n-    private FsStats.Info fs;\n-    private Set<PluginInfo> plugins;\n-\n-    private ClusterStatsNodes() {\n-    }\n-\n-    public ClusterStatsNodes(ClusterStatsNodeResponse[] nodeResponses) {\n-        this.counts = new Counts();\n-        this.versions = new HashSet<Version>();\n-        this.os = new OsStats();\n-        this.jvm = new JvmStats();\n-        this.fs = new FsStats.Info();\n-        this.plugins = new HashSet<PluginInfo>();\n-        this.process = new ProcessStats();\n-\n-        Set<InetAddress> seenAddresses = new HashSet<InetAddress>(nodeResponses.length);\n-\n-        for (ClusterStatsNodeResponse nodeResponse : nodeResponses) {\n-\n-            counts.addNodeInfo(nodeResponse.nodeInfo());\n-            versions.add(nodeResponse.nodeInfo().getVersion());\n-            process.addNodeStats(nodeResponse.nodeStats());\n-            jvm.addNodeInfoStats(nodeResponse.nodeInfo(), nodeResponse.nodeStats());\n-            plugins.addAll(nodeResponse.nodeInfo().getPlugins().getInfos());\n-\n-            // now do the stats that should be deduped by hardware (implemented by ip deduping)\n-            TransportAddress publishAddress = nodeResponse.nodeInfo().getTransport().address().publishAddress();\n-            InetAddress inetAddress = null;\n-            if (publishAddress.uniqueAddressTypeId() == 1) {\n-                inetAddress = ((InetSocketTransportAddress) publishAddress).address().getAddress();\n-            }\n-\n-            if (!seenAddresses.add(inetAddress)) {\n-                continue;\n-            }\n-\n-            os.addNodeInfo(nodeResponse.nodeInfo());\n-            if (nodeResponse.nodeStats().getFs() != null) {\n-                fs.add(nodeResponse.nodeStats().getFs().total());\n-            }\n-        }\n-    }\n-\n-\n-    public Counts getCounts() {\n-        return this.counts;\n-    }\n-\n-    public Set<Version> getVersions() {\n-        return versions;\n-    }\n-\n-    public OsStats getOs() {\n-        return os;\n-    }\n-\n-    public ProcessStats getProcess() {\n-        return process;\n-    }\n-\n-    public JvmStats getJvm() {\n-        return jvm;\n-    }\n-\n-    public FsStats.Info getFs() {\n-        return fs;\n-    }\n-\n-    public Set<PluginInfo> getPlugins() {\n-        return plugins;\n-    }\n-\n-\n-    @Override\n-    public void readFrom(StreamInput in) throws IOException {\n-        counts = Counts.readCounts(in);\n-\n-        int size = in.readVInt();\n-        versions = new HashSet<Version>(size);\n-        for (; size > 0; size--) {\n-            versions.add(Version.readVersion(in));\n-        }\n-\n-        os = OsStats.readOsStats(in);\n-        process = ProcessStats.readStats(in);\n-        jvm = JvmStats.readJvmStats(in);\n-        fs = FsStats.Info.readInfoFrom(in);\n-\n-        size = in.readVInt();\n-        plugins = new HashSet<PluginInfo>(size);\n-        for (; size > 0; size--) {\n-            plugins.add(PluginInfo.readPluginInfo(in));\n-        }\n-    }\n-\n-    @Override\n-    public void writeTo(StreamOutput out) throws IOException {\n-        counts.writeTo(out);\n-        out.writeVInt(versions.size());\n-        for (Version v : versions) Version.writeVersion(v, out);\n-        os.writeTo(out);\n-        process.writeTo(out);\n-        jvm.writeTo(out);\n-        fs.writeTo(out);\n-        out.writeVInt(plugins.size());\n-        for (PluginInfo p : plugins) {\n-            p.writeTo(out);\n-        }\n-    }\n-\n-    public static ClusterStatsNodes readNodeStats(StreamInput in) throws IOException {\n-        ClusterStatsNodes nodeStats = new ClusterStatsNodes();\n-        nodeStats.readFrom(in);\n-        return nodeStats;\n-    }\n-\n-    static final class Fields {\n-        static final XContentBuilderString COUNT = new XContentBuilderString(\"count\");\n-        static final XContentBuilderString VERSIONS = new XContentBuilderString(\"versions\");\n-        static final XContentBuilderString OS = new XContentBuilderString(\"os\");\n-        static final XContentBuilderString PROCESS = new XContentBuilderString(\"process\");\n-        static final XContentBuilderString JVM = new XContentBuilderString(\"jvm\");\n-        static final XContentBuilderString FS = new XContentBuilderString(\"fs\");\n-        static final XContentBuilderString PLUGINS = new XContentBuilderString(\"plugins\");\n-    }\n-\n-    @Override\n-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-        builder.startObject(Fields.COUNT);\n-        counts.toXContent(builder, params);\n-        builder.endObject();\n-\n-        builder.startArray(Fields.VERSIONS);\n-        for (Version v : versions) {\n-            builder.value(v.toString());\n-        }\n-        builder.endArray();\n-\n-        builder.startObject(Fields.OS);\n-        os.toXContent(builder, params);\n-        builder.endObject();\n-\n-        builder.startObject(Fields.PROCESS);\n-        process.toXContent(builder, params);\n-        builder.endObject();\n-\n-        builder.startObject(Fields.JVM);\n-        jvm.toXContent(builder, params);\n-        builder.endObject();\n-\n-        builder.field(Fields.FS);\n-        fs.toXContent(builder, params);\n-\n-        builder.startArray(Fields.PLUGINS);\n-        for (PluginInfo pluginInfo : plugins) {\n-            pluginInfo.toXContent(builder, params);\n-        }\n-        builder.endArray();\n-        return builder;\n-    }\n-\n-    public static class Counts implements Streamable, ToXContent {\n-        int total;\n-        int masterOnly;\n-        int dataOnly;\n-        int masterData;\n-        int client;\n-\n-        public void addNodeInfo(NodeInfo nodeInfo) {\n-            total++;\n-            DiscoveryNode node = nodeInfo.getNode();\n-            if (node.masterNode()) {\n-                if (node.dataNode()) {\n-                    masterData++;\n-                } else {\n-                    masterOnly++;\n-                }\n-            } else if (node.dataNode()) {\n-                dataOnly++;\n-            } else if (node.clientNode()) {\n-                client++;\n-            }\n-        }\n-\n-        public int getTotal() {\n-            return total;\n-        }\n-\n-        public int getMasterOnly() {\n-            return masterOnly;\n-        }\n-\n-        public int getDataOnly() {\n-            return dataOnly;\n-        }\n-\n-        public int getMasterData() {\n-            return masterData;\n-        }\n-\n-        public int getClient() {\n-            return client;\n-        }\n-\n-        public static Counts readCounts(StreamInput in) throws IOException {\n-            Counts c = new Counts();\n-            c.readFrom(in);\n-            return c;\n-        }\n-\n-        @Override\n-        public void readFrom(StreamInput in) throws IOException {\n-            total = in.readVInt();\n-            masterOnly = in.readVInt();\n-            dataOnly = in.readVInt();\n-            masterData = in.readVInt();\n-            client = in.readVInt();\n-        }\n-\n-        @Override\n-        public void writeTo(StreamOutput out) throws IOException {\n-            out.writeVInt(total);\n-            out.writeVInt(masterOnly);\n-            out.writeVInt(dataOnly);\n-            out.writeVInt(masterData);\n-            out.writeVInt(client);\n-        }\n-\n-        static final class Fields {\n-            static final XContentBuilderString TOTAL = new XContentBuilderString(\"total\");\n-            static final XContentBuilderString MASTER_ONLY = new XContentBuilderString(\"master_only\");\n-            static final XContentBuilderString DATA_ONLY = new XContentBuilderString(\"data_only\");\n-            static final XContentBuilderString MASTER_DATA = new XContentBuilderString(\"master_data\");\n-            static final XContentBuilderString CLIENT = new XContentBuilderString(\"client\");\n-        }\n-\n-        @Override\n-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-            builder.field(Fields.TOTAL, total);\n-            builder.field(Fields.MASTER_ONLY, masterOnly);\n-            builder.field(Fields.DATA_ONLY, dataOnly);\n-            builder.field(Fields.MASTER_DATA, masterData);\n-            builder.field(Fields.CLIENT, client);\n-            return builder;\n-        }\n-    }\n-\n-    public static class OsStats implements ToXContent, Streamable {\n-\n-        int availableProcessors;\n-        long availableMemory;\n-        ObjectIntOpenHashMap<OsInfo.Cpu> cpus;\n-\n-        public OsStats() {\n-            cpus = new ObjectIntOpenHashMap<org.elasticsearch.monitor.os.OsInfo.Cpu>();\n-        }\n-\n-        public void addNodeInfo(NodeInfo nodeInfo) {\n-            availableProcessors += nodeInfo.getOs().availableProcessors();\n-            if (nodeInfo.getOs() == null) {\n-                return;\n-            }\n-            if (nodeInfo.getOs().cpu() != null) {\n-                cpus.addTo(nodeInfo.getOs().cpu(), 1);\n-            }\n-            if (nodeInfo.getOs().getMem() != null && nodeInfo.getOs().getMem().getTotal().bytes() != -1) {\n-                availableMemory += nodeInfo.getOs().getMem().getTotal().bytes();\n-            }\n-        }\n-\n-        public int getAvailableProcessors() {\n-            return availableProcessors;\n-        }\n-\n-        public ByteSizeValue getAvailableMemory() {\n-            return new ByteSizeValue(availableMemory);\n-        }\n-\n-        public ObjectIntOpenHashMap<OsInfo.Cpu> getCpus() {\n-            return cpus;\n-        }\n-\n-        @Override\n-        public void readFrom(StreamInput in) throws IOException {\n-            availableProcessors = in.readVInt();\n-            availableMemory = in.readLong();\n-            int size = in.readVInt();\n-            cpus = new ObjectIntOpenHashMap<OsInfo.Cpu>(size);\n-            for (; size > 0; size--) {\n-                cpus.addTo(OsInfo.Cpu.readCpu(in), in.readVInt());\n-            }\n-        }\n-\n-        @Override\n-        public void writeTo(StreamOutput out) throws IOException {\n-            out.writeVInt(availableProcessors);\n-            out.writeLong(availableMemory);\n-            out.writeVInt(cpus.size());\n-            for (ObjectIntCursor<OsInfo.Cpu> c : cpus) {\n-                c.key.writeTo(out);\n-                out.writeVInt(c.value);\n-            }\n-\n-        }\n-\n-        public static OsStats readOsStats(StreamInput in) throws IOException {\n-            OsStats os = new OsStats();\n-            os.readFrom(in);\n-            return os;\n-        }\n-\n-        static final class Fields {\n-            static final XContentBuilderString AVAILABLE_PROCESSORS = new XContentBuilderString(\"available_processors\");\n-            static final XContentBuilderString MEM = new XContentBuilderString(\"mem\");\n-            static final XContentBuilderString TOTAL = new XContentBuilderString(\"total\");\n-            static final XContentBuilderString TOTAL_IN_BYTES = new XContentBuilderString(\"total_in_bytes\");\n-            static final XContentBuilderString CPU = new XContentBuilderString(\"cpu\");\n-            static final XContentBuilderString COUNT = new XContentBuilderString(\"count\");\n-        }\n-\n-        @Override\n-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-            builder.field(Fields.AVAILABLE_PROCESSORS, availableProcessors);\n-            builder.startObject(Fields.MEM);\n-            builder.byteSizeField(Fields.TOTAL_IN_BYTES, Fields.TOTAL, availableMemory);\n-            builder.endObject();\n-\n-            builder.startArray(Fields.CPU);\n-            for (ObjectIntCursor<OsInfo.Cpu> cpu : cpus) {\n-                builder.startObject();\n-                cpu.key.toXContent(builder, params);\n-                builder.field(Fields.COUNT, cpu.value);\n-                builder.endObject();\n-            }\n-            builder.endArray();\n-\n-            return builder;\n-        }\n-    }\n-\n-    public static class ProcessStats implements ToXContent, Streamable {\n-\n-        int count;\n-        int cpuPercent;\n-        long totalOpenFileDescriptors;\n-\n-        public void addNodeStats(NodeStats nodeStats) {\n-            if (nodeStats.getProcess() == null) {\n-                return;\n-            }\n-            count++;\n-            if (nodeStats.getProcess().cpu() != null) {\n-                // with no sigar, this may not be available\n-                cpuPercent += nodeStats.getProcess().cpu().getPercent();\n-            }\n-            totalOpenFileDescriptors += nodeStats.getProcess().openFileDescriptors();\n-        }\n-\n-        /**\n-         * Cpu usage in percentages - 100 is 1 core.\n-         */\n-        public int getCpuPercent() {\n-            return cpuPercent;\n-        }\n-\n-        public long getAvgOpenFileDescriptors() {\n-            if (count == 0) {\n-                return -1;\n-            }\n-            return totalOpenFileDescriptors / count;\n-        }\n-\n-        @Override\n-        public void readFrom(StreamInput in) throws IOException {\n-            count = in.readVInt();\n-            cpuPercent = in.readVInt();\n-            totalOpenFileDescriptors = in.readVLong();\n-        }\n-\n-        @Override\n-        public void writeTo(StreamOutput out) throws IOException {\n-            out.writeVInt(count);\n-            out.writeVInt(cpuPercent);\n-            out.writeVLong(totalOpenFileDescriptors);\n-        }\n-\n-        public static ProcessStats readStats(StreamInput in) throws IOException {\n-            ProcessStats cpu = new ProcessStats();\n-            cpu.readFrom(in);\n-            return cpu;\n-        }\n-\n-        static final class Fields {\n-            static final XContentBuilderString CPU = new XContentBuilderString(\"cpu\");\n-            static final XContentBuilderString PERCENT = new XContentBuilderString(\"percent\");\n-            static final XContentBuilderString AVG_OPEN_FD = new XContentBuilderString(\"avg_open_file_descriptors\");\n-        }\n-\n-        @Override\n-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-            builder.startObject(Fields.CPU).field(Fields.PERCENT, cpuPercent).endObject();\n-            builder.field(Fields.AVG_OPEN_FD, getAvgOpenFileDescriptors());\n-            return builder;\n-        }\n-    }\n-\n-    public static class JvmStats implements Streamable, ToXContent {\n-\n-        ObjectIntOpenHashMap<JvmVersion> versions;\n-        long threads;\n-        long maxUptime;\n-        long heapUsed;\n-        long heapMax;\n-\n-        JvmStats() {\n-            versions = new ObjectIntOpenHashMap<JvmVersion>();\n-            threads = 0;\n-            maxUptime = 0;\n-            heapMax = 0;\n-            heapUsed = 0;\n-        }\n-\n-        public ObjectIntOpenHashMap<JvmVersion> getVersions() {\n-            return versions;\n-        }\n-\n-        /**\n-         * The total number of threads in the cluster\n-         */\n-        public long getThreads() {\n-            return threads;\n-        }\n-\n-        /**\n-         * The maximum uptime of a node in the cluster\n-         */\n-        public TimeValue getMaxUpTime() {\n-            return new TimeValue(maxUptime);\n-        }\n-\n-        /**\n-         * Total heap used in the cluster\n-         */\n-        public ByteSizeValue getHeapUsed() {\n-            return new ByteSizeValue(heapUsed);\n-        }\n-\n-        /**\n-         * Maximum total heap available to the cluster\n-         */\n-        public ByteSizeValue getHeapMax() {\n-            return new ByteSizeValue(heapMax);\n-        }\n-\n-        public void addNodeInfoStats(NodeInfo nodeInfo, NodeStats nodeStats) {\n-            versions.addTo(new JvmVersion(nodeInfo.getJvm()), 1);\n-            org.elasticsearch.monitor.jvm.JvmStats js = nodeStats.getJvm();\n-            if (js == null) {\n-                return;\n-            }\n-            if (js.threads() != null) {\n-                threads += js.threads().count();\n-            }\n-            maxUptime = Math.max(maxUptime, js.uptime().millis());\n-            if (js.mem() != null) {\n-                heapUsed += js.mem().getHeapUsed().bytes();\n-                heapMax += js.mem().getHeapMax().bytes();\n-            }\n-        }\n-\n-        @Override\n-        public void readFrom(StreamInput in) throws IOException {\n-            int size = in.readVInt();\n-            versions = new ObjectIntOpenHashMap<JvmVersion>(size);\n-            for (; size > 0; size--) {\n-                versions.addTo(JvmVersion.readJvmVersion(in), in.readVInt());\n-            }\n-            threads = in.readVLong();\n-            maxUptime = in.readVLong();\n-            heapUsed = in.readVLong();\n-            heapMax = in.readVLong();\n-        }\n-\n-        @Override\n-        public void writeTo(StreamOutput out) throws IOException {\n-            out.writeVInt(versions.size());\n-            for (ObjectIntCursor<JvmVersion> v : versions) {\n-                v.key.writeTo(out);\n-                out.writeVInt(v.value);\n-            }\n-\n-            out.writeVLong(threads);\n-            out.writeVLong(maxUptime);\n-            out.writeVLong(heapUsed);\n-            out.writeVLong(heapMax);\n-        }\n-\n-        public static JvmStats readJvmStats(StreamInput in) throws IOException {\n-            JvmStats jvmStats = new JvmStats();\n-            jvmStats.readFrom(in);\n-            return jvmStats;\n-        }\n-\n-        static final class Fields {\n-            static final XContentBuilderString VERSIONS = new XContentBuilderString(\"versions\");\n-            static final XContentBuilderString VERSION = new XContentBuilderString(\"version\");\n-            static final XContentBuilderString VM_NAME = new XContentBuilderString(\"vm_name\");\n-            static final XContentBuilderString VM_VERSION = new XContentBuilderString(\"vm_version\");\n-            static final XContentBuilderString VM_VENDOR = new XContentBuilderString(\"vm_vendor\");\n-            static final XContentBuilderString COUNT = new XContentBuilderString(\"count\");\n-            static final XContentBuilderString THREADS = new XContentBuilderString(\"threads\");\n-            static final XContentBuilderString MAX_UPTIME = new XContentBuilderString(\"max_uptime\");\n-            static final XContentBuilderString MAX_UPTIME_IN_MILLIS = new XContentBuilderString(\"max_uptime_in_millis\");\n-            static final XContentBuilderString MEM = new XContentBuilderString(\"mem\");\n-            static final XContentBuilderString HEAP_USED = new XContentBuilderString(\"heap_used\");\n-            static final XContentBuilderString HEAP_USED_IN_BYTES = new XContentBuilderString(\"heap_used_in_bytes\");\n-            static final XContentBuilderString HEAP_MAX = new XContentBuilderString(\"heap_max\");\n-            static final XContentBuilderString HEAP_MAX_IN_BYTES = new XContentBuilderString(\"heap_max_in_bytes\");\n-        }\n-\n-        @Override\n-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-            builder.timeValueField(Fields.MAX_UPTIME_IN_MILLIS, Fields.MAX_UPTIME, maxUptime);\n-            builder.startArray(Fields.VERSIONS);\n-            for (ObjectIntCursor<JvmVersion> v : versions) {\n-                builder.startObject();\n-                builder.field(Fields.VERSION, v.key.version);\n-                builder.field(Fields.VM_NAME, v.key.vmName);\n-                builder.field(Fields.VM_VERSION, v.key.vmVersion);\n-                builder.field(Fields.VM_VENDOR, v.key.vmVendor);\n-                builder.field(Fields.COUNT, v.value);\n-                builder.endObject();\n-            }\n-            builder.endArray();\n-            builder.startObject(Fields.MEM);\n-            builder.byteSizeField(Fields.HEAP_USED_IN_BYTES, Fields.HEAP_USED, heapUsed);\n-            builder.byteSizeField(Fields.HEAP_MAX_IN_BYTES, Fields.HEAP_MAX, heapMax);\n-            builder.endObject();\n-\n-            builder.field(Fields.THREADS, threads);\n-            return builder;\n-        }\n-    }\n-\n-    public static class JvmVersion implements Streamable {\n-        String version;\n-        String vmName;\n-        String vmVersion;\n-        String vmVendor;\n-\n-        JvmVersion(JvmInfo jvmInfo) {\n-            version = jvmInfo.version();\n-            vmName = jvmInfo.vmName();\n-            vmVersion = jvmInfo.vmVersion();\n-            vmVendor = jvmInfo.vmVendor();\n-        }\n-\n-        JvmVersion() {\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) {\n-                return true;\n-            }\n-            if (o == null || getClass() != o.getClass()) {\n-                return false;\n-            }\n-\n-            JvmVersion jvm = (JvmVersion) o;\n-\n-            return vmVersion.equals(jvm.vmVersion) && vmVendor.equals(jvm.vmVendor);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return vmVersion.hashCode();\n-        }\n-\n-        public static JvmVersion readJvmVersion(StreamInput in) throws IOException {\n-            JvmVersion jvm = new JvmVersion();\n-            jvm.readFrom(in);\n-            return jvm;\n-        }\n-\n-        @Override\n-        public void readFrom(StreamInput in) throws IOException {\n-            version = in.readString();\n-            vmName = in.readString();\n-            vmVersion = in.readString();\n-            vmVendor = in.readString();\n-        }\n-\n-        @Override\n-        public void writeTo(StreamOutput out) throws IOException {\n-            out.writeString(version);\n-            out.writeString(vmName);\n-            out.writeString(vmVersion);\n-            out.writeString(vmVendor);\n-        }\n-    }\n-\n-\n-}\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java b/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java\ndeleted file mode 100644\nindex 8e7d8dc3247..00000000000\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java\n+++ /dev/null\n@@ -1,51 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.action.admin.cluster.stats;\n-\n-import org.elasticsearch.action.support.nodes.NodesOperationRequest;\n-import org.elasticsearch.common.io.stream.StreamInput;\n-import org.elasticsearch.common.io.stream.StreamOutput;\n-\n-import java.io.IOException;\n-\n-/**\n- * A request to get cluster level stats.\n- */\n-public class ClusterStatsRequest extends NodesOperationRequest<ClusterStatsRequest> {\n-\n-    /**\n-     * Get stats from nodes based on the nodes ids specified. If none are passed, stats\n-     * based on all nodes will be returned.\n-     */\n-    public ClusterStatsRequest(String... nodesIds) {\n-        super(nodesIds);\n-    }\n-\n-    @Override\n-    public void readFrom(StreamInput in) throws IOException {\n-        super.readFrom(in);\n-    }\n-\n-    @Override\n-    public void writeTo(StreamOutput out) throws IOException {\n-        super.writeTo(out);\n-    }\n-\n-}\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequestBuilder.java b/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequestBuilder.java\ndeleted file mode 100644\nindex a06f0d84869..00000000000\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequestBuilder.java\n+++ /dev/null\n@@ -1,40 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.action.admin.cluster.stats;\n-\n-import org.elasticsearch.action.ActionListener;\n-import org.elasticsearch.action.support.nodes.NodesOperationRequestBuilder;\n-import org.elasticsearch.client.ClusterAdminClient;\n-import org.elasticsearch.client.internal.InternalClusterAdminClient;\n-\n-/**\n- *\n- */\n-public class ClusterStatsRequestBuilder extends NodesOperationRequestBuilder<ClusterStatsRequest, ClusterStatsResponse, ClusterStatsRequestBuilder> {\n-\n-    public ClusterStatsRequestBuilder(ClusterAdminClient clusterClient) {\n-        super((InternalClusterAdminClient) clusterClient, new ClusterStatsRequest());\n-    }\n-\n-    @Override\n-    protected void doExecute(ActionListener<ClusterStatsResponse> listener) {\n-        ((ClusterAdminClient) client).clusterStats(request, listener);\n-    }\n-}\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java b/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java\ndeleted file mode 100644\nindex be74ffc48ec..00000000000\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java\n+++ /dev/null\n@@ -1,143 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.action.admin.cluster.stats;\n-\n-import org.elasticsearch.action.support.nodes.NodesOperationResponse;\n-import org.elasticsearch.cluster.ClusterName;\n-import org.elasticsearch.common.io.stream.StreamInput;\n-import org.elasticsearch.common.io.stream.StreamOutput;\n-import org.elasticsearch.common.xcontent.ToXContent;\n-import org.elasticsearch.common.xcontent.XContentBuilder;\n-import org.elasticsearch.common.xcontent.XContentBuilderString;\n-import org.elasticsearch.common.xcontent.XContentFactory;\n-\n-import java.io.IOException;\n-import java.util.Iterator;\n-import java.util.Map;\n-\n-/**\n- *\n- */\n-public class ClusterStatsResponse extends NodesOperationResponse<ClusterStatsNodeResponse> implements ToXContent {\n-\n-    ClusterStatsNodes nodesStats;\n-    ClusterStatsIndices indicesStats;\n-    String clusterUUID;\n-    long timestamp;\n-\n-\n-    ClusterStatsResponse() {\n-    }\n-\n-    public ClusterStatsResponse(long timestamp, ClusterName clusterName, String clusterUUID, ClusterStatsNodeResponse[] nodes) {\n-        super(clusterName, null);\n-        this.timestamp = timestamp;\n-        this.clusterUUID = clusterUUID;\n-        nodesStats = new ClusterStatsNodes(nodes);\n-        indicesStats = new ClusterStatsIndices(nodes);\n-    }\n-\n-    public long getTimestamp() {\n-        return this.timestamp;\n-    }\n-\n-    public ClusterStatsNodes getNodesStats() {\n-        return nodesStats;\n-    }\n-\n-    public ClusterStatsIndices getIndicesStats() {\n-        return indicesStats;\n-    }\n-\n-    @Override\n-    public ClusterStatsNodeResponse[] getNodes() {\n-        throw new UnsupportedOperationException();\n-    }\n-\n-    @Override\n-    public Map<String, ClusterStatsNodeResponse> getNodesMap() {\n-        throw new UnsupportedOperationException();\n-    }\n-\n-    @Override\n-    public ClusterStatsNodeResponse getAt(int position) {\n-        throw new UnsupportedOperationException();\n-    }\n-\n-    @Override\n-    public Iterator<ClusterStatsNodeResponse> iterator() {\n-        throw new UnsupportedOperationException();\n-    }\n-\n-    @Override\n-    public void readFrom(StreamInput in) throws IOException {\n-        super.readFrom(in);\n-        timestamp = in.readVLong();\n-        clusterUUID = in.readString();\n-        nodesStats = ClusterStatsNodes.readNodeStats(in);\n-        indicesStats = ClusterStatsIndices.readIndicesStats(in);\n-    }\n-\n-    @Override\n-    public void writeTo(StreamOutput out) throws IOException {\n-        super.writeTo(out);\n-        out.writeVLong(timestamp);\n-        out.writeString(clusterUUID);\n-        nodesStats.writeTo(out);\n-        indicesStats.writeTo(out);\n-    }\n-\n-    static final class Fields {\n-        static final XContentBuilderString NODES = new XContentBuilderString(\"nodes\");\n-        static final XContentBuilderString INDICES = new XContentBuilderString(\"indices\");\n-        static final XContentBuilderString UUID = new XContentBuilderString(\"uuid\");\n-        static final XContentBuilderString CLUSTER_NAME = new XContentBuilderString(\"cluster_name\");\n-    }\n-\n-    @Override\n-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-        builder.field(\"timestamp\", getTimestamp());\n-        builder.field(Fields.CLUSTER_NAME, getClusterName().value());\n-        if (params.paramAsBoolean(\"output_uuid\", false)) {\n-            builder.field(Fields.UUID, clusterUUID);\n-        }\n-\n-        builder.startObject(Fields.INDICES);\n-        indicesStats.toXContent(builder, params);\n-        builder.endObject();\n-        builder.startObject(Fields.NODES);\n-        nodesStats.toXContent(builder, params);\n-        builder.endObject();\n-        return builder;\n-    }\n-\n-    @Override\n-    public String toString() {\n-        try {\n-            XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();\n-            builder.startObject();\n-            toXContent(builder, EMPTY_PARAMS);\n-            builder.endObject();\n-            return builder.string();\n-        } catch (IOException e) {\n-            return \"{ \\\"error\\\" : \\\"\" + e.getMessage() + \"\\\"}\";\n-        }\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java b/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java\ndeleted file mode 100644\nindex 9f5f097e9e6..00000000000\n--- a/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java\n+++ /dev/null\n@@ -1,166 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.action.admin.cluster.stats;\n-\n-import org.elasticsearch.ElasticSearchException;\n-import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;\n-import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;\n-import org.elasticsearch.action.admin.indices.stats.CommonStatsFlags;\n-import org.elasticsearch.action.admin.indices.stats.ShardStats;\n-import org.elasticsearch.action.support.nodes.NodeOperationRequest;\n-import org.elasticsearch.action.support.nodes.TransportNodesOperationAction;\n-import org.elasticsearch.cluster.ClusterName;\n-import org.elasticsearch.cluster.ClusterService;\n-import org.elasticsearch.common.inject.Inject;\n-import org.elasticsearch.common.io.stream.StreamInput;\n-import org.elasticsearch.common.io.stream.StreamOutput;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.index.service.IndexService;\n-import org.elasticsearch.index.shard.service.IndexShard;\n-import org.elasticsearch.indices.IndicesService;\n-import org.elasticsearch.node.service.NodeService;\n-import org.elasticsearch.threadpool.ThreadPool;\n-import org.elasticsearch.transport.TransportService;\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.concurrent.atomic.AtomicReferenceArray;\n-\n-/**\n- *\n- */\n-public class TransportClusterStatsAction extends TransportNodesOperationAction<ClusterStatsRequest, ClusterStatsResponse,\n-        TransportClusterStatsAction.ClusterStatsNodeRequest, ClusterStatsNodeResponse> {\n-\n-    private static final CommonStatsFlags SHARD_STATS_FLAGS = new CommonStatsFlags(CommonStatsFlags.Flag.Docs, CommonStatsFlags.Flag.Store,\n-            CommonStatsFlags.Flag.FieldData, CommonStatsFlags.Flag.FilterCache, CommonStatsFlags.Flag.IdCache,\n-            CommonStatsFlags.Flag.Completion, CommonStatsFlags.Flag.Segments, CommonStatsFlags.Flag.Percolate);\n-\n-    private final NodeService nodeService;\n-    private final IndicesService indicesService;\n-\n-\n-    @Inject\n-    public TransportClusterStatsAction(Settings settings, ClusterName clusterName, ThreadPool threadPool,\n-                                       ClusterService clusterService, TransportService transportService,\n-                                       NodeService nodeService, IndicesService indicesService) {\n-        super(settings, clusterName, threadPool, clusterService, transportService);\n-        this.nodeService = nodeService;\n-        this.indicesService = indicesService;\n-    }\n-\n-    @Override\n-    protected String executor() {\n-        return ThreadPool.Names.MANAGEMENT;\n-    }\n-\n-    @Override\n-    protected String transportAction() {\n-        return ClusterStatsAction.NAME;\n-    }\n-\n-    @Override\n-    protected ClusterStatsResponse newResponse(ClusterStatsRequest clusterStatsRequest, AtomicReferenceArray responses) {\n-        final List<ClusterStatsNodeResponse> nodeStats = new ArrayList<ClusterStatsNodeResponse>(responses.length());\n-        for (int i = 0; i < responses.length(); i++) {\n-            Object resp = responses.get(i);\n-            if (resp instanceof ClusterStatsNodeResponse) {\n-                nodeStats.add((ClusterStatsNodeResponse) resp);\n-            }\n-        }\n-        return new ClusterStatsResponse(System.currentTimeMillis(), clusterName,\n-                clusterService.state().metaData().uuid(), nodeStats.toArray(new ClusterStatsNodeResponse[nodeStats.size()]));\n-    }\n-\n-    @Override\n-    protected ClusterStatsRequest newRequest() {\n-        return new ClusterStatsRequest();\n-    }\n-\n-    @Override\n-    protected ClusterStatsNodeRequest newNodeRequest() {\n-        return new ClusterStatsNodeRequest();\n-    }\n-\n-    @Override\n-    protected ClusterStatsNodeRequest newNodeRequest(String nodeId, ClusterStatsRequest request) {\n-        return new ClusterStatsNodeRequest(nodeId, request);\n-    }\n-\n-    @Override\n-    protected ClusterStatsNodeResponse newNodeResponse() {\n-        return new ClusterStatsNodeResponse();\n-    }\n-\n-    @Override\n-    protected ClusterStatsNodeResponse nodeOperation(ClusterStatsNodeRequest nodeRequest) throws ElasticSearchException {\n-        NodeInfo nodeInfo = nodeService.info(false, true, false, true, false, false, true, false, true);\n-        NodeStats nodeStats = nodeService.stats(CommonStatsFlags.NONE, false, true, true, false, false, true, false, false);\n-        List<ShardStats> shardsStats = new ArrayList<ShardStats>();\n-        for (String index : indicesService.indices()) {\n-            IndexService indexService = indicesService.indexService(index);\n-            if (indexService == null) {\n-                continue;\n-            }\n-            for (IndexShard indexShard : indexService) {\n-                if (indexShard.routingEntry().active()) {\n-                    // only report on fully started shards\n-                    shardsStats.add(new ShardStats(indexShard, SHARD_STATS_FLAGS));\n-                }\n-            }\n-\n-        }\n-\n-        return new ClusterStatsNodeResponse(nodeInfo.getNode(), nodeInfo, nodeStats, shardsStats.toArray(new ShardStats[shardsStats.size()]));\n-\n-    }\n-\n-    @Override\n-    protected boolean accumulateExceptions() {\n-        return false;\n-    }\n-\n-    static class ClusterStatsNodeRequest extends NodeOperationRequest {\n-\n-        ClusterStatsRequest request;\n-\n-        ClusterStatsNodeRequest() {\n-        }\n-\n-        ClusterStatsNodeRequest(String nodeId, ClusterStatsRequest request) {\n-            super(request, nodeId);\n-            this.request = request;\n-        }\n-\n-        @Override\n-        public void readFrom(StreamInput in) throws IOException {\n-            super.readFrom(in);\n-            request = new ClusterStatsRequest();\n-            request.readFrom(in);\n-        }\n-\n-        @Override\n-        public void writeTo(StreamOutput out) throws IOException {\n-            super.writeTo(out);\n-            request.writeTo(out);\n-        }\n-    }\n-}\ndiff --git a/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java b/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java\nindex b423a714c24..5ebd125eb49 100644\n--- a/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java\n+++ b/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java\n@@ -40,20 +40,6 @@ public class CommonStatsFlags implements Streamable, Cloneable {\n     private String[] fieldDataFields = null;\n     private String[] completionDataFields = null;\n \n-\n-    /**\n-     * @param flags flags to set. If no flags are supplied, default flags will be set.\n-     */\n-    public CommonStatsFlags(Flag... flags) {\n-        if (flags.length > 0) {\n-            clear();\n-            for (Flag f : flags) {\n-                this.flags.add(f);\n-            }\n-        }\n-    }\n-\n-\n     /**\n      * Sets all flags to return all stats.\n      */\n@@ -150,7 +136,6 @@ public class CommonStatsFlags implements Streamable, Cloneable {\n         flags.add(flag);\n     }\n \n-\n     public CommonStatsFlags set(Flag flag, boolean add) {\n         if (add) {\n             set(flag);\ndiff --git a/src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateResponse.java b/src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateResponse.java\nindex ae11260340e..ad9f757a0d6 100644\n--- a/src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateResponse.java\n+++ b/src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateResponse.java\n@@ -19,7 +19,7 @@\n \n package org.elasticsearch.action.admin.indices.template.delete;\n \n-import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.action.ActionResponse;\n import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n \n@@ -28,24 +28,30 @@ import java.io.IOException;\n /**\n  * A response for a delete index template.\n  */\n-public class DeleteIndexTemplateResponse extends AcknowledgedResponse {\n+public class DeleteIndexTemplateResponse extends ActionResponse {\n+\n+    private boolean acknowledged;\n \n     DeleteIndexTemplateResponse() {\n     }\n \n     DeleteIndexTemplateResponse(boolean acknowledged) {\n-        super(acknowledged);\n+        this.acknowledged = acknowledged;\n+    }\n+\n+    public boolean isAcknowledged() {\n+        return acknowledged;\n     }\n \n     @Override\n     public void readFrom(StreamInput in) throws IOException {\n         super.readFrom(in);\n-        readAcknowledged(in);\n+        acknowledged = in.readBoolean();\n     }\n \n     @Override\n     public void writeTo(StreamOutput out) throws IOException {\n         super.writeTo(out);\n-        writeAcknowledged(out);\n+        out.writeBoolean(acknowledged);\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateResponse.java b/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateResponse.java\nindex eb81beac396..0584c1bb9ce 100644\n--- a/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateResponse.java\n+++ b/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateResponse.java\n@@ -19,7 +19,7 @@\n \n package org.elasticsearch.action.admin.indices.template.put;\n \n-import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.action.ActionResponse;\n import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n \n@@ -28,24 +28,30 @@ import java.io.IOException;\n /**\n  * A response for a put index template action.\n  */\n-public class PutIndexTemplateResponse extends AcknowledgedResponse {\n+public class PutIndexTemplateResponse extends ActionResponse {\n+\n+    private boolean acknowledged;\n \n     PutIndexTemplateResponse() {\n     }\n \n     PutIndexTemplateResponse(boolean acknowledged) {\n-        super(acknowledged);\n+        this.acknowledged = acknowledged;\n+    }\n+\n+    public boolean isAcknowledged() {\n+        return acknowledged;\n     }\n \n     @Override\n     public void readFrom(StreamInput in) throws IOException {\n         super.readFrom(in);\n-        readAcknowledged(in);\n+        acknowledged = in.readBoolean();\n     }\n \n     @Override\n     public void writeTo(StreamOutput out) throws IOException {\n         super.writeTo(out);\n-        writeAcknowledged(out);\n+        out.writeBoolean(acknowledged);\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java b/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java\nindex 45cc8309b37..7c9b74e4aa1 100644\n--- a/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java\n+++ b/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java\n@@ -35,7 +35,10 @@ import org.elasticsearch.action.support.TransportAction;\n import org.elasticsearch.cluster.ClusterService;\n import org.elasticsearch.cluster.ClusterState;\n import org.elasticsearch.cluster.node.DiscoveryNode;\n-import org.elasticsearch.cluster.routing.*;\n+import org.elasticsearch.cluster.routing.MutableShardRouting;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.index.engine.DocumentMissingException;\n@@ -93,13 +96,13 @@ public class TransportMoreLikeThisAction extends TransportAction<MoreLikeThisReq\n         // update to the concrete index\n         final String concreteIndex = clusterState.metaData().concreteIndex(request.index());\n \n-        Iterable<MutableShardRouting> routingNode = clusterState.getRoutingNodes().routingNodeIter(clusterService.localNode().getId());\n+        RoutingNode routingNode = clusterState.getRoutingNodes().nodesToShards().get(clusterService.localNode().getId());\n         if (routingNode == null) {\n             redirect(request, concreteIndex, listener, clusterState);\n             return;\n         }\n         boolean hasIndexLocally = false;\n-        for (MutableShardRouting shardRouting : routingNode) {\n+        for (MutableShardRouting shardRouting : routingNode.shards()) {\n             if (concreteIndex.equals(shardRouting.index())) {\n                 hasIndexLocally = true;\n                 break;\ndiff --git a/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java\nindex 95d69fc02c2..2157f42422b 100644\n--- a/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java\n+++ b/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java\n@@ -239,11 +239,11 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se\n     }\n \n     /**\n-     * Sets a filter that will be executed after the query has been executed and only has affect on the search hits\n-     * (not aggregations or facets). This filter is always executed as last filtering mechanism.\n+     * Sets a filter on the query executed that only applies to the search query\n+     * (and not facets for example).\n      */\n-    public SearchRequestBuilder setPostFilter(FilterBuilder postFilter) {\n-        sourceBuilder().postFilter(postFilter);\n+    public SearchRequestBuilder setFilter(FilterBuilder filter) {\n+        sourceBuilder().filter(filter);\n         return this;\n     }\n \n@@ -251,8 +251,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchRequestBuilder setPostFilter(String postFilter) {\n-        sourceBuilder().postFilter(postFilter);\n+    public SearchRequestBuilder setFilter(String filter) {\n+        sourceBuilder().filter(filter);\n         return this;\n     }\n \n@@ -260,8 +260,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchRequestBuilder setPostFilter(BytesReference postFilter) {\n-        sourceBuilder().postFilter(postFilter);\n+    public SearchRequestBuilder setFilter(BytesReference filter) {\n+        sourceBuilder().filter(filter);\n         return this;\n     }\n \n@@ -269,8 +269,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchRequestBuilder setPostFilter(byte[] postFilter) {\n-        sourceBuilder().postFilter(postFilter);\n+    public SearchRequestBuilder setFilter(byte[] filter) {\n+        sourceBuilder().filter(filter);\n         return this;\n     }\n \n@@ -278,8 +278,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchRequestBuilder setPostFilter(byte[] postFilter, int postFilterOffset, int postFilterLength) {\n-        sourceBuilder().postFilter(postFilter, postFilterOffset, postFilterLength);\n+    public SearchRequestBuilder setFilter(byte[] filter, int filterOffset, int filterLength) {\n+        sourceBuilder().filter(filter, filterOffset, filterLength);\n         return this;\n     }\n \n@@ -287,8 +287,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchRequestBuilder setPostFilter(XContentBuilder postFilter) {\n-        sourceBuilder().postFilter(postFilter);\n+    public SearchRequestBuilder setFilter(XContentBuilder filter) {\n+        sourceBuilder().filter(filter);\n         return this;\n     }\n \n@@ -296,8 +296,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchRequestBuilder setPostFilter(Map postFilter) {\n-        sourceBuilder().postFilter(postFilter);\n+    public SearchRequestBuilder setFilter(Map filter) {\n+        sourceBuilder().filter(filter);\n         return this;\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java b/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java\nindex 7f5367fc37f..45a48789ecb 100644\n--- a/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java\n+++ b/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java\n@@ -214,15 +214,7 @@ public abstract class TransportNodesOperationAction<Request extends NodesOperati\n         }\n \n         private void finishHim() {\n-            Response finalResponse;\n-            try {\n-                finalResponse = newResponse(request, responses);\n-            } catch (Throwable t) {\n-                logger.debug(\"failed to combine responses from nodes\", t);\n-                listener.onFailure(t);\n-                return;\n-            }\n-            listener.onResponse(finalResponse);\n+            listener.onResponse(newResponse(request, responses));\n         }\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/client/ClusterAdminClient.java b/src/main/java/org/elasticsearch/client/ClusterAdminClient.java\nindex 6f3cd907b8e..a5588222a66 100644\n--- a/src/main/java/org/elasticsearch/client/ClusterAdminClient.java\n+++ b/src/main/java/org/elasticsearch/client/ClusterAdminClient.java\n@@ -72,9 +72,6 @@ import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotR\n import org.elasticsearch.action.admin.cluster.state.ClusterStateRequest;\n import org.elasticsearch.action.admin.cluster.state.ClusterStateRequestBuilder;\n import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsRequest;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsRequestBuilder;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;\n import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksRequest;\n import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksRequestBuilder;\n import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksResponse;\n@@ -191,30 +188,10 @@ public interface ClusterAdminClient {\n      */\n     NodesInfoRequestBuilder prepareNodesInfo(String... nodesIds);\n \n-    /**\n-     * Cluster wide aggregated stats.\n-     *\n-     * @param request The cluster stats request\n-     * @return The result future\n-     * @see org.elasticsearch.client.Requests#clusterStatsRequest\n-     */\n-    ActionFuture<ClusterStatsResponse> clusterStats(ClusterStatsRequest request);\n-\n-    /**\n-     * Cluster wide aggregated stats\n-     *\n-     * @param request  The cluster stats request\n-     * @param listener A listener to be notified with a result\n-     * @see org.elasticsearch.client.Requests#clusterStatsRequest()\n-     */\n-    void clusterStats(ClusterStatsRequest request, ActionListener<ClusterStatsResponse> listener);\n-\n-    ClusterStatsRequestBuilder prepareClusterStats();\n-\n     /**\n      * Nodes stats of the cluster.\n      *\n-     * @param request The nodes stats request\n+     * @param request The nodes info request\n      * @return The result future\n      * @see org.elasticsearch.client.Requests#nodesStatsRequest(String...)\n      */\ndiff --git a/src/main/java/org/elasticsearch/client/Requests.java b/src/main/java/org/elasticsearch/client/Requests.java\nindex 91b1b128e3a..e63c03e010b 100644\n--- a/src/main/java/org/elasticsearch/client/Requests.java\n+++ b/src/main/java/org/elasticsearch/client/Requests.java\n@@ -35,7 +35,6 @@ import org.elasticsearch.action.admin.cluster.snapshots.delete.DeleteSnapshotReq\n import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequest;\n import org.elasticsearch.action.admin.cluster.state.ClusterStateRequest;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsRequest;\n import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest;\n import org.elasticsearch.action.admin.indices.cache.clear.ClearIndicesCacheRequest;\n import org.elasticsearch.action.admin.indices.close.CloseIndexRequest;\n@@ -426,16 +425,6 @@ public class Requests {\n         return new NodesStatsRequest(nodesIds);\n     }\n \n-    /**\n-     * Creates a cluster stats request.\n-     *\n-     * @return The cluster stats request\n-     * @see org.elasticsearch.client.ClusterAdminClient#clusterStats(org.elasticsearch.action.admin.cluster.stats.ClusterStatsRequest)\n-     */\n-    public static ClusterStatsRequest clusterStatsRequest() {\n-        return new ClusterStatsRequest();\n-    }\n-\n     /**\n      * Shuts down all nodes in the cluster.\n      */\ndiff --git a/src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java b/src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java\nindex 177abf0e6b3..e4cbf9bfea8 100644\n--- a/src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java\n+++ b/src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java\n@@ -89,10 +89,6 @@ import org.elasticsearch.action.admin.cluster.state.ClusterStateAction;\n import org.elasticsearch.action.admin.cluster.state.ClusterStateRequest;\n import org.elasticsearch.action.admin.cluster.state.ClusterStateRequestBuilder;\n import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsAction;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsRequest;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsRequestBuilder;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;\n import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksAction;\n import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksRequest;\n import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksRequestBuilder;\n@@ -199,21 +195,6 @@ public abstract class AbstractClusterAdminClient implements InternalClusterAdmin\n         return new NodesStatsRequestBuilder(this).setNodesIds(nodesIds);\n     }\n \n-    @Override\n-    public ActionFuture<ClusterStatsResponse> clusterStats(ClusterStatsRequest request) {\n-        return execute(ClusterStatsAction.INSTANCE, request);\n-    }\n-\n-    @Override\n-    public void clusterStats(ClusterStatsRequest request, ActionListener<ClusterStatsResponse> listener) {\n-        execute(ClusterStatsAction.INSTANCE, request, listener);\n-    }\n-\n-    @Override\n-    public ClusterStatsRequestBuilder prepareClusterStats() {\n-        return new ClusterStatsRequestBuilder(this);\n-    }\n-\n     @Override\n     public ActionFuture<NodesHotThreadsResponse> nodesHotThreads(NodesHotThreadsRequest request) {\n         return execute(NodesHotThreadsAction.INSTANCE, request);\ndiff --git a/src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java b/src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java\nindex 26cb27c1d27..b14dd716828 100644\n--- a/src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java\n+++ b/src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java\n@@ -89,5 +89,4 @@ public class InternalTransportClusterAdminClient extends AbstractClusterAdminCli\n             }\n         }, listener);\n     }\n-\n }\ndiff --git a/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java b/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java\nindex 3ab16f53d2e..6a15044657a 100644\n--- a/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java\n+++ b/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java\n@@ -303,15 +303,7 @@ public class IndexTemplateMetaData {\n                     currentFieldName = parser.currentName();\n                 } else if (token == XContentParser.Token.START_OBJECT) {\n                     if (\"settings\".equals(currentFieldName)) {\n-                        ImmutableSettings.Builder templateSettingsBuilder = ImmutableSettings.settingsBuilder();\n-                        for (Map.Entry<String, String> entry : SettingsLoader.Helper.loadNestedFromMap(parser.mapOrdered()).entrySet()) {\n-                            if (!entry.getKey().startsWith(\"index.\")) {\n-                                templateSettingsBuilder.put(\"index.\" + entry.getKey(), entry.getValue());\n-                            } else {\n-                                templateSettingsBuilder.put(entry.getKey(), entry.getValue());\n-                            }\n-                        }\n-                        builder.settings(templateSettingsBuilder.build());\n+                        builder.settings(ImmutableSettings.settingsBuilder().put(SettingsLoader.Helper.loadNestedFromMap(parser.mapOrdered())));\n                     } else if (\"mappings\".equals(currentFieldName)) {\n                         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n                             if (token == XContentParser.Token.FIELD_NAME) {\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/ImmutableShardRouting.java b/src/main/java/org/elasticsearch/cluster/routing/ImmutableShardRouting.java\nindex 7553786fb9c..7a3c5d99db3 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/ImmutableShardRouting.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/ImmutableShardRouting.java\n@@ -63,9 +63,6 @@ public class ImmutableShardRouting implements Streamable, Serializable, ShardRou\n         this(copy.index(), copy.id(), copy.currentNodeId(), copy.primary(), copy.state(), copy.version());\n         this.relocatingNodeId = copy.relocatingNodeId();\n         this.restoreSource = copy.restoreSource();\n-        if (copy instanceof ImmutableShardRouting) {\n-            this.shardIdentifier = ((ImmutableShardRouting) copy).shardIdentifier;\n-        }\n     }\n \n     public ImmutableShardRouting(ShardRouting copy, long version) {\n@@ -73,9 +70,6 @@ public class ImmutableShardRouting implements Streamable, Serializable, ShardRou\n         this.relocatingNodeId = copy.relocatingNodeId();\n         this.restoreSource = copy.restoreSource();\n         this.version = version;\n-        if (copy instanceof ImmutableShardRouting) {\n-            this.shardIdentifier = ((ImmutableShardRouting) copy).shardIdentifier;\n-        }\n     }\n \n     public ImmutableShardRouting(String index, int shardId, String currentNodeId,\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java b/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java\nindex b778327d575..a9ec64c01eb 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java\n@@ -19,23 +19,19 @@\n \n package org.elasticsearch.cluster.routing;\n \n-import com.carrotsearch.hppc.cursors.IntCursor;\n-import com.carrotsearch.hppc.cursors.IntObjectCursor;\n import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Sets;\n import com.google.common.collect.UnmodifiableIterator;\n import org.elasticsearch.ElasticSearchIllegalStateException;\n import org.elasticsearch.cluster.metadata.IndexMetaData;\n import org.elasticsearch.cluster.metadata.MetaData;\n-import org.elasticsearch.common.collect.ImmutableOpenIntMap;\n import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n import org.elasticsearch.index.shard.ShardId;\n \n import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Set;\n+import java.util.*;\n import java.util.concurrent.atomic.AtomicInteger;\n \n import static com.google.common.collect.Lists.newArrayList;\n@@ -61,20 +57,20 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n \n     // note, we assume that when the index routing is created, ShardRoutings are created for all possible number of\n     // shards with state set to UNASSIGNED\n-    private final ImmutableOpenIntMap<IndexShardRoutingTable> shards;\n+    private final ImmutableMap<Integer, IndexShardRoutingTable> shards;\n \n     private final ImmutableList<ShardRouting> allShards;\n     private final ImmutableList<ShardRouting> allActiveShards;\n \n     private final AtomicInteger counter = new AtomicInteger();\n \n-    IndexRoutingTable(String index, ImmutableOpenIntMap<IndexShardRoutingTable> shards) {\n+    IndexRoutingTable(String index, Map<Integer, IndexShardRoutingTable> shards) {\n         this.index = index;\n-        this.shards = shards;\n+        this.shards = ImmutableMap.copyOf(shards);\n         ImmutableList.Builder<ShardRouting> allShards = ImmutableList.builder();\n         ImmutableList.Builder<ShardRouting> allActiveShards = ImmutableList.builder();\n-        for (IntObjectCursor<IndexShardRoutingTable> cursor : shards) {\n-            for (ShardRouting shardRouting : cursor.value) {\n+        for (IndexShardRoutingTable indexShardRoutingTable : shards.values()) {\n+            for (ShardRouting shardRouting : indexShardRoutingTable) {\n                 allShards.add(shardRouting);\n                 if (shardRouting.active()) {\n                     allActiveShards.add(shardRouting);\n@@ -111,8 +107,8 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n      */\n     public IndexRoutingTable normalizeVersions() {\n         IndexRoutingTable.Builder builder = new Builder(this.index);\n-        for (IntObjectCursor<IndexShardRoutingTable> cursor : shards) {\n-            builder.addIndexShard(cursor.value.normalizeVersions());\n+        for (IndexShardRoutingTable shardTable : shards.values()) {\n+            builder.addIndexShard(shardTable.normalizeVersions());\n         }\n         return builder.build();\n     }\n@@ -123,18 +119,6 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n             return;\n         }\n         IndexMetaData indexMetaData = metaData.index(index());\n-        for (String failure : validate(indexMetaData)) {\n-            validation.addIndexFailure(index, failure);\n-        }\n-\n-    }\n-\n-    /**\n-     * validate based on a meta data, returning failures found\n-     */\n-    public List<String> validate(IndexMetaData indexMetaData) {\n-        ArrayList<String> failures = new ArrayList<String>();\n-\n         // check the number of shards\n         if (indexMetaData.numberOfShards() != shards().size()) {\n             Set<Integer> expected = Sets.newHashSet();\n@@ -144,27 +128,26 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n             for (IndexShardRoutingTable indexShardRoutingTable : this) {\n                 expected.remove(indexShardRoutingTable.shardId().id());\n             }\n-            failures.add(\"Wrong number of shards in routing table, missing: \" + expected);\n+            validation.addIndexFailure(index(), \"Wrong number of shards in routing table, missing: \" + expected);\n         }\n         // check the replicas\n         for (IndexShardRoutingTable indexShardRoutingTable : this) {\n             int routingNumberOfReplicas = indexShardRoutingTable.size() - 1;\n             if (routingNumberOfReplicas != indexMetaData.numberOfReplicas()) {\n-                failures.add(\"Shard [\" + indexShardRoutingTable.shardId().id()\n+                validation.addIndexFailure(index(), \"Shard [\" + indexShardRoutingTable.shardId().id()\n                         + \"] routing table has wrong number of replicas, expected [\" + indexMetaData.numberOfReplicas() + \"], got [\" + routingNumberOfReplicas + \"]\");\n             }\n             for (ShardRouting shardRouting : indexShardRoutingTable) {\n                 if (!shardRouting.index().equals(index())) {\n-                    failures.add(\"shard routing has an index [\" + shardRouting.index() + \"] that is different than the routing table\");\n+                    validation.addIndexFailure(index(), \"shard routing has an index [\" + shardRouting.index() + \"] that is different than the routing table\");\n                 }\n             }\n         }\n-        return failures;\n     }\n \n     @Override\n     public UnmodifiableIterator<IndexShardRoutingTable> iterator() {\n-        return shards.valuesIt();\n+        return shards.values().iterator();\n     }\n \n     /**\n@@ -199,11 +182,11 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n         return nodes.size();\n     }\n \n-    public ImmutableOpenIntMap<IndexShardRoutingTable> shards() {\n+    public ImmutableMap<Integer, IndexShardRoutingTable> shards() {\n         return shards;\n     }\n \n-    public ImmutableOpenIntMap<IndexShardRoutingTable> getShards() {\n+    public ImmutableMap<Integer, IndexShardRoutingTable> getShards() {\n         return shards();\n     }\n \n@@ -324,7 +307,7 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n     public static class Builder {\n \n         private final String index;\n-        private final ImmutableOpenIntMap.Builder<IndexShardRoutingTable> shards = ImmutableOpenIntMap.builder();\n+        private final Map<Integer, IndexShardRoutingTable> shards = new HashMap<Integer, IndexShardRoutingTable>();\n \n         public Builder(String index) {\n             this.index = index;\n@@ -427,8 +410,7 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n         }\n \n         public Builder addReplica() {\n-            for (IntCursor cursor : shards.keys()) {\n-                int shardId = cursor.value;\n+            for (int shardId : shards.keySet()) {\n                 // version 0, will get updated when reroute will happen\n                 ImmutableShardRouting shard = new ImmutableShardRouting(index, shardId, null, false, ShardRoutingState.UNASSIGNED, 0);\n                 shards.put(shardId,\n@@ -439,8 +421,7 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n         }\n \n         public Builder removeReplica() {\n-            for (IntCursor cursor : shards.keys()) {\n-                int shardId = cursor.value;\n+            for (int shardId : shards.keySet()) {\n                 IndexShardRoutingTable indexShard = shards.get(shardId);\n                 if (indexShard.replicaShards().isEmpty()) {\n                     // nothing to do here!\n@@ -505,7 +486,7 @@ public class IndexRoutingTable implements Iterable<IndexShardRoutingTable> {\n         }\n \n         public IndexRoutingTable build() throws RoutingValidationException {\n-            IndexRoutingTable indexRoutingTable = new IndexRoutingTable(index, shards.build());\n+            IndexRoutingTable indexRoutingTable = new IndexRoutingTable(index, ImmutableMap.copyOf(shards));\n             indexRoutingTable.validate();\n             return indexRoutingTable;\n         }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/MutableShardRouting.java b/src/main/java/org/elasticsearch/cluster/routing/MutableShardRouting.java\nindex a80672b67bb..0ba8f78b6ee 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/MutableShardRouting.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/MutableShardRouting.java\n@@ -55,9 +55,8 @@ public class MutableShardRouting extends ImmutableShardRouting {\n      *\n      * @param nodeId id of the node to assign this shard to\n      */\n-    void assignToNode(String nodeId) {\n+    public void assignToNode(String nodeId) {\n         version++;\n-\n         if (currentNodeId == null) {\n             assert state == ShardRoutingState.UNASSIGNED;\n \n@@ -77,7 +76,7 @@ public class MutableShardRouting extends ImmutableShardRouting {\n      *\n      * @param relocatingNodeId id of the node to relocate the shard\n      */\n-    void relocate(String relocatingNodeId) {\n+    public void relocate(String relocatingNodeId) {\n         version++;\n         assert state == ShardRoutingState.STARTED;\n         state = ShardRoutingState.RELOCATING;\n@@ -88,7 +87,7 @@ public class MutableShardRouting extends ImmutableShardRouting {\n      * Cancel relocation of a shard. The shards state must be set\n      * to <code>RELOCATING</code>.\n      */\n-    void cancelRelocation() {\n+    public void cancelRelocation() {\n         version++;\n         assert state == ShardRoutingState.RELOCATING;\n         assert assignedToNode();\n@@ -102,7 +101,7 @@ public class MutableShardRouting extends ImmutableShardRouting {\n      * Set the shards state to <code>UNASSIGNED</code>.\n      * //TODO document the state\n      */\n-    void deassignNode() {\n+    public void deassignNode() {\n         version++;\n         assert state != ShardRoutingState.UNASSIGNED;\n \n@@ -116,7 +115,7 @@ public class MutableShardRouting extends ImmutableShardRouting {\n      * <code>INITIALIZING</code> or <code>RELOCATING</code>. Any relocation will be\n      * canceled.\n      */\n-    void moveToStarted() {\n+    public void moveToStarted() {\n         version++;\n         assert state == ShardRoutingState.INITIALIZING || state == ShardRoutingState.RELOCATING;\n         relocatingNodeId = null;\n@@ -128,7 +127,7 @@ public class MutableShardRouting extends ImmutableShardRouting {\n      * Make the shard primary unless it's not Primary\n      * //TODO: doc exception\n      */\n-    void moveToPrimary() {\n+    public void moveToPrimary() {\n         version++;\n         if (primary) {\n             throw new IllegalShardRoutingStateException(this, \"Already primary, can't move to primary\");\n@@ -139,7 +138,7 @@ public class MutableShardRouting extends ImmutableShardRouting {\n     /**\n      * Set the primary shard to non-primary\n      */\n-    void moveFromPrimary() {\n+    public void moveFromPrimary() {\n         version++;\n         if (!primary) {\n             throw new IllegalShardRoutingStateException(this, \"Not primary, can't move to replica\");\n@@ -147,14 +146,12 @@ public class MutableShardRouting extends ImmutableShardRouting {\n         primary = false;\n     }\n \n-    private long hashVersion = version-1;\n-    private int hashCode = 0;\n-\n-    @Override\n-    public int hashCode() {\n-        hashCode = (hashVersion != version ? super.hashCode() : hashCode);\n-        hashVersion = version;\n-        return hashCode;\n+    public void restoreFrom(RestoreSource restoreSource) {\n+        version++;\n+        if (!primary) {\n+            throw new IllegalShardRoutingStateException(this, \"Not primary, can't restore from snapshot to replica\");\n+        }\n+        this.restoreSource = restoreSource;\n     }\n }\n \ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java b/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java\nindex 5979fb24a9d..674b657c663 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java\n@@ -19,12 +19,10 @@\n \n package org.elasticsearch.cluster.routing;\n \n-import com.google.common.collect.Iterators;\n import org.elasticsearch.ElasticSearchIllegalStateException;\n import org.elasticsearch.cluster.node.DiscoveryNode;\n \n import java.util.ArrayList;\n-import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n \n@@ -54,10 +52,6 @@ public class RoutingNode implements Iterable<MutableShardRouting> {\n \n     @Override\n     public Iterator<MutableShardRouting> iterator() {\n-        return Iterators.unmodifiableIterator(shards.iterator());\n-    }\n-\n-    Iterator<MutableShardRouting> mutableIterator() {\n         return shards.iterator();\n     }\n \n@@ -78,22 +72,39 @@ public class RoutingNode implements Iterable<MutableShardRouting> {\n         return this.nodeId;\n     }\n \n-    public int size() {\n-        return shards.size();\n+    /**\n+     * Get a list of shards hosted on this node  \n+     * @return list of shards\n+     */\n+    public List<MutableShardRouting> shards() {\n+        return this.shards;\n     }\n \n     /**\n      * Add a new shard to this node\n      * @param shard Shard to crate on this Node\n      */\n-    void add(MutableShardRouting shard) {\n-        // TODO use Set with ShardIds for faster lookup.\n+    public void add(MutableShardRouting shard) {\n         for (MutableShardRouting shardRouting : shards) {\n             if (shardRouting.shardId().equals(shard.shardId())) {\n                 throw new ElasticSearchIllegalStateException(\"Trying to add a shard [\" + shard.shardId().index().name() + \"][\" + shard.shardId().id() + \"] to a node [\" + nodeId + \"] where it already exists\");\n             }\n         }\n         shards.add(shard);\n+        shard.assignToNode(node.id());\n+    }\n+\n+    /**\n+     * Remove a shard from this node\n+     * @param shardId id of the shard to remove\n+     */\n+    public void removeByShardId(int shardId) {\n+        for (Iterator<MutableShardRouting> it = shards.iterator(); it.hasNext(); ) {\n+            MutableShardRouting shard = it.next();\n+            if (shard.id() == shardId) {\n+                it.remove();\n+            }\n+        }\n     }\n \n     /**\n@@ -152,6 +163,21 @@ public class RoutingNode implements Iterable<MutableShardRouting> {\n         return shards;\n     }\n \n+    /**\n+     * Get the number of shard that not match the given states\n+     * @param state set states to exclude\n+     * @return number of shards which state is listed\n+     */\n+    public int numberOfShardsNotWithState(ShardRoutingState state) {\n+        int count = 0;\n+        for (MutableShardRouting shardEntry : this) {\n+            if (shardEntry.state() != state) {\n+                count++;\n+            }\n+        }\n+        return count;\n+    }\n+\n     /**\n      * The number of shards on this node that will not be eventually relocated.\n      */\n@@ -174,16 +200,4 @@ public class RoutingNode implements Iterable<MutableShardRouting> {\n         }\n         return sb.toString();\n     }\n-\n-    public MutableShardRouting get(int i) {\n-        return shards.get(i) ;\n-    }\n-\n-    public Collection<MutableShardRouting> copyShards() {\n-        return new ArrayList<MutableShardRouting>(shards);\n-    }\n-\n-    public boolean isEmpty() {\n-        return shards.isEmpty();\n-    }\n }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java\nindex 837a9efa6ce..c2d4e0cb78b 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java\n@@ -23,21 +23,18 @@ import com.carrotsearch.hppc.ObjectIntOpenHashMap;\n import com.carrotsearch.hppc.cursors.ObjectCursor;\n import com.google.common.base.Predicate;\n import com.google.common.collect.ImmutableSet;\n-import com.google.common.collect.Iterators;\n import com.google.common.collect.Sets;\n import org.elasticsearch.cluster.ClusterState;\n import org.elasticsearch.cluster.block.ClusterBlocks;\n import org.elasticsearch.cluster.metadata.IndexMetaData;\n import org.elasticsearch.cluster.metadata.MetaData;\n import org.elasticsearch.cluster.node.DiscoveryNode;\n-import org.elasticsearch.common.collect.IdentityHashSet;\n import org.elasticsearch.index.shard.ShardId;\n \n import java.util.*;\n \n import static com.google.common.collect.Lists.newArrayList;\n import static com.google.common.collect.Maps.newHashMap;\n-import static com.google.common.collect.Sets.newHashSet;\n \n /**\n  * {@link RoutingNodes} represents a copy the routing information contained in\n@@ -53,17 +50,9 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n \n     private final Map<String, RoutingNode> nodesToShards = newHashMap();\n \n-    private final UnassignedShards unassignedShards = new UnassignedShards();\n+    private final List<MutableShardRouting> unassigned = newArrayList();\n \n-    private final List<MutableShardRouting> ignoredUnassignedShards = newArrayList();\n-\n-    private final Map<ShardId, Set<MutableShardRouting>> assignedShards = newHashMap();\n-\n-    private int inactivePrimaryCount = 0;\n-\n-    private int inactiveShardCount = 0;\n-\n-    private int relocatingShards = 0;\n+    private final List<MutableShardRouting> ignoredUnassigned = newArrayList();\n \n     private Set<ShardId> clearPostAllocationFlag;\n \n@@ -73,7 +62,6 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n         this.metaData = clusterState.metaData();\n         this.blocks = clusterState.blocks();\n         this.routingTable = clusterState.routingTable();\n-\n         Map<String, List<MutableShardRouting>> nodesToShards = newHashMap();\n         // fill in the nodeToShards with the \"live\" nodes\n         for (ObjectCursor<DiscoveryNode> cursor : clusterState.nodes().dataNodes().values()) {\n@@ -81,46 +69,29 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n         }\n \n         // fill in the inverse of node -> shards allocated\n-        // also fill replicaSet information\n         for (IndexRoutingTable indexRoutingTable : routingTable.indicesRouting().values()) {\n             for (IndexShardRoutingTable indexShard : indexRoutingTable) {\n                 for (ShardRouting shard : indexShard) {\n-                    // to get all the shards belonging to an index, including the replicas,\n-                    // we define a replica set and keep track of it. A replica set is identified\n-                    // by the ShardId, as this is common for primary and replicas.\n-                    // A replica Set might have one (and not more) replicas with the state of RELOCATING.\n                     if (shard.assignedToNode()) {\n                         List<MutableShardRouting> entries = nodesToShards.get(shard.currentNodeId());\n                         if (entries == null) {\n                             entries = newArrayList();\n                             nodesToShards.put(shard.currentNodeId(), entries);\n                         }\n-                        MutableShardRouting sr = new MutableShardRouting(shard);\n-                        entries.add(sr);\n-                        activeShardsAdd(sr);\n+                        entries.add(new MutableShardRouting(shard));\n                         if (shard.relocating()) {\n                             entries = nodesToShards.get(shard.relocatingNodeId());\n-                            relocatingShards++;\n                             if (entries == null) {\n                                 entries = newArrayList();\n                                 nodesToShards.put(shard.relocatingNodeId(), entries);\n                             }\n                             // add the counterpart shard with relocatingNodeId reflecting the source from which\n                             // it's relocating from.\n-                            sr = new MutableShardRouting(shard.index(), shard.id(), shard.relocatingNodeId(),\n-                                    shard.currentNodeId(), shard.primary(), ShardRoutingState.INITIALIZING, shard.version());\n-                            entries.add(sr);\n-                            activeShardsAdd(sr);\n-                        } else if (!shard.active()) { // shards that are initializing without being relocated\n-                            if (shard.primary()) {\n-                                inactivePrimaryCount++;\n-                            }\n-                            inactiveShardCount++;\n+                            entries.add(new MutableShardRouting(shard.index(), shard.id(), shard.relocatingNodeId(),\n+                                    shard.currentNodeId(), shard.primary(), ShardRoutingState.INITIALIZING, shard.version()));\n                         }\n                     } else {\n-                        MutableShardRouting sr = new MutableShardRouting(shard);\n-                        activeShardsAdd(sr);\n-                        unassignedShards.add(sr);\n+                        unassigned.add(new MutableShardRouting(shard));\n                     }\n                 }\n             }\n@@ -133,7 +104,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n \n     @Override\n     public Iterator<RoutingNode> iterator() {\n-        return Iterators.unmodifiableIterator(nodesToShards.values().iterator());\n+        return nodesToShards.values().iterator();\n     }\n \n     public RoutingTable routingTable() {\n@@ -173,19 +144,27 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n     }\n \n     public boolean hasUnassigned() {\n-        return !unassignedShards.isEmpty();\n+        return !unassigned.isEmpty();\n     }\n \n     public List<MutableShardRouting> ignoredUnassigned() {\n-        return this.ignoredUnassignedShards;\n+        return this.ignoredUnassigned;\n     }\n \n-    public UnassignedShards unassigned() {\n-        return this.unassignedShards;\n+    public List<MutableShardRouting> unassigned() {\n+        return this.unassigned;\n     }\n \n-    public RoutingNodesIterator nodes() {\n-        return new RoutingNodesIterator(nodesToShards.values().iterator());\n+    public List<MutableShardRouting> getUnassigned() {\n+        return unassigned();\n+    }\n+\n+    public Map<String, RoutingNode> nodesToShards() {\n+        return nodesToShards;\n+    }\n+\n+    public Map<String, RoutingNode> getNodesToShards() {\n+        return nodesToShards();\n     }\n \n     /**\n@@ -225,63 +204,58 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n         return nodesPerAttributesCounts;\n     }\n \n-    public boolean hasUnassignedPrimaries() {\n-        return unassignedShards.numPrimaries() > 0;\n-    }\n-\n-    public boolean hasUnassignedShards() {\n-        return !unassignedShards.isEmpty();\n-    }\n-\n-    public boolean hasInactivePrimaries() {\n-        return inactivePrimaryCount > 0;\n-    }\n-\n-    public boolean hasInactiveShards() {\n-        return inactiveShardCount > 0;\n-    }\n-\n-    public int getRelocatingShardCount() {\n-        return relocatingShards;\n-    }\n-\n-    /**\n-     * Returns the active primary shard for the given ShardRouting or <code>null</code> if\n-     * no primary is found or the primary is not active.\n-     */\n-    public MutableShardRouting activePrimary(ShardRouting shard) {\n+    public MutableShardRouting findPrimaryForReplica(ShardRouting shard) {\n         assert !shard.primary();\n-        for (MutableShardRouting shardRouting : activeShards(shard.shardId())) {\n-            if (shardRouting.primary()) {\n-                if (shardRouting.active()) {\n+        for (RoutingNode routingNode : nodesToShards.values()) {\n+            List<MutableShardRouting> shards = routingNode.shards();\n+            for (int i = 0; i < shards.size(); i++) {\n+                MutableShardRouting shardRouting = shards.get(i);\n+                if (shardRouting.shardId().equals(shard.shardId()) && shardRouting.primary()) {\n                     return shardRouting;\n                 }\n-                break;\n             }\n         }\n         return null;\n     }\n \n-    /**\n-     * Returns <code>true</code> iff all replicas are active for the given shard routing. Otherwise <code>false</code>\n-     */\n-    public boolean allReplicasActive(ShardRouting shardRouting) {\n-        final Set<MutableShardRouting> shards = activeShards(shardRouting.shardId());\n-        if (shards.isEmpty() || shards.size() < this.routingTable.index(shardRouting.index()).shard(shardRouting.id()).size()) {\n-            return false; // if we are empty nothing is active if we have less than total at least one is unassigned\n+    public List<MutableShardRouting> shardsRoutingFor(ShardRouting shardRouting) {\n+        return shardsRoutingFor(shardRouting.index(), shardRouting.id());\n+    }\n+\n+    public List<MutableShardRouting> shardsRoutingFor(String index, int shardId) {\n+        List<MutableShardRouting> shards = newArrayList();\n+        for (RoutingNode routingNode : this) {\n+            List<MutableShardRouting> nShards = routingNode.shards();\n+            for (int i = 0; i < nShards.size(); i++) {\n+                MutableShardRouting shardRouting = nShards.get(i);\n+                if (shardRouting.index().equals(index) && shardRouting.id() == shardId) {\n+                    shards.add(shardRouting);\n+                }\n+            }\n         }\n-        for (MutableShardRouting shard : shards) {\n-            if (!shard.active()) {\n-                return false;\n+        for (int i = 0; i < unassigned.size(); i++) {\n+            MutableShardRouting shardRouting = unassigned.get(i);\n+            if (shardRouting.index().equals(index) && shardRouting.id() == shardId) {\n+                shards.add(shardRouting);\n             }\n         }\n-        return true;\n+        return shards;\n+    }\n+\n+    public int numberOfShardsOfType(ShardRoutingState state) {\n+        int count = 0;\n+        for (RoutingNode routingNode : this) {\n+            count += routingNode.numberOfShardsWithState(state);\n+        }\n+        return count;\n     }\n \n     public List<MutableShardRouting> shards(Predicate<MutableShardRouting> predicate) {\n         List<MutableShardRouting> shards = newArrayList();\n         for (RoutingNode routingNode : this) {\n-            for (MutableShardRouting shardRouting : routingNode) {\n+            List<MutableShardRouting> nodeShards = routingNode.shards();\n+            for (int i = 0; i < nodeShards.size(); i++) {\n+                MutableShardRouting shardRouting = nodeShards.get(i);\n                 if (predicate.apply(shardRouting)) {\n                     shards.add(shardRouting);\n                 }\n@@ -291,7 +265,6 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n     }\n \n     public List<MutableShardRouting> shardsWithState(ShardRoutingState... state) {\n-        // TODO these are used on tests only - move into utils class\n         List<MutableShardRouting> shards = newArrayList();\n         for (RoutingNode routingNode : this) {\n             shards.addAll(routingNode.shardsWithState(state));\n@@ -300,7 +273,6 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n     }\n \n     public List<MutableShardRouting> shardsWithState(String index, ShardRoutingState... state) {\n-        // TODO these are used on tests only - move into utils class\n         List<MutableShardRouting> shards = newArrayList();\n         for (RoutingNode routingNode : this) {\n             shards.addAll(routingNode.shardsWithState(index, state));\n@@ -314,435 +286,9 @@ public class RoutingNodes implements Iterable<RoutingNode> {\n             sb.append(routingNode.prettyPrint());\n         }\n         sb.append(\"---- unassigned\\n\");\n-        for (MutableShardRouting shardEntry : unassignedShards) {\n+        for (MutableShardRouting shardEntry : unassigned) {\n             sb.append(\"--------\").append(shardEntry.shortSummary()).append('\\n');\n         }\n         return sb.toString();\n     }\n-\n-    /**\n-     * Assign a shard to a node. This will increment the inactiveShardCount counter\n-     * and the inactivePrimaryCount counter if the shard is the primary.\n-     * In case the shard is already assigned and started, it will be marked as \n-     * relocating, which is accounted for, too, so the number of concurrent relocations\n-     * can be retrieved easily.\n-     * This method can be called several times for the same shard, only the first time\n-     * will change the state.\n-     *\n-     * INITIALIZING => INITIALIZING\n-     * UNASSIGNED   => INITIALIZING\n-     * STARTED      => RELOCATING\n-     * RELOCATING   => RELOCATING\n-     *\n-     * @param shard the shard to be assigned\n-     * @param nodeId the nodeId this shard should initialize on or relocate from\n-     */\n-    public void assign(MutableShardRouting shard, String nodeId) {\n-        // state will not change if the shard is already initializing.\n-        ShardRoutingState oldState = shard.state();\n-        shard.assignToNode(nodeId);\n-        node(nodeId).add(shard);\n-        if (oldState == ShardRoutingState.UNASSIGNED) {\n-            inactiveShardCount++;\n-            if (shard.primary()) {\n-                inactivePrimaryCount++;\n-            }\n-        }\n-\n-        if (shard.state() == ShardRoutingState.RELOCATING) {\n-            relocatingShards++;\n-        }\n-        activeShardsAdd(shard);\n-    }\n-\n-    /**\n-     * Relocate a shard to another node.\n-     */\n-    public void relocate(MutableShardRouting shard, String nodeId) {\n-        relocatingShards++;\n-        shard.relocate(nodeId);\n-    }\n-\n-    /**\n-     * Mark a shard as started and adjusts internal statistics.\n-     */\n-    public void started(MutableShardRouting shard) {\n-        if (!shard.active() && shard.relocatingNodeId() == null) {\n-            inactiveShardCount--;\n-            if (shard.primary()) {\n-                inactivePrimaryCount--;\n-            }\n-        } else if (shard.relocating()) {\n-            relocatingShards--;\n-        }\n-        assert !shard.started();\n-        shard.moveToStarted();\n-    }\n-\n-    /**\n-     * Cancels a relocation of a shard that shard must relocating.\n-     */\n-    public void cancelRelocation(MutableShardRouting shard) {\n-        relocatingShards--;\n-        shard.cancelRelocation();\n-    }\n-\n-    /**\n-     * swaps the status of a shard, making replicas primary and vice versa.\n-     *\n-     * @param shards the shard to have its primary status swapped.\n-     */\n-    public void swapPrimaryFlag(MutableShardRouting... shards) {\n-        for (MutableShardRouting shard : shards) {\n-            if (shard.primary()) {\n-                shard.moveFromPrimary();\n-                if (shard.unassigned()) {\n-                    unassignedShards.primaries--;\n-                }\n-            } else {\n-                shard.moveToPrimary();\n-                if (shard.unassigned()) {\n-                    unassignedShards.primaries++;\n-                }\n-            }\n-        }\n-    }\n-\n-    private static final Set<MutableShardRouting> EMPTY = Collections.emptySet();\n-\n-    private Set<MutableShardRouting> activeShards(ShardId shardId) {\n-        final Set<MutableShardRouting> replicaSet = assignedShards.get(shardId);\n-        return replicaSet == null ? EMPTY : Collections.unmodifiableSet(replicaSet);\n-    }\n-\n-    /**\n-     * Cancels the give shard from the Routing nodes internal statistics and cancels\n-     * the relocation if the shard is relocating.\n-     * @param shard\n-     */\n-    private void remove(MutableShardRouting shard) {\n-        if (!shard.active() && shard.relocatingNodeId() == null) {\n-            inactiveShardCount--;\n-            assert inactiveShardCount >= 0;\n-            if (shard.primary()) {\n-                inactivePrimaryCount--;\n-            }\n-        } else if (shard.relocating()) {\n-            cancelRelocation(shard);\n-        }\n-        activeShardsRemove(shard);\n-    }\n-\n-    private void activeShardsAdd(MutableShardRouting shard) {\n-        if (shard.unassigned()) {\n-            // no unassigned\n-            return;\n-        }\n-        Set<MutableShardRouting> replicaSet = assignedShards.get(shard.shardId());\n-        if (replicaSet == null) {\n-            replicaSet = new IdentityHashSet<MutableShardRouting>();\n-            assignedShards.put(shard.shardId(), replicaSet);\n-        }\n-        replicaSet.add(shard);\n-    }\n-\n-    private void activeShardsRemove(MutableShardRouting shard) {\n-        Set<MutableShardRouting> replicaSet = assignedShards.get(shard.shardId());\n-        if (replicaSet != null) {\n-            if (replicaSet.contains(shard)) {\n-                replicaSet.remove(shard);\n-            } else {\n-                assert false : \"Illegal state\";\n-                Iterator<MutableShardRouting> iterator = replicaSet.iterator();\n-                while(iterator.hasNext()) {\n-                    if (shard.equals(iterator.next())) {\n-                        iterator.remove();\n-                    }\n-                }\n-            }\n-        }\n-    }\n-\n-    public boolean isKnown(DiscoveryNode node) {\n-        return nodesToShards.containsKey(node.getId());\n-    }\n-\n-    public void addNode(DiscoveryNode node) {\n-        RoutingNode routingNode = new RoutingNode(node.id(), node);\n-        nodesToShards.put(routingNode.nodeId(), routingNode);\n-    }\n-\n-    public RoutingNodeIterator routingNodeIter(String nodeId) {\n-        final RoutingNode routingNode = nodesToShards.get(nodeId);\n-        if (routingNode == null) {\n-            return null;\n-        }\n-        assert assertShardStats(this);\n-        return new RoutingNodeIterator(routingNode);\n-    }\n-\n-    public RoutingNode[] toArray() {\n-        return nodesToShards.values().toArray(new RoutingNode[nodesToShards.size()]);\n-    }\n-\n-    public final static class UnassignedShards implements Iterable<MutableShardRouting>  {\n-\n-        private final List<MutableShardRouting> unassigned;\n-        private int primaries = 0;\n-        private long transactionId = 0;\n-        private final UnassignedShards source;\n-        private final long sourceTransactionId;\n-\n-        public UnassignedShards(UnassignedShards other) {\n-            source = other;\n-            sourceTransactionId = other.transactionId;\n-            unassigned = new ArrayList<MutableShardRouting>(other.unassigned);\n-            primaries = other.primaries;\n-        }\n-\n-        public UnassignedShards() {\n-            unassigned = new ArrayList<MutableShardRouting>();\n-            source = null;\n-            sourceTransactionId = -1;\n-        }\n-\n-        public void add(MutableShardRouting mutableShardRouting) {\n-            if(mutableShardRouting.primary()) {\n-                primaries++;\n-            }\n-            unassigned.add(mutableShardRouting);\n-            transactionId++;\n-        }\n-\n-        public void addAll(Collection<MutableShardRouting> mutableShardRoutings) {\n-            for (MutableShardRouting r : mutableShardRoutings) {\n-                add(r);\n-            }\n-        }\n-\n-        public int size() {\n-            return unassigned.size();\n-        }\n-\n-        public int numPrimaries() {\n-            return primaries;\n-        }\n-\n-        @Override\n-        public Iterator<MutableShardRouting> iterator() {\n-            final Iterator<MutableShardRouting> iterator = unassigned.iterator();\n-            return new Iterator<MutableShardRouting>() {\n-                private  MutableShardRouting current;\n-                @Override\n-                public boolean hasNext() {\n-                    return iterator.hasNext();\n-                }\n-\n-                @Override\n-                public MutableShardRouting next() {\n-                    return current = iterator.next();\n-                }\n-\n-                @Override\n-                public void remove() {\n-                    iterator.remove();\n-                    if (current.primary()) {\n-                        primaries--;\n-                    }\n-                    transactionId++;\n-                }\n-            };\n-        }\n-\n-        public boolean isEmpty() {\n-            return unassigned.isEmpty();\n-        }\n-\n-        public void shuffle() {\n-            Collections.shuffle(unassigned);\n-        }\n-\n-        public void clear() {\n-            transactionId++;\n-            unassigned.clear();\n-            primaries = 0;\n-        }\n-\n-        public void transactionEnd(UnassignedShards shards) {\n-           assert shards.source == this && shards.sourceTransactionId == transactionId :\n-                   \"Expected ID: \" + shards.sourceTransactionId + \" actual: \" + transactionId + \" Expected Source: \" + shards.source + \" actual: \" + this;\n-           transactionId++;\n-           this.unassigned.clear();\n-           this.unassigned.addAll(shards.unassigned);\n-           this.primaries = shards.primaries;\n-        }\n-\n-        public UnassignedShards transactionBegin() {\n-            return new UnassignedShards(this);\n-        }\n-\n-        public void copyAll(Collection<MutableShardRouting> others) {\n-            others.addAll(unassigned);\n-        }\n-\n-        public MutableShardRouting[] drain() {\n-            MutableShardRouting[] mutableShardRoutings = unassigned.toArray(new MutableShardRouting[unassigned.size()]);\n-            unassigned.clear();\n-            primaries = 0;\n-            transactionId++;\n-            return mutableShardRoutings;\n-        }\n-    }\n-\n-\n-    /**\n-     * Calculates RoutingNodes statistics by iterating over all {@link MutableShardRouting}s\n-     * in the cluster to ensure the book-keeping is correct.\n-     * For performance reasons, this should only be called from asserts\n-     *\n-     * @return this method always returns <code>true</code> or throws an assertion error. If assertion are not enabled\n-     *         this method does nothing.\n-     */\n-    public static boolean assertShardStats(RoutingNodes routingNodes) {\n-        boolean run = false;\n-        assert (run = true); // only run if assertions are enabled!\n-        if (!run) {\n-            return true;\n-        }\n-        int unassignedPrimaryCount = 0;\n-        int inactivePrimaryCount = 0;\n-        int inactiveShardCount = 0;\n-        int relocating = 0;\n-        final Set<ShardId> seenShards = newHashSet();\n-        Map<String, Integer> indicesAndShards = new HashMap<String, Integer>();\n-        for (RoutingNode node : routingNodes) {\n-            for (MutableShardRouting shard : node) {\n-                if (!shard.active() && shard.relocatingNodeId() == null) {\n-                    if (!shard.relocating()) {\n-                        inactiveShardCount++;\n-                        if (shard.primary()) {\n-                            inactivePrimaryCount++;\n-                        }\n-                    }\n-                }\n-                if (shard.relocating()) {\n-                    relocating++;\n-                }\n-                seenShards.add(shard.shardId());\n-                Integer i = indicesAndShards.get(shard.index());\n-                if (i == null) {\n-                    i = shard.id();\n-                }\n-                indicesAndShards.put(shard.index(), Math.max(i, shard.id()));\n-            }\n-        }\n-        // Assert that the active shard routing are identical.\n-        Set<Map.Entry<String, Integer>> entries = indicesAndShards.entrySet();\n-        Set<MutableShardRouting> shards = newHashSet();\n-        for (Map.Entry<String, Integer> e : entries) {\n-            String index = e.getKey();\n-            for (int i = 0; i < e.getValue(); i++) {\n-                for (RoutingNode routingNode : routingNodes) {\n-                    for (MutableShardRouting shardRouting : routingNode) {\n-                        if (shardRouting.index().equals(index) && shardRouting.id() == i) {\n-                            shards.add(shardRouting);\n-                        }\n-                    }\n-                }\n-                Set<MutableShardRouting> mutableShardRoutings = routingNodes.activeShards(new ShardId(index, i));\n-                for (MutableShardRouting r : mutableShardRoutings) {\n-                    assert shards.contains(r);\n-                    shards.remove(r);\n-                }\n-                assert shards.isEmpty();\n-            }\n-        }\n-\n-        for (MutableShardRouting shard : routingNodes.unassigned()) {\n-            if (shard.primary()) {\n-                unassignedPrimaryCount++;\n-            }\n-            seenShards.add(shard.shardId());\n-        }\n-\n-        assert unassignedPrimaryCount == routingNodes.unassignedShards.numPrimaries() :\n-                \"Unassigned primaries is [\" + unassignedPrimaryCount + \"] but RoutingNodes returned unassigned primaries [\" + routingNodes.unassigned().numPrimaries() + \"]\";\n-        assert inactivePrimaryCount == routingNodes.inactivePrimaryCount :\n-                \"Inactive Primary count [\" + inactivePrimaryCount + \"] but RoutingNodes returned inactive primaries [\" + routingNodes.inactivePrimaryCount + \"]\";\n-        assert inactiveShardCount == routingNodes.inactiveShardCount :\n-                \"Inactive Shard count [\" + inactiveShardCount + \"] but RoutingNodes returned inactive shards [\" + routingNodes.inactiveShardCount + \"]\";\n-        assert routingNodes.getRelocatingShardCount() == relocating : \"Relocating shards mismatch [\" + routingNodes.getRelocatingShardCount() + \"] but expected [\" + relocating + \"]\";\n-        return true;\n-    }\n-\n-\n-    public class RoutingNodesIterator implements Iterator<RoutingNode>, Iterable<MutableShardRouting> {\n-        private RoutingNode current;\n-        private final Iterator<RoutingNode> delegate;\n-\n-        public RoutingNodesIterator(Iterator<RoutingNode> iterator) {\n-            delegate = iterator;\n-        }\n-\n-        @Override\n-        public boolean hasNext() {\n-            return delegate.hasNext();\n-        }\n-\n-        @Override\n-        public RoutingNode next() {\n-            return current = delegate.next();\n-        }\n-\n-        public RoutingNodeIterator nodeShards() {\n-            return new RoutingNodeIterator(current);\n-        }\n-\n-        @Override\n-        public void remove() {\n-           delegate.remove();\n-        }\n-\n-        @Override\n-        public Iterator<MutableShardRouting> iterator() {\n-            return nodeShards();\n-        }\n-    }\n-\n-    public final class RoutingNodeIterator implements Iterator<MutableShardRouting>, Iterable<MutableShardRouting> {\n-        private final RoutingNode iterable;\n-        private MutableShardRouting shard;\n-        private final Iterator<MutableShardRouting> delegate;\n-\n-        public RoutingNodeIterator(RoutingNode iterable) {\n-            this.delegate = iterable.mutableIterator();\n-            this.iterable = iterable;\n-        }\n-\n-        @Override\n-        public boolean hasNext() {\n-            return delegate.hasNext();\n-        }\n-\n-        @Override\n-        public MutableShardRouting next() {\n-            return shard = delegate.next();\n-        }\n-\n-        public void remove() {\n-            delegate.remove();\n-            RoutingNodes.this.remove(shard);\n-        }\n-\n-        @Override\n-        public Iterator<MutableShardRouting> iterator() {\n-            return iterable.iterator();\n-        }\n-\n-        public void moveToUnassigned() {\n-            iterator().remove();\n-            unassigned().add(new MutableShardRouting(shard.index(), shard.id(),\n-                    null, shard.primary(), ShardRoutingState.UNASSIGNED, shard.version() + 1));\n-        }\n-    }\n }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java\nindex 5a765b297ec..edaee306f80 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java\n@@ -34,9 +34,12 @@ import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;\n import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n import org.elasticsearch.common.component.AbstractComponent;\n import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.settings.ImmutableSettings;\n import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.node.settings.NodeSettingsService;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n \n@@ -55,6 +58,16 @@ public class AllocationService extends AbstractComponent {\n     private final ClusterInfoService clusterInfoService;\n     private final ShardsAllocators shardsAllocators;\n \n+    public AllocationService() {\n+        this(ImmutableSettings.Builder.EMPTY_SETTINGS);\n+    }\n+\n+    public AllocationService(Settings settings) {\n+        this(settings,\n+                new AllocationDeciders(settings, new NodeSettingsService(ImmutableSettings.Builder.EMPTY_SETTINGS)),\n+                new ShardsAllocators(settings), ClusterInfoService.EMPTY);\n+    }\n+\n     @Inject\n     public AllocationService(Settings settings, AllocationDeciders allocationDeciders, ShardsAllocators shardsAllocators, ClusterInfoService clusterInfoService) {\n         super(settings);\n@@ -75,7 +88,7 @@ public class AllocationService extends AbstractComponent {\n     public RoutingAllocation.Result applyStartedShards(ClusterState clusterState, List<? extends ShardRouting> startedShards, boolean withReroute) {\n         RoutingNodes routingNodes = clusterState.routingNodes();\n         // shuffle the unassigned nodes, just so we won't have things like poison failed shards\n-        routingNodes.unassigned().shuffle();\n+        Collections.shuffle(routingNodes.unassigned());\n         StartedRerouteAllocation allocation = new StartedRerouteAllocation(allocationDeciders, routingNodes, clusterState.nodes(), startedShards, clusterInfoService.getClusterInfo());\n         boolean changed = applyStartedShards(routingNodes, startedShards);\n         if (!changed) {\n@@ -100,7 +113,7 @@ public class AllocationService extends AbstractComponent {\n     public RoutingAllocation.Result applyFailedShards(ClusterState clusterState, List<ShardRouting> failedShards) {\n         RoutingNodes routingNodes = clusterState.routingNodes();\n         // shuffle the unassigned nodes, just so we won't have things like poison failed shards\n-        routingNodes.unassigned().shuffle();\n+        Collections.shuffle(routingNodes.unassigned());\n         FailedRerouteAllocation allocation = new FailedRerouteAllocation(allocationDeciders, routingNodes, clusterState.nodes(), failedShards, clusterInfoService.getClusterInfo());\n         boolean changed = false;\n         for (ShardRouting failedShard : failedShards) {\n@@ -139,7 +152,7 @@ public class AllocationService extends AbstractComponent {\n     public RoutingAllocation.Result reroute(ClusterState clusterState) {\n         RoutingNodes routingNodes = clusterState.routingNodes();\n         // shuffle the unassigned nodes, just so we won't have things like poison failed shards\n-        routingNodes.unassigned().shuffle();\n+        Collections.shuffle(routingNodes.unassigned());\n         RoutingAllocation allocation = new RoutingAllocation(allocationDeciders, routingNodes, clusterState.nodes(), clusterInfoService.getClusterInfo());\n         if (!reroute(allocation)) {\n             return new RoutingAllocation.Result(false, clusterState.routingTable(), allocation.explanation());\n@@ -155,7 +168,7 @@ public class AllocationService extends AbstractComponent {\n     public RoutingAllocation.Result rerouteWithNoReassign(ClusterState clusterState) {\n         RoutingNodes routingNodes = clusterState.routingNodes();\n         // shuffle the unassigned nodes, just so we won't have things like poison failed shards\n-        routingNodes.unassigned().shuffle();\n+        Collections.shuffle(routingNodes.unassigned());\n         RoutingAllocation allocation = new RoutingAllocation(allocationDeciders, routingNodes, clusterState.nodes(), clusterInfoService.getClusterInfo());\n         boolean changed = false;\n         // first, clear from the shards any node id they used to belong to that is now dead\n@@ -198,7 +211,7 @@ public class AllocationService extends AbstractComponent {\n \n         // rebalance\n         changed |= shardsAllocators.rebalance(allocation);\n-        assert RoutingNodes.assertShardStats(allocation.routingNodes());\n+\n         return changed;\n     }\n \n@@ -209,15 +222,14 @@ public class AllocationService extends AbstractComponent {\n         List<MutableShardRouting> shards = new ArrayList<MutableShardRouting>();\n         int index = 0;\n         boolean found = true;\n-        final RoutingNodes routingNodes = allocation.routingNodes();\n         while (found) {\n             found = false;\n-            for (RoutingNode routingNode : routingNodes) {\n-                if (index >= routingNode.size()) {\n+            for (RoutingNode routingNode : allocation.routingNodes()) {\n+                if (index >= routingNode.shards().size()) {\n                     continue;\n                 }\n                 found = true;\n-                shards.add(routingNode.get(index));\n+                shards.add(routingNode.shards().get(index));\n             }\n             index++;\n         }\n@@ -227,7 +239,7 @@ public class AllocationService extends AbstractComponent {\n             if (!shardRouting.started()) {\n                 continue;\n             }\n-            final RoutingNode routingNode = routingNodes.node(shardRouting.currentNodeId());\n+            RoutingNode routingNode = allocation.routingNodes().node(shardRouting.currentNodeId());\n             Decision decision = allocation.deciders().canRemain(shardRouting, routingNode, allocation);\n             if (decision.type() == Decision.Type.NO) {\n                 logger.debug(\"[{}][{}] allocated on [{}], but can no longer be allocated on it, moving...\", shardRouting.index(), shardRouting.id(), routingNode.node());\n@@ -235,7 +247,6 @@ public class AllocationService extends AbstractComponent {\n                 if (!moved) {\n                     logger.debug(\"[{}][{}] can't move\", shardRouting.index(), shardRouting.id());\n                 } else {\n-                    assert RoutingNodes.assertShardStats(allocation.routingNodes());\n                     changed = true;\n                 }\n             }\n@@ -250,15 +261,16 @@ public class AllocationService extends AbstractComponent {\n             if (shardEntry.primary() && !shardEntry.assignedToNode()) {\n                 boolean elected = false;\n                 // primary and not assigned, go over and find a replica that is assigned and active (since it might be relocating)\n-                for (RoutingNode routingNode : routingNodes) {\n+                for (RoutingNode routingNode : routingNodes.nodesToShards().values()) {\n \n-                    for (MutableShardRouting shardEntry2 : routingNode) {\n+                    for (MutableShardRouting shardEntry2 : routingNode.shards()) {\n                         if (shardEntry.shardId().equals(shardEntry2.shardId()) && shardEntry2.active()) {\n                             assert shardEntry2.assignedToNode();\n                             assert !shardEntry2.primary();\n \n                             changed = true;\n-                            routingNodes.swapPrimaryFlag(shardEntry, shardEntry2);\n+                            shardEntry.moveFromPrimary();\n+                            shardEntry2.moveToPrimary();\n \n                             if (shardEntry2.relocatingNodeId() != null) {\n                                 // its also relocating, make sure to move the other routing to primary\n@@ -266,7 +278,7 @@ public class AllocationService extends AbstractComponent {\n                                 if (node != null) {\n                                     for (MutableShardRouting shardRouting : node) {\n                                         if (shardRouting.shardId().equals(shardEntry2.shardId()) && !shardRouting.primary()) {\n-                                            routingNodes.swapPrimaryFlag(shardRouting);\n+                                            shardRouting.moveToPrimary();\n                                             break;\n                                         }\n                                     }\n@@ -289,8 +301,8 @@ public class AllocationService extends AbstractComponent {\n         List<ShardRouting> shardsToFail = null;\n         for (MutableShardRouting shardEntry : routingNodes.unassigned()) {\n             if (shardEntry.primary() && !shardEntry.assignedToNode()) {\n-                for (RoutingNode routingNode : routingNodes) {\n-                    for (MutableShardRouting shardEntry2 : routingNode) {\n+                for (RoutingNode routingNode : routingNodes.nodesToShards().values()) {\n+                    for (MutableShardRouting shardEntry2 : routingNode.shards()) {\n                         if (shardEntry.shardId().equals(shardEntry2.shardId()) && !shardEntry2.active()) {\n                             changed = true;\n                             if (shardsToFail == null) {\n@@ -315,18 +327,18 @@ public class AllocationService extends AbstractComponent {\n      * new nodes);\n      */\n     private void applyNewNodes(RoutingAllocation allocation) {\n-        final RoutingNodes routingNodes = allocation.routingNodes();\n         for (ObjectCursor<DiscoveryNode> cursor : allocation.nodes().dataNodes().values()) {\n             DiscoveryNode node = cursor.value;\n-            if (!routingNodes.isKnown(node)) {\n-                routingNodes.addNode(node);\n+            if (!allocation.routingNodes().nodesToShards().containsKey(node.id())) {\n+                RoutingNode routingNode = new RoutingNode(node.id(), node);\n+                allocation.routingNodes().nodesToShards().put(node.id(), routingNode);\n             }\n         }\n     }\n \n     private boolean deassociateDeadNodes(RoutingAllocation allocation) {\n         boolean changed = false;\n-        for (RoutingNodes.RoutingNodesIterator it = allocation.routingNodes().nodes(); it.hasNext(); ) {\n+        for (Iterator<RoutingNode> it = allocation.routingNodes().nodesToShards().values().iterator(); it.hasNext(); ) {\n             RoutingNode node = it.next();\n             if (allocation.nodes().dataNodes().containsKey(node.nodeId())) {\n                 // its a live node, continue\n@@ -334,7 +346,7 @@ public class AllocationService extends AbstractComponent {\n             }\n             changed = true;\n             // now, go over all the shards routing on the node, and fail them\n-            for (MutableShardRouting shardRouting : node.copyShards()) {\n+            for (MutableShardRouting shardRouting : new ArrayList<MutableShardRouting>(node.shards())) {\n                 applyFailedShard(allocation, shardRouting, false);\n             }\n             // its a dead node, remove it, note, its important to remove it *after* we apply failed shard\n@@ -350,17 +362,17 @@ public class AllocationService extends AbstractComponent {\n         for (ShardRouting startedShard : startedShardEntries) {\n             assert startedShard.state() == INITIALIZING;\n \n-            // retrieve the relocating node id before calling startedShard().\n+            // retrieve the relocating node id before calling moveToStarted().\n             String relocatingNodeId = null;\n \n-            RoutingNodes.RoutingNodeIterator currentRoutingNode = routingNodes.routingNodeIter(startedShard.currentNodeId());\n+            RoutingNode currentRoutingNode = routingNodes.nodesToShards().get(startedShard.currentNodeId());\n             if (currentRoutingNode != null) {\n                 for (MutableShardRouting shard : currentRoutingNode) {\n                     if (shard.shardId().equals(startedShard.shardId())) {\n                         relocatingNodeId = shard.relocatingNodeId();\n                         if (!shard.started()) {\n                             dirty = true;\n-                            routingNodes.started(shard);\n+                            shard.moveToStarted();\n                         }\n                         break;\n                     }\n@@ -375,14 +387,15 @@ public class AllocationService extends AbstractComponent {\n                 continue;\n             }\n \n-            RoutingNodes.RoutingNodeIterator sourceRoutingNode = routingNodes.routingNodeIter(relocatingNodeId);\n+            RoutingNode sourceRoutingNode = routingNodes.nodesToShards().get(relocatingNodeId);\n             if (sourceRoutingNode != null) {\n-                while (sourceRoutingNode.hasNext()) {\n-                    MutableShardRouting shard = sourceRoutingNode.next();\n+                Iterator<MutableShardRouting> shardsIter = sourceRoutingNode.iterator();\n+                while (shardsIter.hasNext()) {\n+                    MutableShardRouting shard = shardsIter.next();\n                     if (shard.shardId().equals(startedShard.shardId())) {\n                         if (shard.relocating()) {\n                             dirty = true;\n-                            sourceRoutingNode.remove();\n+                            shardsIter.remove();\n                             break;\n                         }\n                     }\n@@ -406,20 +419,21 @@ public class AllocationService extends AbstractComponent {\n             return false;\n         }\n \n-        RoutingNodes routingNodes = allocation.routingNodes();\n         if (failedShard.relocatingNodeId() != null) {\n             // the shard is relocating, either in initializing (recovery from another node) or relocating (moving to another node)\n             if (failedShard.state() == INITIALIZING) {\n                 // the shard is initializing and recovering from another node\n                 boolean dirty = false;\n                 // first, we need to cancel the current node that is being initialized\n-                RoutingNodes.RoutingNodeIterator initializingNode = routingNodes.routingNodeIter(failedShard.currentNodeId());\n+                RoutingNode initializingNode = allocation.routingNodes().node(failedShard.currentNodeId());\n                 if (initializingNode != null) {\n-                    while(initializingNode.hasNext()) {\n-                        MutableShardRouting shardRouting = initializingNode.next();\n+                    for (Iterator<MutableShardRouting> it = initializingNode.iterator(); it.hasNext(); ) {\n+                        MutableShardRouting shardRouting = it.next();\n                         if (shardRouting.equals(failedShard)) {\n                             dirty = true;\n-                            initializingNode.remove();\n+                            it.remove();\n+                            shardRouting.deassignNode();\n+\n                             if (addToIgnoreList) {\n                                 // make sure we ignore this shard on the relevant node\n                                 allocation.addIgnoreShardForNode(failedShard.shardId(), failedShard.currentNodeId());\n@@ -431,12 +445,13 @@ public class AllocationService extends AbstractComponent {\n                 }\n                 if (dirty) {\n                     // now, find the node that we are relocating *from*, and cancel its relocation\n-                    RoutingNode relocatingFromNode = routingNodes.node(failedShard.relocatingNodeId());\n+                    RoutingNode relocatingFromNode = allocation.routingNodes().node(failedShard.relocatingNodeId());\n                     if (relocatingFromNode != null) {\n-                        for (MutableShardRouting shardRouting : relocatingFromNode) {\n-                            if (shardRouting.shardId().equals(failedShard.shardId()) && shardRouting.relocating()) {\n+                        for (Iterator<MutableShardRouting> it = relocatingFromNode.iterator(); it.hasNext(); ) {\n+                            MutableShardRouting shardRouting = it.next();\n+                            if (shardRouting.shardId().equals(failedShard.shardId()) && shardRouting.state() == RELOCATING) {\n                                 dirty = true;\n-                                routingNodes.cancelRelocation(shardRouting);\n+                                shardRouting.cancelRelocation();\n                                 break;\n                             }\n                         }\n@@ -449,19 +464,20 @@ public class AllocationService extends AbstractComponent {\n                 // first, we need to cancel the current relocation from the current node\n                 // now, find the node that we are recovering from, cancel the relocation, remove it from the node\n                 // and add it to the unassigned shards list...\n-                RoutingNodes.RoutingNodeIterator relocatingFromNode = routingNodes.routingNodeIter(failedShard.currentNodeId());\n+                RoutingNode relocatingFromNode = allocation.routingNodes().node(failedShard.currentNodeId());\n                 if (relocatingFromNode != null) {\n-                    while(relocatingFromNode.hasNext()) {\n-                        MutableShardRouting shardRouting = relocatingFromNode.next();\n+                    for (Iterator<MutableShardRouting> it = relocatingFromNode.iterator(); it.hasNext(); ) {\n+                        MutableShardRouting shardRouting = it.next();\n                         if (shardRouting.equals(failedShard)) {\n                             dirty = true;\n-                            relocatingFromNode.remove();\n+                            shardRouting.cancelRelocation();\n+                            it.remove();\n                             if (addToIgnoreList) {\n                                 // make sure we ignore this shard on the relevant node\n                                 allocation.addIgnoreShardForNode(failedShard.shardId(), failedShard.currentNodeId());\n                             }\n \n-                            routingNodes.unassigned().add(new MutableShardRouting(failedShard.index(), failedShard.id(),\n+                            allocation.routingNodes().unassigned().add(new MutableShardRouting(failedShard.index(), failedShard.id(),\n                                     null, failedShard.primary(), ShardRoutingState.UNASSIGNED, failedShard.version() + 1));\n                             break;\n                         }\n@@ -469,13 +485,14 @@ public class AllocationService extends AbstractComponent {\n                 }\n                 if (dirty) {\n                     // next, we need to find the target initializing shard that is recovering from, and remove it...\n-                    RoutingNodes.RoutingNodeIterator initializingNode = routingNodes.routingNodeIter(failedShard.relocatingNodeId());\n+                    RoutingNode initializingNode = allocation.routingNodes().node(failedShard.relocatingNodeId());\n                     if (initializingNode != null) {\n-                        while (initializingNode.hasNext()) {\n-                            MutableShardRouting shardRouting = initializingNode.next();\n+                        for (Iterator<MutableShardRouting> it = initializingNode.iterator(); it.hasNext(); ) {\n+                            MutableShardRouting shardRouting = it.next();\n                             if (shardRouting.shardId().equals(failedShard.shardId()) && shardRouting.state() == INITIALIZING) {\n                                 dirty = true;\n-                                initializingNode.remove();\n+                                shardRouting.deassignNode();\n+                                it.remove();\n                             }\n                         }\n                     }\n@@ -487,23 +504,25 @@ public class AllocationService extends AbstractComponent {\n         } else {\n             // the shard is not relocating, its either started, or initializing, just cancel it and move on...\n             boolean dirty = false;\n-            RoutingNodes.RoutingNodeIterator node = routingNodes.routingNodeIter(failedShard.currentNodeId());\n+            RoutingNode node = allocation.routingNodes().node(failedShard.currentNodeId());\n             if (node != null) {\n-                while(node.hasNext()) {\n-                    MutableShardRouting shardRouting = node.next();\n+                for (Iterator<MutableShardRouting> it = node.iterator(); it.hasNext(); ) {\n+                    MutableShardRouting shardRouting = it.next();\n                     if (shardRouting.equals(failedShard)) {\n                         dirty = true;\n                         if (addToIgnoreList) {\n                             // make sure we ignore this shard on the relevant node\n                             allocation.addIgnoreShardForNode(failedShard.shardId(), failedShard.currentNodeId());\n                         }\n-                        node.remove();\n+\n+                        it.remove();\n+\n                         // move all the shards matching the failed shard to the end of the unassigned list\n                         // so we give a chance for other allocations and won't create poison failed allocations\n                         // that can keep other shards from being allocated (because of limits applied on how many\n                         // shards we can start per node)\n                         List<MutableShardRouting> shardsToMove = Lists.newArrayList();\n-                        for (Iterator<MutableShardRouting> unassignedIt = routingNodes.unassigned().iterator(); unassignedIt.hasNext(); ) {\n+                        for (Iterator<MutableShardRouting> unassignedIt = allocation.routingNodes().unassigned().iterator(); unassignedIt.hasNext(); ) {\n                             MutableShardRouting unassignedShardRouting = unassignedIt.next();\n                             if (unassignedShardRouting.shardId().equals(failedShard.shardId())) {\n                                 unassignedIt.remove();\n@@ -511,10 +530,10 @@ public class AllocationService extends AbstractComponent {\n                             }\n                         }\n                         if (!shardsToMove.isEmpty()) {\n-                            routingNodes.unassigned().addAll(shardsToMove);\n+                            allocation.routingNodes().unassigned().addAll(shardsToMove);\n                         }\n \n-                        routingNodes.unassigned().add(new MutableShardRouting(failedShard.index(), failedShard.id(), null,\n+                        allocation.routingNodes().unassigned().add(new MutableShardRouting(failedShard.index(), failedShard.id(), null,\n                                 null, failedShard.restoreSource(), failedShard.primary(), ShardRoutingState.UNASSIGNED, failedShard.version() + 1));\n \n                         break;\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java\nindex c2bb966ced7..c6a96c3ee05 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java\n@@ -20,18 +20,19 @@\n package org.elasticsearch.cluster.routing.allocation.allocator;\n \n import com.google.common.base.Predicate;\n-import org.apache.lucene.util.ArrayUtil;\n import org.apache.lucene.util.IntroSorter;\n import org.elasticsearch.ElasticSearchIllegalArgumentException;\n import org.elasticsearch.cluster.metadata.MetaData;\n-import org.elasticsearch.cluster.routing.*;\n+import org.elasticsearch.cluster.routing.MutableShardRouting;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.RoutingNodes;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n import org.elasticsearch.cluster.routing.allocation.FailedRerouteAllocation;\n import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.cluster.routing.allocation.StartedRerouteAllocation;\n import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;\n import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n import org.elasticsearch.cluster.routing.allocation.decider.Decision.Type;\n-import org.elasticsearch.common.collect.IdentityHashSet;\n import org.elasticsearch.common.component.AbstractComponent;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.logging.ESLogger;\n@@ -265,7 +266,6 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n         private final Map<String, ModelNode> nodes = new HashMap<String, ModelNode>();\n         private final HashSet<String> indices = new HashSet<String>();\n         private final RoutingAllocation allocation;\n-        private final RoutingNodes routingNodes;\n         private final WeightFunction weight;\n \n         private final float threshold;\n@@ -284,11 +284,10 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n             this.allocation = allocation;\n             this.weight = weight;\n             this.threshold = threshold;\n-            this.routingNodes = allocation.routingNodes();\n-            for (RoutingNode node : routingNodes) {\n+            for (RoutingNode node : allocation.routingNodes()) {\n                 nodes.put(node.nodeId(), new ModelNode(node.nodeId()));\n             }\n-            metaData = routingNodes.metaData();\n+            metaData = allocation.routingNodes().metaData();\n         }\n \n         /**\n@@ -336,7 +335,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n             return new NodeSorter(nodesArray(), weight, this);\n         }\n \n-        private boolean initialize(RoutingNodes routing, RoutingNodes.UnassignedShards unassigned) {\n+        private boolean initialize(RoutingNodes routing, List<MutableShardRouting> unassigned) {\n             if (logger.isTraceEnabled()) {\n                 logger.trace(\"Start distributing Shards\");\n             }\n@@ -367,88 +366,86 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n             if (logger.isTraceEnabled()) {\n                 logger.trace(\"Start balancing cluster\");\n             }\n-            final RoutingNodes.UnassignedShards unassigned = routingNodes.unassigned().transactionBegin();\n-            boolean changed = initialize(routingNodes, unassigned);\n-            if (!changed) {\n-                NodeSorter sorter = newNodeSorter();\n-                if (nodes.size() > 1) { /* skip if we only have one node */\n-                    for (String index : buildWeightOrderedIndidces(Operation.BALANCE, sorter)) {\n-                        sorter.reset(Operation.BALANCE, index);\n-                        final float[] weights = sorter.weights;\n-                        final ModelNode[] modelNodes = sorter.modelNodes;\n-                        int lowIdx = 0;\n-                        int highIdx = weights.length - 1;\n-                        while (true) {\n-                            final ModelNode minNode = modelNodes[lowIdx];\n-                            final ModelNode maxNode = modelNodes[highIdx];\n-                            advance_range:\n-                            if (maxNode.numShards(index) > 0) {\n-                                float delta = weights[highIdx] - weights[lowIdx];\n-                                delta = delta <= threshold ? delta : sorter.weight(Operation.THRESHOLD_CHECK, maxNode) - sorter.weight(Operation.THRESHOLD_CHECK, minNode);\n-                                if (delta <= threshold) {\n-                                    if (lowIdx > 0 && highIdx-1 > 0 // is there a chance for a higher delta?\n-                                        && (weights[highIdx-1] - weights[0] > threshold) // check if we need to break at all\n-                                        ) {\n-                                        /* This is a special case if allocations from the \"heaviest\" to the \"lighter\" nodes is not possible\n-                                         * due to some allocation decider restrictions like zone awareness. if one zone has for instance\n-                                         * less nodes than another zone. so one zone is horribly overloaded from a balanced perspective but we\n-                                         * can't move to the \"lighter\" shards since otherwise the zone would go over capacity.\n-                                         *\n-                                         * This break jumps straight to the condition below were we start moving from the high index towards\n-                                         * the low index to shrink the window we are considering for balance from the other direction.\n-                                         * (check shrinking the window from MAX to MIN)\n-                                         * See #3580\n-                                         */\n-                                        break advance_range;\n-                                    }\n-                                    if (logger.isTraceEnabled()) {\n-                                        logger.trace(\"Stop balancing index [{}]  min_node [{}] weight: [{}]  max_node [{}] weight: [{}]  delta: [{}]\",\n-                                                index, maxNode.getNodeId(), weights[highIdx], minNode.getNodeId(), weights[lowIdx], delta);\n-                                    }\n-                                    break;\n+            final TransactionalList<MutableShardRouting> unassigned = new TransactionalList<MutableShardRouting>(allocation.routingNodes().unassigned());\n+            boolean changed = initialize(allocation.routingNodes(), unassigned);\n+            NodeSorter sorter = newNodeSorter();\n+            if (nodes.size() > 1) { /* skip if we only have one node */\n+                for (String index : buildWeightOrderedIndidces(Operation.BALANCE, sorter)) {\n+                    sorter.reset(Operation.BALANCE, index);\n+                    final float[] weights = sorter.weights;\n+                    final ModelNode[] modelNodes = sorter.modelNodes;\n+                    int lowIdx = 0;\n+                    int highIdx = weights.length - 1;\n+                    while (true) {\n+                        final ModelNode minNode = modelNodes[lowIdx];\n+                        final ModelNode maxNode = modelNodes[highIdx];\n+                        advance_range: \n+                        if (maxNode.numShards(index) > 0) {\n+                            float delta = weights[highIdx] - weights[lowIdx];\n+                            delta = delta <= threshold ? delta : sorter.weight(Operation.THRESHOLD_CHECK, maxNode) - sorter.weight(Operation.THRESHOLD_CHECK, minNode);\n+                            if (delta <= threshold) {\n+                                if (lowIdx > 0 && highIdx-1 > 0 // is there a chance for a higher delta? \n+                                    && (weights[highIdx-1] - weights[0] > threshold) // check if we need to break at all\n+                                    ) {\n+                                    /* This is a special case if allocations from the \"heaviest\" to the \"lighter\" nodes is not possible \n+                                     * due to some allocation decider restrictions like zone awareness. if one zone has for instance \n+                                     * less nodes than another zone. so one zone is horribly overloaded from a balanced perspective but we\n+                                     * can't move to the \"lighter\" shards since otherwise the zone would go over capacity.\n+                                     * \n+                                     * This break jumps straight to the condition below were we start moving from the high index towards \n+                                     * the low index to shrink the window we are considering for balance from the other direction. \n+                                     * (check shrinking the window from MAX to MIN)\n+                                     * See #3580\n+                                     */\n+                                    break advance_range;\n                                 }\n                                 if (logger.isTraceEnabled()) {\n-                                    logger.trace(\"Balancing from node [{}] weight: [{}] to node [{}] weight: [{}]  delta: [{}]\",\n-                                            maxNode.getNodeId(), weights[highIdx], minNode.getNodeId(), weights[lowIdx], delta);\n-                                }\n-                                /* pass the delta to the replication function to prevent relocations that only swap the weights of the two nodes.\n-                                 * a relocation must bring us closer to the balance if we only achive the same delta the relocation is useless */\n-                                if (tryRelocateShard(Operation.BALANCE, minNode, maxNode, index, delta)) {\n-                                    /*\n-                                     * TODO we could be a bit smarter here, we don't need to fully sort necessarily\n-                                     * we could just find the place to insert linearly but the win might be minor\n-                                     * compared to the added complexity\n-                                     */\n-                                    weights[lowIdx] = sorter.weight(Operation.BALANCE, modelNodes[lowIdx]);\n-                                    weights[highIdx] = sorter.weight(Operation.BALANCE, modelNodes[highIdx]);\n-                                    sorter.sort(0, weights.length);\n-                                    lowIdx = 0;\n-                                    highIdx = weights.length - 1;\n-                                    changed = true;\n-                                    continue;\n+                                    logger.trace(\"Stop balancing index [{}]  min_node [{}] weight: [{}]  max_node [{}] weight: [{}]  delta: [{}]\",\n+                                            index, maxNode.getNodeId(), weights[highIdx], minNode.getNodeId(), weights[lowIdx], delta);\n                                 }\n+                                break;\n                             }\n-                            if (lowIdx < highIdx - 1) {\n-                                /* Shrinking the window from MIN to MAX\n-                                 * we can't move from any shard from the min node lets move on to the next node\n-                                 * and see if the threshold still holds. We either don't have any shard of this\n-                                 * index on this node of allocation deciders prevent any relocation.*/\n-                                lowIdx++;\n-                            } else if (lowIdx > 0) {\n-                                /* Shrinking the window from MAX to MIN\n-                                 * now we go max to min since obviously we can't move anything to the max node\n-                                 * lets pick the next highest */\n+                            if (logger.isTraceEnabled()) {\n+                                logger.trace(\"Balancing from node [{}] weight: [{}] to node [{}] weight: [{}]  delta: [{}]\",\n+                                        maxNode.getNodeId(), weights[highIdx], minNode.getNodeId(), weights[lowIdx], delta);\n+                            }\n+                            /* pass the delta to the replication function to prevent relocations that only swap the weights of the two nodes.\n+                             * a relocation must bring us closer to the balance if we only achive the same delta the relocation is useless */\n+                            if (tryRelocateShard(Operation.BALANCE, minNode, maxNode, index, delta)) {\n+                                /*\n+                                 * TODO we could be a bit smarter here, we don't need to fully sort necessarily\n+                                 * we could just find the place to insert linearly but the win might be minor\n+                                 * compared to the added complexity\n+                                 */\n+                                weights[lowIdx] = sorter.weight(Operation.BALANCE, modelNodes[lowIdx]);\n+                                weights[highIdx] = sorter.weight(Operation.BALANCE, modelNodes[highIdx]);\n+                                sorter.sort(0, weights.length);\n                                 lowIdx = 0;\n-                                highIdx--;\n-                            } else {\n-                                /* we are done here, we either can't relocate anymore or we are balanced */\n-                                break;\n+                                highIdx = weights.length - 1;\n+                                changed = true;\n+                                continue;\n                             }\n                         }\n+                        if (lowIdx < highIdx - 1) { \n+                            /* Shrinking the window from MIN to MAX\n+                             * we can't move from any shard from the min node lets move on to the next node\n+                             * and see if the threshold still holds. We either don't have any shard of this\n+                             * index on this node of allocation deciders prevent any relocation.*/\n+                            lowIdx++;\n+                        } else if (lowIdx > 0) {\n+                            /* Shrinking the window from MAX to MIN\n+                             * now we go max to min since obviously we can't move anything to the max node \n+                             * lets pick the next highest */\n+                            lowIdx = 0;\n+                            highIdx--;\n+                        } else {\n+                            /* we are done here, we either can't relocate anymore or we are balanced */\n+                            break;\n+                        }\n                     }\n                 }\n             }\n-            routingNodes.unassigned().transactionEnd(unassigned);\n+            unassigned.commit();\n             return changed;\n         }\n \n@@ -515,7 +512,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n          *\n          * @return <code>true</code> iff the shard has successfully been moved.\n          */\n-        public boolean move(MutableShardRouting shard, RoutingNode node ) {\n+        public boolean move(MutableShardRouting shard, RoutingNode node) {\n             if (nodes.isEmpty() || !shard.started()) {\n                 /* with no nodes or a not started shard this is pointless */\n                 return false;\n@@ -523,44 +520,43 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n             if (logger.isTraceEnabled()) {\n                 logger.trace(\"Try moving shard [{}] from [{}]\", shard, node);\n             }\n-            final RoutingNodes.UnassignedShards unassigned = routingNodes.unassigned().transactionBegin();\n-            boolean changed = initialize(routingNodes, unassigned);\n-            if (!changed) {\n-                final ModelNode sourceNode = nodes.get(node.nodeId());\n-                assert sourceNode != null;\n-                final NodeSorter sorter = newNodeSorter();\n-                sorter.reset(Operation.MOVE, shard.getIndex());\n-                final ModelNode[] nodes = sorter.modelNodes;\n-                assert sourceNode.containsShard(shard);\n-                /*\n-                 * the sorter holds the minimum weight node first for the shards index.\n-                 * We now walk through the nodes until we find a node to allocate the shard.\n-                 * This is not guaranteed to be balanced after this operation we still try best effort to\n-                 * allocate on the minimal eligible node.\n-                 */\n-\n-                for (ModelNode currentNode : nodes) {\n-                    if (currentNode.getNodeId().equals(node.nodeId())) {\n-                        continue;\n-                    }\n-                    RoutingNode target = routingNodes.node(currentNode.getNodeId());\n-                    Decision decision = allocation.deciders().canAllocate(shard, target, allocation);\n-                    if (decision.type() == Type.YES) { // TODO maybe we can respect throttling here too?\n-                        sourceNode.removeShard(shard);\n-                        final MutableShardRouting initializingShard = new MutableShardRouting(shard.index(), shard.id(), currentNode.getNodeId(),\n-                                shard.currentNodeId(), shard.restoreSource(), shard.primary(), INITIALIZING, shard.version() + 1);\n-                        currentNode.addShard(initializingShard, decision);\n-                        routingNodes.assign(initializingShard, target.nodeId());\n-                        routingNodes.relocate(shard, target.nodeId()); // set the node to relocate after we added the initializing shard\n-                        if (logger.isTraceEnabled()) {\n-                            logger.trace(\"Moved shard [{}] to node [{}]\", shard, currentNode.getNodeId());\n-                        }\n-                        changed = true;\n-                        break;\n+            final TransactionalList<MutableShardRouting> unassigned = new TransactionalList<MutableShardRouting>(allocation.routingNodes().unassigned());\n+            boolean changed = initialize(allocation.routingNodes(), unassigned);\n+\n+            final ModelNode sourceNode = nodes.get(node.nodeId());\n+            assert sourceNode != null;\n+            final NodeSorter sorter = newNodeSorter();\n+            sorter.reset(Operation.MOVE, shard.getIndex());\n+            final ModelNode[] nodes = sorter.modelNodes;\n+            assert sourceNode.containsShard(shard);\n+            /*\n+             * the sorter holds the minimum weight node first for the shards index.\n+             * We now walk through the nodes until we find a node to allocate the shard.\n+             * This is not guaranteed to be balanced after this operation we still try best effort to \n+             * allocate on the minimal eligible node.\n+             */\n+            \n+            for (ModelNode currentNode : nodes) {\n+                if (currentNode.getNodeId().equals(node.nodeId())) {\n+                    continue;\n+                }\n+                RoutingNode target = allocation.routingNodes().node(currentNode.getNodeId());\n+                Decision decision = allocation.deciders().canAllocate(shard, target, allocation);\n+                if (decision.type() == Type.YES) { // TODO maybe we can respect throttling here too?\n+                    sourceNode.removeShard(shard);\n+                    final MutableShardRouting initializingShard = new MutableShardRouting(shard.index(), shard.id(), currentNode.getNodeId(),\n+                            shard.currentNodeId(), shard.restoreSource(), shard.primary(), INITIALIZING, shard.version() + 1);\n+                    currentNode.addShard(initializingShard, decision);\n+                    target.add(initializingShard);\n+                    shard.relocate(target.nodeId()); // set the node to relocate after we added the initializing shard\n+                    if (logger.isTraceEnabled()) {\n+                        logger.trace(\"Moved shard [{}] to node [{}]\", shard, currentNode.getNodeId());\n                     }\n+                    changed = true;\n+                    break;\n                 }\n             }\n-            routingNodes.unassigned().transactionEnd(unassigned);\n+            unassigned.commit();\n             return changed;\n         }\n \n@@ -593,7 +589,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n          * Allocates all given shards on the minimal eligable node for the shards index\n          * with respect to the weight function. All given shards must be unassigned.\n          */\n-        private boolean allocateUnassigned(RoutingNodes.UnassignedShards unassigned, List<MutableShardRouting> ignoredUnassigned) {\n+        private boolean allocateUnassigned(List<MutableShardRouting> unassigned, List<MutableShardRouting> ignoredUnassigned) {\n             assert !nodes.isEmpty();\n             if (logger.isTraceEnabled()) {\n                 logger.trace(\"Start allocating unassigned shards\");\n@@ -607,58 +603,42 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n              * TODO: We could be smarter here and group the shards by index and then\n              * use the sorter to save some iterations. \n              */\n+            final RoutingNodes routingNodes = allocation.routingNodes();\n             final AllocationDeciders deciders = allocation.deciders();\n-            final Comparator<MutableShardRouting> comparator = new Comparator<MutableShardRouting>() {\n+            final Set<MutableShardRouting> currentRound = new TreeSet<MutableShardRouting>(new Comparator<MutableShardRouting>() {\n                 @Override\n                 public int compare(MutableShardRouting o1,\n                                    MutableShardRouting o2) {\n-                    if (o1.primary() ^ o2.primary()) {\n-                        return o1.primary() ? -1 : o2.primary() ? 1 : 0;\n-                    }\n                     final int indexCmp;\n                     if ((indexCmp = o1.index().compareTo(o2.index())) == 0) {\n+                        if (o1.getId() - o2.getId() == 0) {\n+                            return o1.primary() ? -1 : o2.primary() ? 1 : 0;\n+                        }\n                         return o1.getId() - o2.getId();\n+\n                     }\n                     return indexCmp;\n                 }\n-            };\n-            /*\n-             * we use 2 arrays and move replicas to the second array once we allocated an identical\n-             * replica in the current iteration to make sure all indices get allocated in the same manner.\n-             * The arrays are sorted by primaries first and then by index and shard ID so a 2 indices with 2 replica and 1 shard would look like:\n-             * [(0,P,IDX1), (0,P,IDX2), (0,R,IDX1), (0,R,IDX1), (0,R,IDX2), (0,R,IDX2)]\n-             * if we allocate for instance (0, R, IDX1) we move the second replica to the secondary array and proceed with\n-             * the next replica. If we could not find a node to allocate (0,R,IDX1) we move all it's replicas to ingoreUnassigned.\n-             */\n-            MutableShardRouting[] primary = unassigned.drain();\n-            MutableShardRouting[] secondary = new MutableShardRouting[primary.length];\n-            int secondaryLength = 0;\n-            int primaryLength = primary.length;\n-            ArrayUtil.timSort(primary, comparator);\n-            final Set<ModelNode> values = new IdentityHashSet<ModelNode>(nodes.values());\n+            });\n             do {\n-                for (int i = 0; i < primaryLength; i++) {\n-                    MutableShardRouting shard = primary[i];\n-                    if (!shard.primary()) {\n-                        boolean drop = deciders.canAllocate(shard, allocation).type() == Type.NO;\n-                        if (drop) {\n-                            ignoredUnassigned.add(shard);\n-                            while(i < primaryLength-1 && comparator.compare(primary[i], primary[i+1]) == 0) {\n-                                ignoredUnassigned.add(primary[++i]);\n-                            }\n-                            continue;\n-                        } else {\n-                            while(i < primaryLength-1 && comparator.compare(primary[i], primary[i+1]) == 0) {\n-                                secondary[secondaryLength++] = primary[++i];\n-                            }\n-                        }\n+                Iterator<MutableShardRouting> iterator = unassigned.iterator();\n+                while (iterator.hasNext()) {\n+                    /* we treat every index equally here once chunk a time such that we fill up\n+                     * nodes with all indices at the same time. Only on shard of a shard a time.\n+                \t * Although there might be a primary and a shard of a shard in the set but\n+                \t * primaries will be started first.*/\n+                    if (currentRound.add(iterator.next())) {\n+                        iterator.remove();\n                     }\n-                    assert !shard.assignedToNode() : shard;\n+                }\n+                boolean iterationChanged = false;\n+                for (MutableShardRouting shard : currentRound) {\n+                    assert !shard.assignedToNode();\n                     /* find an node with minimal weight we can allocate on*/\n                     float minWeight = Float.POSITIVE_INFINITY;\n                     ModelNode minNode = null;\n                     Decision decision = null;\n-                    for (ModelNode node : values) {\n+                    for (ModelNode node : nodes.values()) {\n                         /*\n                          * The shard we add is removed below to simulate the\n \t                     * addition for weight calculation we use Decision.ALWAYS to\n@@ -718,22 +698,15 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n                     }\n                     assert decision != null && minNode != null || decision == null && minNode == null;\n                     if (minNode != null) {\n+                        iterationChanged = true;\n                         minNode.addShard(shard, decision);\n                         if (decision.type() == Type.YES) {\n                             if (logger.isTraceEnabled()) {\n                                 logger.trace(\"Assigned shard [{}] to [{}]\", shard, minNode.getNodeId());\n                             }\n-                            routingNodes.assign(shard, routingNodes.node(minNode.getNodeId()).nodeId());\n+                            routingNodes.node(minNode.getNodeId()).add(shard);\n                             changed = true;\n                             continue; // don't add to ignoreUnassigned\n-                        } else {\n-                            final RoutingNode node = routingNodes.node(minNode.getNodeId());\n-                            if (deciders.canAllocate(node, allocation).type() != Type.YES) {\n-                                if (logger.isTraceEnabled()) {\n-                                    logger.trace(\"Can not allocate on node [{}] remove from round decisin [{}]\", node, decision.type());\n-                                }\n-                                values.remove(minNode);\n-                            }\n                         }\n                         if (logger.isTraceEnabled()) {\n                             logger.trace(\"No eligable node found to assign shard [{}] decision [{}]\", shard, decision.type());\n@@ -742,18 +715,14 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n                         logger.trace(\"No Node found to assign shard [{}]\", shard);\n                     }\n                     ignoredUnassigned.add(shard);\n-                    if (!shard.primary()) { // we could not allocate it and we are a replica - check if we can ignore the other replicas\n-                        while(secondaryLength > 0 && comparator.compare(shard, secondary[secondaryLength-1]) == 0) {\n-                            ignoredUnassigned.add(secondary[--secondaryLength]);\n-                        }\n-                    }\n                 }\n-                primaryLength = secondaryLength;\n-                MutableShardRouting[] tmp = primary;\n-                primary = secondary;\n-                secondary = tmp;\n-                secondaryLength = 0;\n-            } while (primaryLength > 0);\n+                if (!iterationChanged && !unassigned.isEmpty()) {\n+                    ignoredUnassigned.addAll(unassigned);\n+                    unassigned.clear();\n+                    return changed;\n+                }\n+                currentRound.clear();\n+            } while (!unassigned.isEmpty());\n             // clear everything we have either added it or moved to ingoreUnassigned\n             return changed;\n         }\n@@ -771,7 +740,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n                     logger.trace(\"Try relocating shard for index index [{}] from node [{}] to node [{}]\", idx, maxNode.getNodeId(),\n                             minNode.getNodeId());\n                 }\n-                final RoutingNode node = routingNodes.node(minNode.getNodeId());\n+                final RoutingNode node = allocation.routingNodes().node(minNode.getNodeId());\n                 MutableShardRouting candidate = null;\n                 final AllocationDeciders deciders = allocation.deciders();\n                 /* make a copy since we modify this list in the loop */\n@@ -812,14 +781,14 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n                         }\n                         /* now allocate on the cluster - if we are started we need to relocate the shard */\n                         if (candidate.started()) {\n-                            RoutingNode lowRoutingNode = routingNodes.node(minNode.getNodeId());\n-                            routingNodes.assign(new MutableShardRouting(candidate.index(), candidate.id(), lowRoutingNode.nodeId(), candidate\n-                                    .currentNodeId(), candidate.restoreSource(), candidate.primary(), INITIALIZING, candidate.version() + 1), lowRoutingNode.nodeId());\n-                            routingNodes.relocate(candidate, lowRoutingNode.nodeId());\n+                            RoutingNode lowRoutingNode = allocation.routingNodes().node(minNode.getNodeId());\n+                            lowRoutingNode.add(new MutableShardRouting(candidate.index(), candidate.id(), lowRoutingNode.nodeId(), candidate\n+                                    .currentNodeId(), candidate.restoreSource(), candidate.primary(), INITIALIZING, candidate.version() + 1));\n+                            candidate.relocate(lowRoutingNode.nodeId());\n \n                         } else {\n                             assert candidate.unassigned();\n-                            routingNodes.assign(candidate, routingNodes.node(minNode.getNodeId()).nodeId());\n+                            allocation.routingNodes().node(minNode.getNodeId()).add(candidate);\n                         }\n                         return true;\n \n@@ -1074,4 +1043,37 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards\n             return weights[weights.length - 1] - weights[0];\n         }\n     }\n+    \n+    /**\n+     * A list that makes a full copy of the original list and applies all\n+     * modification to the copied list once {@link TransactionalList#commit()}\n+     * is called.\n+     * \n+     */\n+    @SuppressWarnings(\"serial\")\n+    private static final class TransactionalList<T> extends ArrayList<T> {\n+        \n+        private final List<T> originalList;\n+        private List<T> assertingList; // only with assert\n+\n+        TransactionalList(List<T> originalList) {\n+            super(originalList);\n+            assert copyAsseringList(originalList);\n+            this.originalList = originalList;\n+        }\n+        \n+        private boolean copyAsseringList(List<T> orig) {\n+            this.assertingList = new ArrayList<T>(orig);\n+            return true;\n+        }\n+        \n+        public void commit() {\n+            /* Ensure that the actual source list is not modified while\n+             * the transaction is running */\n+            assert assertingList.equals(originalList) : \"The list was modified outside of the scope\";\n+            originalList.clear();\n+            originalList.addAll(this);    \n+            \n+        }\n+    }\n }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/EvenShardsCountAllocator.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/EvenShardsCountAllocator.java\nindex 6e91ef571d8..6d0ae1573ba 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/EvenShardsCountAllocator.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/EvenShardsCountAllocator.java\n@@ -102,13 +102,13 @@ public class EvenShardsCountAllocator extends AbstractComponent implements Shard\n \n                 Decision decision = allocation.deciders().canAllocate(shard, node, allocation);\n                 if (decision.type() == Decision.Type.YES) {\n-                    int numberOfShardsToAllocate = routingNodes.requiredAverageNumberOfShardsPerNode() - node.size();\n+                    int numberOfShardsToAllocate = routingNodes.requiredAverageNumberOfShardsPerNode() - node.shards().size();\n                     if (numberOfShardsToAllocate <= 0) {\n                         continue;\n                     }\n \n                     changed = true;\n-                    allocation.routingNodes().assign(shard, node.nodeId());\n+                    node.add(shard);\n                     unassignedIterator.remove();\n                     break;\n                 }\n@@ -123,7 +123,7 @@ public class EvenShardsCountAllocator extends AbstractComponent implements Shard\n                 Decision decision = allocation.deciders().canAllocate(shard, routingNode, allocation);\n                 if (decision.type() == Decision.Type.YES) {\n                     changed = true;\n-                    allocation.routingNodes().assign(shard, routingNode.nodeId());\n+                    routingNode.add(shard);\n                     it.remove();\n                     break;\n                 }\n@@ -156,7 +156,7 @@ public class EvenShardsCountAllocator extends AbstractComponent implements Shard\n                     continue;\n                 }\n \n-                if (lowRoutingNode.size() >= averageNumOfShards) {\n+                if (lowRoutingNode.shards().size() >= averageNumOfShards) {\n                     lowIndex++;\n                     continue;\n                 }\n@@ -173,11 +173,11 @@ public class EvenShardsCountAllocator extends AbstractComponent implements Shard\n                     Decision allocateDecision = allocation.deciders().canAllocate(startedShard, lowRoutingNode, allocation);\n                     if (allocateDecision.type() == Decision.Type.YES) {\n                         changed = true;\n-                        allocation.routingNodes().assign(new MutableShardRouting(startedShard.index(), startedShard.id(),\n+                        lowRoutingNode.add(new MutableShardRouting(startedShard.index(), startedShard.id(),\n                                 lowRoutingNode.nodeId(), startedShard.currentNodeId(), startedShard.restoreSource(),\n-                                startedShard.primary(), INITIALIZING, startedShard.version() + 1), lowRoutingNode.nodeId());\n+                                startedShard.primary(), INITIALIZING, startedShard.version() + 1));\n \n-                        allocation.routingNodes().relocate(startedShard, lowRoutingNode.nodeId());\n+                        startedShard.relocate(lowRoutingNode.nodeId());\n                         relocated = true;\n                         relocationPerformed = true;\n                         break;\n@@ -210,11 +210,11 @@ public class EvenShardsCountAllocator extends AbstractComponent implements Shard\n             }\n             Decision decision = allocation.deciders().canAllocate(shardRouting, nodeToCheck, allocation);\n             if (decision.type() == Decision.Type.YES) {\n-                allocation.routingNodes().assign(new MutableShardRouting(shardRouting.index(), shardRouting.id(),\n+                nodeToCheck.add(new MutableShardRouting(shardRouting.index(), shardRouting.id(),\n                         nodeToCheck.nodeId(), shardRouting.currentNodeId(), shardRouting.restoreSource(),\n-                        shardRouting.primary(), INITIALIZING, shardRouting.version() + 1), nodeToCheck.nodeId());\n+                        shardRouting.primary(), INITIALIZING, shardRouting.version() + 1));\n \n-                allocation.routingNodes().relocate(shardRouting, nodeToCheck.nodeId());\n+                shardRouting.relocate(nodeToCheck.nodeId());\n                 changed = true;\n                 break;\n             }\n@@ -227,13 +227,13 @@ public class EvenShardsCountAllocator extends AbstractComponent implements Shard\n         // create count per node id, taking into account relocations\n         final ObjectIntOpenHashMap<String> nodeCounts = new ObjectIntOpenHashMap<String>();\n         for (RoutingNode node : allocation.routingNodes()) {\n-            for (int i = 0; i < node.size(); i++) {\n-                ShardRouting shardRouting = node.get(i);\n+            for (int i = 0; i < node.shards().size(); i++) {\n+                ShardRouting shardRouting = node.shards().get(i);\n                 String nodeId = shardRouting.relocating() ? shardRouting.relocatingNodeId() : shardRouting.currentNodeId();\n                 nodeCounts.addTo(nodeId, 1);\n             }\n         }\n-        RoutingNode[] nodes = allocation.routingNodes().toArray();\n+        RoutingNode[] nodes = allocation.routingNodes().nodesToShards().values().toArray(new RoutingNode[allocation.routingNodes().nodesToShards().values().size()]);\n         Arrays.sort(nodes, new Comparator<RoutingNode>() {\n             @Override\n             public int compare(RoutingNode o1, RoutingNode o2) {\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/ShardsAllocators.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/ShardsAllocators.java\nindex 437d2fd7326..9efcf7efb21 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/ShardsAllocators.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/ShardsAllocators.java\n@@ -21,7 +21,6 @@ package org.elasticsearch.cluster.routing.allocation.allocator;\n \n import org.elasticsearch.cluster.routing.MutableShardRouting;\n import org.elasticsearch.cluster.routing.RoutingNode;\n-import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.allocation.FailedRerouteAllocation;\n import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.cluster.routing.allocation.StartedRerouteAllocation;\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java\nindex ff3edb7c8d0..ce615cec71c 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java\n@@ -193,7 +193,7 @@ public class AllocateAllocationCommand implements AllocationCommand {\n                 continue;\n             }\n             it.remove();\n-            allocation.routingNodes().assign(shardRouting, routingNode.nodeId());\n+            routingNode.add(shardRouting);\n             if (shardRouting.primary()) {\n                 // we need to clear the post allocation flag, since its an explicit allocation of the primary shard\n                 // and we want to force allocate it (and create a new index for it)\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java\nindex abefc4ce206..5857cbc6780 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java\n@@ -25,7 +25,6 @@ import org.elasticsearch.ElasticSearchParseException;\n import org.elasticsearch.cluster.node.DiscoveryNode;\n import org.elasticsearch.cluster.routing.MutableShardRouting;\n import org.elasticsearch.cluster.routing.RoutingNode;\n-import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.ShardRoutingState;\n import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.common.io.stream.StreamInput;\n@@ -36,6 +35,7 @@ import org.elasticsearch.common.xcontent.XContentParser;\n import org.elasticsearch.index.shard.ShardId;\n \n import java.io.IOException;\n+import java.util.Iterator;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;\n@@ -161,8 +161,9 @@ public class CancelAllocationCommand implements AllocationCommand {\n     @Override\n     public void execute(RoutingAllocation allocation) throws ElasticSearchException {\n         DiscoveryNode discoNode = allocation.nodes().resolveNode(node);\n+\n         boolean found = false;\n-        for (RoutingNodes.RoutingNodeIterator it = allocation.routingNodes().routingNodeIter(discoNode.id()); it.hasNext(); ) {\n+        for (Iterator<MutableShardRouting> it = allocation.routingNodes().node(discoNode.id()).iterator(); it.hasNext(); ) {\n             MutableShardRouting shardRouting = it.next();\n             if (!shardRouting.shardId().equals(shardId)) {\n                 continue;\n@@ -172,31 +173,35 @@ public class CancelAllocationCommand implements AllocationCommand {\n                 if (shardRouting.initializing()) {\n                     // the shard is initializing and recovering from another node, simply cancel the recovery\n                     it.remove();\n+                    shardRouting.deassignNode();\n                     // and cancel the relocating state from the shard its being relocated from\n                     RoutingNode relocatingFromNode = allocation.routingNodes().node(shardRouting.relocatingNodeId());\n                     if (relocatingFromNode != null) {\n                         for (MutableShardRouting fromShardRouting : relocatingFromNode) {\n-                            if (fromShardRouting.shardId().equals(shardRouting.shardId()) && fromShardRouting.state() == RELOCATING) {\n-                                allocation.routingNodes().cancelRelocation(fromShardRouting);\n+                            if (fromShardRouting.shardId().equals(shardRouting.shardId()) && shardRouting.state() == RELOCATING) {\n+                                fromShardRouting.cancelRelocation();\n                                 break;\n                             }\n                         }\n                     }\n                 } else if (shardRouting.relocating()) {\n-\n                     // the shard is relocating to another node, cancel the recovery on the other node, and deallocate this one\n                     if (!allowPrimary && shardRouting.primary()) {\n                         // can't cancel a primary shard being initialized\n                         throw new ElasticSearchIllegalArgumentException(\"[cancel_allocation] can't cancel \" + shardId + \" on node \" + discoNode + \", shard is primary and initializing its state\");\n                     }\n-                    it.moveToUnassigned();\n+                    it.remove();\n+                    allocation.routingNodes().unassigned().add(new MutableShardRouting(shardRouting.index(), shardRouting.id(),\n+                            null, shardRouting.primary(), ShardRoutingState.UNASSIGNED, shardRouting.version() + 1));\n+\n                     // now, go and find the shard that is initializing on the target node, and cancel it as well...\n-                    RoutingNodes.RoutingNodeIterator initializingNode = allocation.routingNodes().routingNodeIter(shardRouting.relocatingNodeId());\n+                    RoutingNode initializingNode = allocation.routingNodes().node(shardRouting.relocatingNodeId());\n                     if (initializingNode != null) {\n-                        while (initializingNode.hasNext()) {\n-                            MutableShardRouting initializingShardRouting = initializingNode.next();\n+                        for (Iterator<MutableShardRouting> itX = initializingNode.iterator(); itX.hasNext(); ) {\n+                            MutableShardRouting initializingShardRouting = itX.next();\n                             if (initializingShardRouting.shardId().equals(shardRouting.shardId()) && initializingShardRouting.state() == INITIALIZING) {\n-                                initializingNode.remove();\n+                                shardRouting.deassignNode();\n+                                itX.remove();\n                             }\n                         }\n                     }\n@@ -212,6 +217,7 @@ public class CancelAllocationCommand implements AllocationCommand {\n                         null, shardRouting.primary(), ShardRoutingState.UNASSIGNED, shardRouting.version() + 1));\n             }\n         }\n+\n         if (!found) {\n             throw new ElasticSearchIllegalArgumentException(\"[cancel_allocation] can't cancel \" + shardId + \", failed to find it on node \" + discoNode);\n         }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java\nindex af0f2d3e763..feba47b575e 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java\n@@ -167,11 +167,11 @@ public class MoveAllocationCommand implements AllocationCommand {\n                 // its being throttled, maybe have a flag to take it into account and fail? for now, just do it since the \"user\" wants it...\n             }\n \n-            allocation.routingNodes().assign(new MutableShardRouting(shardRouting.index(), shardRouting.id(),\n+            toRoutingNode.add(new MutableShardRouting(shardRouting.index(), shardRouting.id(),\n                     toRoutingNode.nodeId(), shardRouting.currentNodeId(), shardRouting.restoreSource(),\n-                    shardRouting.primary(), ShardRoutingState.INITIALIZING, shardRouting.version() + 1), toRoutingNode.nodeId());\n+                    shardRouting.primary(), ShardRoutingState.INITIALIZING, shardRouting.version() + 1));\n \n-            allocation.routingNodes().relocate(shardRouting, toRoutingNode.nodeId());\n+            shardRouting.relocate(toRoutingNode.nodeId());\n         }\n \n         if (!found) {\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecider.java\nindex b2247e9de4a..2209e9d04dd 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecider.java\n@@ -64,20 +64,4 @@ public abstract class AllocationDecider extends AbstractComponent {\n     public Decision canRemain(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {\n         return Decision.ALWAYS;\n     }\n-\n-    /**\n-     * Returns a {@link Decision} whether the given shard routing can be allocated at all at this state of the\n-     * {@link RoutingAllocation}. The default is {@link Decision#ALWAYS}.\n-     */\n-    public Decision canAllocate(ShardRouting shardRouting, RoutingAllocation allocation) {\n-        return Decision.ALWAYS;\n-    }\n-\n-    /**\n-     * Returns a {@link Decision} whether the given node can allow any allocation at all at this state of the\n-     * {@link RoutingAllocation}. The default is {@link Decision#ALWAYS}.\n-     */\n-    public Decision canAllocate(RoutingNode node, RoutingAllocation allocation) {\n-        return Decision.ALWAYS;\n-    }\n }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java\nindex 91c0889027d..3eb129a7e72 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java\n@@ -19,11 +19,13 @@\n \n package org.elasticsearch.cluster.routing.allocation.decider;\n \n+import com.google.common.collect.ImmutableSet;\n import org.elasticsearch.cluster.routing.RoutingNode;\n import org.elasticsearch.cluster.routing.ShardRouting;\n import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.node.settings.NodeSettingsService;\n \n import java.util.Set;\n \n@@ -35,14 +37,34 @@ public class AllocationDeciders extends AllocationDecider {\n \n     private final AllocationDecider[] allocations;\n \n-    public AllocationDeciders(Settings settings, AllocationDecider[] allocations) {\n-        super(settings);\n-        this.allocations = allocations;\n+    /**\n+     * Create a new {@link AllocationDeciders} instance\n+     *\n+     * @param settings            settings to use\n+     * @param nodeSettingsService per-node settings to use\n+     */\n+    public AllocationDeciders(Settings settings, NodeSettingsService nodeSettingsService) {\n+        this(settings, ImmutableSet.<AllocationDecider>builder()\n+                .add(new SameShardAllocationDecider(settings))\n+                .add(new FilterAllocationDecider(settings, nodeSettingsService))\n+                .add(new ReplicaAfterPrimaryActiveAllocationDecider(settings))\n+                .add(new ThrottlingAllocationDecider(settings, nodeSettingsService))\n+                .add(new RebalanceOnlyWhenActiveAllocationDecider(settings))\n+                .add(new ClusterRebalanceAllocationDecider(settings))\n+                .add(new ConcurrentRebalanceAllocationDecider(settings, nodeSettingsService))\n+                .add(new DisableAllocationDecider(settings, nodeSettingsService))\n+                .add(new AwarenessAllocationDecider(settings, nodeSettingsService))\n+                .add(new ShardsLimitAllocationDecider(settings))\n+                .add(new DiskThresholdDecider(settings, nodeSettingsService))\n+                .add(new SnapshotInProgressAllocationDecider(settings))\n+                .build()\n+        );\n     }\n \n     @Inject\n     public AllocationDeciders(Settings settings, Set<AllocationDecider> allocations) {\n-        this(settings, allocations.toArray(new AllocationDecider[allocations.size()]));\n+        super(settings);\n+        this.allocations = allocations.toArray(new AllocationDecider[allocations.size()]);\n     }\n \n     @Override\n@@ -50,10 +72,7 @@ public class AllocationDeciders extends AllocationDecider {\n         Decision.Multi ret = new Decision.Multi();\n         for (AllocationDecider allocationDecider : allocations) {\n             Decision decision = allocationDecider.canRebalance(shardRouting, allocation);\n-            // short track if a NO is returned.\n-            if (decision == Decision.NO) {\n-                return decision;\n-            } else if (decision != Decision.ALWAYS) {\n+            if (decision != Decision.ALWAYS) {\n                 ret.add(decision);\n             }\n         }\n@@ -68,15 +87,9 @@ public class AllocationDeciders extends AllocationDecider {\n         Decision.Multi ret = new Decision.Multi();\n         for (AllocationDecider allocationDecider : allocations) {\n             Decision decision = allocationDecider.canAllocate(shardRouting, node, allocation);\n-            // short track if a NO is returned.\n-            if (decision == Decision.NO) {\n-                if (logger.isTraceEnabled()) {\n-                    logger.trace(\"Can not allocate [{}] on node [{}] due to [{}]\", shardRouting, node.nodeId(), allocationDecider.getClass().getSimpleName());\n-                }\n-                return decision;\n-            } else if (decision != Decision.ALWAYS) {\n-                // the assumption is that a decider that returns the static instance Decision#ALWAYS\n-                // does not really implements canAllocate\n+            // the assumption is that a decider that returns the static instance Decision#ALWAYS\n+            // does not really implements canAllocate\n+            if (decision != Decision.ALWAYS) {\n                 ret.add(decision);\n             }\n         }\n@@ -86,49 +99,12 @@ public class AllocationDeciders extends AllocationDecider {\n     @Override\n     public Decision canRemain(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {\n         if (allocation.shouldIgnoreShardForNode(shardRouting.shardId(), node.nodeId())) {\n-            if (logger.isTraceEnabled()) {\n-                logger.trace(\"Shard [{}] should be ignored for node [{}]\", shardRouting, node.nodeId());\n-            }\n             return Decision.NO;\n         }\n         Decision.Multi ret = new Decision.Multi();\n         for (AllocationDecider allocationDecider : allocations) {\n             Decision decision = allocationDecider.canRemain(shardRouting, node, allocation);\n-            // short track if a NO is returned.\n-            if (decision == Decision.NO) {\n-                if (logger.isTraceEnabled()) {\n-                    logger.trace(\"Shard [{}] can not remain on node [{}] due to [{}]\", shardRouting, node.nodeId(), allocationDecider.getClass().getSimpleName());\n-                }\n-                return decision;\n-            } else if (decision != Decision.ALWAYS) {\n-                ret.add(decision);\n-            }\n-        }\n-        return ret;\n-    }\n-\n-    public Decision canAllocate(ShardRouting shardRouting, RoutingAllocation allocation) {\n-        Decision.Multi ret = new Decision.Multi();\n-        for (AllocationDecider allocationDecider : allocations) {\n-            Decision decision = allocationDecider.canAllocate(shardRouting, allocation);\n-            // short track if a NO is returned.\n-            if (decision == Decision.NO) {\n-                return decision;\n-            } else if (decision != Decision.ALWAYS) {\n-                ret.add(decision);\n-            }\n-        }\n-        return ret;\n-    }\n-\n-    public Decision canAllocate(RoutingNode node, RoutingAllocation allocation) {\n-        Decision.Multi ret = new Decision.Multi();\n-        for (AllocationDecider allocationDecider : allocations) {\n-            Decision decision = allocationDecider.canAllocate(node, allocation);\n-            // short track if a NO is returned.\n-            if (decision == Decision.NO) {\n-                return decision;\n-            } else if (decision != Decision.ALWAYS) {\n+            if (decision != Decision.ALWAYS) {\n                 ret.add(decision);\n             }\n         }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java\nindex a07198e675a..999a7392b06 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java\n@@ -19,7 +19,6 @@\n \n package org.elasticsearch.cluster.routing.allocation.decider;\n \n-import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Lists;\n import org.elasticsearch.common.inject.AbstractModule;\n import org.elasticsearch.common.inject.multibindings.Multibinder;\n@@ -52,27 +51,22 @@ public class AllocationDecidersModule extends AbstractModule {\n     @Override\n     protected void configure() {\n         Multibinder<AllocationDecider> allocationMultibinder = Multibinder.newSetBinder(binder(), AllocationDecider.class);\n-        for (Class<? extends AllocationDecider> deciderClass : DEFAULT_ALLOCATION_DECIDERS) {\n-            allocationMultibinder.addBinding().to(deciderClass);\n-        }\n+        allocationMultibinder.addBinding().to(SameShardAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(FilterAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(ReplicaAfterPrimaryActiveAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(ThrottlingAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(RebalanceOnlyWhenActiveAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(ClusterRebalanceAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(ConcurrentRebalanceAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(DisableAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(AwarenessAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(ShardsLimitAllocationDecider.class);\n+        allocationMultibinder.addBinding().to(DiskThresholdDecider.class);\n+        allocationMultibinder.addBinding().to(SnapshotInProgressAllocationDecider.class);\n         for (Class<? extends AllocationDecider> allocation : allocations) {\n             allocationMultibinder.addBinding().to(allocation);\n         }\n \n         bind(AllocationDeciders.class).asEagerSingleton();\n     }\n-\n-    public static final ImmutableSet<Class<? extends AllocationDecider>> DEFAULT_ALLOCATION_DECIDERS = ImmutableSet.<Class<? extends AllocationDecider>>builder().\n-            add(SameShardAllocationDecider.class).\n-            add(FilterAllocationDecider.class).\n-            add(ReplicaAfterPrimaryActiveAllocationDecider.class).\n-            add(ThrottlingAllocationDecider.class).\n-            add(RebalanceOnlyWhenActiveAllocationDecider.class).\n-            add(ClusterRebalanceAllocationDecider.class).\n-            add(ConcurrentRebalanceAllocationDecider.class).\n-            add(DisableAllocationDecider.class).\n-            add(AwarenessAllocationDecider.class).\n-            add(ShardsLimitAllocationDecider.class).\n-            add(DiskThresholdDecider.class).\n-            add(SnapshotInProgressAllocationDecider.class).build();\n }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java\nindex d59a5934285..08c49da0746 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java\n@@ -185,7 +185,8 @@ public class AwarenessAllocationDecider extends AllocationDecider {\n             // build the count of shards per attribute value\n             ObjectIntOpenHashMap<String> shardPerAttribute = new ObjectIntOpenHashMap<String>();\n             for (RoutingNode routingNode : allocation.routingNodes()) {\n-                for (MutableShardRouting nodeShardRouting : routingNode) {\n+                for (int i = 0; i < routingNode.shards().size(); i++) {\n+                    MutableShardRouting nodeShardRouting = routingNode.shards().get(i);\n                     if (nodeShardRouting.shardId().equals(shardRouting.shardId())) {\n                         // if the shard is relocating, then make sure we count it as part of the node it is relocating to\n                         if (nodeShardRouting.relocating()) {\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java\nindex bc6c88e203b..04a86d614ce 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java\n@@ -21,7 +21,6 @@ package org.elasticsearch.cluster.routing.allocation.decider;\n \n import org.elasticsearch.cluster.routing.MutableShardRouting;\n import org.elasticsearch.cluster.routing.RoutingNode;\n-import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.ShardRouting;\n import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.common.inject.Inject;\n@@ -89,26 +88,34 @@ public class ClusterRebalanceAllocationDecider extends AllocationDecider {\n     @Override\n     public Decision canRebalance(ShardRouting shardRouting, RoutingAllocation allocation) {\n         if (type == ClusterRebalanceType.INDICES_PRIMARIES_ACTIVE) {\n-            // check if there are unassigned primaries.\n-            if ( allocation.routingNodes().hasUnassignedPrimaries() ) {\n-                return Decision.NO;\n+            for (MutableShardRouting shard : allocation.routingNodes().unassigned()) {\n+                if (shard.primary()) {\n+                    return Decision.NO;\n+                }\n             }\n-            // check if there are initializing primaries that don't have a relocatingNodeId entry.\n-            if ( allocation.routingNodes().hasInactivePrimaries() ) {\n-                return Decision.NO;\n+            for (RoutingNode node : allocation.routingNodes()) {\n+                List<MutableShardRouting> shards = node.shards();\n+                for (int i = 0; i < shards.size(); i++) {\n+                    MutableShardRouting shard = shards.get(i);\n+                    if (shard.primary() && !shard.active() && shard.relocatingNodeId() == null) {\n+                        return Decision.NO;\n+                    }\n+                }\n             }\n-\n             return Decision.YES;\n         }\n         if (type == ClusterRebalanceType.INDICES_ALL_ACTIVE) {\n-            // check if there are unassigned shards.\n-            if ( allocation.routingNodes().hasUnassignedShards() ) {\n+            if (!allocation.routingNodes().unassigned().isEmpty()) {\n                 return Decision.NO;\n             }\n-            // in case all indices are assigned, are there initializing shards which\n-            // are not relocating?\n-            if ( allocation.routingNodes().hasInactiveShards() ) {\n-                return Decision.NO;\n+            for (RoutingNode node : allocation.routingNodes()) {\n+                List<MutableShardRouting> shards = node.shards();\n+                for (int i = 0; i < shards.size(); i++) {\n+                    MutableShardRouting shard = shards.get(i);\n+                    if (!shard.active() && shard.relocatingNodeId() == null) {\n+                        return Decision.NO;\n+                    }\n+                }\n             }\n         }\n         // type == Type.ALWAYS\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java\nindex 61d702b04cd..51450947aab 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java\n@@ -19,12 +19,17 @@\n \n package org.elasticsearch.cluster.routing.allocation.decider;\n \n+import org.elasticsearch.cluster.routing.MutableShardRouting;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.node.settings.NodeSettingsService;\n \n+import java.util.List;\n+\n /**\n  * Similar to the {@link ClusterRebalanceAllocationDecider} this\n  * {@link AllocationDecider} controls the number of currently in-progress\n@@ -67,9 +72,18 @@ public class ConcurrentRebalanceAllocationDecider extends AllocationDecider {\n         if (clusterConcurrentRebalance == -1) {\n             return Decision.YES;\n         }\n-        if (allocation.routingNodes().getRelocatingShardCount() >= clusterConcurrentRebalance) {\n+        int rebalance = 0;\n+        for (RoutingNode node : allocation.routingNodes()) {\n+            List<MutableShardRouting> shards = node.shards();\n+            for (int i = 0; i < shards.size(); i++) {\n+                if (shards.get(i).state() == ShardRoutingState.RELOCATING) {\n+                    rebalance++;\n+                }\n+            }\n+        }\n+        if (rebalance >= clusterConcurrentRebalance) {\n             return Decision.NO;\n         }\n         return Decision.YES;\n     }\n-}\n+}\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java\nindex 7b7b0c9805e..5bcd0d67fd4 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java\n@@ -110,7 +110,7 @@ public class DiskThresholdDecider extends AllocationDecider {\n     }\n \n     @Inject\n-    public DiskThresholdDecider(Settings settings, NodeSettingsService nodeSettingsService) {\n+    protected DiskThresholdDecider(Settings settings, NodeSettingsService nodeSettingsService) {\n         super(settings);\n         String lowWatermark = settings.get(CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, \"0.7\");\n         String highWatermark = settings.get(CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, \"0.85\");\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/RebalanceOnlyWhenActiveAllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/RebalanceOnlyWhenActiveAllocationDecider.java\nindex bf4a34f5df5..f95d5dc575d 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/RebalanceOnlyWhenActiveAllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/RebalanceOnlyWhenActiveAllocationDecider.java\n@@ -19,11 +19,14 @@\n \n package org.elasticsearch.cluster.routing.allocation.decider;\n \n+import org.elasticsearch.cluster.routing.MutableShardRouting;\n import org.elasticsearch.cluster.routing.ShardRouting;\n import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n \n+import java.util.List;\n+\n /**\n  * Only allow rebalancing when all shards are active within the shard replication group.\n  */\n@@ -36,10 +39,13 @@ public class RebalanceOnlyWhenActiveAllocationDecider extends AllocationDecider\n \n     @Override\n     public Decision canRebalance(ShardRouting shardRouting, RoutingAllocation allocation) {\n+        List<MutableShardRouting> shards = allocation.routingNodes().shardsRoutingFor(shardRouting);\n         // its ok to check for active here, since in relocation, a shard is split into two in routing\n         // nodes, once relocating, and one initializing\n-        if (!allocation.routingNodes().allReplicasActive(shardRouting)) {\n-            return Decision.NO;\n+        for (int i = 0; i < shards.size(); i++) {\n+            if (!shards.get(i).active()) {\n+                return Decision.NO;\n+            }\n         }\n         return Decision.YES;\n     }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ReplicaAfterPrimaryActiveAllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ReplicaAfterPrimaryActiveAllocationDecider.java\nindex f2cfd6f2abc..3914e198be4 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ReplicaAfterPrimaryActiveAllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ReplicaAfterPrimaryActiveAllocationDecider.java\n@@ -38,15 +38,11 @@ public class ReplicaAfterPrimaryActiveAllocationDecider extends AllocationDecide\n \n     @Override\n     public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {\n-        return canAllocate(shardRouting, allocation);\n-    }\n-\n-    public Decision canAllocate(ShardRouting shardRouting, RoutingAllocation allocation) {\n         if (shardRouting.primary()) {\n             return Decision.YES;\n         }\n-        MutableShardRouting primary = allocation.routingNodes().activePrimary(shardRouting);\n-        if (primary == null) {\n+        MutableShardRouting primary = allocation.routingNodes().findPrimaryForReplica(shardRouting);\n+        if (primary == null || !primary.active()) {\n             return Decision.NO;\n         }\n         return Decision.YES;\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SameShardAllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SameShardAllocationDecider.java\nindex e49ab2e6e43..efbb1d65f27 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SameShardAllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SameShardAllocationDecider.java\n@@ -26,6 +26,8 @@ import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n \n+import java.util.List;\n+\n /**\n  * An allocation decider that prevents the more than prevents multiple instances\n  * of the same shard to be allocated on a single <tt>host</tt>. The cluster setting can\n@@ -52,9 +54,10 @@ public class SameShardAllocationDecider extends AllocationDecider {\n \n     @Override\n     public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {\n-        for (MutableShardRouting nodeShard : node) {\n+        List<MutableShardRouting> shards = node.shards();\n+        for (int i = 0; i < shards.size(); i++) {\n             // we do not allow for two shards of the same shard id to exists on the same node\n-            if (nodeShard.shardId().equals(shardRouting.shardId())) {\n+            if (shards.get(i).shardId().equals(shardRouting.shardId())) {\n                 return Decision.NO;\n             }\n         }\n@@ -68,8 +71,9 @@ public class SameShardAllocationDecider extends AllocationDecider {\n                     if (!checkNode.node().address().sameHost(node.node().address())) {\n                         continue;\n                     }\n-                    for (MutableShardRouting nodeShard : checkNode) {\n-                        if (nodeShard.shardId().equals(shardRouting.shardId())) {\n+                    shards = checkNode.shards();\n+                    for (int i = 0; i < shards.size(); i++) {\n+                        if (shards.get(i).shardId().equals(shardRouting.shardId())) {\n                             return Decision.NO;\n                         }\n                     }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java\nindex d5f243114b4..23c611c8935 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java\n@@ -28,6 +28,8 @@ import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n \n+import java.util.List;\n+\n /**\n  * This {@link AllocationDecider} limits the number of shards per node on a per\n  * index basis. The allocator prevents a single node to hold more than\n@@ -69,7 +71,9 @@ public class ShardsLimitAllocationDecider extends AllocationDecider {\n         }\n \n         int nodeCount = 0;\n-        for (MutableShardRouting nodeShard : node) {\n+        List<MutableShardRouting> shards = node.shards();\n+        for (int i = 0; i < shards.size(); i++) {\n+            MutableShardRouting nodeShard = shards.get(i);\n             if (!nodeShard.index().equals(shardRouting.index())) {\n                 continue;\n             }\n@@ -94,7 +98,9 @@ public class ShardsLimitAllocationDecider extends AllocationDecider {\n         }\n \n         int nodeCount = 0;\n-        for (MutableShardRouting nodeShard : node) {;\n+        List<MutableShardRouting> shards = node.shards();\n+        for (int i = 0; i < shards.size(); i++) {\n+            MutableShardRouting nodeShard = shards.get(i);\n             if (!nodeShard.index().equals(shardRouting.index())) {\n                 continue;\n             }\ndiff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java\nindex c9987668072..0d556d093f8 100644\n--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java\n+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java\n@@ -28,6 +28,8 @@ import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.node.settings.NodeSettingsService;\n \n+import java.util.List;\n+\n /**\n  * {@link ThrottlingAllocationDecider} controls the recovery process per node in\n  * the cluster. It exposes two settings via the cluster update API that allow\n@@ -77,7 +79,9 @@ public class ThrottlingAllocationDecider extends AllocationDecider {\n                 // primary is unassigned, means we are going to do recovery from gateway\n                 // count *just the primary* currently doing recovery on the node and check against concurrent_recoveries\n                 int primariesInRecovery = 0;\n-                for (MutableShardRouting shard : node) {\n+                List<MutableShardRouting> shards = node.shards();\n+                for (int i = 0; i < shards.size(); i++) {\n+                    MutableShardRouting shard = shards.get(i);\n                     // when a primary shard is INITIALIZING, it can be because of *initial recovery* or *relocation from another node*\n                     // we only count initial recoveries here, so we need to make sure that relocating node is null\n                     if (shard.state() == ShardRoutingState.INITIALIZING && shard.primary() && shard.relocatingNodeId() == null) {\n@@ -95,16 +99,15 @@ public class ThrottlingAllocationDecider extends AllocationDecider {\n         // either primary or replica doing recovery (from peer shard)\n \n         // count the number of recoveries on the node, its for both target (INITIALIZING) and source (RELOCATING)\n-        return canAllocate(node, allocation);\n-    }\n-\n-    public Decision canAllocate(RoutingNode node, RoutingAllocation allocation) {\n         int currentRecoveries = 0;\n-        for (MutableShardRouting shard : node) {\n+        List<MutableShardRouting> shards = node.shards();\n+        for (int i = 0; i < shards.size(); i++) {\n+            MutableShardRouting shard = shards.get(i);\n             if (shard.state() == ShardRoutingState.INITIALIZING || shard.state() == ShardRoutingState.RELOCATING) {\n                 currentRecoveries++;\n             }\n         }\n+\n         if (currentRecoveries >= concurrentRecoveries) {\n             return Decision.THROTTLE;\n         } else {\ndiff --git a/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java b/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java\ndeleted file mode 100644\nindex d30d14970ab..00000000000\n--- a/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java\n+++ /dev/null\n@@ -1,329 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.common.collect;\n-\n-import com.carrotsearch.hppc.*;\n-import com.carrotsearch.hppc.cursors.IntCursor;\n-import com.carrotsearch.hppc.cursors.IntObjectCursor;\n-import com.carrotsearch.hppc.cursors.ObjectCursor;\n-import com.carrotsearch.hppc.predicates.IntPredicate;\n-import com.carrotsearch.hppc.procedures.IntObjectProcedure;\n-import com.google.common.collect.UnmodifiableIterator;\n-\n-import java.util.Iterator;\n-import java.util.Map;\n-\n-/**\n- * An immutable map implementation based on open hash map.\n- * <p/>\n- * Can be constructed using a {@link #builder()}, or using {@link #builder(org.elasticsearch.common.collect.ImmutableOpenIntMap)} (which is an optimized\n- * option to copy over existing content and modify it).\n- */\n-public final class ImmutableOpenIntMap<VType> implements Iterable<IntObjectCursor<VType>> {\n-\n-    private final IntObjectOpenHashMap<VType> map;\n-\n-    private ImmutableOpenIntMap(IntObjectOpenHashMap<VType> map) {\n-        this.map = map;\n-    }\n-\n-    /**\n-     * @return Returns the value associated with the given key or the default value\n-     * for the key type, if the key is not associated with any value.\n-     * <p/>\n-     * <b>Important note:</b> For primitive type values, the value returned for a non-existing\n-     * key may not be the default value of the primitive type (it may be any value previously\n-     * assigned to that slot).\n-     */\n-    public VType get(int key) {\n-        return map.get(key);\n-    }\n-\n-    /**\n-     * Returns <code>true</code> if this container has an association to a value for\n-     * the given key.\n-     */\n-    public boolean containsKey(int key) {\n-        return map.containsKey(key);\n-    }\n-\n-    /**\n-     * @return Returns the current size (number of assigned keys) in the container.\n-     */\n-    public int size() {\n-        return map.size();\n-    }\n-\n-    /**\n-     * @return Return <code>true</code> if this hash map contains no assigned keys.\n-     */\n-    public boolean isEmpty() {\n-        return map.isEmpty();\n-    }\n-\n-    /**\n-     * Returns a cursor over the entries (key-value pairs) in this map. The iterator is\n-     * implemented as a cursor and it returns <b>the same cursor instance</b> on every\n-     * call to {@link java.util.Iterator#next()}. To read the current key and value use the cursor's\n-     * public fields. An example is shown below.\n-     * <pre>\n-     * for (IntShortCursor c : intShortMap)\n-     * {\n-     *     System.out.println(&quot;index=&quot; + c.index\n-     *       + &quot; key=&quot; + c.key\n-     *       + &quot; value=&quot; + c.value);\n-     * }\n-     * </pre>\n-     * <p/>\n-     * <p>The <code>index</code> field inside the cursor gives the internal index inside\n-     * the container's implementation. The interpretation of this index depends on\n-     * to the container.\n-     */\n-    @Override\n-    public Iterator<IntObjectCursor<VType>> iterator() {\n-        return map.iterator();\n-    }\n-\n-    /**\n-     * Returns a specialized view of the keys of this associated container.\n-     * The view additionally implements {@link com.carrotsearch.hppc.ObjectLookupContainer}.\n-     */\n-    public IntLookupContainer keys() {\n-        return map.keys();\n-    }\n-\n-    /**\n-     * Returns a direct iterator over the keys.\n-     */\n-    public UnmodifiableIterator<Integer> keysIt() {\n-        final Iterator<IntCursor> iterator = map.keys().iterator();\n-        return new UnmodifiableIterator<Integer>() {\n-            @Override\n-            public boolean hasNext() {\n-                return iterator.hasNext();\n-            }\n-\n-            @Override\n-            public Integer next() {\n-                return iterator.next().value;\n-            }\n-        };\n-    }\n-\n-    /**\n-     * @return Returns a container with all values stored in this map.\n-     */\n-    public ObjectContainer<VType> values() {\n-        return map.values();\n-    }\n-\n-    /**\n-     * Returns a direct iterator over the keys.\n-     */\n-    public UnmodifiableIterator<VType> valuesIt() {\n-        final Iterator<ObjectCursor<VType>> iterator = map.values().iterator();\n-        return new UnmodifiableIterator<VType>() {\n-            @Override\n-            public boolean hasNext() {\n-                return iterator.hasNext();\n-            }\n-\n-            @Override\n-            public VType next() {\n-                return iterator.next().value;\n-            }\n-        };\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return map.toString();\n-    }\n-\n-    @Override\n-    public boolean equals(Object o) {\n-        if (this == o) return true;\n-        if (o == null || getClass() != o.getClass()) return false;\n-\n-        ImmutableOpenIntMap that = (ImmutableOpenIntMap) o;\n-\n-        if (!map.equals(that.map)) return false;\n-\n-        return true;\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return map.hashCode();\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static final ImmutableOpenIntMap EMPTY = new ImmutableOpenIntMap(new IntObjectOpenHashMap());\n-\n-    @SuppressWarnings(\"unchecked\")\n-    public static <VType> ImmutableOpenIntMap<VType> of() {\n-        return EMPTY;\n-    }\n-\n-    public static <VType> Builder<VType> builder() {\n-        return new Builder<VType>();\n-    }\n-\n-    public static <VType> Builder<VType> builder(int size) {\n-        return new Builder<VType>(size);\n-    }\n-\n-    public static <VType> Builder<VType> builder(ImmutableOpenIntMap<VType> map) {\n-        return new Builder<VType>(map);\n-    }\n-\n-    public static class Builder<VType> implements IntObjectMap<VType> {\n-\n-        private IntObjectOpenHashMap<VType> map;\n-\n-        public Builder() {\n-            //noinspection unchecked\n-            this(EMPTY);\n-        }\n-\n-        public Builder(int size) {\n-            this.map = new IntObjectOpenHashMap<VType>(size);\n-        }\n-\n-        public Builder(ImmutableOpenIntMap<VType> map) {\n-            this.map = map.map.clone();\n-        }\n-\n-        /**\n-         * Builds a new instance of the\n-         */\n-        public ImmutableOpenIntMap<VType> build() {\n-            IntObjectOpenHashMap<VType> map = this.map;\n-            this.map = null; // nullify the map, so any operation post build will fail! (hackish, but safest)\n-            return new ImmutableOpenIntMap<VType>(map);\n-        }\n-\n-        /**\n-         * Puts all the entries in the map to the builder.\n-         */\n-        public Builder<VType> putAll(Map<Integer, VType> map) {\n-            for (Map.Entry<Integer, VType> entry : map.entrySet()) {\n-                this.map.put(entry.getKey(), entry.getValue());\n-            }\n-            return this;\n-        }\n-\n-        /**\n-         * A put operation that can be used in the fluent pattern.\n-         */\n-        public Builder<VType> fPut(int key, VType value) {\n-            map.put(key, value);\n-            return this;\n-        }\n-\n-        @Override\n-        public VType put(int key, VType value) {\n-            return map.put(key, value);\n-        }\n-\n-        @Override\n-        public VType get(int key) {\n-            return map.get(key);\n-        }\n-\n-        @Override\n-        public VType getOrDefault(int kType, VType vType) {\n-            return map.getOrDefault(kType, vType);\n-        }\n-\n-        /**\n-         * Remove that can be used in the fluent pattern.\n-         */\n-        public Builder<VType> fRemove(int key) {\n-            map.remove(key);\n-            return this;\n-        }\n-\n-        @Override\n-        public VType remove(int key) {\n-            return map.remove(key);\n-        }\n-\n-        @Override\n-        public Iterator<IntObjectCursor<VType>> iterator() {\n-            return map.iterator();\n-        }\n-\n-        @Override\n-        public boolean containsKey(int key) {\n-            return map.containsKey(key);\n-        }\n-\n-        @Override\n-        public int size() {\n-            return map.size();\n-        }\n-\n-        @Override\n-        public boolean isEmpty() {\n-            return map.isEmpty();\n-        }\n-\n-        @Override\n-        public void clear() {\n-            map.clear();\n-        }\n-\n-        @Override\n-        public int putAll(IntObjectAssociativeContainer<? extends VType> container) {\n-            return map.putAll(container);\n-        }\n-\n-        @Override\n-        public int putAll(Iterable<? extends IntObjectCursor<? extends VType>> iterable) {\n-            return map.putAll(iterable);\n-        }\n-\n-        @Override\n-        public int removeAll(IntContainer container) {\n-            return map.removeAll(container);\n-        }\n-\n-        @Override\n-        public int removeAll(IntPredicate predicate) {\n-            return map.removeAll(predicate);\n-        }\n-\n-        @Override\n-        public <T extends IntObjectProcedure<? super VType>> T forEach(T procedure) {\n-            return map.forEach(procedure);\n-        }\n-\n-        @Override\n-        public IntCollection keys() {\n-            return map.keys();\n-        }\n-\n-        @Override\n-        public ObjectContainer<VType> values() {\n-            return map.values();\n-        }\n-    }\n-}\ndiff --git a/src/main/java/org/elasticsearch/common/collect/ImmutableOpenLongMap.java b/src/main/java/org/elasticsearch/common/collect/ImmutableOpenLongMap.java\ndeleted file mode 100644\nindex 7f5ff7e7b54..00000000000\n--- a/src/main/java/org/elasticsearch/common/collect/ImmutableOpenLongMap.java\n+++ /dev/null\n@@ -1,329 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.common.collect;\n-\n-import com.carrotsearch.hppc.*;\n-import com.carrotsearch.hppc.cursors.LongCursor;\n-import com.carrotsearch.hppc.cursors.LongObjectCursor;\n-import com.carrotsearch.hppc.cursors.ObjectCursor;\n-import com.carrotsearch.hppc.predicates.LongPredicate;\n-import com.carrotsearch.hppc.procedures.LongObjectProcedure;\n-import com.google.common.collect.UnmodifiableIterator;\n-\n-import java.util.Iterator;\n-import java.util.Map;\n-\n-/**\n- * An immutable map implementation based on open hash map.\n- * <p/>\n- * Can be constructed using a {@link #builder()}, or using {@link #builder(org.elasticsearch.common.collect.ImmutableOpenLongMap)} (which is an optimized\n- * option to copy over existing content and modify it).\n- */\n-public final class ImmutableOpenLongMap<VType> implements Iterable<LongObjectCursor<VType>> {\n-\n-    private final LongObjectOpenHashMap<VType> map;\n-\n-    private ImmutableOpenLongMap(LongObjectOpenHashMap<VType> map) {\n-        this.map = map;\n-    }\n-\n-    /**\n-     * @return Returns the value associated with the given key or the default value\n-     * for the key type, if the key is not associated with any value.\n-     * <p/>\n-     * <b>Important note:</b> For primitive type values, the value returned for a non-existing\n-     * key may not be the default value of the primitive type (it may be any value previously\n-     * assigned to that slot).\n-     */\n-    public VType get(long key) {\n-        return map.get(key);\n-    }\n-\n-    /**\n-     * Returns <code>true</code> if this container has an association to a value for\n-     * the given key.\n-     */\n-    public boolean containsKey(long key) {\n-        return map.containsKey(key);\n-    }\n-\n-    /**\n-     * @return Returns the current size (number of assigned keys) in the container.\n-     */\n-    public int size() {\n-        return map.size();\n-    }\n-\n-    /**\n-     * @return Return <code>true</code> if this hash map contains no assigned keys.\n-     */\n-    public boolean isEmpty() {\n-        return map.isEmpty();\n-    }\n-\n-    /**\n-     * Returns a cursor over the entries (key-value pairs) in this map. The iterator is\n-     * implemented as a cursor and it returns <b>the same cursor instance</b> on every\n-     * call to {@link java.util.Iterator#next()}. To read the current key and value use the cursor's\n-     * public fields. An example is shown below.\n-     * <pre>\n-     * for (IntShortCursor c : intShortMap)\n-     * {\n-     *     System.out.println(&quot;index=&quot; + c.index\n-     *       + &quot; key=&quot; + c.key\n-     *       + &quot; value=&quot; + c.value);\n-     * }\n-     * </pre>\n-     * <p/>\n-     * <p>The <code>index</code> field inside the cursor gives the internal index inside\n-     * the container's implementation. The interpretation of this index depends on\n-     * to the container.\n-     */\n-    @Override\n-    public Iterator<LongObjectCursor<VType>> iterator() {\n-        return map.iterator();\n-    }\n-\n-    /**\n-     * Returns a specialized view of the keys of this associated container.\n-     * The view additionally implements {@link com.carrotsearch.hppc.ObjectLookupContainer}.\n-     */\n-    public LongLookupContainer keys() {\n-        return map.keys();\n-    }\n-\n-    /**\n-     * Returns a direct iterator over the keys.\n-     */\n-    public UnmodifiableIterator<Long> keysIt() {\n-        final Iterator<LongCursor> iterator = map.keys().iterator();\n-        return new UnmodifiableIterator<Long>() {\n-            @Override\n-            public boolean hasNext() {\n-                return iterator.hasNext();\n-            }\n-\n-            @Override\n-            public Long next() {\n-                return iterator.next().value;\n-            }\n-        };\n-    }\n-\n-    /**\n-     * @return Returns a container with all values stored in this map.\n-     */\n-    public ObjectContainer<VType> values() {\n-        return map.values();\n-    }\n-\n-    /**\n-     * Returns a direct iterator over the keys.\n-     */\n-    public UnmodifiableIterator<VType> valuesIt() {\n-        final Iterator<ObjectCursor<VType>> iterator = map.values().iterator();\n-        return new UnmodifiableIterator<VType>() {\n-            @Override\n-            public boolean hasNext() {\n-                return iterator.hasNext();\n-            }\n-\n-            @Override\n-            public VType next() {\n-                return iterator.next().value;\n-            }\n-        };\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return map.toString();\n-    }\n-\n-    @Override\n-    public boolean equals(Object o) {\n-        if (this == o) return true;\n-        if (o == null || getClass() != o.getClass()) return false;\n-\n-        ImmutableOpenLongMap that = (ImmutableOpenLongMap) o;\n-\n-        if (!map.equals(that.map)) return false;\n-\n-        return true;\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return map.hashCode();\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static final ImmutableOpenLongMap EMPTY = new ImmutableOpenLongMap(new LongObjectOpenHashMap());\n-\n-    @SuppressWarnings(\"unchecked\")\n-    public static <VType> ImmutableOpenLongMap<VType> of() {\n-        return EMPTY;\n-    }\n-\n-    public static <VType> Builder<VType> builder() {\n-        return new Builder<VType>();\n-    }\n-\n-    public static <VType> Builder<VType> builder(int size) {\n-        return new Builder<VType>(size);\n-    }\n-\n-    public static <VType> Builder<VType> builder(ImmutableOpenLongMap<VType> map) {\n-        return new Builder<VType>(map);\n-    }\n-\n-    public static class Builder<VType> implements LongObjectMap<VType> {\n-\n-        private LongObjectOpenHashMap<VType> map;\n-\n-        public Builder() {\n-            //noinspection unchecked\n-            this(EMPTY);\n-        }\n-\n-        public Builder(int size) {\n-            this.map = new LongObjectOpenHashMap<VType>(size);\n-        }\n-\n-        public Builder(ImmutableOpenLongMap<VType> map) {\n-            this.map = map.map.clone();\n-        }\n-\n-        /**\n-         * Builds a new instance of the\n-         */\n-        public ImmutableOpenLongMap<VType> build() {\n-            LongObjectOpenHashMap<VType> map = this.map;\n-            this.map = null; // nullify the map, so any operation post build will fail! (hackish, but safest)\n-            return new ImmutableOpenLongMap<VType>(map);\n-        }\n-\n-        /**\n-         * Puts all the entries in the map to the builder.\n-         */\n-        public Builder<VType> putAll(Map<Long, VType> map) {\n-            for (Map.Entry<Long, VType> entry : map.entrySet()) {\n-                this.map.put(entry.getKey(), entry.getValue());\n-            }\n-            return this;\n-        }\n-\n-        /**\n-         * A put operation that can be used in the fluent pattern.\n-         */\n-        public Builder<VType> fPut(long key, VType value) {\n-            map.put(key, value);\n-            return this;\n-        }\n-\n-        @Override\n-        public VType put(long key, VType value) {\n-            return map.put(key, value);\n-        }\n-\n-        @Override\n-        public VType get(long key) {\n-            return map.get(key);\n-        }\n-\n-        @Override\n-        public VType getOrDefault(long kType, VType vType) {\n-            return map.getOrDefault(kType, vType);\n-        }\n-\n-        /**\n-         * Remove that can be used in the fluent pattern.\n-         */\n-        public Builder<VType> fRemove(long key) {\n-            map.remove(key);\n-            return this;\n-        }\n-\n-        @Override\n-        public VType remove(long key) {\n-            return map.remove(key);\n-        }\n-\n-        @Override\n-        public Iterator<LongObjectCursor<VType>> iterator() {\n-            return map.iterator();\n-        }\n-\n-        @Override\n-        public boolean containsKey(long key) {\n-            return map.containsKey(key);\n-        }\n-\n-        @Override\n-        public int size() {\n-            return map.size();\n-        }\n-\n-        @Override\n-        public boolean isEmpty() {\n-            return map.isEmpty();\n-        }\n-\n-        @Override\n-        public void clear() {\n-            map.clear();\n-        }\n-\n-        @Override\n-        public int putAll(LongObjectAssociativeContainer<? extends VType> container) {\n-            return map.putAll(container);\n-        }\n-\n-        @Override\n-        public int putAll(Iterable<? extends LongObjectCursor<? extends VType>> iterable) {\n-            return map.putAll(iterable);\n-        }\n-\n-        @Override\n-        public int removeAll(LongContainer container) {\n-            return map.removeAll(container);\n-        }\n-\n-        @Override\n-        public int removeAll(LongPredicate predicate) {\n-            return map.removeAll(predicate);\n-        }\n-\n-        @Override\n-        public <T extends LongObjectProcedure<? super VType>> T forEach(T procedure) {\n-            return map.forEach(procedure);\n-        }\n-\n-        @Override\n-        public LongCollection keys() {\n-            return map.keys();\n-        }\n-\n-        @Override\n-        public ObjectContainer<VType> values() {\n-            return map.values();\n-        }\n-    }\n-}\ndiff --git a/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java b/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java\nindex aafa4b54860..2624717af4e 100644\n--- a/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java\n+++ b/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java\n@@ -248,7 +248,7 @@ public enum DistanceUnit {\n         public final double value;\n         public final DistanceUnit unit;\n \n-        public Distance(double value, DistanceUnit unit) {\n+        private Distance(double value, DistanceUnit unit) {\n             super();\n             this.value = value;\n             this.unit = unit;\ndiff --git a/src/main/java/org/elasticsearch/common/util/BigArrays.java b/src/main/java/org/elasticsearch/common/util/BigArrays.java\nindex 4dc6d98869c..9fef20bd15b 100644\n--- a/src/main/java/org/elasticsearch/common/util/BigArrays.java\n+++ b/src/main/java/org/elasticsearch/common/util/BigArrays.java\n@@ -131,13 +131,6 @@ public enum BigArrays {\n             assert indexIsInt(index);\n             return array[(int) index] += inc;\n         }\n-\n-        @Override\n-        public void fill(long fromIndex, long toIndex, long value) {\n-            assert indexIsInt(fromIndex);\n-            assert indexIsInt(toIndex);\n-            Arrays.fill(array, (int) fromIndex, (int) toIndex, value);\n-        }\n     }\n \n     private static class DoubleArrayWrapper implements DoubleArray {\ndiff --git a/src/main/java/org/elasticsearch/common/util/BigLongArray.java b/src/main/java/org/elasticsearch/common/util/BigLongArray.java\nindex 0e1b882fe25..3faf67e7a1a 100644\n--- a/src/main/java/org/elasticsearch/common/util/BigLongArray.java\n+++ b/src/main/java/org/elasticsearch/common/util/BigLongArray.java\n@@ -19,7 +19,6 @@\n \n package org.elasticsearch.common.util;\n \n-import com.google.common.base.Preconditions;\n import org.apache.lucene.util.ArrayUtil;\n import org.apache.lucene.util.RamUsageEstimator;\n \n@@ -93,20 +92,4 @@ final class BigLongArray extends AbstractBigArray implements LongArray {\n         this.size = newSize;\n     }\n \n-    @Override\n-    public void fill(long fromIndex, long toIndex, long value) {\n-        Preconditions.checkArgument(fromIndex <= toIndex);\n-        final int fromPage = pageIndex(fromIndex);\n-        final int toPage = pageIndex(toIndex - 1);\n-        if (fromPage == toPage) {\n-            Arrays.fill(pages[fromPage], indexInPage(fromIndex), indexInPage(toIndex - 1) + 1, value);\n-        } else {\n-            Arrays.fill(pages[fromPage], indexInPage(fromIndex), pages[fromPage].length, value);\n-            for (int i = fromPage + 1; i < toPage; ++i) {\n-                Arrays.fill(pages[i], value);\n-            }\n-            Arrays.fill(pages[toPage], 0, indexInPage(toIndex - 1) + 1, value);\n-        }\n-    }\n-\n }\ndiff --git a/src/main/java/org/elasticsearch/common/util/LongArray.java b/src/main/java/org/elasticsearch/common/util/LongArray.java\nindex d986cb7534d..b00058ecf8c 100644\n--- a/src/main/java/org/elasticsearch/common/util/LongArray.java\n+++ b/src/main/java/org/elasticsearch/common/util/LongArray.java\n@@ -39,9 +39,4 @@ public interface LongArray extends BigArray {\n      */\n     public abstract long increment(long index, long inc);\n \n-    /**\n-     * Fill slots between <code>fromIndex</code> inclusive to <code>toIndex</code> exclusive with <code>value</code>.\n-     */\n-    public abstract void fill(long fromIndex, long toIndex, long value);\n-\n }\ndiff --git a/src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java b/src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java\nindex e1a27ea5186..86301ec8ea0 100644\n--- a/src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java\n+++ b/src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java\n@@ -53,8 +53,8 @@ public class EsThreadPoolExecutor extends ThreadPoolExecutor {\n             } else {\n                 this.listener = listener;\n             }\n+            shutdown();\n         }\n-        shutdown();\n     }\n \n     @Override\ndiff --git a/src/main/java/org/elasticsearch/gateway/blobstore/BlobReuseExistingGatewayAllocator.java b/src/main/java/org/elasticsearch/gateway/blobstore/BlobReuseExistingGatewayAllocator.java\nindex f1421fe0d7d..d0c6a3397a2 100644\n--- a/src/main/java/org/elasticsearch/gateway/blobstore/BlobReuseExistingGatewayAllocator.java\n+++ b/src/main/java/org/elasticsearch/gateway/blobstore/BlobReuseExistingGatewayAllocator.java\n@@ -214,9 +214,8 @@ public class BlobReuseExistingGatewayAllocator extends AbstractComponent impleme\n                 } else {\n                     // if its backup, see if there is a primary that *is* allocated, and try and assign a location that is closest to it\n                     // note, since we replicate operations, this might not be the same (different flush intervals)\n-                    MutableShardRouting primaryShard = routingNodes.activePrimary(shard);\n-                    if (primaryShard != null) {\n-                        assert primaryShard.active();\n+                    MutableShardRouting primaryShard = routingNodes.findPrimaryForReplica(shard);\n+                    if (primaryShard != null && primaryShard.active()) {\n                         DiscoveryNode primaryNode = nodes.get(primaryShard.currentNodeId());\n                         if (primaryNode != null) {\n                             TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryNodeStore = shardStores.get(primaryNode);\n@@ -253,11 +252,12 @@ public class BlobReuseExistingGatewayAllocator extends AbstractComponent impleme\n                     }\n                     // we found a match\n                     changed = true;\n-                    allocation.routingNodes().assign(shard, lastNodeMatched.nodeId());\n+                    lastNodeMatched.add(shard);\n                     unassignedIterator.remove();\n                 }\n             }\n         }\n+\n         return changed;\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/gateway/local/LocalGatewayAllocator.java b/src/main/java/org/elasticsearch/gateway/local/LocalGatewayAllocator.java\nindex b4943ca03f7..259e4c23bea 100644\n--- a/src/main/java/org/elasticsearch/gateway/local/LocalGatewayAllocator.java\n+++ b/src/main/java/org/elasticsearch/gateway/local/LocalGatewayAllocator.java\n@@ -214,7 +214,7 @@ public class LocalGatewayAllocator extends AbstractComponent implements GatewayA\n                     // we found a match\n                     changed = true;\n                     // make sure we create one with the version from the recovered state\n-                    allocation.routingNodes().assign(new MutableShardRouting(shard, highestVersion), node.nodeId());\n+                    node.add(new MutableShardRouting(shard, highestVersion));\n                     unassignedIterator.remove();\n \n                     // found a node, so no throttling, no \"no\", and break out of the loop\n@@ -234,7 +234,7 @@ public class LocalGatewayAllocator extends AbstractComponent implements GatewayA\n                     // we found a match\n                     changed = true;\n                     // make sure we create one with the version from the recovered state\n-                    allocation.routingNodes().assign(new MutableShardRouting(shard, highestVersion), node.nodeId());\n+                    node.add(new MutableShardRouting(shard, highestVersion));\n                     unassignedIterator.remove();\n                 }\n             } else {\n@@ -311,9 +311,8 @@ public class LocalGatewayAllocator extends AbstractComponent implements GatewayA\n                 }\n \n                 if (!shard.primary()) {\n-                    MutableShardRouting primaryShard = routingNodes.activePrimary(shard);\n-                    if (primaryShard != null) {\n-                        assert primaryShard.active();\n+                    MutableShardRouting primaryShard = routingNodes.findPrimaryForReplica(shard);\n+                    if (primaryShard != null && primaryShard.active()) {\n                         DiscoveryNode primaryNode = nodes.get(primaryShard.currentNodeId());\n                         if (primaryNode != null) {\n                             TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryNodeStore = shardStores.get(primaryNode);\n@@ -352,11 +351,12 @@ public class LocalGatewayAllocator extends AbstractComponent implements GatewayA\n                     }\n                     // we found a match\n                     changed = true;\n-                    allocation.routingNodes().assign(shard, lastNodeMatched.nodeId());\n+                    lastNodeMatched.add(shard);\n                     unassignedIterator.remove();\n                 }\n             }\n         }\n+\n         return changed;\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java b/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java\nindex bb57e1ce2f3..4ad49bed9e1 100644\n--- a/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java\n+++ b/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java\n@@ -58,8 +58,8 @@ public class NettyHttpChannel implements HttpChannel {\n         // Decide whether to close the connection or not.\n         boolean http10 = request.getProtocolVersion().equals(HttpVersion.HTTP_1_0);\n         boolean close =\n-                HttpHeaders.Values.CLOSE.equalsIgnoreCase(request.headers().get(HttpHeaders.Names.CONNECTION)) ||\n-                        (http10 && !HttpHeaders.Values.KEEP_ALIVE.equalsIgnoreCase(request.headers().get(HttpHeaders.Names.CONNECTION)));\n+                HttpHeaders.Values.CLOSE.equalsIgnoreCase(request.getHeader(HttpHeaders.Names.CONNECTION)) ||\n+                        (http10 && !HttpHeaders.Values.KEEP_ALIVE.equalsIgnoreCase(request.getHeader(HttpHeaders.Names.CONNECTION)));\n \n         // Build the response object.\n         HttpResponseStatus status = getStatus(response.status());\n@@ -67,27 +67,27 @@ public class NettyHttpChannel implements HttpChannel {\n         if (http10) {\n             resp = new DefaultHttpResponse(HttpVersion.HTTP_1_0, status);\n             if (!close) {\n-                resp.headers().add(HttpHeaders.Names.CONNECTION, \"Keep-Alive\");\n+                resp.addHeader(HttpHeaders.Names.CONNECTION, \"Keep-Alive\");\n             }\n         } else {\n             resp = new DefaultHttpResponse(HttpVersion.HTTP_1_1, status);\n         }\n-        if (RestUtils.isBrowser(request.headers().get(HttpHeaders.Names.USER_AGENT))) {\n+        if (RestUtils.isBrowser(request.getHeader(HttpHeaders.Names.USER_AGENT))) {\n             if (transport.settings().getAsBoolean(\"http.cors.enabled\", true)) {\n                 // Add support for cross-origin Ajax requests (CORS)\n-                resp.headers().add(\"Access-Control-Allow-Origin\", transport.settings().get(\"http.cors.allow-origin\", \"*\"));\n+                resp.addHeader(\"Access-Control-Allow-Origin\", transport.settings().get(\"http.cors.allow-origin\", \"*\"));\n                 if (request.getMethod() == HttpMethod.OPTIONS) {\n                     // Allow Ajax requests based on the CORS \"preflight\" request\n-                    resp.headers().add(\"Access-Control-Max-Age\", transport.settings().getAsInt(\"http.cors.max-age\", 1728000));\n-                    resp.headers().add(\"Access-Control-Allow-Methods\", transport.settings().get(\"http.cors.allow-methods\", \"OPTIONS, HEAD, GET, POST, PUT, DELETE\"));\n-                    resp.headers().add(\"Access-Control-Allow-Headers\", transport.settings().get(\"http.cors.allow-headers\", \"X-Requested-With, Content-Type, Content-Length\"));\n+                    resp.addHeader(\"Access-Control-Max-Age\", transport.settings().getAsInt(\"http.cors.max-age\", 1728000));\n+                    resp.addHeader(\"Access-Control-Allow-Methods\", transport.settings().get(\"http.cors.allow-methods\", \"OPTIONS, HEAD, GET, POST, PUT, DELETE\"));\n+                    resp.addHeader(\"Access-Control-Allow-Headers\", transport.settings().get(\"http.cors.allow-headers\", \"X-Requested-With, Content-Type, Content-Length\"));\n                 }\n             }\n         }\n \n-        String opaque = request.headers().get(\"X-Opaque-Id\");\n+        String opaque = request.getHeader(\"X-Opaque-Id\");\n         if (opaque != null) {\n-            resp.headers().add(\"X-Opaque-Id\", opaque);\n+            resp.addHeader(\"X-Opaque-Id\", opaque);\n         }\n \n         // Add all custom headers\n@@ -95,7 +95,7 @@ public class NettyHttpChannel implements HttpChannel {\n         if (customHeaders != null) {\n             for (Map.Entry<String, List<String>> headerEntry : customHeaders.entrySet()) {\n                 for (String headerValue : headerEntry.getValue()) {\n-                    resp.headers().add(headerEntry.getKey(), headerValue);\n+                    resp.addHeader(headerEntry.getKey(), headerValue);\n                 }\n             }\n         }\n@@ -134,12 +134,12 @@ public class NettyHttpChannel implements HttpChannel {\n             buf = ChannelBuffers.wrappedBuffer(prefixBuf, buf, suffixBuf);\n         }\n         resp.setContent(buf);\n-        resp.headers().add(HttpHeaders.Names.CONTENT_TYPE, response.contentType());\n+        resp.setHeader(HttpHeaders.Names.CONTENT_TYPE, response.contentType());\n \n-        resp.headers().add(HttpHeaders.Names.CONTENT_LENGTH, String.valueOf(buf.readableBytes()));\n+        resp.setHeader(HttpHeaders.Names.CONTENT_LENGTH, String.valueOf(buf.readableBytes()));\n \n         if (transport.resetCookies) {\n-            String cookieString = request.headers().get(HttpHeaders.Names.COOKIE);\n+            String cookieString = request.getHeader(HttpHeaders.Names.COOKIE);\n             if (cookieString != null) {\n                 CookieDecoder cookieDecoder = new CookieDecoder();\n                 Set<Cookie> cookies = cookieDecoder.decode(cookieString);\n@@ -149,7 +149,7 @@ public class NettyHttpChannel implements HttpChannel {\n                     for (Cookie cookie : cookies) {\n                         cookieEncoder.addCookie(cookie);\n                     }\n-                    resp.headers().add(HttpHeaders.Names.SET_COOKIE, cookieEncoder.encode());\n+                    resp.addHeader(HttpHeaders.Names.SET_COOKIE, cookieEncoder.encode());\n                 }\n             }\n         }\ndiff --git a/src/main/java/org/elasticsearch/http/netty/NettyHttpRequest.java b/src/main/java/org/elasticsearch/http/netty/NettyHttpRequest.java\nindex a136ad06be6..4bd7ff23feb 100644\n--- a/src/main/java/org/elasticsearch/http/netty/NettyHttpRequest.java\n+++ b/src/main/java/org/elasticsearch/http/netty/NettyHttpRequest.java\n@@ -121,7 +121,7 @@ public class NettyHttpRequest extends AbstractRestRequest implements HttpRequest\n \n     @Override\n     public String header(String name) {\n-        return request.headers().get(name);\n+        return request.getHeader(name);\n     }\n \n     @Override\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java\nindex bbccb2a9887..c527d9c3f97 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java\n@@ -170,7 +170,7 @@ public interface IndexFieldData<FD extends AtomicFieldData> extends IndexCompone\n \n     interface Builder {\n \n-        IndexFieldData build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache);\n+        IndexFieldData build(Index index, @IndexSettings Settings indexSettings, FieldMapper.Names fieldNames, FieldDataType type, IndexFieldDataCache cache);\n     }\n \n     public interface WithOrdinals<FD extends AtomicFieldData.WithOrdinals> extends IndexFieldData<FD> {\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java b/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java\nindex 9c68546cad7..cf10906c7d9 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java\n@@ -20,7 +20,6 @@\n package org.elasticsearch.index.fielddata;\n \n import com.google.common.collect.ImmutableMap;\n-import com.google.common.collect.Maps;\n import org.apache.lucene.index.IndexReader;\n import org.elasticsearch.ElasticSearchIllegalArgumentException;\n import org.elasticsearch.common.collect.MapBuilder;\n@@ -37,20 +36,12 @@ import org.elasticsearch.index.service.IndexService;\n import org.elasticsearch.index.settings.IndexSettings;\n import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;\n \n-import java.util.Map;\n import java.util.concurrent.ConcurrentMap;\n \n /**\n  */\n public class IndexFieldDataService extends AbstractIndexComponent {\n \n-    private static final String DISABLED_FORMAT = \"disabled\";\n-    private static final String DOC_VALUES_FORMAT = \"doc_values\";\n-    private static final String ARRAY_FORMAT = \"array\";\n-    private static final String PAGED_BYTES_FORMAT = \"paged_bytes\";\n-    private static final String FST_FORMAT = \"fst\";\n-    private static final String COMPRESSED_FORMAT = \"compressed\";\n-\n     private final static ImmutableMap<String, IndexFieldData.Builder> buildersByType;\n     private final static ImmutableMap<String, IndexFieldData.Builder> docValuesBuildersByType;\n     private final static ImmutableMap<Tuple<String, String>, IndexFieldData.Builder> buildersByTypeAndFormat;\n@@ -78,49 +69,30 @@ public class IndexFieldDataService extends AbstractIndexComponent {\n                 .immutableMap();\n \n         buildersByTypeAndFormat = MapBuilder.<Tuple<String, String>, IndexFieldData.Builder>newMapBuilder()\n-                .put(Tuple.tuple(\"string\", PAGED_BYTES_FORMAT), new PagedBytesIndexFieldData.Builder())\n-                .put(Tuple.tuple(\"string\", FST_FORMAT), new FSTBytesIndexFieldData.Builder())\n-                .put(Tuple.tuple(\"string\", DOC_VALUES_FORMAT), new DocValuesIndexFieldData.Builder())\n-                .put(Tuple.tuple(\"string\", DISABLED_FORMAT), new DisabledIndexFieldData.Builder())\n-\n-                .put(Tuple.tuple(\"float\", ARRAY_FORMAT), new FloatArrayIndexFieldData.Builder())\n-                .put(Tuple.tuple(\"float\", DOC_VALUES_FORMAT), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.FLOAT))\n-                .put(Tuple.tuple(\"float\", DISABLED_FORMAT), new DisabledIndexFieldData.Builder())\n-\n-                .put(Tuple.tuple(\"double\", ARRAY_FORMAT), new DoubleArrayIndexFieldData.Builder())\n-                .put(Tuple.tuple(\"double\", DOC_VALUES_FORMAT), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.DOUBLE))\n-                .put(Tuple.tuple(\"double\", DISABLED_FORMAT), new DisabledIndexFieldData.Builder())\n-\n-                .put(Tuple.tuple(\"byte\", ARRAY_FORMAT), new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BYTE))\n-                .put(Tuple.tuple(\"byte\", DOC_VALUES_FORMAT), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.BYTE))\n-                .put(Tuple.tuple(\"byte\", DISABLED_FORMAT), new DisabledIndexFieldData.Builder())\n-\n-                .put(Tuple.tuple(\"short\", ARRAY_FORMAT), new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.SHORT))\n-                .put(Tuple.tuple(\"short\", DOC_VALUES_FORMAT), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.SHORT))\n-                .put(Tuple.tuple(\"short\", DISABLED_FORMAT), new DisabledIndexFieldData.Builder())\n-\n-                .put(Tuple.tuple(\"int\", ARRAY_FORMAT), new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.INT))\n-                .put(Tuple.tuple(\"int\", DOC_VALUES_FORMAT), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.INT))\n-                .put(Tuple.tuple(\"int\", DISABLED_FORMAT), new DisabledIndexFieldData.Builder())\n-\n-                .put(Tuple.tuple(\"long\", ARRAY_FORMAT), new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.LONG))\n-                .put(Tuple.tuple(\"long\", DOC_VALUES_FORMAT), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.LONG))\n-                .put(Tuple.tuple(\"long\", DISABLED_FORMAT), new DisabledIndexFieldData.Builder())\n-\n-                .put(Tuple.tuple(\"geo_point\", ARRAY_FORMAT), new GeoPointDoubleArrayIndexFieldData.Builder())\n-                .put(Tuple.tuple(\"geo_point\", DISABLED_FORMAT), new DisabledIndexFieldData.Builder())\n-                .put(Tuple.tuple(\"geo_point\", COMPRESSED_FORMAT), new GeoPointCompressedIndexFieldData.Builder())\n-\n+                .put(Tuple.tuple(\"string\", \"paged_bytes\"), new PagedBytesIndexFieldData.Builder())\n+                .put(Tuple.tuple(\"string\", \"fst\"), new FSTBytesIndexFieldData.Builder())\n+                .put(Tuple.tuple(\"string\", \"doc_values\"), new DocValuesIndexFieldData.Builder())\n+                .put(Tuple.tuple(\"float\", \"array\"), new FloatArrayIndexFieldData.Builder())\n+                .put(Tuple.tuple(\"float\", \"doc_values\"), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.FLOAT))\n+                .put(Tuple.tuple(\"double\", \"array\"), new DoubleArrayIndexFieldData.Builder())\n+                .put(Tuple.tuple(\"double\", \"doc_values\"), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.DOUBLE))\n+                .put(Tuple.tuple(\"byte\", \"array\"), new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BYTE))\n+                .put(Tuple.tuple(\"byte\", \"doc_values\"), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.BYTE))\n+                .put(Tuple.tuple(\"short\", \"array\"), new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.SHORT))\n+                .put(Tuple.tuple(\"short\", \"doc_values\"), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.SHORT))\n+                .put(Tuple.tuple(\"int\", \"array\"), new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.INT))\n+                .put(Tuple.tuple(\"int\", \"doc_values\"), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.INT))\n+                .put(Tuple.tuple(\"long\", \"array\"), new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.LONG))\n+                .put(Tuple.tuple(\"long\", \"doc_values\"), new DocValuesIndexFieldData.Builder().numericType(IndexNumericFieldData.NumericType.LONG))\n+                .put(Tuple.tuple(\"geo_point\", \"array\"), new GeoPointDoubleArrayIndexFieldData.Builder())\n                 .immutableMap();\n     }\n \n     private final IndicesFieldDataCache indicesFieldDataCache;\n     private final ConcurrentMap<String, IndexFieldData<?>> loadedFieldData = ConcurrentCollections.newConcurrentMap();\n-    private final Map<String, IndexFieldDataCache> fieldDataCaches = Maps.newHashMap(); // no need for concurrency support, always used under lock\n \n     IndexService indexService;\n \n-    // public for testing\n     public IndexFieldDataService(Index index) {\n         this(index, ImmutableSettings.Builder.EMPTY_SETTINGS, new IndicesFieldDataCache(ImmutableSettings.Builder.EMPTY_SETTINGS));\n     }\n@@ -142,10 +114,6 @@ public class IndexFieldDataService extends AbstractIndexComponent {\n                 fieldData.clear();\n             }\n             loadedFieldData.clear();\n-            for (IndexFieldDataCache cache : fieldDataCaches.values()) {\n-                cache.clear();\n-            }\n-            fieldDataCaches.clear();\n         }\n     }\n \n@@ -155,29 +123,12 @@ public class IndexFieldDataService extends AbstractIndexComponent {\n             if (fieldData != null) {\n                 fieldData.clear();\n             }\n-            IndexFieldDataCache cache = fieldDataCaches.remove(fieldName);\n-            if (cache != null) {\n-                cache.clear();\n-            }\n         }\n     }\n \n     public void clear(IndexReader reader) {\n-        synchronized (loadedFieldData) {\n-            for (IndexFieldData<?> indexFieldData : loadedFieldData.values()) {\n-                indexFieldData.clear(reader);\n-            }\n-            for (IndexFieldDataCache cache : fieldDataCaches.values()) {\n-                cache.clear(reader);\n-            }\n-        }\n-    }\n-\n-    public void onMappingUpdate() {\n-        // synchronize to make sure to not miss field data instances that are being loaded\n-        synchronized (loadedFieldData) {\n-            // important: do not clear fieldDataCaches: the cache may be reused\n-            loadedFieldData.clear();\n+        for (IndexFieldData<?> indexFieldData : loadedFieldData.values()) {\n+            indexFieldData.clear(reader);\n         }\n     }\n \n@@ -212,29 +163,25 @@ public class IndexFieldDataService extends AbstractIndexComponent {\n                         throw new ElasticSearchIllegalArgumentException(\"failed to find field data builder for field \" + fieldNames.fullName() + \", and type \" + type.getType());\n                     }\n \n-                    IndexFieldDataCache cache = fieldDataCaches.get(fieldNames.indexName());\n-                    if (cache == null) {\n-                        //  we default to node level cache, which in turn defaults to be unbounded\n-                        // this means changing the node level settings is simple, just set the bounds there\n-                        String cacheType = type.getSettings().get(\"cache\", indexSettings.get(\"index.fielddata.cache\", \"node\"));\n-                        if (\"resident\".equals(cacheType)) {\n-                            cache = new IndexFieldDataCache.Resident(indexService, fieldNames, type);\n-                        } else if (\"soft\".equals(cacheType)) {\n-                            cache = new IndexFieldDataCache.Soft(indexService, fieldNames, type);\n-                        } else if (\"node\".equals(cacheType)) {\n-                            cache = indicesFieldDataCache.buildIndexFieldDataCache(indexService, index, fieldNames, type);\n-                        } else {\n-                            throw new ElasticSearchIllegalArgumentException(\"cache type not supported [\" + cacheType + \"] for field [\" + fieldNames.fullName() + \"]\");\n-                        }\n-                        fieldDataCaches.put(fieldNames.indexName(), cache);\n+                    IndexFieldDataCache cache;\n+                    //  we default to node level cache, which in turn defaults to be unbounded\n+                    // this means changing the node level settings is simple, just set the bounds there\n+                    String cacheType = type.getSettings().get(\"cache\", indexSettings.get(\"index.fielddata.cache\", \"node\"));\n+                    if (\"resident\".equals(cacheType)) {\n+                        cache = new IndexFieldDataCache.Resident(indexService, fieldNames, type);\n+                    } else if (\"soft\".equals(cacheType)) {\n+                        cache = new IndexFieldDataCache.Soft(indexService, fieldNames, type);\n+                    } else if (\"node\".equals(cacheType)) {\n+                        cache = indicesFieldDataCache.buildIndexFieldDataCache(indexService, index, fieldNames, type);\n+                    } else {\n+                        throw new ElasticSearchIllegalArgumentException(\"cache type not supported [\" + cacheType + \"] for field [\" + fieldNames.fullName() + \"]\");\n                     }\n \n-                    fieldData = builder.build(index, indexSettings, mapper, cache);\n+                    fieldData = builder.build(index, indexSettings, fieldNames, type, cache);\n                     loadedFieldData.put(fieldNames.indexName(), fieldData);\n                 }\n             }\n         }\n         return (IFD) fieldData;\n     }\n-\n }\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractGeoPointIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractGeoPointIndexFieldData.java\ndeleted file mode 100644\nindex d73f0d4912c..00000000000\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractGeoPointIndexFieldData.java\n+++ /dev/null\n@@ -1,145 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.index.fielddata.plain;\n-\n-import org.apache.lucene.util.BytesRef;\n-import org.apache.lucene.util.BytesRefIterator;\n-import org.apache.lucene.util.CharsRef;\n-import org.apache.lucene.util.UnicodeUtil;\n-import org.elasticsearch.ElasticSearchIllegalArgumentException;\n-import org.elasticsearch.common.Nullable;\n-import org.elasticsearch.common.geo.GeoPoint;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.index.Index;\n-import org.elasticsearch.index.fielddata.*;\n-import org.elasticsearch.index.fielddata.fieldcomparator.SortMode;\n-import org.elasticsearch.index.mapper.FieldMapper.Names;\n-\n-import java.io.IOException;\n-\n-abstract class AbstractGeoPointIndexFieldData extends AbstractIndexFieldData<AtomicGeoPointFieldData<ScriptDocValues>> implements IndexGeoPointFieldData<AtomicGeoPointFieldData<ScriptDocValues>> {\n-\n-    protected static class Empty extends AtomicGeoPointFieldData<ScriptDocValues> {\n-\n-        private final int numDocs;\n-\n-        Empty(int numDocs) {\n-            this.numDocs = numDocs;\n-        }\n-\n-        @Override\n-        public boolean isMultiValued() {\n-            return false;\n-        }\n-\n-        @Override\n-        public boolean isValuesOrdered() {\n-            return false;\n-        }\n-\n-        @Override\n-        public long getNumberUniqueValues() {\n-            return 0;\n-        }\n-\n-        @Override\n-        public long getMemorySizeInBytes() {\n-            return 0;\n-        }\n-\n-        @Override\n-        public BytesValues getBytesValues(boolean needsHashes) {\n-            return BytesValues.EMPTY;\n-        }\n-\n-        @Override\n-        public GeoPointValues getGeoPointValues() {\n-            return GeoPointValues.EMPTY;\n-        }\n-\n-        @Override\n-        public ScriptDocValues getScriptValues() {\n-            return ScriptDocValues.EMPTY;\n-        }\n-\n-        @Override\n-        public int getNumDocs() {\n-            return numDocs;\n-        }\n-\n-        @Override\n-        public void close() {\n-            // no-op\n-        }\n-    }\n-\n-    protected static class GeoPointEnum {\n-\n-        private final BytesRefIterator termsEnum;\n-        private final GeoPoint next;\n-        private final CharsRef spare;\n-\n-        protected GeoPointEnum(BytesRefIterator termsEnum) {\n-            this.termsEnum = termsEnum;\n-            next = new GeoPoint();\n-            spare = new CharsRef();\n-        }\n-\n-        public GeoPoint next() throws IOException {\n-            final BytesRef term = termsEnum.next();\n-            if (term == null) {\n-                return null;\n-            }\n-            UnicodeUtil.UTF8toUTF16(term, spare);\n-            int commaIndex = -1;\n-            for (int i = 0; i < spare.length; i++) {\n-                if (spare.chars[spare.offset + i] == ',') { // safes a string creation \n-                    commaIndex = i;\n-                    break;\n-                }\n-            }\n-            if (commaIndex == -1) {\n-                assert false;\n-                return next.reset(0, 0);\n-            }\n-            final double lat = Double.parseDouble(new String(spare.chars, spare.offset, (commaIndex - spare.offset)));\n-            final double lon = Double.parseDouble(new String(spare.chars, (spare.offset + (commaIndex + 1)), spare.length - ((commaIndex + 1) - spare.offset)));\n-            return next.reset(lat, lon);\n-        }\n-\n-    }\n-\n-    public AbstractGeoPointIndexFieldData(Index index, Settings indexSettings, Names fieldNames, FieldDataType fieldDataType, IndexFieldDataCache cache) {\n-        super(index, indexSettings, fieldNames, fieldDataType, cache);\n-    }\n-\n-    @Override\n-    public boolean valuesOrdered() {\n-        // because we might have single values? we can dynamically update a flag to reflect that\n-        // based on the atomic field data loaded\n-        return false;\n-    }\n-\n-    @Override\n-    public final XFieldComparatorSource comparatorSource(@Nullable Object missingValue, SortMode sortMode) {\n-        throw new ElasticSearchIllegalArgumentException(\"can't sort on geo_point field without using specific sorting feature, like geo_distance\");\n-    }\n-\n-}\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java\ndeleted file mode 100644\nindex 9f324ad5670..00000000000\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java\n+++ /dev/null\n@@ -1,68 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.index.fielddata.plain;\n-\n-import org.apache.lucene.index.AtomicReaderContext;\n-import org.elasticsearch.ElasticSearchIllegalStateException;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.index.Index;\n-import org.elasticsearch.index.fielddata.*;\n-import org.elasticsearch.index.fielddata.fieldcomparator.SortMode;\n-import org.elasticsearch.index.mapper.FieldMapper;\n-import org.elasticsearch.index.mapper.FieldMapper.Names;\n-import org.elasticsearch.index.settings.IndexSettings;\n-\n-/**\n- * A field data implementation that forbids loading and will throw an {@link ElasticSearchIllegalStateException} if you try to load\n- * {@link AtomicFieldData} instances.\n- */\n-public final class DisabledIndexFieldData extends AbstractIndexFieldData<AtomicFieldData<?>> {\n-\n-    public static class Builder implements IndexFieldData.Builder {\n-        @Override\n-        public IndexFieldData<AtomicFieldData<?>> build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            return new DisabledIndexFieldData(index, indexSettings, mapper.names(), mapper.fieldDataType(), cache);\n-        }\n-    }\n-\n-    public DisabledIndexFieldData(Index index, Settings indexSettings, Names fieldNames, FieldDataType fieldDataType, IndexFieldDataCache cache) {\n-        super(index, indexSettings, fieldNames, fieldDataType, cache);\n-    }\n-\n-    @Override\n-    public boolean valuesOrdered() {\n-        return false;\n-    }\n-\n-    @Override\n-    public AtomicFieldData<?> loadDirect(AtomicReaderContext context) throws Exception {\n-        throw fail();\n-    }\n-\n-    @Override\n-    public IndexFieldData.XFieldComparatorSource comparatorSource(Object missingValue, SortMode sortMode) {\n-        throw fail();\n-    }\n-\n-    private ElasticSearchIllegalStateException fail() {\n-        return new ElasticSearchIllegalStateException(\"Field data loading is forbidden on \" + getFieldNames().name());\n-    }\n-\n-}\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java\nindex 8d75fd5623e..c0ee85ad4c2 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java\n@@ -24,10 +24,10 @@ import org.apache.lucene.index.IndexReader;\n import org.elasticsearch.ElasticSearchIllegalArgumentException;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.fielddata.FieldDataType;\n import org.elasticsearch.index.fielddata.IndexFieldData;\n import org.elasticsearch.index.fielddata.IndexFieldDataCache;\n import org.elasticsearch.index.fielddata.IndexNumericFieldData.NumericType;\n-import org.elasticsearch.index.mapper.FieldMapper;\n import org.elasticsearch.index.mapper.FieldMapper.Names;\n import org.elasticsearch.index.mapper.internal.IdFieldMapper;\n import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;\n@@ -93,9 +93,8 @@ public abstract class DocValuesIndexFieldData {\n         }\n \n         @Override\n-        public IndexFieldData<?> build(Index index, Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            final FieldMapper.Names fieldNames = mapper.names();\n-            final Settings fdSettings = mapper.fieldDataType().getSettings();\n+        public IndexFieldData<?> build(Index index, Settings indexSettings, Names fieldNames, FieldDataType type, IndexFieldDataCache cache) {\n+            final Settings fdSettings = type.getSettings();\n             final Map<String, Settings> filter = fdSettings.getGroups(\"filter\");\n             if (filter != null && !filter.isEmpty()) {\n                 throw new ElasticSearchIllegalArgumentException(\"Doc values field data doesn't support filters [\" + fieldNames.name() + \"]\");\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java\nindex 5cb5312660a..1c099026842 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java\n@@ -43,8 +43,8 @@ public class DoubleArrayIndexFieldData extends AbstractIndexFieldData<DoubleArra\n     public static class Builder implements IndexFieldData.Builder {\n \n         @Override\n-        public IndexFieldData<?> build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            return new DoubleArrayIndexFieldData(index, indexSettings, mapper.names(), mapper.fieldDataType(), cache);\n+        public IndexFieldData<?> build(Index index, @IndexSettings Settings indexSettings, FieldMapper.Names fieldNames, FieldDataType type, IndexFieldDataCache cache) {\n+            return new DoubleArrayIndexFieldData(index, indexSettings, fieldNames, type, cache);\n         }\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java\nindex 0a42d364ccf..4c19be218bc 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java\n@@ -43,8 +43,8 @@ public class FSTBytesIndexFieldData extends AbstractBytesIndexFieldData<FSTBytes\n     public static class Builder implements IndexFieldData.Builder {\n \n         @Override\n-        public IndexFieldData<FSTBytesAtomicFieldData> build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            return new FSTBytesIndexFieldData(index, indexSettings, mapper.names(), mapper.fieldDataType(), cache);\n+        public IndexFieldData<FSTBytesAtomicFieldData> build(Index index, @IndexSettings Settings indexSettings, FieldMapper.Names fieldNames, FieldDataType type, IndexFieldDataCache cache) {\n+            return new FSTBytesIndexFieldData(index, indexSettings, fieldNames, type, cache);\n         }\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java\nindex b0f60b85059..31f3db43183 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java\n@@ -43,8 +43,8 @@ public class FloatArrayIndexFieldData extends AbstractIndexFieldData<FloatArrayA\n     public static class Builder implements IndexFieldData.Builder {\n \n         @Override\n-        public IndexFieldData<?> build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            return new FloatArrayIndexFieldData(index, indexSettings, mapper.names(), mapper.fieldDataType(), cache);\n+        public IndexFieldData<?> build(Index index, @IndexSettings Settings indexSettings, FieldMapper.Names fieldNames, FieldDataType type, IndexFieldDataCache cache) {\n+            return new FloatArrayIndexFieldData(index, indexSettings, fieldNames, type, cache);\n         }\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedAtomicFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedAtomicFieldData.java\ndeleted file mode 100644\nindex 4899593e23d..00000000000\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedAtomicFieldData.java\n+++ /dev/null\n@@ -1,281 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.index.fielddata.plain;\n-\n-import org.apache.lucene.util.FixedBitSet;\n-import org.apache.lucene.util.RamUsageEstimator;\n-import org.apache.lucene.util.packed.PagedMutable;\n-import org.elasticsearch.common.geo.GeoPoint;\n-import org.elasticsearch.index.fielddata.AtomicGeoPointFieldData;\n-import org.elasticsearch.index.fielddata.GeoPointValues;\n-import org.elasticsearch.index.fielddata.ScriptDocValues;\n-import org.elasticsearch.index.fielddata.ordinals.Ordinals;\n-import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;\n-\n-/**\n- * Field data atomic impl for geo points with lossy compression.\n- */\n-public abstract class GeoPointCompressedAtomicFieldData extends AtomicGeoPointFieldData<ScriptDocValues> {\n-\n-    private final int numDocs;\n-\n-    protected long size = -1;\n-\n-    public GeoPointCompressedAtomicFieldData(int numDocs) {\n-        this.numDocs = numDocs;\n-    }\n-\n-    @Override\n-    public void close() {\n-    }\n-\n-    @Override\n-    public int getNumDocs() {\n-        return numDocs;\n-    }\n-\n-    @Override\n-    public ScriptDocValues getScriptValues() {\n-        return new ScriptDocValues.GeoPoints(getGeoPointValues());\n-    }\n-\n-    static class WithOrdinals extends GeoPointCompressedAtomicFieldData {\n-\n-        private final GeoPointFieldMapper.Encoding encoding;\n-        private final PagedMutable lon, lat;\n-        private final Ordinals ordinals;\n-\n-        public WithOrdinals(GeoPointFieldMapper.Encoding encoding, PagedMutable lon, PagedMutable lat, int numDocs, Ordinals ordinals) {\n-            super(numDocs);\n-            this.encoding = encoding;\n-            this.lon = lon;\n-            this.lat = lat;\n-            this.ordinals = ordinals;\n-        }\n-\n-        @Override\n-        public boolean isMultiValued() {\n-            return ordinals.isMultiValued();\n-        }\n-\n-        @Override\n-        public boolean isValuesOrdered() {\n-            return true;\n-        }\n-\n-        @Override\n-        public long getNumberUniqueValues() {\n-            return ordinals.getNumOrds();\n-        }\n-\n-        @Override\n-        public long getMemorySizeInBytes() {\n-            if (size == -1) {\n-                size = RamUsageEstimator.NUM_BYTES_INT/*size*/ + RamUsageEstimator.NUM_BYTES_INT/*numDocs*/ + lon.ramBytesUsed() + lat.ramBytesUsed();\n-            }\n-            return size;\n-        }\n-\n-        @Override\n-        public GeoPointValues getGeoPointValues() {\n-            return new GeoPointValuesWithOrdinals(encoding, lon, lat, ordinals.ordinals());\n-        }\n-\n-        public static class GeoPointValuesWithOrdinals extends GeoPointValues {\n-\n-            private final GeoPointFieldMapper.Encoding encoding;\n-            private final PagedMutable lon, lat;\n-            private final Ordinals.Docs ordinals;\n-\n-            private final GeoPoint scratch = new GeoPoint();\n-\n-            GeoPointValuesWithOrdinals(GeoPointFieldMapper.Encoding encoding, PagedMutable lon, PagedMutable lat, Ordinals.Docs ordinals) {\n-                super(ordinals.isMultiValued());\n-                this.encoding = encoding;\n-                this.lon = lon;\n-                this.lat = lat;\n-                this.ordinals = ordinals;\n-            }\n-\n-            @Override\n-            public GeoPoint nextValue() {\n-                final long ord = ordinals.nextOrd();\n-                assert ord > 0;\n-                return encoding.decode(lat.get(ord), lon.get(ord), scratch);\n-            }\n-\n-            @Override\n-            public int setDocument(int docId) {\n-                this.docId = docId;\n-                return ordinals.setDocument(docId);\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Assumes unset values are marked in bitset, and docId is used as the index to the value array.\n-     */\n-    public static class SingleFixedSet extends GeoPointCompressedAtomicFieldData {\n-\n-        private final GeoPointFieldMapper.Encoding encoding;\n-        private final PagedMutable lon, lat;\n-        private final FixedBitSet set;\n-        private final long numOrds;\n-\n-        public SingleFixedSet(GeoPointFieldMapper.Encoding encoding, PagedMutable lon, PagedMutable lat, int numDocs, FixedBitSet set, long numOrds) {\n-            super(numDocs);\n-            this.encoding = encoding;\n-            this.lon = lon;\n-            this.lat = lat;\n-            this.set = set;\n-            this.numOrds = numOrds;\n-        }\n-\n-        @Override\n-        public boolean isMultiValued() {\n-            return false;\n-        }\n-\n-        @Override\n-        public boolean isValuesOrdered() {\n-            return false;\n-        }\n-\n-        @Override\n-        public long getNumberUniqueValues() {\n-            return numOrds;\n-        }\n-\n-        @Override\n-        public long getMemorySizeInBytes() {\n-            if (size == -1) {\n-                size = RamUsageEstimator.NUM_BYTES_INT/*size*/ + RamUsageEstimator.NUM_BYTES_INT/*numDocs*/ + lon.ramBytesUsed() + lat.ramBytesUsed() + RamUsageEstimator.sizeOf(set.getBits());\n-            }\n-            return size;\n-        }\n-\n-        @Override\n-        public GeoPointValues getGeoPointValues() {\n-            return new GeoPointValuesSingleFixedSet(encoding, lon, lat, set);\n-        }\n-\n-\n-        static class GeoPointValuesSingleFixedSet extends GeoPointValues {\n-\n-            private final GeoPointFieldMapper.Encoding encoding;\n-            private final PagedMutable lat, lon;\n-            private final FixedBitSet set;\n-            private final GeoPoint scratch = new GeoPoint();\n-\n-\n-            GeoPointValuesSingleFixedSet(GeoPointFieldMapper.Encoding encoding, PagedMutable lon, PagedMutable lat, FixedBitSet set) {\n-                super(false);\n-                this.encoding = encoding;\n-                this.lon = lon;\n-                this.lat = lat;\n-                this.set = set;\n-            }\n-\n-            @Override\n-            public int setDocument(int docId) {\n-                this.docId = docId;\n-                return set.get(docId) ? 1 : 0;\n-            }\n-\n-            @Override\n-            public GeoPoint nextValue() {\n-                return encoding.decode(lat.get(docId), lon.get(docId), scratch);\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Assumes all the values are \"set\", and docId is used as the index to the value array.\n-     */\n-    public static class Single extends GeoPointCompressedAtomicFieldData {\n-\n-        private final GeoPointFieldMapper.Encoding encoding;\n-        private final PagedMutable lon, lat;\n-        private final long numOrds;\n-\n-        public Single(GeoPointFieldMapper.Encoding encoding, PagedMutable lon, PagedMutable lat, int numDocs, long numOrds) {\n-            super(numDocs);\n-            this.encoding = encoding;\n-            this.lon = lon;\n-            this.lat = lat;\n-            this.numOrds = numOrds;\n-        }\n-\n-        @Override\n-        public boolean isMultiValued() {\n-            return false;\n-        }\n-\n-        @Override\n-        public boolean isValuesOrdered() {\n-            return false;\n-        }\n-\n-        @Override\n-        public long getNumberUniqueValues() {\n-            return numOrds;\n-        }\n-\n-        @Override\n-        public long getMemorySizeInBytes() {\n-            if (size == -1) {\n-                size = RamUsageEstimator.NUM_BYTES_INT/*size*/ + RamUsageEstimator.NUM_BYTES_INT/*numDocs*/ + (lon.ramBytesUsed() + lat.ramBytesUsed());\n-            }\n-            return size;\n-        }\n-\n-        @Override\n-        public GeoPointValues getGeoPointValues() {\n-            return new GeoPointValuesSingle(encoding, lon, lat);\n-        }\n-\n-        static class GeoPointValuesSingle extends GeoPointValues {\n-\n-            private final GeoPointFieldMapper.Encoding encoding;\n-            private final PagedMutable lon, lat;\n-\n-            private final GeoPoint scratch = new GeoPoint();\n-\n-\n-            GeoPointValuesSingle(GeoPointFieldMapper.Encoding encoding, PagedMutable lon, PagedMutable lat) {\n-                super(false);\n-                this.encoding = encoding;\n-                this.lon = lon;\n-                this.lat = lat;\n-            }\n-\n-            @Override\n-            public int setDocument(int docId) {\n-                this.docId = docId;\n-                return 1;\n-            }\n-\n-            @Override\n-            public GeoPoint nextValue() {\n-                return encoding.decode(lat.get(docId), lon.get(docId), scratch);\n-            }\n-        }\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java\ndeleted file mode 100644\nindex 601c27c1276..00000000000\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java\n+++ /dev/null\n@@ -1,137 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.index.fielddata.plain;\n-\n-import org.apache.lucene.index.AtomicReader;\n-import org.apache.lucene.index.AtomicReaderContext;\n-import org.apache.lucene.index.Terms;\n-import org.apache.lucene.util.FixedBitSet;\n-import org.apache.lucene.util.packed.PackedInts;\n-import org.apache.lucene.util.packed.PagedMutable;\n-import org.elasticsearch.common.geo.GeoPoint;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.common.unit.DistanceUnit;\n-import org.elasticsearch.common.unit.DistanceUnit.Distance;\n-import org.elasticsearch.common.util.BigArrays;\n-import org.elasticsearch.index.Index;\n-import org.elasticsearch.index.fielddata.*;\n-import org.elasticsearch.index.fielddata.ordinals.Ordinals;\n-import org.elasticsearch.index.fielddata.ordinals.Ordinals.Docs;\n-import org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder;\n-import org.elasticsearch.index.mapper.FieldMapper;\n-import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;\n-import org.elasticsearch.index.settings.IndexSettings;\n-\n-/**\n- */\n-public class GeoPointCompressedIndexFieldData extends AbstractGeoPointIndexFieldData {\n-\n-    private static final String PRECISION_KEY = \"precision\";\n-    private static final Distance DEFAULT_PRECISION_VALUE = new Distance(1, DistanceUnit.CENTIMETERS);\n-\n-    public static class Builder implements IndexFieldData.Builder {\n-\n-        @Override\n-        public IndexFieldData<?> build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            FieldDataType type = mapper.fieldDataType();\n-            final String precisionAsString = type.getSettings().get(PRECISION_KEY);\n-            final Distance precision;\n-            if (precisionAsString != null) {\n-                precision = Distance.parseDistance(precisionAsString, DistanceUnit.METERS);\n-            } else {\n-                precision = DEFAULT_PRECISION_VALUE;\n-            }\n-            return new GeoPointCompressedIndexFieldData(index, indexSettings, mapper.names(), mapper.fieldDataType(), cache, precision);\n-        }\n-    }\n-\n-    private final GeoPointFieldMapper.Encoding encoding;\n-\n-    public GeoPointCompressedIndexFieldData(Index index, @IndexSettings Settings indexSettings, FieldMapper.Names fieldNames, FieldDataType fieldDataType, IndexFieldDataCache cache, Distance precision) {\n-        super(index, indexSettings, fieldNames, fieldDataType, cache);\n-        this.encoding = GeoPointFieldMapper.Encoding.of(precision);\n-    }\n-\n-    @Override\n-    public AtomicGeoPointFieldData<ScriptDocValues> loadDirect(AtomicReaderContext context) throws Exception {\n-        AtomicReader reader = context.reader();\n-\n-        Terms terms = reader.terms(getFieldNames().indexName());\n-        if (terms == null) {\n-            return new Empty(reader.maxDoc());\n-        }\n-        final long initialSize;\n-        if (terms.size() >= 0) {\n-            initialSize = 1 + terms.size();\n-        } else { // codec doesn't expose size\n-            initialSize = 1 + Math.min(1 << 12, reader.maxDoc());\n-        }\n-        final int pageSize = Integer.highestOneBit(BigArrays.PAGE_SIZE_IN_BYTES * 8 / encoding.numBitsPerCoordinate() - 1) << 1;\n-        PagedMutable lat = new PagedMutable(initialSize, pageSize, encoding.numBitsPerCoordinate(), PackedInts.COMPACT);\n-        PagedMutable lon = new PagedMutable(initialSize, pageSize, encoding.numBitsPerCoordinate(), PackedInts.COMPACT);\n-        final float acceptableTransientOverheadRatio = fieldDataType.getSettings().getAsFloat(\"acceptable_transient_overhead_ratio\", OrdinalsBuilder.DEFAULT_ACCEPTABLE_OVERHEAD_RATIO);\n-        OrdinalsBuilder builder = new OrdinalsBuilder(terms.size(), reader.maxDoc(), acceptableTransientOverheadRatio);\n-        try {\n-            final GeoPointEnum iter = new GeoPointEnum(builder.buildFromTerms(terms.iterator(null)));\n-            GeoPoint point;\n-            long ord = 0;\n-            while ((point = iter.next()) != null) {\n-                ++ord;\n-                if (lat.size() <= ord) {\n-                    final long newSize = BigArrays.overSize(ord + 1);\n-                    lat = lat.resize(newSize);\n-                    lon = lon.resize(newSize);\n-                }\n-                lat.set(ord, encoding.encodeCoordinate(point.getLat()));\n-                lon.set(ord, encoding.encodeCoordinate(point.getLon()));\n-            }\n-\n-            Ordinals build = builder.build(fieldDataType.getSettings());\n-            if (!build.isMultiValued() && CommonSettings.removeOrdsOnSingleValue(fieldDataType)) {\n-                Docs ordinals = build.ordinals();\n-                int maxDoc = reader.maxDoc();\n-                PagedMutable sLat = new PagedMutable(reader.maxDoc(), pageSize, encoding.numBitsPerCoordinate(), PackedInts.COMPACT);\n-                PagedMutable sLon = new PagedMutable(reader.maxDoc(), pageSize, encoding.numBitsPerCoordinate(), PackedInts.COMPACT);\n-                for (int i = 0; i < maxDoc; i++) {\n-                    final long nativeOrdinal = ordinals.getOrd(i);\n-                    sLat.set(i, lat.get(nativeOrdinal));\n-                    sLon.set(i, lon.get(nativeOrdinal));\n-                }\n-                FixedBitSet set = builder.buildDocsWithValuesSet();\n-                if (set == null) {\n-                    return new GeoPointCompressedAtomicFieldData.Single(encoding, sLon, sLat, reader.maxDoc(), ordinals.getNumOrds());\n-                } else {\n-                    return new GeoPointCompressedAtomicFieldData.SingleFixedSet(encoding, sLon, sLat, reader.maxDoc(), set, ordinals.getNumOrds());\n-                }\n-            } else {\n-                if (lat.size() != build.getMaxOrd()) {\n-                    lat = lat.resize(build.getMaxOrd());\n-                    lon = lon.resize(build.getMaxOrd());\n-                }\n-                return new GeoPointCompressedAtomicFieldData.WithOrdinals(encoding, lon, lat, reader.maxDoc(), build);\n-            }\n-        } finally {\n-            builder.close();\n-        }\n-\n-    }\n-\n-\n-}\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayAtomicFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayAtomicFieldData.java\nindex 16385ccc383..0c28b979413 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayAtomicFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayAtomicFieldData.java\n@@ -24,6 +24,7 @@ import org.apache.lucene.util.RamUsageEstimator;\n import org.elasticsearch.common.geo.GeoPoint;\n import org.elasticsearch.common.util.BigDoubleArrayList;\n import org.elasticsearch.index.fielddata.AtomicGeoPointFieldData;\n+import org.elasticsearch.index.fielddata.BytesValues;\n import org.elasticsearch.index.fielddata.GeoPointValues;\n import org.elasticsearch.index.fielddata.ScriptDocValues;\n import org.elasticsearch.index.fielddata.ordinals.Ordinals;\n@@ -32,6 +33,10 @@ import org.elasticsearch.index.fielddata.ordinals.Ordinals;\n  */\n public abstract class GeoPointDoubleArrayAtomicFieldData extends AtomicGeoPointFieldData<ScriptDocValues> {\n \n+    public static GeoPointDoubleArrayAtomicFieldData empty(int numDocs) {\n+        return new Empty(numDocs);\n+    }\n+\n     private final int numDocs;\n \n     protected long size = -1;\n@@ -54,6 +59,48 @@ public abstract class GeoPointDoubleArrayAtomicFieldData extends AtomicGeoPointF\n         return new ScriptDocValues.GeoPoints(getGeoPointValues());\n     }\n \n+    static class Empty extends GeoPointDoubleArrayAtomicFieldData {\n+\n+        Empty(int numDocs) {\n+            super(numDocs);\n+        }\n+\n+        @Override\n+        public boolean isMultiValued() {\n+            return false;\n+        }\n+\n+        @Override\n+        public boolean isValuesOrdered() {\n+            return false;\n+        }\n+\n+        @Override\n+        public long getNumberUniqueValues() {\n+            return 0;\n+        }\n+\n+        @Override\n+        public long getMemorySizeInBytes() {\n+            return 0;\n+        }\n+\n+        @Override\n+        public BytesValues getBytesValues(boolean needsHashes) {\n+            return BytesValues.EMPTY;\n+        }\n+\n+        @Override\n+        public GeoPointValues getGeoPointValues() {\n+            return GeoPointValues.EMPTY;\n+        }\n+\n+        @Override\n+        public ScriptDocValues getScriptValues() {\n+            return ScriptDocValues.EMPTY;\n+        }\n+    }\n+\n     static class WithOrdinals extends GeoPointDoubleArrayAtomicFieldData {\n \n         private final BigDoubleArrayList lon, lat;\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java\nindex 4ac005f0580..b6dc7c78f20 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java\n@@ -22,12 +22,14 @@ package org.elasticsearch.index.fielddata.plain;\n import org.apache.lucene.index.AtomicReader;\n import org.apache.lucene.index.AtomicReaderContext;\n import org.apache.lucene.index.Terms;\n-import org.apache.lucene.util.FixedBitSet;\n-import org.elasticsearch.common.geo.GeoPoint;\n+import org.apache.lucene.util.*;\n+import org.elasticsearch.ElasticSearchIllegalArgumentException;\n+import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.util.BigDoubleArrayList;\n import org.elasticsearch.index.Index;\n import org.elasticsearch.index.fielddata.*;\n+import org.elasticsearch.index.fielddata.fieldcomparator.SortMode;\n import org.elasticsearch.index.fielddata.ordinals.Ordinals;\n import org.elasticsearch.index.fielddata.ordinals.Ordinals.Docs;\n import org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder;\n@@ -36,13 +38,13 @@ import org.elasticsearch.index.settings.IndexSettings;\n \n /**\n  */\n-public class GeoPointDoubleArrayIndexFieldData extends AbstractGeoPointIndexFieldData {\n+public class GeoPointDoubleArrayIndexFieldData extends AbstractIndexFieldData<GeoPointDoubleArrayAtomicFieldData> implements IndexGeoPointFieldData<GeoPointDoubleArrayAtomicFieldData> {\n \n     public static class Builder implements IndexFieldData.Builder {\n \n         @Override\n-        public IndexFieldData<?> build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            return new GeoPointDoubleArrayIndexFieldData(index, indexSettings, mapper.names(), mapper.fieldDataType(), cache);\n+        public IndexFieldData<?> build(Index index, @IndexSettings Settings indexSettings, FieldMapper.Names fieldNames, FieldDataType type, IndexFieldDataCache cache) {\n+            return new GeoPointDoubleArrayIndexFieldData(index, indexSettings, fieldNames, type, cache);\n         }\n     }\n \n@@ -51,25 +53,43 @@ public class GeoPointDoubleArrayIndexFieldData extends AbstractGeoPointIndexFiel\n     }\n \n     @Override\n-    public AtomicGeoPointFieldData<ScriptDocValues> loadDirect(AtomicReaderContext context) throws Exception {\n+    public boolean valuesOrdered() {\n+        // because we might have single values? we can dynamically update a flag to reflect that\n+        // based on the atomic field data loaded\n+        return false;\n+    }\n+\n+    @Override\n+    public GeoPointDoubleArrayAtomicFieldData loadDirect(AtomicReaderContext context) throws Exception {\n         AtomicReader reader = context.reader();\n \n         Terms terms = reader.terms(getFieldNames().indexName());\n         if (terms == null) {\n-            return new Empty(reader.maxDoc());\n+            return GeoPointDoubleArrayAtomicFieldData.empty(reader.maxDoc());\n         }\n+        // TODO: how can we guess the number of terms? numerics end up creating more terms per value...\n         final BigDoubleArrayList lat = new BigDoubleArrayList();\n         final BigDoubleArrayList lon = new BigDoubleArrayList();\n         lat.add(0); // first \"t\" indicates null value\n         lon.add(0); // first \"t\" indicates null value\n         final float acceptableTransientOverheadRatio = fieldDataType.getSettings().getAsFloat(\"acceptable_transient_overhead_ratio\", OrdinalsBuilder.DEFAULT_ACCEPTABLE_OVERHEAD_RATIO);\n         OrdinalsBuilder builder = new OrdinalsBuilder(terms.size(), reader.maxDoc(), acceptableTransientOverheadRatio);\n+        final CharsRef spare = new CharsRef();\n         try {\n-            final GeoPointEnum iter = new GeoPointEnum(builder.buildFromTerms(terms.iterator(null)));\n-            GeoPoint point;\n-            while ((point = iter.next()) != null) {\n-                lat.add(point.getLat());\n-                lon.add(point.getLon());\n+            BytesRefIterator iter = builder.buildFromTerms(terms.iterator(null));\n+            BytesRef term;\n+            while ((term = iter.next()) != null) {\n+                UnicodeUtil.UTF8toUTF16(term, spare);\n+                boolean parsed = false;\n+                for (int i = spare.offset; i < spare.length; i++) {\n+                    if (spare.chars[i] == ',') { // safes a string creation \n+                        lat.add(Double.parseDouble(new String(spare.chars, spare.offset, (i - spare.offset))));\n+                        lon.add(Double.parseDouble(new String(spare.chars, (spare.offset + (i + 1)), spare.length - ((i + 1) - spare.offset))));\n+                        parsed = true;\n+                        break;\n+                    }\n+                }\n+                assert parsed;\n             }\n \n             Ordinals build = builder.build(fieldDataType.getSettings());\n@@ -99,4 +119,9 @@ public class GeoPointDoubleArrayIndexFieldData extends AbstractGeoPointIndexFiel\n         }\n \n     }\n+\n+    @Override\n+    public XFieldComparatorSource comparatorSource(@Nullable Object missingValue, SortMode sortMode) {\n+        throw new ElasticSearchIllegalArgumentException(\"can't sort on geo_point field without using specific sorting feature, like geo_distance\");\n+    }\n }\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java\nindex 499cee8f5ad..04ba5dc9089 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java\n@@ -58,8 +58,8 @@ public class PackedArrayIndexFieldData extends AbstractIndexFieldData<AtomicNume\n         }\n \n         @Override\n-        public IndexFieldData<AtomicNumericFieldData> build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            return new PackedArrayIndexFieldData(index, indexSettings, mapper.names(), mapper.fieldDataType(), cache, numericType);\n+        public IndexFieldData<AtomicNumericFieldData> build(Index index, @IndexSettings Settings indexSettings, FieldMapper.Names fieldNames, FieldDataType type, IndexFieldDataCache cache) {\n+            return new PackedArrayIndexFieldData(index, indexSettings, fieldNames, type, cache, numericType);\n         }\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java b/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java\nindex ba47e202051..6ddf519e0f7 100644\n--- a/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java\n+++ b/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java\n@@ -40,8 +40,8 @@ public class PagedBytesIndexFieldData extends AbstractBytesIndexFieldData<PagedB\n     public static class Builder implements IndexFieldData.Builder {\n \n         @Override\n-        public IndexFieldData<PagedBytesAtomicFieldData> build(Index index, @IndexSettings Settings indexSettings, FieldMapper<?> mapper, IndexFieldDataCache cache) {\n-            return new PagedBytesIndexFieldData(index, indexSettings, mapper.names(), mapper.fieldDataType(), cache);\n+        public IndexFieldData<PagedBytesAtomicFieldData> build(Index index, @IndexSettings Settings indexSettings, FieldMapper.Names fieldNames, FieldDataType type, IndexFieldDataCache cache) {\n+            return new PagedBytesIndexFieldData(index, indexSettings, fieldNames, type, cache);\n         }\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java b/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java\nindex 7808b60e08c..85300b437d9 100644\n--- a/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java\n+++ b/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java\n@@ -29,15 +29,12 @@ import org.apache.lucene.document.FieldType;\n import org.apache.lucene.index.IndexableField;\n import org.apache.lucene.search.Filter;\n import org.apache.lucene.util.CloseableThreadLocal;\n-import org.elasticsearch.ElasticSearchGenerationException;\n import org.elasticsearch.common.Booleans;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.Preconditions;\n import org.elasticsearch.common.bytes.BytesReference;\n import org.elasticsearch.common.collect.MapBuilder;\n import org.elasticsearch.common.compress.CompressedString;\n-import org.elasticsearch.common.compress.CompressorFactory;\n-import org.elasticsearch.common.io.stream.BytesStreamOutput;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.text.StringAndBytesText;\n import org.elasticsearch.common.text.Text;\n@@ -658,17 +655,15 @@ public class DocumentMapper implements ToXContent {\n         return new MergeResult(mergeContext.buildConflicts());\n     }\n \n-    public CompressedString refreshSource() throws ElasticSearchGenerationException {\n+    public CompressedString refreshSource() throws FailedToGenerateSourceMapperException {\n         try {\n-            BytesStreamOutput bStream = new BytesStreamOutput();\n-            XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON, CompressorFactory.defaultCompressor().streamOutput(bStream));\n+            XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);\n             builder.startObject();\n             toXContent(builder, ToXContent.EMPTY_PARAMS);\n             builder.endObject();\n-            builder.close();\n-            return mappingSource = new CompressedString(bStream.bytes());\n+            return mappingSource = new CompressedString(builder.bytes());\n         } catch (Exception e) {\n-            throw new ElasticSearchGenerationException(\"failed to serialize source for type [\" + type + \"]\", e);\n+            throw new FailedToGenerateSourceMapperException(e.getMessage(), e);\n         }\n     }\n \ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesUtils.java b/src/main/java/org/elasticsearch/index/mapper/FailedToGenerateSourceMapperException.java\nsimilarity index 61%\nrename from src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesUtils.java\nrename to src/main/java/org/elasticsearch/index/mapper/FailedToGenerateSourceMapperException.java\nindex 1d8a0d53ad2..11521150757 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesUtils.java\n+++ b/src/main/java/org/elasticsearch/index/mapper/FailedToGenerateSourceMapperException.java\n@@ -16,21 +16,19 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.elasticsearch.cluster.routing.allocation;\n \n-import org.elasticsearch.cluster.routing.RoutingNode;\n-import org.elasticsearch.cluster.routing.RoutingNodes;\n-import org.elasticsearch.cluster.routing.ShardRoutingState;\n+package org.elasticsearch.index.mapper;\n \n /**\n+ *\n  */\n-public class RoutingNodesUtils {\n+public class FailedToGenerateSourceMapperException extends MapperException {\n+\n+    public FailedToGenerateSourceMapperException(String message) {\n+        super(message);\n+    }\n \n-    public static int numberOfShardsOfType(RoutingNodes nodes, ShardRoutingState state) {\n-        int count = 0;\n-        for (RoutingNode routingNode : nodes) {\n-            count += routingNode.numberOfShardsWithState(state);\n-        }\n-        return count;\n+    public FailedToGenerateSourceMapperException(String message, Throwable cause) {\n+        super(message, cause);\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/index/mapper/MapperService.java b/src/main/java/org/elasticsearch/index/mapper/MapperService.java\nindex 7014cb947c8..f7bb47f3598 100644\n--- a/src/main/java/org/elasticsearch/index/mapper/MapperService.java\n+++ b/src/main/java/org/elasticsearch/index/mapper/MapperService.java\n@@ -48,7 +48,6 @@ import org.elasticsearch.index.Index;\n import org.elasticsearch.index.analysis.AnalysisService;\n import org.elasticsearch.index.codec.docvaluesformat.DocValuesFormatService;\n import org.elasticsearch.index.codec.postingsformat.PostingsFormatService;\n-import org.elasticsearch.index.fielddata.IndexFieldDataService;\n import org.elasticsearch.index.mapper.internal.TypeFieldMapper;\n import org.elasticsearch.index.mapper.object.ObjectMapper;\n import org.elasticsearch.index.search.nested.NonNestedDocsFilter;\n@@ -77,7 +76,6 @@ public class MapperService extends AbstractIndexComponent implements Iterable<Do\n     public static final String DEFAULT_MAPPING = \"_default_\";\n \n     private final AnalysisService analysisService;\n-    private final IndexFieldDataService fieldDataService;\n \n     /**\n      * Will create types automatically if they do not exists in the mapping definition yet\n@@ -107,11 +105,10 @@ public class MapperService extends AbstractIndexComponent implements Iterable<Do\n     private final List<DocumentTypeListener> typeListeners = new CopyOnWriteArrayList<DocumentTypeListener>();\n \n     @Inject\n-    public MapperService(Index index, @IndexSettings Settings indexSettings, Environment environment, AnalysisService analysisService, IndexFieldDataService fieldDataService,\n+    public MapperService(Index index, @IndexSettings Settings indexSettings, Environment environment, AnalysisService analysisService,\n                          PostingsFormatService postingsFormatService, DocValuesFormatService docValuesFormatService, SimilarityLookupService similarityLookupService) {\n         super(index, indexSettings);\n         this.analysisService = analysisService;\n-        this.fieldDataService = fieldDataService;\n         this.documentParser = new DocumentMapperParser(index, indexSettings, analysisService, postingsFormatService, docValuesFormatService, similarityLookupService);\n         this.searchAnalyzer = new SmartIndexNameSearchAnalyzer(analysisService.defaultSearchAnalyzer());\n         this.searchQuoteAnalyzer = new SmartIndexNameSearchQuoteAnalyzer(analysisService.defaultSearchQuoteAnalyzer());\n@@ -281,7 +278,6 @@ public class MapperService extends AbstractIndexComponent implements Iterable<Do\n                         logger.debug(\"merging mapping for type [{}] resulted in conflicts: [{}]\", mapper.type(), Arrays.toString(result.conflicts()));\n                     }\n                 }\n-                fieldDataService.onMappingUpdate();\n                 return oldMapper;\n             } else {\n                 FieldMapperListener.Aggregator fieldMappersAgg = new FieldMapperListener.Aggregator();\ndiff --git a/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java b/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java\nindex 2254ca02579..4c1cfde3fdb 100644\n--- a/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java\n+++ b/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java\n@@ -19,19 +19,15 @@\n \n package org.elasticsearch.index.mapper.geo;\n \n-import org.apache.lucene.document.Field;\n import org.apache.lucene.document.FieldType;\n import org.apache.lucene.index.FieldInfo.IndexOptions;\n import org.elasticsearch.ElasticSearchIllegalArgumentException;\n-import org.elasticsearch.ElasticSearchIllegalStateException;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.Strings;\n-import org.elasticsearch.common.geo.GeoDistance;\n import org.elasticsearch.common.geo.GeoHashUtils;\n import org.elasticsearch.common.geo.GeoPoint;\n import org.elasticsearch.common.geo.GeoUtils;\n import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.common.unit.DistanceUnit;\n import org.elasticsearch.common.xcontent.XContentBuilder;\n import org.elasticsearch.common.xcontent.XContentParser;\n import org.elasticsearch.common.xcontent.support.XContentMapValues;\n@@ -45,16 +41,14 @@ import org.elasticsearch.index.mapper.core.DoubleFieldMapper;\n import org.elasticsearch.index.mapper.core.NumberFieldMapper;\n import org.elasticsearch.index.mapper.core.StringFieldMapper;\n import org.elasticsearch.index.mapper.object.ArrayValueMapperParser;\n-import org.elasticsearch.index.similarity.SimilarityProvider;\n \n import java.io.IOException;\n-import java.util.List;\n import java.util.Locale;\n import java.util.Map;\n \n import static org.elasticsearch.index.mapper.MapperBuilders.*;\n-import static org.elasticsearch.index.mapper.core.TypeParsers.parseField;\n import static org.elasticsearch.index.mapper.core.TypeParsers.parsePathType;\n+import static org.elasticsearch.index.mapper.core.TypeParsers.parseStore;\n \n /**\n  * Parsing: We handle:\n@@ -66,7 +60,7 @@ import static org.elasticsearch.index.mapper.core.TypeParsers.parsePathType;\n  * \"lon\" : 2.1\n  * }\n  */\n-public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implements ArrayValueMapperParser {\n+public class GeoPointFieldMapper implements Mapper, ArrayValueMapperParser {\n \n     public static final String CONTENT_TYPE = \"geo_point\";\n \n@@ -85,7 +79,7 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         public static final boolean ENABLE_LATLON = false;\n         public static final boolean ENABLE_GEOHASH = false;\n         public static final boolean ENABLE_GEOHASH_PREFIX = false;\n-        public static final int GEO_HASH_PRECISION = GeoHashUtils.PRECISION;\n+        public static final int PRECISION = GeoHashUtils.PRECISION;\n         public static final boolean NORMALIZE_LAT = true;\n         public static final boolean NORMALIZE_LON = true;\n         public static final boolean VALIDATE_LAT = true;\n@@ -102,7 +96,7 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         }\n     }\n \n-    public static class Builder extends AbstractFieldMapper.Builder<Builder, GeoPointFieldMapper> {\n+    public static class Builder extends Mapper.Builder<Builder, GeoPointFieldMapper> {\n \n         private ContentPath.Type pathType = Defaults.PATH_TYPE;\n \n@@ -114,7 +108,9 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n \n         private Integer precisionStep;\n \n-        private int geoHashPrecision = Defaults.GEO_HASH_PRECISION;\n+        private int precision = Defaults.PRECISION;\n+\n+        private boolean store = Defaults.STORE;\n \n         boolean validateLat = Defaults.VALIDATE_LAT;\n         boolean validateLon = Defaults.VALIDATE_LON;\n@@ -122,7 +118,7 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         boolean normalizeLon = Defaults.NORMALIZE_LON;\n \n         public Builder(String name) {\n-            super(name, new FieldType(Defaults.FIELD_TYPE));\n+            super(name);\n             this.builder = this;\n         }\n \n@@ -151,14 +147,14 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n             return this;\n         }\n \n-        public Builder geoHashPrecision(int precision) {\n-            this.geoHashPrecision = precision;\n+        public Builder precision(int precision) {\n+            this.precision = precision;\n             return this;\n         }\n \n-        public Builder fieldDataSettings(Settings settings) {\n-            this.fieldDataSettings = settings;\n-            return builder;\n+        public Builder store(boolean store) {\n+            this.store = store;\n+            return this;\n         }\n \n         @Override\n@@ -166,6 +162,9 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n             ContentPath.Type origPathType = context.path().pathType();\n             context.path().pathType(pathType);\n \n+            GeoStringFieldMapper geoStringMapper = new GeoStringFieldMapper.Builder(name)\n+                    .includeInAll(false).store(store).build(context);\n+\n             DoubleFieldMapper latMapper = null;\n             DoubleFieldMapper lonMapper = null;\n \n@@ -177,8 +176,8 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n                     latMapperBuilder.precisionStep(precisionStep);\n                     lonMapperBuilder.precisionStep(precisionStep);\n                 }\n-                latMapper = (DoubleFieldMapper) latMapperBuilder.includeInAll(false).store(fieldType.stored()).build(context);\n-                lonMapper = (DoubleFieldMapper) lonMapperBuilder.includeInAll(false).store(fieldType.stored()).build(context);\n+                latMapper = (DoubleFieldMapper) latMapperBuilder.includeInAll(false).store(store).build(context);\n+                lonMapper = (DoubleFieldMapper) lonMapperBuilder.includeInAll(false).store(store).build(context);\n             }\n             StringFieldMapper geohashMapper = null;\n             if (enableGeoHash) {\n@@ -188,11 +187,9 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n \n             context.path().pathType(origPathType);\n \n-            // this is important: even if geo points feel like they need to be tokenized to distinguish lat from lon, we actually want to\n-            // store them as a single token.\n-            fieldType.setTokenized(false);\n-\n-            return new GeoPointFieldMapper(buildNames(context), fieldType, indexAnalyzer, searchAnalyzer, postingsProvider, docValuesProvider, similarity, fieldDataSettings, context.indexSettings(), origPathType, enableLatLon, enableGeoHash, enableGeohashPrefix, precisionStep, geoHashPrecision, latMapper, lonMapper, geohashMapper, validateLon, validateLat, normalizeLon, normalizeLat);\n+            return new GeoPointFieldMapper(name, pathType, enableLatLon, enableGeoHash, enableGeohashPrefix, precisionStep, precision,\n+                    latMapper, lonMapper, geohashMapper, geoStringMapper,\n+                    validateLon, validateLat, normalizeLon, normalizeLat);\n         }\n     }\n \n@@ -200,12 +197,14 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         @Override\n         public Mapper.Builder<?, ?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException {\n             Builder builder = geoPointField(name);\n-            parseField(builder, name, node, parserContext);\n+\n             for (Map.Entry<String, Object> entry : node.entrySet()) {\n                 String fieldName = Strings.toUnderscoreCase(entry.getKey());\n                 Object fieldNode = entry.getValue();\n                 if (fieldName.equals(\"path\")) {\n                     builder.pathType(parsePathType(name, fieldNode.toString()));\n+                } else if (fieldName.equals(\"store\")) {\n+                    builder.store(parseStore(name, fieldNode.toString()));\n                 } else if (fieldName.equals(\"lat_lon\")) {\n                     builder.enableLatLon(XContentMapValues.nodeBooleanValue(fieldNode));\n                 } else if (fieldName.equals(\"geohash\")) {\n@@ -218,7 +217,7 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n                 } else if (fieldName.equals(\"precision_step\")) {\n                     builder.precisionStep(XContentMapValues.nodeIntegerValue(fieldNode));\n                 } else if (fieldName.equals(\"geohash_precision\")) {\n-                    builder.geoHashPrecision(XContentMapValues.nodeIntegerValue(fieldNode));\n+                    builder.precision(XContentMapValues.nodeIntegerValue(fieldNode));\n                 } else if (fieldName.equals(\"validate\")) {\n                     builder.validateLat = XContentMapValues.nodeBooleanValue(fieldNode);\n                     builder.validateLon = XContentMapValues.nodeBooleanValue(fieldNode);\n@@ -239,130 +238,7 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         }\n     }\n \n-    /**\n-     * A byte-aligned fixed-length encoding for latitudes and longitudes.\n-     */\n-    public static final class Encoding {\n-\n-        // With 14 bytes we already have better precision than a double since a double has 11 bits of exponent\n-        private static final int MAX_NUM_BYTES = 14;\n-\n-        private static final Encoding[] INSTANCES;\n-        static {\n-            INSTANCES = new Encoding[MAX_NUM_BYTES + 1];\n-            for (int numBytes = 2; numBytes <= MAX_NUM_BYTES; numBytes += 2) {\n-                INSTANCES[numBytes] = new Encoding(numBytes);\n-            }\n-        }\n-\n-        /** Get an instance based on the number of bytes that has been used to encode values. */\n-        public static final Encoding of(int numBytesPerValue) {\n-            final Encoding instance = INSTANCES[numBytesPerValue];\n-            if (instance == null) {\n-                throw new ElasticSearchIllegalStateException(\"No encoding for \" + numBytesPerValue + \" bytes per value\");\n-            }\n-            return instance;\n-        }\n-\n-        /** Get an instance based on the expected precision. Here are examples of the number of required bytes per value depending on the\n-         *  expected precision:<ul>\n-         *  <li>1km: 4 bytes</li>\n-         *  <li>3m: 6 bytes</li>\n-         *  <li>1m: 8 bytes</li>\n-         *  <li>1cm: 8 bytes</li>\n-         *  <li>1mm: 10 bytes</li></ul> */\n-        public static final Encoding of(DistanceUnit.Distance precision) {\n-            for (Encoding encoding : INSTANCES) {\n-                if (encoding != null && encoding.precision().compareTo(precision) <= 0) {\n-                    return encoding;\n-                }\n-            }\n-            return INSTANCES[MAX_NUM_BYTES];\n-        }\n-\n-        private final DistanceUnit.Distance precision;\n-        private final int numBytes;\n-        private final int numBytesPerCoordinate;\n-        private final double factor;\n-\n-        private Encoding(int numBytes) {\n-            assert numBytes >= 1 && numBytes <= MAX_NUM_BYTES;\n-            assert (numBytes & 1) == 0; // we don't support odd numBytes for the moment\n-            this.numBytes = numBytes;\n-            this.numBytesPerCoordinate = numBytes / 2;\n-            this.factor = Math.pow(2, - numBytesPerCoordinate * 8 + 9);\n-            assert (1L << (numBytesPerCoordinate * 8 - 1)) * factor > 180 && (1L << (numBytesPerCoordinate * 8 - 2)) * factor < 180 : numBytesPerCoordinate + \" \" + factor;\n-            if (numBytes == MAX_NUM_BYTES) {\n-                // no precision loss compared to a double\n-                precision = new DistanceUnit.Distance(0, DistanceUnit.METERS);\n-            } else {\n-                precision = new DistanceUnit.Distance(\n-                        GeoDistance.PLANE.calculate(0, 0, factor / 2, factor / 2, DistanceUnit.METERS), // factor/2 because we use Math.round instead of a cast to convert the double to a long\n-                        DistanceUnit.METERS);\n-            }\n-        }\n-\n-        public DistanceUnit.Distance precision() {\n-            return precision;\n-        }\n-\n-        /** The number of bytes required to encode a single geo point. */\n-        public final int numBytes() {\n-            return numBytes;\n-        }\n-\n-        /** The number of bits required to encode a single coordinate of a geo point. */\n-        public int numBitsPerCoordinate() {\n-            return numBytesPerCoordinate << 3;\n-        }\n-\n-        /** Return the bits that encode a latitude/longitude. */\n-        public long encodeCoordinate(double lat) {\n-            return Math.round((lat + 180) / factor);\n-        }\n-\n-        /** Decode a sequence of bits into the original coordinate. */\n-        public double decodeCoordinate(long bits) {\n-            return bits * factor - 180;\n-        }\n-\n-        private void encodeBits(long bits, byte[] out, int offset) {\n-            for (int i = 0; i < numBytesPerCoordinate; ++i) {\n-                out[offset++] = (byte) bits;\n-                bits >>>= 8;\n-            }\n-            assert bits == 0;\n-        }\n-\n-        private long decodeBits(byte [] in, int offset) {\n-            long r = in[offset++] & 0xFFL;\n-            for (int i = 1; i < numBytesPerCoordinate; ++i) {\n-                r = (in[offset++] & 0xFFL) << (i * 8);\n-            }\n-            return r;\n-        }\n-\n-        /** Encode a geo point into a byte-array, over {@link #numBytes()} bytes. */\n-        public void encode(double lat, double lon, byte[] out, int offset) {\n-            encodeBits(encodeCoordinate(lat), out, offset);\n-            encodeBits(encodeCoordinate(lon), out, offset + numBytesPerCoordinate);\n-        }\n-\n-        /** Decode a geo point from a byte-array, reading {@link #numBytes()} bytes. */\n-        public GeoPoint decode(byte[] in, int offset, GeoPoint out) {\n-            final long latBits = decodeBits(in, offset);\n-            final long lonBits = decodeBits(in, offset + numBytesPerCoordinate);\n-            return decode(latBits, lonBits, out);\n-        }\n-\n-        /** Decode a geo point from the bits of the encoded latitude and longitudes. */\n-        public GeoPoint decode(long latBits, long lonBits, GeoPoint out) {\n-            final double lat = decodeCoordinate(latBits);\n-            final double lon = decodeCoordinate(lonBits);\n-            return out.reset(lat, lon);\n-        }\n-\n-    }\n+    private final String name;\n \n     private final ContentPath.Type pathType;\n \n@@ -374,7 +250,7 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n \n     private final Integer precisionStep;\n \n-    private final int geoHashPrecision;\n+    private final int precision;\n \n     private final DoubleFieldMapper latMapper;\n \n@@ -382,56 +258,43 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n \n     private final StringFieldMapper geohashMapper;\n \n+    private final GeoStringFieldMapper geoStringMapper;\n+\n     private final boolean validateLon;\n     private final boolean validateLat;\n \n     private final boolean normalizeLon;\n     private final boolean normalizeLat;\n \n-    public GeoPointFieldMapper(FieldMapper.Names names, FieldType fieldType,\n-            NamedAnalyzer indexAnalyzer, NamedAnalyzer searchAnalyzer,\n-            PostingsFormatProvider postingsFormat, DocValuesFormatProvider docValuesFormat,\n-            SimilarityProvider similarity, @Nullable Settings fieldDataSettings, Settings indexSettings,\n-            ContentPath.Type pathType, boolean enableLatLon, boolean enableGeoHash, boolean enableGeohashPrefix, Integer precisionStep, int geoHashPrecision,\n-            DoubleFieldMapper latMapper, DoubleFieldMapper lonMapper, StringFieldMapper geohashMapper,\n-            boolean validateLon, boolean validateLat,\n-            boolean normalizeLon, boolean normalizeLat) {\n-        super(names, 1f, fieldType, null, indexAnalyzer, postingsFormat, docValuesFormat, similarity, fieldDataSettings, indexSettings);\n+    public GeoPointFieldMapper(String name, ContentPath.Type pathType, boolean enableLatLon, boolean enableGeoHash, boolean enableGeohashPrefix, Integer precisionStep, int precision,\n+                               DoubleFieldMapper latMapper, DoubleFieldMapper lonMapper, StringFieldMapper geohashMapper, GeoStringFieldMapper geoStringMapper,\n+                               boolean validateLon, boolean validateLat,\n+                               boolean normalizeLon, boolean normalizeLat) {\n+        this.name = name;\n         this.pathType = pathType;\n         this.enableLatLon = enableLatLon;\n         this.enableGeoHash = enableGeoHash || enableGeohashPrefix; // implicitly enable geohashes if geohash_prefix is set\n         this.enableGeohashPrefix = enableGeohashPrefix;\n         this.precisionStep = precisionStep;\n-        this.geoHashPrecision = geoHashPrecision;\n+        this.precision = precision;\n \n         this.latMapper = latMapper;\n         this.lonMapper = lonMapper;\n+        this.geoStringMapper = geoStringMapper;\n         this.geohashMapper = geohashMapper;\n \n+        this.geoStringMapper.geoMapper = this;\n+\n         this.validateLat = validateLat;\n         this.validateLon = validateLon;\n \n         this.normalizeLat = normalizeLat;\n         this.normalizeLon = normalizeLon;\n-\n-        if (hasDocValues()) {\n-            throw new ElasticSearchIllegalStateException(\"Geo points don't support doc values\"); // yet\n-        }\n     }\n \n     @Override\n-    protected String contentType() {\n-        return CONTENT_TYPE;\n-    }\n-\n-    @Override\n-    public FieldType defaultFieldType() {\n-        return Defaults.FIELD_TYPE;\n-    }\n-\n-    @Override\n-    public FieldDataType defaultFieldDataType() {\n-        return new FieldDataType(\"geo_point\");\n+    public String name() {\n+        return this.name;\n     }\n \n     public DoubleFieldMapper latMapper() {\n@@ -442,6 +305,10 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         return lonMapper;\n     }\n \n+    public GeoStringFieldMapper stringMapper() {\n+        return this.geoStringMapper;\n+    }\n+\n     public StringFieldMapper geoHashStringMapper() {\n         return this.geohashMapper;\n     }\n@@ -454,25 +321,11 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         return enableGeohashPrefix;\n     }\n \n-    @Override\n-    public GeoPoint value(Object value) {\n-        if (value instanceof GeoPoint) {\n-            return (GeoPoint) value;\n-        } else {\n-            return GeoPoint.parseFromLatLon(value.toString());\n-        }\n-    }\n-\n-    @Override\n-    protected void parseCreateField(ParseContext context, List<Field> fields) throws IOException {\n-        throw new UnsupportedOperationException(\"Parsing is implemented in parse(), this method should NEVER be called\");\n-    }\n-\n     @Override\n     public void parse(ParseContext context) throws IOException {\n         ContentPath.Type origPathType = context.path().pathType();\n         context.path().pathType(pathType);\n-        context.path().add(name());\n+        context.path().add(name);\n \n         XContentParser.Token token = context.parser().currentToken();\n         if (token == XContentParser.Token.START_ARRAY) {\n@@ -560,7 +413,7 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n     }\n \n     private void parseGeohashField(ParseContext context, String geohash) throws IOException {\n-        int len = Math.min(geoHashPrecision, geohash.length());\n+        int len = Math.min(precision, geohash.length());\n         int min = enableGeohashPrefix ? 1 : geohash.length();\n \n         for (int i = len; i >= min; i--) {\n@@ -580,21 +433,19 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n \n         if (validateLat) {\n             if (lat > 90.0 || lat < -90.0) {\n-                throw new ElasticSearchIllegalArgumentException(\"illegal latitude value [\" + lat + \"] for \" + name());\n+                throw new ElasticSearchIllegalArgumentException(\"illegal latitude value [\" + lat + \"] for \" + name);\n             }\n         }\n         if (validateLon) {\n             if (lon > 180.0 || lon < -180) {\n-                throw new ElasticSearchIllegalArgumentException(\"illegal longitude value [\" + lon + \"] for \" + name());\n+                throw new ElasticSearchIllegalArgumentException(\"illegal longitude value [\" + lon + \"] for \" + name);\n             }\n         }\n \n-        if (fieldType.indexed() || fieldType.stored()) {\n-            Field field = new Field(names.indexName(), Double.toString(lat) + ',' + Double.toString(lon), fieldType);\n-            context.doc().add(field);\n-        }\n+        context.externalValue(Double.toString(lat) + ',' + Double.toString(lon));\n+        geoStringMapper.parse(context);\n         if (enableGeoHash) {\n-            parseGeohashField(context, GeoHashUtils.encode(lat, lon, geoHashPrecision));\n+            parseGeohashField(context, GeoHashUtils.encode(lat, lon, precision));\n         }\n         if (enableLatLon) {\n             context.externalValue(lat);\n@@ -613,19 +464,17 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n \n         if (validateLat) {\n             if (point.lat() > 90.0 || point.lat() < -90.0) {\n-                throw new ElasticSearchIllegalArgumentException(\"illegal latitude value [\" + point.lat() + \"] for \" + name());\n+                throw new ElasticSearchIllegalArgumentException(\"illegal latitude value [\" + point.lat() + \"] for \" + name);\n             }\n         }\n         if (validateLon) {\n             if (point.lon() > 180.0 || point.lon() < -180) {\n-                throw new ElasticSearchIllegalArgumentException(\"illegal longitude value [\" + point.lon() + \"] for \" + name());\n+                throw new ElasticSearchIllegalArgumentException(\"illegal longitude value [\" + point.lon() + \"] for \" + name);\n             }\n         }\n \n-        if (fieldType.indexed() || fieldType.stored()) {\n-            Field field = new Field(names.indexName(), Double.toString(point.lat()) + ',' + Double.toString(point.lon()), fieldType);\n-            context.doc().add(field);\n-        }\n+        context.externalValue(Double.toString(point.lat()) + ',' + Double.toString(point.lon()));\n+        geoStringMapper.parse(context);\n         if (enableGeoHash) {\n             parseGeohashField(context, geohash);\n         }\n@@ -648,17 +497,19 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         if (geohashMapper != null) {\n             geohashMapper.close();\n         }\n+        if (geoStringMapper != null) {\n+            geoStringMapper.close();\n+        }\n     }\n \n     @Override\n     public void merge(Mapper mergeWith, MergeContext mergeContext) throws MergeMappingException {\n-        super.merge(mergeWith, mergeContext);\n-        // TODO: geo-specific properties\n+        // TODO\n     }\n \n     @Override\n     public void traverse(FieldMapperListener fieldMapperListener) {\n-        super.traverse(fieldMapperListener);\n+        geoStringMapper.traverse(fieldMapperListener);\n         if (enableGeoHash) {\n             geohashMapper.traverse(fieldMapperListener);\n         }\n@@ -673,8 +524,8 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n     }\n \n     @Override\n-    protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException {\n-        super.doXContentBody(builder, includeDefaults, params);\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject(name);\n         builder.field(\"type\", CONTENT_TYPE);\n         if (pathType != Defaults.PATH_TYPE) {\n             builder.field(\"path\", pathType.name().toLowerCase(Locale.ROOT));\n@@ -688,8 +539,11 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n         if (enableGeohashPrefix != Defaults.ENABLE_GEOHASH_PREFIX) {\n             builder.field(\"geohash_prefix\", enableGeohashPrefix);\n         }\n-        if (geoHashPrecision != Defaults.GEO_HASH_PRECISION) {\n-            builder.field(\"geohash_precision\", geoHashPrecision);\n+        if (geoStringMapper.fieldType().stored() != Defaults.STORE) {\n+            builder.field(\"store\", geoStringMapper.fieldType().stored());\n+        }\n+        if (precision != Defaults.PRECISION) {\n+            builder.field(\"geohash_precision\", precision);\n         }\n         if (precisionStep != null) {\n             builder.field(\"precision_step\", precisionStep);\n@@ -714,6 +568,66 @@ public class GeoPointFieldMapper extends AbstractFieldMapper<GeoPoint> implement\n                 builder.field(\"normalize_lon\", normalizeLon);\n             }\n         }\n+\n+        builder.endObject();\n+        return builder;\n     }\n \n+    public static class GeoStringFieldMapper extends StringFieldMapper {\n+\n+        public static class Builder extends AbstractFieldMapper.Builder<Builder, StringFieldMapper> {\n+\n+            protected String nullValue = Defaults.NULL_VALUE;\n+\n+            public Builder(String name) {\n+                super(name, new FieldType(GeoPointFieldMapper.Defaults.FIELD_TYPE));\n+                builder = this;\n+            }\n+\n+            public Builder nullValue(String nullValue) {\n+                this.nullValue = nullValue;\n+                return this;\n+            }\n+\n+            @Override\n+            public GeoStringFieldMapper build(BuilderContext context) {\n+                GeoStringFieldMapper fieldMapper = new GeoStringFieldMapper(buildNames(context), boost, fieldType, nullValue,\n+                        indexAnalyzer, searchAnalyzer, postingsProvider, docValuesProvider, fieldDataSettings, context.indexSettings());\n+                fieldMapper.includeInAll(includeInAll);\n+                return fieldMapper;\n+            }\n+        }\n+\n+        GeoPointFieldMapper geoMapper;\n+\n+        public GeoStringFieldMapper(Names names, float boost, FieldType fieldType, String nullValue,\n+                                    NamedAnalyzer indexAnalyzer, NamedAnalyzer searchAnalyzer,\n+                                    PostingsFormatProvider postingsProvider, DocValuesFormatProvider docValuesProvider,\n+                                    @Nullable Settings fieldDataSettings, Settings indexSettings) {\n+            super(names, boost, fieldType, nullValue, indexAnalyzer, searchAnalyzer, searchAnalyzer, Defaults.POSITION_OFFSET_GAP, Defaults.IGNORE_ABOVE,\n+                    postingsProvider, docValuesProvider, null, fieldDataSettings, indexSettings);\n+            if (hasDocValues()) {\n+                throw new MapperParsingException(\"Field [\" + names.fullName() + \"] cannot have doc values\");\n+            }\n+        }\n+\n+        @Override\n+        public FieldType defaultFieldType() {\n+            return GeoPointFieldMapper.Defaults.FIELD_TYPE;\n+        }\n+\n+        @Override\n+        public boolean hasDocValues() {\n+            return false;\n+        }\n+\n+        @Override\n+        public FieldDataType defaultFieldDataType() {\n+            return new FieldDataType(\"geo_point\");\n+        }\n+\n+        public GeoPointFieldMapper geoMapper() {\n+            return geoMapper;\n+        }\n+    }\n }\ndiff --git a/src/main/java/org/elasticsearch/index/percolator/stats/PercolateStats.java b/src/main/java/org/elasticsearch/index/percolator/stats/PercolateStats.java\nindex eb613a73fb0..b126116883e 100644\n--- a/src/main/java/org/elasticsearch/index/percolator/stats/PercolateStats.java\n+++ b/src/main/java/org/elasticsearch/index/percolator/stats/PercolateStats.java\n@@ -93,7 +93,6 @@ public class PercolateStats implements Streamable, ToXContent {\n         builder.field(Fields.CURRENT, current);\n         builder.field(Fields.MEMORY_SIZE_IN_BYTES, memorySizeInBytes);\n         builder.field(Fields.MEMORY_SIZE, getMemorySize());\n-        builder.field(Fields.QUERIES, getNumQueries());\n         builder.endObject();\n         return builder;\n     }\n@@ -118,7 +117,6 @@ public class PercolateStats implements Streamable, ToXContent {\n         static final XContentBuilderString CURRENT = new XContentBuilderString(\"current\");\n         static final XContentBuilderString MEMORY_SIZE_IN_BYTES = new XContentBuilderString(\"memory_size_in_bytes\");\n         static final XContentBuilderString MEMORY_SIZE = new XContentBuilderString(\"memory_size\");\n-        static final XContentBuilderString QUERIES = new XContentBuilderString(\"queries\");\n     }\n \n     public static PercolateStats readPercolateStats(StreamInput in) throws IOException {\ndiff --git a/src/main/java/org/elasticsearch/index/query/FieldQueryBuilder.java b/src/main/java/org/elasticsearch/index/query/FieldQueryBuilder.java\nnew file mode 100644\nindex 00000000000..5c805514ce7\n--- /dev/null\n+++ b/src/main/java/org/elasticsearch/index/query/FieldQueryBuilder.java\n@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to ElasticSearch and Shay Banon under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. ElasticSearch licenses this\n+ * file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.index.query;\n+\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+\n+/**\n+ * A query that executes the query string against a field. It is a simplified\n+ * version of {@link QueryStringQueryBuilder} that simply runs against\n+ * a single field.\n+ */\n+public class FieldQueryBuilder extends BaseQueryBuilder implements BoostableQueryBuilder<FieldQueryBuilder> {\n+\n+    public static enum Operator {\n+        OR,\n+        AND\n+    }\n+\n+    private final String name;\n+\n+    private final Object query;\n+\n+    private Operator defaultOperator;\n+\n+    private String analyzer;\n+\n+    private Boolean autoGeneratePhraseQueries;\n+\n+    private Boolean allowLeadingWildcard;\n+\n+    private Boolean lowercaseExpandedTerms;\n+\n+    private Boolean enablePositionIncrements;\n+\n+    private Boolean analyzeWildcard;\n+\n+    private int fuzzyPrefixLength = -1;\n+    private float fuzzyMinSim = -1;\n+    private int fuzzyMaxExpansions = -1;\n+    private String fuzzyRewrite;\n+\n+    private float boost = -1;\n+\n+    private int phraseSlop = -1;\n+\n+    private boolean extraSet = false;\n+\n+    private String rewrite;\n+\n+    private String minimumShouldMatch;\n+\n+    private String queryName;\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public FieldQueryBuilder(String name, String query) {\n+        this(name, (Object) query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public FieldQueryBuilder(String name, int query) {\n+        this(name, (Object) query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public FieldQueryBuilder(String name, long query) {\n+        this(name, (Object) query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public FieldQueryBuilder(String name, float query) {\n+        this(name, (Object) query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public FieldQueryBuilder(String name, double query) {\n+        this(name, (Object) query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public FieldQueryBuilder(String name, boolean query) {\n+        this(name, (Object) query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public FieldQueryBuilder(String name, Object query) {\n+        this.name = name;\n+        this.query = query;\n+    }\n+\n+    /**\n+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal\n+     * weightings) have their score multiplied by the boost provided.\n+     */\n+    public FieldQueryBuilder boost(float boost) {\n+        this.boost = boost;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    /**\n+     * Sets the boolean operator of the query parser used to parse the query string.\n+     * <p/>\n+     * <p>In default mode ({@link FieldQueryBuilder.Operator#OR}) terms without any modifiers\n+     * are considered optional: for example <code>capital of Hungary</code> is equal to\n+     * <code>capital OR of OR Hungary</code>.\n+     * <p/>\n+     * <p>In {@link FieldQueryBuilder.Operator#AND} mode terms are considered to be in conjunction: the\n+     * above mentioned query is parsed as <code>capital AND of AND Hungary</code>\n+     */\n+    public FieldQueryBuilder defaultOperator(Operator defaultOperator) {\n+        this.defaultOperator = defaultOperator;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    /**\n+     * The optional analyzer used to analyze the query string. Note, if a field has search analyzer\n+     * defined for it, then it will be used automatically. Defaults to the smart search analyzer.\n+     */\n+    public FieldQueryBuilder analyzer(String analyzer) {\n+        this.analyzer = analyzer;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    /**\n+     * Set to true if phrase queries will be automatically generated\n+     * when the analyzer returns more than one term from whitespace\n+     * delimited text.\n+     * NOTE: this behavior may not be suitable for all languages.\n+     * <p/>\n+     * Set to false if phrase queries should only be generated when\n+     * surrounded by double quotes.\n+     */\n+    public void autoGeneratePhraseQueries(boolean autoGeneratePhraseQueries) {\n+        this.autoGeneratePhraseQueries = autoGeneratePhraseQueries;\n+    }\n+\n+    /**\n+     * Should leading wildcards be allowed or not. Defaults to <tt>true</tt>.\n+     */\n+    public FieldQueryBuilder allowLeadingWildcard(boolean allowLeadingWildcard) {\n+        this.allowLeadingWildcard = allowLeadingWildcard;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    /**\n+     * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically\n+     * lower-cased or not.  Default is <tt>true</tt>.\n+     */\n+    public FieldQueryBuilder lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {\n+        this.lowercaseExpandedTerms = lowercaseExpandedTerms;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    /**\n+     * Set to <tt>true</tt> to enable position increments in result query. Defaults to\n+     * <tt>true</tt>.\n+     * <p/>\n+     * <p>When set, result phrase and multi-phrase queries will be aware of position increments.\n+     * Useful when e.g. a StopFilter increases the position increment of the token that follows an omitted token.\n+     */\n+    public FieldQueryBuilder enablePositionIncrements(boolean enablePositionIncrements) {\n+        this.enablePositionIncrements = enablePositionIncrements;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    /**\n+     * Set the minimum similarity for fuzzy queries. Default is 0.5f.\n+     */\n+    public FieldQueryBuilder fuzzyMinSim(float fuzzyMinSim) {\n+        this.fuzzyMinSim = fuzzyMinSim;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    /**\n+     * Set the prefix length for fuzzy queries. Default is 0.\n+     */\n+    public FieldQueryBuilder fuzzyPrefixLength(int fuzzyPrefixLength) {\n+        this.fuzzyPrefixLength = fuzzyPrefixLength;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    public FieldQueryBuilder fuzzyMaxExpansions(int fuzzyMaxExpansions) {\n+        this.fuzzyMaxExpansions = fuzzyMaxExpansions;\n+        return this;\n+    }\n+\n+    public FieldQueryBuilder fuzzyRewrite(String fuzzyRewrite) {\n+        this.fuzzyRewrite = fuzzyRewrite;\n+        return this;\n+    }\n+\n+\n+    /**\n+     * Sets the default slop for phrases.  If zero, then exact phrase matches\n+     * are required. Default value is zero.\n+     */\n+    public FieldQueryBuilder phraseSlop(int phraseSlop) {\n+        this.phraseSlop = phraseSlop;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    /**\n+     * Set to <tt>true</tt> to enable analysis on wildcard and prefix queries.\n+     */\n+    public FieldQueryBuilder analyzeWildcard(boolean analyzeWildcard) {\n+        this.analyzeWildcard = analyzeWildcard;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    public FieldQueryBuilder rewrite(String rewrite) {\n+        this.rewrite = rewrite;\n+        extraSet = true;\n+        return this;\n+    }\n+\n+    public FieldQueryBuilder minimumShouldMatch(String minimumShouldMatch) {\n+        this.minimumShouldMatch = minimumShouldMatch;\n+        return this;\n+    }\n+\n+    /**\n+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.\n+     */\n+    public FieldQueryBuilder queryName(String queryName) {\n+        this.queryName = queryName;\n+        this.extraSet = true;\n+        return this;\n+    }\n+\n+    @Override\n+    public void doXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject(FieldQueryParser.NAME);\n+        if (!extraSet) {\n+            builder.field(name, query);\n+        } else {\n+            builder.startObject(name);\n+            builder.field(\"query\", query);\n+            if (defaultOperator != null) {\n+                builder.field(\"default_operator\", defaultOperator.name().toLowerCase(Locale.ROOT));\n+            }\n+            if (analyzer != null) {\n+                builder.field(\"analyzer\", analyzer);\n+            }\n+            if (autoGeneratePhraseQueries != null) {\n+                builder.field(\"auto_generate_phrase_queries\", autoGeneratePhraseQueries);\n+            }\n+            if (allowLeadingWildcard != null) {\n+                builder.field(\"allow_leading_wildcard\", allowLeadingWildcard);\n+            }\n+            if (lowercaseExpandedTerms != null) {\n+                builder.field(\"lowercase_expanded_terms\", lowercaseExpandedTerms);\n+            }\n+            if (enablePositionIncrements != null) {\n+                builder.field(\"enable_position_increments\", enablePositionIncrements);\n+            }\n+            if (fuzzyMinSim != -1) {\n+                builder.field(\"fuzzy_min_sim\", fuzzyMinSim);\n+            }\n+            if (boost != -1) {\n+                builder.field(\"boost\", boost);\n+            }\n+            if (fuzzyPrefixLength != -1) {\n+                builder.field(\"fuzzy_prefix_length\", fuzzyPrefixLength);\n+            }\n+            if (fuzzyMaxExpansions != -1) {\n+                builder.field(\"fuzzy_max_expansions\", fuzzyMaxExpansions);\n+            }\n+            if (fuzzyRewrite != null) {\n+                builder.field(\"fuzzy_rewrite\", fuzzyRewrite);\n+            }\n+            if (phraseSlop != -1) {\n+                builder.field(\"phrase_slop\", phraseSlop);\n+            }\n+            if (analyzeWildcard != null) {\n+                builder.field(\"analyze_wildcard\", analyzeWildcard);\n+            }\n+            if (rewrite != null) {\n+                builder.field(\"rewrite\", rewrite);\n+            }\n+            if (minimumShouldMatch != null) {\n+                builder.field(\"minimum_should_match\", minimumShouldMatch);\n+            }\n+            if (queryName != null) {\n+                builder.field(\"_name\", queryName);\n+            }\n+            builder.endObject();\n+        }\n+        builder.endObject();\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/index/query/FieldQueryParser.java b/src/main/java/org/elasticsearch/index/query/FieldQueryParser.java\nnew file mode 100644\nindex 00000000000..2404119997f\n--- /dev/null\n+++ b/src/main/java/org/elasticsearch/index/query/FieldQueryParser.java\n@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to ElasticSearch and Shay Banon under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. ElasticSearch licenses this\n+ * file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.index.query;\n+\n+import org.apache.lucene.queryparser.classic.MapperQueryParser;\n+import org.apache.lucene.queryparser.classic.ParseException;\n+import org.apache.lucene.queryparser.classic.QueryParserSettings;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.Query;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.lucene.search.Queries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.index.analysis.NamedAnalyzer;\n+import org.elasticsearch.index.query.support.QueryParsers;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;\n+import static org.elasticsearch.common.lucene.search.Queries.optimizeQuery;\n+\n+/**\n+ *\n+ */\n+public class FieldQueryParser implements QueryParser {\n+\n+    public static final String NAME = \"field\";\n+\n+    private final boolean defaultAnalyzeWildcard;\n+    private final boolean defaultAllowLeadingWildcard;\n+\n+    @Inject\n+    public FieldQueryParser(Settings settings) {\n+        this.defaultAnalyzeWildcard = settings.getAsBoolean(\"indices.query.query_string.analyze_wildcard\", QueryParserSettings.DEFAULT_ANALYZE_WILDCARD);\n+        this.defaultAllowLeadingWildcard = settings.getAsBoolean(\"indices.query.query_string.allowLeadingWildcard\", QueryParserSettings.DEFAULT_ALLOW_LEADING_WILDCARD);\n+    }\n+\n+    @Override\n+    public String[] names() {\n+        return new String[]{NAME};\n+    }\n+\n+    @Override\n+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {\n+        XContentParser parser = parseContext.parser();\n+\n+        XContentParser.Token token = parser.nextToken();\n+        if (token != XContentParser.Token.FIELD_NAME) {\n+            throw new QueryParsingException(parseContext.index(), \"[field] query malformed, no field\");\n+        }\n+        String fieldName = parser.currentName();\n+\n+        QueryParserSettings qpSettings = new QueryParserSettings();\n+        qpSettings.defaultField(fieldName);\n+        qpSettings.analyzeWildcard(defaultAnalyzeWildcard);\n+        qpSettings.allowLeadingWildcard(defaultAllowLeadingWildcard);\n+        String queryName = null;\n+\n+        token = parser.nextToken();\n+        if (token == XContentParser.Token.START_OBJECT) {\n+            String currentFieldName = null;\n+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n+                if (token == XContentParser.Token.FIELD_NAME) {\n+                    currentFieldName = parser.currentName();\n+                } else if (token.isValue()) {\n+                    if (\"query\".equals(currentFieldName)) {\n+                        qpSettings.queryString(parser.text());\n+                    } else if (\"boost\".equals(currentFieldName)) {\n+                        qpSettings.boost(parser.floatValue());\n+                    } else if (\"enable_position_increments\".equals(currentFieldName) || \"enablePositionIncrements\".equals(currentFieldName)) {\n+                        qpSettings.enablePositionIncrements(parser.booleanValue());\n+                    } else if (\"allow_leading_wildcard\".equals(currentFieldName) || \"allowLeadingWildcard\".equals(currentFieldName)) {\n+                        qpSettings.allowLeadingWildcard(parser.booleanValue());\n+                    } else if (\"auto_generate_phrase_queries\".equals(currentFieldName) || \"autoGeneratePhraseQueries\".equals(currentFieldName)) {\n+                        qpSettings.autoGeneratePhraseQueries(parser.booleanValue());\n+                    } else if (\"lowercase_expanded_terms\".equals(currentFieldName) || \"lowercaseExpandedTerms\".equals(currentFieldName)) {\n+                        qpSettings.lowercaseExpandedTerms(parser.booleanValue());\n+                    } else if (\"phrase_slop\".equals(currentFieldName) || \"phraseSlop\".equals(currentFieldName)) {\n+                        qpSettings.phraseSlop(parser.intValue());\n+                    } else if (\"analyzer\".equals(currentFieldName)) {\n+                        NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());\n+                        if (analyzer == null) {\n+                            throw new QueryParsingException(parseContext.index(), \"[query_string] analyzer [\" + parser.text() + \"] not found\");\n+                        }\n+                        qpSettings.forcedAnalyzer(analyzer);\n+                    } else if (\"quote_analyzer\".equals(currentFieldName) || \"quoteAnalyzer\".equals(currentFieldName)) {\n+                        NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());\n+                        if (analyzer == null) {\n+                            throw new QueryParsingException(parseContext.index(), \"[query_string] analyzer [\" + parser.text() + \"] not found\");\n+                        }\n+                        qpSettings.forcedQuoteAnalyzer(analyzer);\n+                    } else if (\"default_operator\".equals(currentFieldName) || \"defaultOperator\".equals(currentFieldName)) {\n+                        String op = parser.text();\n+                        if (\"or\".equalsIgnoreCase(op)) {\n+                            qpSettings.defaultOperator(org.apache.lucene.queryparser.classic.QueryParser.Operator.OR);\n+                        } else if (\"and\".equalsIgnoreCase(op)) {\n+                            qpSettings.defaultOperator(org.apache.lucene.queryparser.classic.QueryParser.Operator.AND);\n+                        } else {\n+                            throw new QueryParsingException(parseContext.index(), \"Query default operator [\" + op + \"] is not allowed\");\n+                        }\n+                    } else if (\"fuzzy_min_sim\".equals(currentFieldName) || \"fuzzyMinSim\".equals(currentFieldName)) {\n+                        qpSettings.fuzzyMinSim(parser.floatValue());\n+                    } else if (\"fuzzy_prefix_length\".equals(currentFieldName) || \"fuzzyPrefixLength\".equals(currentFieldName)) {\n+                        qpSettings.fuzzyPrefixLength(parser.intValue());\n+                    } else if (\"fuzzy_max_expansions\".equals(currentFieldName) || \"fuzzyMaxExpansions\".equals(currentFieldName)) {\n+                        qpSettings.fuzzyMaxExpansions(parser.intValue());\n+                    } else if (\"fuzzy_rewrite\".equals(currentFieldName) || \"fuzzyRewrite\".equals(currentFieldName)) {\n+                        qpSettings.fuzzyRewriteMethod(QueryParsers.parseRewriteMethod(parser.textOrNull()));\n+                    } else if (\"escape\".equals(currentFieldName)) {\n+                        qpSettings.escape(parser.booleanValue());\n+                    } else if (\"analyze_wildcard\".equals(currentFieldName) || \"analyzeWildcard\".equals(currentFieldName)) {\n+                        qpSettings.analyzeWildcard(parser.booleanValue());\n+                    } else if (\"rewrite\".equals(currentFieldName)) {\n+                        qpSettings.rewriteMethod(QueryParsers.parseRewriteMethod(parser.textOrNull()));\n+                    } else if (\"minimum_should_match\".equals(currentFieldName) || \"minimumShouldMatch\".equals(currentFieldName)) {\n+                        qpSettings.minimumShouldMatch(parser.textOrNull());\n+                    } else if (\"quote_field_suffix\".equals(currentFieldName) || \"quoteFieldSuffix\".equals(currentFieldName)) {\n+                        qpSettings.quoteFieldSuffix(parser.textOrNull());\n+                    } else if (\"_name\".equals(currentFieldName)) {\n+                        queryName = parser.text();\n+                    } else {\n+                        throw new QueryParsingException(parseContext.index(), \"[field] query does not support [\" + currentFieldName + \"]\");\n+                    }\n+                }\n+            }\n+            parser.nextToken();\n+        } else {\n+            qpSettings.queryString(parser.text());\n+            // move to the next token\n+            parser.nextToken();\n+        }\n+\n+        qpSettings.defaultAnalyzer(parseContext.mapperService().searchAnalyzer());\n+        qpSettings.defaultQuoteAnalyzer(parseContext.mapperService().searchQuoteAnalyzer());\n+\n+        if (qpSettings.queryString() == null) {\n+            throw new QueryParsingException(parseContext.index(), \"No value specified for term query\");\n+        }\n+\n+        if (qpSettings.escape()) {\n+            qpSettings.queryString(org.apache.lucene.queryparser.classic.QueryParser.escape(qpSettings.queryString()));\n+        }\n+\n+        qpSettings.queryTypes(parseContext.queryTypes());\n+\n+        Query query = parseContext.indexCache().queryParserCache().get(qpSettings);\n+        if (query != null) {\n+            return query;\n+        }\n+\n+        MapperQueryParser queryParser = parseContext.queryParser(qpSettings);\n+\n+        try {\n+            query = queryParser.parse(qpSettings.queryString());\n+            if (query == null) {\n+                return null;\n+            }\n+            query.setBoost(qpSettings.boost());\n+            query = optimizeQuery(fixNegativeQueryIfNeeded(query));\n+            if (query instanceof BooleanQuery) {\n+                Queries.applyMinimumShouldMatch((BooleanQuery) query, qpSettings.minimumShouldMatch());\n+            }\n+            parseContext.indexCache().queryParserCache().put(qpSettings, query);\n+            if (queryName != null) {\n+                parseContext.addNamedQuery(queryName, query);\n+            }\n+            return query;\n+        } catch (ParseException e) {\n+            throw new QueryParsingException(parseContext.index(), \"Failed to parse query [\" + qpSettings.queryString() + \"]\", e);\n+        }\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java b/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java\nindex 41ba2c3c27e..5b041f9667c 100644\n--- a/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java\n+++ b/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java\n@@ -19,8 +19,9 @@\n \n package org.elasticsearch.index.query;\n \n-import org.apache.lucene.search.Filter;\n import org.elasticsearch.ElasticSearchParseException;\n+\n+import org.apache.lucene.search.Filter;\n import org.elasticsearch.common.geo.GeoPoint;\n import org.elasticsearch.common.geo.GeoUtils;\n import org.elasticsearch.common.inject.Inject;\n@@ -125,10 +126,10 @@ public class GeoBoundingBoxFilterParser implements FilterParser {\n             throw new QueryParsingException(parseContext.index(), \"failed to find geo_point field [\" + fieldName + \"]\");\n         }\n         FieldMapper<?> mapper = smartMappers.mapper();\n-        if (!(mapper instanceof GeoPointFieldMapper)) {\n+        if (!(mapper instanceof GeoPointFieldMapper.GeoStringFieldMapper)) {\n             throw new QueryParsingException(parseContext.index(), \"field [\" + fieldName + \"] is not a geo_point field\");\n         }\n-        GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper) mapper);\n+        GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper.GeoStringFieldMapper) mapper).geoMapper();\n \n         Filter filter;\n         if (\"indexed\".equals(type)) {\ndiff --git a/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java b/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java\nindex 7e265b896d8..d546b8bf1eb 100644\n--- a/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java\n+++ b/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java\n@@ -156,14 +156,14 @@ public class GeoDistanceFilterParser implements FilterParser {\n         if (smartMappers == null || !smartMappers.hasMapper()) {\n             throw new QueryParsingException(parseContext.index(), \"failed to find geo_point field [\" + fieldName + \"]\");\n         }\n-        FieldMapper<?> mapper = smartMappers.mapper();\n-        if (!(mapper instanceof GeoPointFieldMapper)) {\n+        FieldMapper mapper = smartMappers.mapper();\n+        if (!(mapper instanceof GeoPointFieldMapper.GeoStringFieldMapper)) {\n             throw new QueryParsingException(parseContext.index(), \"field [\" + fieldName + \"] is not a geo_point field\");\n         }\n-        GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper) mapper);\n+        GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper.GeoStringFieldMapper) mapper).geoMapper();\n \n \n-        IndexGeoPointFieldData<?> indexFieldData = parseContext.fieldData().getForField(mapper);\n+        IndexGeoPointFieldData indexFieldData = parseContext.fieldData().getForField(mapper);\n         Filter filter = new GeoDistanceFilter(point.lat(), point.lon(), distance, geoDistance, indexFieldData, geoMapper, optimizeBbox);\n         if (cache) {\n             filter = parseContext.cacheFilter(filter, cacheKey);\ndiff --git a/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java b/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java\nindex 4906f94e4f1..02c094bebf2 100644\n--- a/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java\n+++ b/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java\n@@ -199,13 +199,13 @@ public class GeoDistanceRangeFilterParser implements FilterParser {\n         if (smartMappers == null || !smartMappers.hasMapper()) {\n             throw new QueryParsingException(parseContext.index(), \"failed to find geo_point field [\" + fieldName + \"]\");\n         }\n-        FieldMapper<?> mapper = smartMappers.mapper();\n-        if (!(mapper instanceof GeoPointFieldMapper)) {\n+        FieldMapper mapper = smartMappers.mapper();\n+        if (!(mapper instanceof GeoPointFieldMapper.GeoStringFieldMapper)) {\n             throw new QueryParsingException(parseContext.index(), \"field [\" + fieldName + \"] is not a geo_point field\");\n         }\n-        GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper) mapper);\n+        GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper.GeoStringFieldMapper) mapper).geoMapper();\n \n-        IndexGeoPointFieldData<?> indexFieldData = parseContext.fieldData().getForField(mapper);\n+        IndexGeoPointFieldData indexFieldData = parseContext.fieldData().getForField(mapper);\n         Filter filter = new GeoDistanceRangeFilter(point, from, to, includeLower, includeUpper, geoDistance, geoMapper, indexFieldData, optimizeBbox);\n         if (cache) {\n             filter = parseContext.cacheFilter(filter, cacheKey);\ndiff --git a/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java b/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java\nindex 09c355cd993..3f337d061a1 100644\n--- a/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java\n+++ b/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java\n@@ -21,6 +21,7 @@ package org.elasticsearch.index.query;\n \n import com.google.common.collect.Lists;\n import org.apache.lucene.search.Filter;\n+import org.elasticsearch.common.geo.GeoHashUtils;\n import org.elasticsearch.common.geo.GeoPoint;\n import org.elasticsearch.common.geo.GeoUtils;\n import org.elasticsearch.common.inject.Inject;\n@@ -127,12 +128,12 @@ public class GeoPolygonFilterParser implements FilterParser {\n         if (smartMappers == null || !smartMappers.hasMapper()) {\n             throw new QueryParsingException(parseContext.index(), \"failed to find geo_point field [\" + fieldName + \"]\");\n         }\n-        FieldMapper<?> mapper = smartMappers.mapper();\n-        if (!(mapper instanceof GeoPointFieldMapper)) {\n+        FieldMapper mapper = smartMappers.mapper();\n+        if (!(mapper instanceof GeoPointFieldMapper.GeoStringFieldMapper)) {\n             throw new QueryParsingException(parseContext.index(), \"field [\" + fieldName + \"] is not a geo_point field\");\n         }\n \n-        IndexGeoPointFieldData<?> indexFieldData = parseContext.fieldData().getForField(mapper);\n+        IndexGeoPointFieldData indexFieldData = parseContext.fieldData().getForField(mapper);\n         Filter filter = new GeoPolygonFilter(points.toArray(new GeoPoint[points.size()]), indexFieldData);\n         if (cache) {\n             filter = parseContext.cacheFilter(filter, cacheKey);\ndiff --git a/src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java b/src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java\nindex fa9b87d1709..d37d3cdaed9 100644\n--- a/src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java\n+++ b/src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java\n@@ -239,11 +239,11 @@ public class GeohashCellFilter {\n             }\n \n             FieldMapper<?> mapper = smartMappers.mapper();\n-            if (!(mapper instanceof GeoPointFieldMapper)) {\n+            if (!(mapper instanceof GeoPointFieldMapper.GeoStringFieldMapper)) {\n                 throw new QueryParsingException(parseContext.index(), \"field [\" + fieldName + \"] is not a geo_point field\");\n             }\n \n-            GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper) mapper);\n+            GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper.GeoStringFieldMapper) mapper).geoMapper();\n             if (!geoMapper.isEnableGeohashPrefix()) {\n                 throw new QueryParsingException(parseContext.index(), \"can't execute geohash_cell on field [\" + fieldName + \"], geohash_prefix is not enabled\");\n             }\ndiff --git a/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java b/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java\nindex a106e2d0844..765286a5861 100644\n--- a/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java\n+++ b/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java\n@@ -45,7 +45,8 @@ public class MatchQueryParser implements QueryParser {\n     @Override\n     public String[] names() {\n         return new String[]{\n-                NAME, \"match_phrase\", \"matchPhrase\", \"match_phrase_prefix\", \"matchPhrasePrefix\", \"matchFuzzy\", \"match_fuzzy\", \"fuzzy_match\"\n+                NAME, \"match_phrase\", \"matchPhrase\", \"match_phrase_prefix\", \"matchPhrasePrefix\", \"matchFuzzy\", \"match_fuzzy\", \"fuzzy_match\",\n+                \"text\", \"text_phrase\", \"textPhrase\", \"text_phrase_prefix\", \"textPhrasePrefix\", \"fuzzyText\", \"fuzzy_text\"\n         };\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/index/query/QueryBuilders.java b/src/main/java/org/elasticsearch/index/query/QueryBuilders.java\nindex 9e1b1a0248a..b97f68d2084 100644\n--- a/src/main/java/org/elasticsearch/index/query/QueryBuilders.java\n+++ b/src/main/java/org/elasticsearch/index/query/QueryBuilders.java\n@@ -38,6 +38,28 @@ public abstract class QueryBuilders {\n         return new MatchAllQueryBuilder();\n     }\n \n+    /**\n+     * Creates a text query with type \"BOOLEAN\" for the provided field name and text.\n+     *\n+     * @param name The field name.\n+     * @param text The query text (to be analyzed).\n+     * @deprecated use {@link #textQuery(String, Object)} instead\n+     */\n+    public static MatchQueryBuilder text(String name, Object text) {\n+        return textQuery(name, text);\n+    }\n+\n+    /**\n+     * Creates a text query with type \"BOOLEAN\" for the provided field name and text.\n+     *\n+     * @param name The field name.\n+     * @param text The query text (to be analyzed).\n+     * @deprecated Use {@link #matchQuery(String, Object)}\n+     */\n+    public static MatchQueryBuilder textQuery(String name, Object text) {\n+        return new MatchQueryBuilder(name, text).type(MatchQueryBuilder.Type.BOOLEAN);\n+    }\n+\n     /**\n      * Creates a match query with type \"BOOLEAN\" for the provided field name and text.\n      *\n@@ -240,6 +262,89 @@ public abstract class QueryBuilders {\n         return new FuzzyQueryBuilder(name, value);\n     }\n \n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name The name of the field\n+     */\n+    public static FieldQueryBuilder fieldQuery(String name, String query) {\n+        return new FieldQueryBuilder(name, query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public static FieldQueryBuilder fieldQuery(String name, int query) {\n+        return new FieldQueryBuilder(name, query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public static FieldQueryBuilder fieldQuery(String name, long query) {\n+        return new FieldQueryBuilder(name, query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public static FieldQueryBuilder fieldQuery(String name, float query) {\n+        return new FieldQueryBuilder(name, query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public static FieldQueryBuilder fieldQuery(String name, double query) {\n+        return new FieldQueryBuilder(name, query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public static FieldQueryBuilder fieldQuery(String name, boolean query) {\n+        return new FieldQueryBuilder(name, query);\n+    }\n+\n+    /**\n+     * A query that executes the query string against a field. It is a simplified\n+     * version of {@link QueryStringQueryBuilder} that simply runs against\n+     * a single field.\n+     *\n+     * @param name  The name of the field\n+     * @param query The query string\n+     */\n+    public static FieldQueryBuilder fieldQuery(String name, Object query) {\n+        return new FieldQueryBuilder(name, query);\n+    }\n+\n     /**\n      * A Query that matches documents containing terms with a specified prefix.\n      *\ndiff --git a/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java b/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java\nindex f0abe4edc24..146318fe3af 100644\n--- a/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java\n+++ b/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java\n@@ -19,7 +19,6 @@\n \n package org.elasticsearch.index.query;\n \n-import org.apache.lucene.analysis.Analyzer;\n import org.apache.lucene.queryparser.XSimpleQueryParser;\n import org.apache.lucene.search.BooleanClause;\n import org.apache.lucene.search.Query;\n@@ -28,6 +27,7 @@ import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.regex.Regex;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.index.analysis.NamedAnalyzer;\n \n import java.io.IOException;\n import java.util.HashMap;\n@@ -37,23 +37,23 @@ import java.util.Map;\n  * SimpleQueryStringParser is a query parser that acts similar to a query_string\n  * query, but won't throw exceptions for any weird string syntax. It supports\n  * the following:\n- * <p/>\n+ *\n  * <ul>\n- * <li>'{@code +}' specifies {@code AND} operation: <tt>token1+token2</tt>\n- * <li>'{@code |}' specifies {@code OR} operation: <tt>token1|token2</tt>\n- * <li>'{@code -}' negates a single token: <tt>-token0</tt>\n- * <li>'{@code \"}' creates phrases of terms: <tt>\"term1 term2 ...\"</tt>\n- * <li>'{@code *}' at the end of terms specifies prefix query: <tt>term*</tt>\n- * <li>'{@code (}' and '{@code)}' specifies precedence: <tt>token1 + (token2 | token3)</tt>\n+ *  <li>'{@code +}' specifies {@code AND} operation: <tt>token1+token2</tt>\n+ *  <li>'{@code |}' specifies {@code OR} operation: <tt>token1|token2</tt>\n+ *  <li>'{@code -}' negates a single token: <tt>-token0</tt>\n+ *  <li>'{@code \"}' creates phrases of terms: <tt>\"term1 term2 ...\"</tt>\n+ *  <li>'{@code *}' at the end of terms specifies prefix query: <tt>term*</tt>\n+ *  <li>'{@code (}' and '{@code )}' specifies precedence: <tt>token1 + (token2 | token3)</tt>\n  * </ul>\n- * <p/>\n+ *\n  * See: {@link XSimpleQueryParser} for more information.\n- * <p/>\n+ *\n  * This query supports these options:\n- * <p/>\n+ *\n  * Required:\n  * {@code query} - query text to be converted into other queries\n- * <p/>\n+ *\n  * Optional:\n  * {@code analyzer} - anaylzer to be used for analyzing tokens to determine\n  * which kind of query they should be converted into, defaults to \"standard\"\n@@ -85,9 +85,9 @@ public class SimpleQueryStringParser implements QueryParser {\n         String field = null;\n         Map<String, Float> fieldsAndWeights = null;\n         BooleanClause.Occur defaultOperator = null;\n-        Analyzer analyzer = null;\n+        NamedAnalyzer analyzer = null;\n+        XContentParser.Token token = null;\n \n-        XContentParser.Token token;\n         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n             if (token == XContentParser.Token.FIELD_NAME) {\n                 currentFieldName = parser.currentName();\n@@ -132,7 +132,8 @@ public class SimpleQueryStringParser implements QueryParser {\n                 } else if (\"analyzer\".equals(currentFieldName)) {\n                     analyzer = parseContext.analysisService().analyzer(parser.text());\n                     if (analyzer == null) {\n-                        throw new QueryParsingException(parseContext.index(), \"[\" + NAME + \"] analyzer [\" + parser.text() + \"] not found\");\n+                        throw new QueryParsingException(parseContext.index(),\n+                                \"[\" + NAME + \"] analyzer [\" + parser.text() + \"] not found\");\n                     }\n                 } else if (\"field\".equals(currentFieldName)) {\n                     field = parser.text();\n@@ -163,13 +164,13 @@ public class SimpleQueryStringParser implements QueryParser {\n         }\n \n         // Use the default field (_all) if no fields specified\n-        if (fieldsAndWeights == null) {\n+        if (queryBody != null && fieldsAndWeights == null) {\n             field = parseContext.defaultField();\n         }\n \n         // Use standard analyzer by default\n         if (analyzer == null) {\n-            analyzer = parseContext.mapperService().searchAnalyzer();\n+            analyzer = parseContext.analysisService().analyzer(\"standard\");\n         }\n \n         XSimpleQueryParser sqp;\ndiff --git a/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java b/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java\nindex d2675bfd9a4..864fdb7787e 100644\n--- a/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java\n+++ b/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java\n@@ -39,7 +39,7 @@ import org.elasticsearch.index.mapper.FieldMapper;\n import org.elasticsearch.index.mapper.MapperService;\n import org.elasticsearch.index.mapper.core.DateFieldMapper;\n import org.elasticsearch.index.mapper.core.NumberFieldMapper;\n-import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;\n+import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper.GeoStringFieldMapper;\n import org.elasticsearch.index.query.QueryParseContext;\n import org.elasticsearch.index.query.QueryParsingException;\n import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;\n@@ -147,8 +147,8 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {\n         // dates and time need special handling\n         if (mapper instanceof DateFieldMapper) {\n             return parseDateVariable(fieldName, parser, parseContext, (DateFieldMapper) mapper);\n-        } else if (mapper instanceof GeoPointFieldMapper) {\n-            return parseGeoVariable(fieldName, parser, parseContext, (GeoPointFieldMapper) mapper);\n+        } else if (mapper instanceof GeoStringFieldMapper) {\n+            return parseGeoVariable(fieldName, parser, parseContext, (GeoStringFieldMapper) mapper);\n         } else if (mapper instanceof NumberFieldMapper<?>) {\n             return parseNumberVariable(fieldName, parser, parseContext, (NumberFieldMapper<?>) mapper);\n         } else {\n@@ -193,7 +193,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {\n     }\n \n     private ScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,\n-            GeoPointFieldMapper mapper) throws IOException {\n+            GeoStringFieldMapper mapper) throws IOException {\n         XContentParser.Token token;\n         String parameterName = null;\n         GeoPoint origin = new GeoPoint();\ndiff --git a/src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java b/src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java\nindex 619768855f3..2757e06813d 100644\n--- a/src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java\n+++ b/src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java\n@@ -302,6 +302,7 @@ public class ChildrenQuery extends Query {\n                     }\n \n                     HashedBytesArray uid = idTypeCache.idByDoc(currentDocId);\n+                    currentScore = uidToScore.get(uid);\n                     if (uidToScore.containsKey(uid)) {\n                         // Can use lget b/c uidToScore is only used by one thread at the time (via CacheRecycler)\n                         currentScore = uidToScore.lget();\ndiff --git a/src/main/java/org/elasticsearch/index/service/InternalIndexService.java b/src/main/java/org/elasticsearch/index/service/InternalIndexService.java\nindex 26dcb24ee28..5e63e25e362 100644\n--- a/src/main/java/org/elasticsearch/index/service/InternalIndexService.java\n+++ b/src/main/java/org/elasticsearch/index/service/InternalIndexService.java\n@@ -351,7 +351,6 @@ public class InternalIndexService extends AbstractIndexComponent implements Inde\n \n         IndexShard indexShard = shardInjector.getInstance(IndexShard.class);\n \n-        indicesLifecycle.indexShardStateChanged(indexShard, null, \"shard created\");\n         indicesLifecycle.afterIndexShardCreated(indexShard);\n \n         shards = newMapBuilder(shards).put(shardId.id(), indexShard).immutableMap();\ndiff --git a/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java b/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java\nindex 3e841c1775e..1eaf50f6d32 100644\n--- a/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java\n+++ b/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java\n@@ -289,7 +289,8 @@ public class InternalIndexShard extends AbstractIndexShardComponent implements I\n                 synchronized (mutex) {\n                     // do the check under a mutex, so we make sure to only change to STARTED if in POST_RECOVERY\n                     if (state == IndexShardState.POST_RECOVERY) {\n-                        changeState(IndexShardState.STARTED, \"global state is [\" + newRouting.state() + \"]\");\n+                        logger.debug(\"state: [{}]->[{}], reason [global state is [{}]]\", state, IndexShardState.STARTED, newRouting.state());\n+                        state = IndexShardState.STARTED;\n                         movedToStarted = true;\n                     } else {\n                         logger.debug(\"state [{}] not changed, not in POST_RECOVERY, global state is [{}]\", state, newRouting.state());\n@@ -313,6 +314,7 @@ public class InternalIndexShard extends AbstractIndexShardComponent implements I\n     public IndexShardState recovering(String reason) throws IndexShardStartedException,\n             IndexShardRelocatedException, IndexShardRecoveringException, IndexShardClosedException {\n         synchronized (mutex) {\n+            IndexShardState returnValue = state;\n             if (state == IndexShardState.CLOSED) {\n                 throw new IndexShardClosedException(shardId);\n             }\n@@ -328,7 +330,9 @@ public class InternalIndexShard extends AbstractIndexShardComponent implements I\n             if (state == IndexShardState.POST_RECOVERY) {\n                 throw new IndexShardRecoveringException(shardId);\n             }\n-            return changeState(IndexShardState.RECOVERING, reason);\n+            logger.debug(\"state: [{}]->[{}], reason [{}]\", state, IndexShardState.RECOVERING, reason);\n+            state = IndexShardState.RECOVERING;\n+            return returnValue;\n         }\n     }\n \n@@ -337,7 +341,8 @@ public class InternalIndexShard extends AbstractIndexShardComponent implements I\n             if (state != IndexShardState.STARTED) {\n                 throw new IndexShardNotStartedException(shardId, state);\n             }\n-            changeState(IndexShardState.RELOCATED, reason);\n+            logger.debug(\"state: [{}]->[{}], reason [{}]\", state, IndexShardState.RELOCATED, reason);\n+            state = IndexShardState.RELOCATED;\n         }\n         return this;\n     }\n@@ -347,20 +352,6 @@ public class InternalIndexShard extends AbstractIndexShardComponent implements I\n         return state;\n     }\n \n-    /**\n-     * Changes the state of the current shard\n-     * @param newState the new shard state\n-     * @param reason the reason for the state change\n-     * @return the previous shard state\n-     */\n-    private IndexShardState changeState(IndexShardState newState, String reason) {\n-        logger.debug(\"state: [{}]->[{}], reason [{}]\", state, newState, reason);\n-        IndexShardState previousState = state;\n-        state = newState;\n-        this.indicesLifecycle.indexShardStateChanged(this, previousState, reason);\n-        return previousState;\n-    }\n-\n     @Override\n     public Engine.Create prepareCreate(SourceToParse source) throws ElasticSearchException {\n         long startTime = System.nanoTime();\n@@ -662,7 +653,10 @@ public class InternalIndexShard extends AbstractIndexShardComponent implements I\n                     mergeScheduleFuture = null;\n                 }\n             }\n-            changeState(IndexShardState.CLOSED, reason);\n+            if (logger.isDebugEnabled()) {\n+                logger.debug(\"state: [{}]->[{}], reason [{}]\", state, IndexShardState.CLOSED, reason);\n+            }\n+            state = IndexShardState.CLOSED;\n         }\n     }\n \n@@ -687,7 +681,8 @@ public class InternalIndexShard extends AbstractIndexShardComponent implements I\n             }\n             engine.start();\n             startScheduledTasksIfNeeded();\n-            changeState(IndexShardState.POST_RECOVERY, reason);\n+            logger.debug(\"state: [{}]->[{}], reason [{}]\", state, IndexShardState.POST_RECOVERY, reason);\n+            state = IndexShardState.POST_RECOVERY;\n         }\n         indicesLifecycle.afterIndexShardPostRecovery(this);\n         return this;\n@@ -730,7 +725,8 @@ public class InternalIndexShard extends AbstractIndexShardComponent implements I\n         translog.clearUnreferenced();\n         engine.refresh(new Engine.Refresh(\"recovery_finalization\").force(true));\n         synchronized (mutex) {\n-            changeState(IndexShardState.POST_RECOVERY, \"post recovery\");\n+            logger.debug(\"state: [{}]->[{}], reason [post recovery]\", state, IndexShardState.POST_RECOVERY);\n+            state = IndexShardState.POST_RECOVERY;\n         }\n         indicesLifecycle.afterIndexShardPostRecovery(this);\n         startScheduledTasksIfNeeded();\ndiff --git a/src/main/java/org/elasticsearch/indices/IndicesLifecycle.java b/src/main/java/org/elasticsearch/indices/IndicesLifecycle.java\nindex 978ac25a777..9c95dd56133 100644\n--- a/src/main/java/org/elasticsearch/indices/IndicesLifecycle.java\n+++ b/src/main/java/org/elasticsearch/indices/IndicesLifecycle.java\n@@ -23,7 +23,6 @@ import org.elasticsearch.cluster.routing.ShardRouting;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.index.Index;\n import org.elasticsearch.index.service.IndexService;\n-import org.elasticsearch.index.shard.IndexShardState;\n import org.elasticsearch.index.shard.ShardId;\n import org.elasticsearch.index.shard.service.IndexShard;\n \n@@ -133,19 +132,6 @@ public interface IndicesLifecycle {\n         public void afterIndexShardClosed(ShardId shardId) {\n \n         }\n-\n-        /**\n-         * Called after a shard's {@link org.elasticsearch.index.shard.IndexShardState} changes.\n-         * The order of concurrent events is preserved. The execution must be lightweight.\n-         *\n-         * @param indexShard the shard the new state was applied to\n-         * @param previousState the previous index shard state if there was one, null otherwise\n-         * @param currentState the new shard state\n-         * @param reason the reason for the state change if there is one, null otherwise\n-         */\n-        public void indexShardStateChanged(IndexShard indexShard, @Nullable IndexShardState previousState, IndexShardState currentState, @Nullable String reason) {\n-\n-        }\n     }\n \n }\ndiff --git a/src/main/java/org/elasticsearch/indices/InternalIndicesLifecycle.java b/src/main/java/org/elasticsearch/indices/InternalIndicesLifecycle.java\nindex b933d87853b..b196f9ed1e2 100644\n--- a/src/main/java/org/elasticsearch/indices/InternalIndicesLifecycle.java\n+++ b/src/main/java/org/elasticsearch/indices/InternalIndicesLifecycle.java\n@@ -26,7 +26,6 @@ import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.index.Index;\n import org.elasticsearch.index.service.IndexService;\n-import org.elasticsearch.index.shard.IndexShardState;\n import org.elasticsearch.index.shard.ShardId;\n import org.elasticsearch.index.shard.service.IndexShard;\n \n@@ -161,14 +160,4 @@ public class InternalIndicesLifecycle extends AbstractComponent implements Indic\n             }\n         }\n     }\n-\n-    public void indexShardStateChanged(IndexShard indexShard, @Nullable IndexShardState previousState, @Nullable String reason) {\n-        for (Listener listener : listeners) {\n-            try {\n-                listener.indexShardStateChanged(indexShard, previousState, indexShard.state(), reason);\n-            } catch (Throwable t) {\n-                logger.warn(\"{} failed to invoke index shard state changed callback\", t, indexShard.shardId());\n-            }\n-        }\n-    }\n }\ndiff --git a/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java b/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java\nindex d559b94fe9b..8e22cdf5e08 100644\n--- a/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java\n+++ b/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java\n@@ -263,7 +263,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic\n     }\n \n     private void applyDeletedShards(final ClusterChangedEvent event) {\n-        RoutingNodes.RoutingNodeIterator routingNode = event.state().readOnlyRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());\n+        RoutingNode routingNode = event.state().readOnlyRoutingNodes().nodesToShards().get(event.state().nodes().localNodeId());\n         if (routingNode == null) {\n             return;\n         }\n@@ -276,7 +276,8 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic\n             }\n             // now, go over and delete shards that needs to get deleted\n             newShardIds.clear();\n-            for (MutableShardRouting shard : routingNode) {\n+            List<MutableShardRouting> shards = routingNode.shards();\n+            for (MutableShardRouting shard : shards) {\n                 if (shard.index().equals(index)) {\n                     newShardIds.add(shard.id());\n                 }\n@@ -303,7 +304,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic\n \n     private void applyNewIndices(final ClusterChangedEvent event) {\n         // we only create indices for shards that are allocated\n-        RoutingNodes.RoutingNodeIterator routingNode = event.state().readOnlyRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());\n+        RoutingNode routingNode = event.state().readOnlyRoutingNodes().nodesToShards().get(event.state().nodes().localNodeId());\n         if (routingNode == null) {\n             return;\n         }\n@@ -501,14 +502,14 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic\n         }\n \n         RoutingTable routingTable = event.state().routingTable();\n-        RoutingNodes.RoutingNodeIterator routingNode = event.state().readOnlyRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());;\n-        if (routingNode == null) {\n+        RoutingNode routingNodes = event.state().readOnlyRoutingNodes().nodesToShards().get(event.state().nodes().localNodeId());\n+        if (routingNodes == null) {\n             failedShards.clear();\n             return;\n         }\n         DiscoveryNodes nodes = event.state().nodes();\n \n-        for (final ShardRouting shardRouting : routingNode) {\n+        for (final ShardRouting shardRouting : routingNodes) {\n             final IndexService indexService = indicesService.indexService(shardRouting.index());\n             if (indexService == null) {\n                 // got deleted on us, ignore\n@@ -563,8 +564,8 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic\n \n     private void cleanFailedShards(final ClusterChangedEvent event) {\n         RoutingTable routingTable = event.state().routingTable();\n-        RoutingNodes.RoutingNodeIterator routingNode = event.state().readOnlyRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());;\n-        if (routingNode == null) {\n+        RoutingNode routingNodes = event.state().readOnlyRoutingNodes().nodesToShards().get(event.state().nodes().localNodeId());\n+        if (routingNodes == null) {\n             failedShards.clear();\n             return;\n         }\ndiff --git a/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java b/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java\nindex 309910361cc..e25577fdd4d 100644\n--- a/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java\n+++ b/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java\n@@ -82,6 +82,7 @@ public class IndicesQueriesModule extends AbstractModule {\n         qpBinders.addBinding().to(TermsQueryParser.class).asEagerSingleton();\n         qpBinders.addBinding().to(FuzzyQueryParser.class).asEagerSingleton();\n         qpBinders.addBinding().to(RegexpQueryParser.class).asEagerSingleton();\n+        qpBinders.addBinding().to(FieldQueryParser.class).asEagerSingleton();\n         qpBinders.addBinding().to(RangeQueryParser.class).asEagerSingleton();\n         qpBinders.addBinding().to(PrefixQueryParser.class).asEagerSingleton();\n         qpBinders.addBinding().to(WildcardQueryParser.class).asEagerSingleton();\ndiff --git a/src/main/java/org/elasticsearch/monitor/fs/FsStats.java b/src/main/java/org/elasticsearch/monitor/fs/FsStats.java\nindex bb2c227e4db..44c2c223acc 100644\n--- a/src/main/java/org/elasticsearch/monitor/fs/FsStats.java\n+++ b/src/main/java/org/elasticsearch/monitor/fs/FsStats.java\n@@ -20,7 +20,6 @@\n package org.elasticsearch.monitor.fs;\n \n import com.google.common.collect.Iterators;\n-import org.elasticsearch.Version;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.io.stream.StreamInput;\n@@ -57,19 +56,9 @@ public class FsStats implements Iterable<FsStats.Info>, Streamable, ToXContent {\n         double diskQueue = -1;\n         double diskServiceTime = -1;\n \n-        static public Info readInfoFrom(StreamInput in) throws IOException {\n-            Info i = new Info();\n-            i.readFrom(in);\n-            return i;\n-        }\n-\n         @Override\n         public void readFrom(StreamInput in) throws IOException {\n-            if (in.getVersion().after(Version.V_0_90_7)) {\n-                path = in.readOptionalString();\n-            } else {\n-                path = in.readString();\n-            }\n+            path = in.readString();\n             mount = in.readOptionalString();\n             dev = in.readOptionalString();\n             total = in.readLong();\n@@ -85,11 +74,7 @@ public class FsStats implements Iterable<FsStats.Info>, Streamable, ToXContent {\n \n         @Override\n         public void writeTo(StreamOutput out) throws IOException {\n-            if (out.getVersion().after(Version.V_0_90_7)) {\n-                out.writeOptionalString(path); // total aggregates do not have a path\n-            } else {\n-                out.writeString(path);\n-            }\n+            out.writeString(path);\n             out.writeOptionalString(mount);\n             out.writeOptionalString(dev);\n             out.writeLong(total);\n@@ -339,7 +324,8 @@ public class FsStats implements Iterable<FsStats.Info>, Streamable, ToXContent {\n         timestamp = in.readVLong();\n         infos = new Info[in.readVInt()];\n         for (int i = 0; i < infos.length; i++) {\n-            infos[i] = Info.readInfoFrom(in);\n+            infos[i] = new Info();\n+            infos[i].readFrom(in);\n         }\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/monitor/os/OsInfo.java b/src/main/java/org/elasticsearch/monitor/os/OsInfo.java\nindex 89f62d28bf2..a1a9a251ec7 100644\n--- a/src/main/java/org/elasticsearch/monitor/os/OsInfo.java\n+++ b/src/main/java/org/elasticsearch/monitor/os/OsInfo.java\n@@ -115,7 +115,13 @@ public class OsInfo implements Streamable, Serializable, ToXContent {\n         builder.field(Fields.AVAILABLE_PROCESSORS, availableProcessors);\n         if (cpu != null) {\n             builder.startObject(Fields.CPU);\n-            cpu.toXContent(builder, params);\n+            builder.field(Fields.VENDOR, cpu.vendor());\n+            builder.field(Fields.MODEL, cpu.model());\n+            builder.field(Fields.MHZ, cpu.mhz());\n+            builder.field(Fields.TOTAL_CORES, cpu.totalCores());\n+            builder.field(Fields.TOTAL_SOCKETS, cpu.totalSockets());\n+            builder.field(Fields.CORES_PER_SOCKET, cpu.coresPerSocket());\n+            builder.byteSizeField(Fields.CACHE_SIZE_IN_BYTES, Fields.CACHE_SIZE, cpu.cacheSize);\n             builder.endObject();\n         }\n         if (mem != null) {\n@@ -245,7 +251,7 @@ public class OsInfo implements Streamable, Serializable, ToXContent {\n \n     }\n \n-    public static class Cpu implements Streamable, Serializable, ToXContent {\n+    public static class Cpu implements Streamable, Serializable {\n \n         String vendor = \"\";\n         String model = \"\";\n@@ -342,36 +348,5 @@ public class OsInfo implements Streamable, Serializable, ToXContent {\n             out.writeInt(coresPerSocket);\n             out.writeLong(cacheSize);\n         }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) {\n-                return true;\n-            }\n-            if (o == null || getClass() != o.getClass()) {\n-                return false;\n-            }\n-\n-            Cpu cpu = (Cpu) o;\n-\n-            return model.equals(cpu.model) && vendor.equals(cpu.vendor);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return model.hashCode();\n-        }\n-\n-        @Override\n-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-            builder.field(Fields.VENDOR, vendor);\n-            builder.field(Fields.MODEL, model);\n-            builder.field(Fields.MHZ, mhz);\n-            builder.field(Fields.TOTAL_CORES, totalCores);\n-            builder.field(Fields.TOTAL_SOCKETS, totalSockets);\n-            builder.field(Fields.CORES_PER_SOCKET, coresPerSocket);\n-            builder.byteSizeField(Fields.CACHE_SIZE_IN_BYTES, Fields.CACHE_SIZE, cacheSize);\n-            return builder;\n-        }\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/percolator/PercolateContext.java b/src/main/java/org/elasticsearch/percolator/PercolateContext.java\nindex f73efb1ee37..496d938d3ab 100644\n--- a/src/main/java/org/elasticsearch/percolator/PercolateContext.java\n+++ b/src/main/java/org/elasticsearch/percolator/PercolateContext.java\n@@ -529,12 +529,12 @@ public class PercolateContext extends SearchContext {\n     }\n \n     @Override\n-    public SearchContext parsedPostFilter(ParsedFilter postFilter) {\n+    public SearchContext parsedFilter(ParsedFilter filter) {\n         throw new UnsupportedOperationException();\n     }\n \n     @Override\n-    public ParsedFilter parsedPostFilter() {\n+    public ParsedFilter parsedFilter() {\n         return null;\n     }\n \ndiff --git a/src/main/java/org/elasticsearch/percolator/PercolatorService.java b/src/main/java/org/elasticsearch/percolator/PercolatorService.java\nindex d521f20d8f5..7d274398368 100644\n--- a/src/main/java/org/elasticsearch/percolator/PercolatorService.java\n+++ b/src/main/java/org/elasticsearch/percolator/PercolatorService.java\n@@ -82,7 +82,6 @@ import org.elasticsearch.search.facet.InternalFacet;\n import org.elasticsearch.search.facet.InternalFacets;\n import org.elasticsearch.search.highlight.HighlightField;\n import org.elasticsearch.search.highlight.HighlightPhase;\n-import org.elasticsearch.search.highlight.SearchContextHighlight;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n@@ -310,11 +309,6 @@ public class PercolatorService extends AbstractComponent {\n             // We need to get the actual source from the request body for highlighting, so parse the request body again\n             // and only get the doc source.\n             if (context.highlight() != null) {\n-                // Enforce highlighting by source, because MemoryIndex doesn't support stored fields.\n-                for (SearchContextHighlight.Field field : context.highlight().fields()) {\n-                    field.forceSource(true);\n-                }\n-\n                 parser.close();\n                 currentFieldName = null;\n                 parser = XContentFactory.xContent(source).createParser(source);\ndiff --git a/src/main/java/org/elasticsearch/rest/action/RestActionModule.java b/src/main/java/org/elasticsearch/rest/action/RestActionModule.java\nindex 612628a911c..fd824bb51ea 100644\n--- a/src/main/java/org/elasticsearch/rest/action/RestActionModule.java\n+++ b/src/main/java/org/elasticsearch/rest/action/RestActionModule.java\n@@ -41,7 +41,6 @@ import org.elasticsearch.rest.action.admin.cluster.snapshots.delete.RestDeleteSn\n import org.elasticsearch.rest.action.admin.cluster.snapshots.get.RestGetSnapshotsAction;\n import org.elasticsearch.rest.action.admin.cluster.snapshots.restore.RestRestoreSnapshotAction;\n import org.elasticsearch.rest.action.admin.cluster.state.RestClusterStateAction;\n-import org.elasticsearch.rest.action.admin.cluster.stats.RestClusterStatsAction;\n import org.elasticsearch.rest.action.admin.cluster.tasks.RestPendingClusterTasksAction;\n import org.elasticsearch.rest.action.admin.indices.alias.RestGetIndicesAliasesAction;\n import org.elasticsearch.rest.action.admin.indices.alias.RestIndicesAliasesAction;\n@@ -126,7 +125,6 @@ public class RestActionModule extends AbstractModule {\n         bind(RestNodesHotThreadsAction.class).asEagerSingleton();\n         bind(RestNodesShutdownAction.class).asEagerSingleton();\n         bind(RestNodesRestartAction.class).asEagerSingleton();\n-        bind(RestClusterStatsAction.class).asEagerSingleton();\n         bind(RestClusterStateAction.class).asEagerSingleton();\n         bind(RestClusterHealthAction.class).asEagerSingleton();\n         bind(RestClusterUpdateSettingsAction.class).asEagerSingleton();\n@@ -222,7 +220,6 @@ public class RestActionModule extends AbstractModule {\n         catActionMultibinder.addBinding().to(RestRecoveryAction.class).asEagerSingleton();\n         catActionMultibinder.addBinding().to(RestHealthAction.class).asEagerSingleton();\n         catActionMultibinder.addBinding().to(org.elasticsearch.rest.action.cat.RestPendingClusterTasksAction.class).asEagerSingleton();\n-        catActionMultibinder.addBinding().to(RestAliasAction.class).asEagerSingleton();\n         // no abstract cat action\n         bind(RestCatAction.class).asEagerSingleton();\n     }\ndiff --git a/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java b/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java\ndeleted file mode 100644\nindex 869fe531760..00000000000\n--- a/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java\n+++ /dev/null\n@@ -1,75 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.rest.action.admin.cluster.stats;\n-\n-import org.elasticsearch.action.ActionListener;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsRequest;\n-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;\n-import org.elasticsearch.client.Client;\n-import org.elasticsearch.common.inject.Inject;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.common.xcontent.XContentBuilder;\n-import org.elasticsearch.rest.*;\n-import org.elasticsearch.rest.action.support.RestXContentBuilder;\n-\n-import java.io.IOException;\n-\n-\n-/**\n- *\n- */\n-public class RestClusterStatsAction extends BaseRestHandler {\n-\n-    @Inject\n-    public RestClusterStatsAction(Settings settings, Client client, RestController controller) {\n-        super(settings, client);\n-        controller.registerHandler(RestRequest.Method.GET, \"/_cluster/stats\", this);\n-        controller.registerHandler(RestRequest.Method.GET, \"/_cluster/stats/nodes/{nodeId}\", this);\n-    }\n-\n-    @Override\n-    public void handleRequest(final RestRequest request, final RestChannel channel) {\n-        ClusterStatsRequest clusterStatsRequest = new ClusterStatsRequest().nodesIds(request.paramAsStringArray(\"nodeId\", null));\n-        clusterStatsRequest.listenerThreaded(false);\n-        client.admin().cluster().clusterStats(clusterStatsRequest, new ActionListener<ClusterStatsResponse>() {\n-            @Override\n-            public void onResponse(ClusterStatsResponse response) {\n-                try {\n-                    XContentBuilder builder = RestXContentBuilder.restContentBuilder(request);\n-                    builder.startObject();\n-                    response.toXContent(builder, request);\n-                    builder.endObject();\n-                    channel.sendResponse(new XContentRestResponse(request, RestStatus.OK, builder));\n-                } catch (Throwable e) {\n-                    onFailure(e);\n-                }\n-            }\n-\n-            @Override\n-            public void onFailure(Throwable e) {\n-                try {\n-                    channel.sendResponse(new XContentThrowableRestResponse(request, e));\n-                } catch (IOException e1) {\n-                    logger.error(\"Failed to send failure response\", e1);\n-                }\n-            }\n-        });\n-    }\n-}\ndiff --git a/src/main/java/org/elasticsearch/rest/action/admin/indices/template/delete/RestDeleteIndexTemplateAction.java b/src/main/java/org/elasticsearch/rest/action/admin/indices/template/delete/RestDeleteIndexTemplateAction.java\nindex d0ab34e7b92..21d855c936a 100644\n--- a/src/main/java/org/elasticsearch/rest/action/admin/indices/template/delete/RestDeleteIndexTemplateAction.java\n+++ b/src/main/java/org/elasticsearch/rest/action/admin/indices/template/delete/RestDeleteIndexTemplateAction.java\n@@ -19,12 +19,20 @@\n \n package org.elasticsearch.rest.action.admin.indices.template.delete;\n \n+import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.admin.indices.template.delete.DeleteIndexTemplateRequest;\n import org.elasticsearch.action.admin.indices.template.delete.DeleteIndexTemplateResponse;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentBuilderString;\n import org.elasticsearch.rest.*;\n+import org.elasticsearch.rest.action.support.RestXContentBuilder;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.rest.RestStatus.OK;\n \n /**\n  *\n@@ -42,6 +50,34 @@ public class RestDeleteIndexTemplateAction extends BaseRestHandler {\n         DeleteIndexTemplateRequest deleteIndexTemplateRequest = new DeleteIndexTemplateRequest(request.param(\"name\"));\n         deleteIndexTemplateRequest.listenerThreaded(false);\n         deleteIndexTemplateRequest.masterNodeTimeout(request.paramAsTime(\"master_timeout\", deleteIndexTemplateRequest.masterNodeTimeout()));\n-        client.admin().indices().deleteTemplate(deleteIndexTemplateRequest, new AcknowledgedRestResponseActionListener<DeleteIndexTemplateResponse>(request, channel, logger));\n+        client.admin().indices().deleteTemplate(deleteIndexTemplateRequest, new ActionListener<DeleteIndexTemplateResponse>() {\n+            @Override\n+            public void onResponse(DeleteIndexTemplateResponse response) {\n+                try {\n+                    XContentBuilder builder = RestXContentBuilder.restContentBuilder(request);\n+                    builder.startObject()\n+                            .field(Fields.OK, true)\n+                            .field(Fields.ACKNOWLEDGED, response.isAcknowledged())\n+                            .endObject();\n+                    channel.sendResponse(new XContentRestResponse(request, OK, builder));\n+                } catch (IOException e) {\n+                    onFailure(e);\n+                }\n+            }\n+\n+            @Override\n+            public void onFailure(Throwable e) {\n+                try {\n+                    channel.sendResponse(new XContentThrowableRestResponse(request, e));\n+                } catch (IOException e1) {\n+                    logger.error(\"Failed to send failure response\", e1);\n+                }\n+            }\n+        });\n+    }\n+\n+    static final class Fields {\n+        static final XContentBuilderString OK = new XContentBuilderString(\"ok\");\n+        static final XContentBuilderString ACKNOWLEDGED = new XContentBuilderString(\"acknowledged\");\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/rest/action/admin/indices/template/put/RestPutIndexTemplateAction.java b/src/main/java/org/elasticsearch/rest/action/admin/indices/template/put/RestPutIndexTemplateAction.java\nindex d663dd10364..a0e6bfa5e0e 100644\n--- a/src/main/java/org/elasticsearch/rest/action/admin/indices/template/put/RestPutIndexTemplateAction.java\n+++ b/src/main/java/org/elasticsearch/rest/action/admin/indices/template/put/RestPutIndexTemplateAction.java\n@@ -19,15 +19,21 @@\n \n package org.elasticsearch.rest.action.admin.indices.template.put;\n \n+import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequest;\n import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateResponse;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.common.inject.Inject;\n import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentBuilderString;\n import org.elasticsearch.rest.*;\n+import org.elasticsearch.rest.action.support.RestXContentBuilder;\n \n import java.io.IOException;\n \n+import static org.elasticsearch.rest.RestStatus.OK;\n+\n /**\n  *\n  */\n@@ -70,6 +76,34 @@ public class RestPutIndexTemplateAction extends BaseRestHandler {\n             return;\n         }\n \n-        client.admin().indices().putTemplate(putRequest, new AcknowledgedRestResponseActionListener<PutIndexTemplateResponse>(request, channel, logger));\n+        client.admin().indices().putTemplate(putRequest, new ActionListener<PutIndexTemplateResponse>() {\n+            @Override\n+            public void onResponse(PutIndexTemplateResponse response) {\n+                try {\n+                    XContentBuilder builder = RestXContentBuilder.restContentBuilder(request);\n+                    builder.startObject()\n+                            .field(Fields.OK, true)\n+                            .field(Fields.ACKNOWLEDGED, response.isAcknowledged())\n+                            .endObject();\n+                    channel.sendResponse(new XContentRestResponse(request, OK, builder));\n+                } catch (IOException e) {\n+                    onFailure(e);\n+                }\n+            }\n+\n+            @Override\n+            public void onFailure(Throwable e) {\n+                try {\n+                    channel.sendResponse(new XContentThrowableRestResponse(request, e));\n+                } catch (IOException e1) {\n+                    logger.error(\"Failed to send failure response\", e1);\n+                }\n+            }\n+        });\n+    }\n+\n+    static final class Fields {\n+        static final XContentBuilderString OK = new XContentBuilderString(\"ok\");\n+        static final XContentBuilderString ACKNOWLEDGED = new XContentBuilderString(\"acknowledged\");\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/rest/action/cat/RestAliasAction.java b/src/main/java/org/elasticsearch/rest/action/cat/RestAliasAction.java\ndeleted file mode 100644\nindex 0649ad27210..00000000000\n--- a/src/main/java/org/elasticsearch/rest/action/cat/RestAliasAction.java\n+++ /dev/null\n@@ -1,132 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.elasticsearch.rest.action.cat;\n-\n-import com.carrotsearch.hppc.cursors.ObjectObjectCursor;\n-import org.elasticsearch.action.ActionListener;\n-import org.elasticsearch.action.admin.cluster.state.ClusterStateRequest;\n-import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;\n-import org.elasticsearch.client.Client;\n-import org.elasticsearch.cluster.metadata.AliasMetaData;\n-import org.elasticsearch.common.Strings;\n-import org.elasticsearch.common.Table;\n-import org.elasticsearch.common.collect.ImmutableOpenMap;\n-import org.elasticsearch.common.inject.Inject;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.rest.RestChannel;\n-import org.elasticsearch.rest.RestController;\n-import org.elasticsearch.rest.RestRequest;\n-import org.elasticsearch.rest.XContentThrowableRestResponse;\n-import org.elasticsearch.rest.action.support.RestTable;\n-\n-import java.io.IOException;\n-import java.util.Iterator;\n-\n-import static org.elasticsearch.rest.RestRequest.Method.GET;\n-\n-/**\n- *\n- */\n-public class RestAliasAction extends AbstractCatAction {\n-\n-    @Inject\n-    public RestAliasAction(Settings settings, Client client, RestController controller) {\n-        super(settings, client);\n-        controller.registerHandler(GET, \"/_cat/aliases\", this);\n-        controller.registerHandler(GET, \"/_cat/aliases/{alias}\", this);\n-    }\n-\n-\n-    @Override\n-    void doRequest(final RestRequest request, final RestChannel channel) {\n-        final ClusterStateRequest clusterStateRequest = new ClusterStateRequest();\n-        clusterStateRequest.filterMetaData(true);\n-        clusterStateRequest.local(request.paramAsBoolean(\"local\", clusterStateRequest.local()));\n-        clusterStateRequest.masterNodeTimeout(request.paramAsTime(\"master_timeout\", clusterStateRequest.masterNodeTimeout()));\n-        clusterStateRequest.filterAll().filterMetaData(false);\n-\n-        client.admin().cluster().state(clusterStateRequest, new ActionListener<ClusterStateResponse>() {\n-\n-            @Override\n-            public void onResponse(ClusterStateResponse response) {\n-                try {\n-                    Table tab = buildTable(request, response);\n-                    channel.sendResponse(RestTable.buildResponse(tab, request, channel));\n-                } catch (Throwable e) {\n-                    onFailure(e);\n-                }\n-            }\n-\n-            @Override\n-            public void onFailure(Throwable e) {\n-                try {\n-                    channel.sendResponse(new XContentThrowableRestResponse(request, e));\n-                } catch (IOException e1) {\n-                    logger.error(\"Failed to send failure response\", e1);\n-                }\n-            }\n-        });\n-    }\n-\n-    @Override\n-    void documentation(StringBuilder sb) {\n-        sb.append(\"/_cat_alias\");\n-        sb.append(\"/_cat_alias/{alias}\");\n-    }\n-\n-    @Override\n-    Table getTableWithHeader(RestRequest request) {\n-        final Table table = new Table();\n-        table.startHeaders();\n-        table.addCell(\"alias\", \"desc:alias name\");\n-        table.addCell(\"index\", \"desc:index alias points to\");\n-        table.addCell(\"filter\", \"desc:filter\");\n-        table.addCell(\"index_routing\", \"desc:index routing\");\n-        table.addCell(\"search_routing\", \"desc:search routing\");\n-        table.endHeaders();\n-        return table;\n-    }\n-\n-    private Table buildTable(RestRequest request, ClusterStateResponse response) {\n-        Table table = getTableWithHeader(request);\n-\n-        for (ObjectObjectCursor<String, ImmutableOpenMap<String, AliasMetaData>> cursor : response.getState().getMetaData().aliases()) {\n-            String aliasName = cursor.key;\n-            Iterator<ObjectObjectCursor<String,AliasMetaData>> iterator = cursor.value.iterator();\n-            while (iterator.hasNext()) {\n-                ObjectObjectCursor<String, AliasMetaData> iteratorCursor = iterator.next();\n-                String indexName = iteratorCursor.key;\n-                AliasMetaData aliasMetaData = iteratorCursor.value;\n-\n-                table.startRow();\n-                table.addCell(aliasName);\n-                table.addCell(indexName);\n-                table.addCell(aliasMetaData.filteringRequired() ? \"*\" : \"-\");\n-                String indexRouting = Strings.hasLength(aliasMetaData.indexRouting()) ? aliasMetaData.indexRouting() : \"-\";\n-                table.addCell(indexRouting);\n-                String searchRouting = Strings.hasLength(aliasMetaData.searchRouting()) ? aliasMetaData.searchRouting() : \"-\";\n-                table.addCell(searchRouting);\n-                table.endRow();\n-            }\n-        }\n-\n-        return table;\n-    }\n-\n-}\ndiff --git a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java\nindex b8ceece6732..2b6f21fc034 100644\n--- a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java\n+++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java\n@@ -19,14 +19,9 @@\n \n package org.elasticsearch.search.aggregations.bucket.terms;\n \n-import org.apache.lucene.index.AtomicReaderContext;\n import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash;\n-import org.elasticsearch.common.lucene.ReaderContextAware;\n-import org.elasticsearch.common.util.BigArrays;\n-import org.elasticsearch.common.util.LongArray;\n import org.elasticsearch.index.fielddata.BytesValues;\n-import org.elasticsearch.index.fielddata.ordinals.Ordinals;\n import org.elasticsearch.search.aggregations.Aggregator;\n import org.elasticsearch.search.aggregations.AggregatorFactories;\n import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;\n@@ -34,7 +29,6 @@ import org.elasticsearch.search.aggregations.bucket.terms.support.BucketPriority\n import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;\n import org.elasticsearch.search.aggregations.support.AggregationContext;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n-import org.elasticsearch.search.aggregations.support.bytes.BytesValuesSource;\n \n import java.io.IOException;\n import java.util.Arrays;\n@@ -43,6 +37,7 @@ import java.util.Collections;\n /**\n  * An aggregator of string values.\n  */\n+// TODO we need a similar aggregator that would use ords, similarly to TermsStringOrdinalsFacetExecutor\n public class StringTermsAggregator extends BucketsAggregator {\n \n     private static final int INITIAL_CAPACITY = 50; // TODO sizing\n@@ -51,7 +46,7 @@ public class StringTermsAggregator extends BucketsAggregator {\n     private final InternalOrder order;\n     private final int requiredSize;\n     private final int shardSize;\n-    protected final BytesRefHash bucketOrds;\n+    private final BytesRefHash bucketOrds;\n     private final IncludeExclude includeExclude;\n \n     public StringTermsAggregator(String name, AggregatorFactories factories, ValuesSource valuesSource,\n@@ -135,56 +130,5 @@ public class StringTermsAggregator extends BucketsAggregator {\n         return new StringTerms(name, order, requiredSize, Collections.<InternalTerms.Bucket>emptyList());\n     }\n \n-    /**\n-     * Extension of StringTermsAggregator that caches bucket ords using terms ordinals.\n-     */\n-    public static class WithOrdinals extends StringTermsAggregator implements ReaderContextAware {\n-\n-        private final BytesValuesSource.WithOrdinals valuesSource;\n-        private BytesValues.WithOrdinals bytesValues;\n-        private Ordinals.Docs ordinals;\n-        private LongArray ordinalToBucket;\n-\n-        public WithOrdinals(String name, AggregatorFactories factories, BytesValuesSource.WithOrdinals valuesSource, InternalOrder order, int requiredSize,\n-                int shardSize, AggregationContext aggregationContext, Aggregator parent) {\n-            super(name, factories, valuesSource, order, requiredSize, shardSize, null, aggregationContext, parent);\n-            this.valuesSource = valuesSource;\n-        }\n-\n-        @Override\n-        public void setNextReader(AtomicReaderContext reader) {\n-            bytesValues = valuesSource.bytesValues();\n-            ordinals = bytesValues.ordinals();\n-            final long maxOrd = ordinals.getMaxOrd();\n-            if (ordinalToBucket == null || ordinalToBucket.size() < maxOrd) {\n-                ordinalToBucket = BigArrays.newLongArray(BigArrays.overSize(maxOrd));\n-            }\n-            ordinalToBucket.fill(0, maxOrd, -1L);\n-        }\n-\n-        @Override\n-        public void collect(int doc, long owningBucketOrdinal) throws IOException {\n-            assert owningBucketOrdinal == 0;\n-            final int valuesCount = ordinals.setDocument(doc);\n-\n-            for (int i = 0; i < valuesCount; ++i) {\n-                final long ord = ordinals.nextOrd();\n-                long bucketOrd = ordinalToBucket.get(ord);\n-                if (bucketOrd < 0) { // unlikely condition on a low-cardinality field\n-                    final BytesRef bytes = bytesValues.getValueByOrd(ord);\n-                    final int hash = bytesValues.currentValueHash();\n-                    assert hash == bytes.hashCode();\n-                    bucketOrd = bucketOrds.add(bytes, hash);\n-                    if (bucketOrd < 0) { // already seen in another segment\n-                        bucketOrd = - 1 - bucketOrd;\n-                    }\n-                    ordinalToBucket.set(ord, bucketOrd);\n-                }\n-\n-                collectBucket(doc, bucketOrd);\n-            }\n-        }\n-    }\n-\n }\n \ndiff --git a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java\nindex 246c07d29a5..5090ca4e415 100644\n--- a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java\n+++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java\n@@ -19,10 +19,8 @@\n \n package org.elasticsearch.search.aggregations.bucket.terms;\n \n-import org.elasticsearch.ElasticSearchIllegalArgumentException;\n import org.elasticsearch.search.aggregations.AggregationExecutionException;\n import org.elasticsearch.search.aggregations.Aggregator;\n-import org.elasticsearch.search.aggregations.Aggregator.BucketAggregationMode;\n import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;\n import org.elasticsearch.search.aggregations.support.AggregationContext;\n import org.elasticsearch.search.aggregations.support.ValueSourceAggregatorFactory;\n@@ -36,22 +34,17 @@ import org.elasticsearch.search.aggregations.support.numeric.NumericValuesSource\n  */\n public class TermsAggregatorFactory extends ValueSourceAggregatorFactory {\n \n-    public static final String EXECUTION_HINT_VALUE_MAP = \"map\";\n-    public static final String EXECUTION_HINT_VALUE_ORDINALS = \"ordinals\";\n-\n     private final InternalOrder order;\n     private final int requiredSize;\n     private final int shardSize;\n     private final IncludeExclude includeExclude;\n-    private final String executionHint;\n \n-    public TermsAggregatorFactory(String name, ValuesSourceConfig valueSourceConfig, InternalOrder order, int requiredSize, int shardSize, IncludeExclude includeExclude, String executionHint) {\n+    public TermsAggregatorFactory(String name, ValuesSourceConfig valueSourceConfig, InternalOrder order, int requiredSize, int shardSize, IncludeExclude includeExclude) {\n         super(name, StringTerms.TYPE.name(), valueSourceConfig);\n         this.order = order;\n         this.requiredSize = requiredSize;\n         this.shardSize = shardSize;\n         this.includeExclude = includeExclude;\n-        this.executionHint = executionHint;\n     }\n \n     @Override\n@@ -59,46 +52,10 @@ public class TermsAggregatorFactory extends ValueSourceAggregatorFactory {\n         return new UnmappedTermsAggregator(name, order, requiredSize, aggregationContext, parent);\n     }\n \n-    private static boolean hasParentBucketAggregator(Aggregator parent) {\n-        if (parent == null) {\n-            return false;\n-        } else if (parent.bucketAggregationMode() == BucketAggregationMode.PER_BUCKET) {\n-            return true;\n-        } else {\n-            return hasParentBucketAggregator(parent.parent());\n-        }\n-    }\n-\n     @Override\n     protected Aggregator create(ValuesSource valuesSource, long expectedBucketsCount, AggregationContext aggregationContext, Aggregator parent) {\n         if (valuesSource instanceof BytesValuesSource) {\n-            if (executionHint != null && !executionHint.equals(EXECUTION_HINT_VALUE_MAP) && !executionHint.equals(EXECUTION_HINT_VALUE_ORDINALS)) {\n-                throw new ElasticSearchIllegalArgumentException(\"execution_hint can only be '\" + EXECUTION_HINT_VALUE_MAP + \"' or '\" + EXECUTION_HINT_VALUE_ORDINALS + \"', not \" + executionHint);\n-            }\n-            String execution = executionHint;\n-            if (!(valuesSource instanceof BytesValuesSource.WithOrdinals)) {\n-                execution = EXECUTION_HINT_VALUE_MAP;\n-            } else if (includeExclude != null) {\n-                execution = EXECUTION_HINT_VALUE_MAP;\n-            }\n-            if (execution == null) {\n-                if ((valuesSource instanceof BytesValuesSource.WithOrdinals)\n-                        && !hasParentBucketAggregator(parent)) {\n-                    execution = EXECUTION_HINT_VALUE_ORDINALS;\n-                } else {\n-                    execution = EXECUTION_HINT_VALUE_MAP;\n-                }\n-            }\n-            assert execution != null;\n-\n-            if (execution.equals(EXECUTION_HINT_VALUE_ORDINALS)) {\n-                assert includeExclude == null;\n-                final StringTermsAggregator.WithOrdinals aggregator = new StringTermsAggregator.WithOrdinals(name, factories, (BytesValuesSource.WithOrdinals) valuesSource, order, requiredSize, shardSize, aggregationContext, parent);\n-                aggregationContext.registerReaderContextAware(aggregator);\n-                return aggregator;\n-            } else {\n-                return new StringTermsAggregator(name, factories, valuesSource, order, requiredSize, shardSize, includeExclude, aggregationContext, parent);\n-            }\n+            return new StringTermsAggregator(name, factories, valuesSource, order, requiredSize, shardSize, includeExclude, aggregationContext, parent);\n         }\n \n         if (includeExclude != null) {\ndiff --git a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java\nindex 4a1bce95a24..44396de3560 100644\n--- a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java\n+++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java\n@@ -20,7 +20,6 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {\n     private int includeFlags;\n     private String excludePattern;\n     private int excludeFlags;\n-    private String executionHint;\n \n     public TermsBuilder(String name) {\n         super(name, \"terms\");\n@@ -103,11 +102,6 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {\n         return this;\n     }\n \n-    public TermsBuilder executionHint(String executionHint) {\n-        this.executionHint = executionHint;\n-        return this;\n-    }\n-\n     @Override\n     protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {\n         if (size >=0) {\n@@ -143,9 +137,6 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {\n                         .endObject();\n             }\n         }\n-        if (executionHint != null) {\n-            builder.field(\"execution_hint\", executionHint);\n-        }\n         return builder;\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java\nindex 477811d4f09..4ea1085aed6 100644\n--- a/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java\n+++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java\n@@ -72,7 +72,6 @@ public class TermsParser implements Aggregator.Parser {\n         int includeFlags = 0; // 0 means no flags\n         String exclude = null;\n         int excludeFlags = 0; // 0 means no flags\n-        String executionHint = null;\n \n \n         XContentParser.Token token;\n@@ -95,8 +94,6 @@ public class TermsParser implements Aggregator.Parser {\n                     include = parser.text();\n                 } else if (\"exclude\".equals(currentFieldName)) {\n                     exclude = parser.text();\n-                } else if (\"execution_hint\".equals(currentFieldName) || \"executionHint\".equals(currentFieldName)) {\n-                    executionHint = parser.text();\n                 }\n             } else if (token == XContentParser.Token.VALUE_BOOLEAN) {\n                 if (\"script_values_unique\".equals(currentFieldName)) {\n@@ -195,14 +192,14 @@ public class TermsParser implements Aggregator.Parser {\n             if (!assumeUnique) {\n                 config.ensureUnique(true);\n             }\n-            return new TermsAggregatorFactory(aggregationName, config, order, requiredSize, shardSize, includeExclude, executionHint);\n+            return new TermsAggregatorFactory(aggregationName, config, order, requiredSize, shardSize, includeExclude);\n         }\n \n         FieldMapper<?> mapper = context.smartNameFieldMapper(field);\n         if (mapper == null) {\n             ValuesSourceConfig<?> config = new ValuesSourceConfig<BytesValuesSource>(BytesValuesSource.class);\n             config.unmapped(true);\n-            return new TermsAggregatorFactory(aggregationName, config, order, requiredSize, shardSize, includeExclude, executionHint);\n+            return new TermsAggregatorFactory(aggregationName, config, order, requiredSize, shardSize, includeExclude);\n         }\n         IndexFieldData<?> indexFieldData = context.fieldData().getForField(mapper);\n \n@@ -244,7 +241,7 @@ public class TermsParser implements Aggregator.Parser {\n             config.ensureUnique(true);\n         }\n \n-        return new TermsAggregatorFactory(aggregationName, config, order, requiredSize, shardSize, includeExclude, executionHint);\n+        return new TermsAggregatorFactory(aggregationName, config, order, requiredSize, shardSize, includeExclude);\n     }\n \n     static InternalOrder resolveOrder(String key, boolean asc) {\ndiff --git a/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java b/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java\nindex 16b3093cda9..54947645c56 100644\n--- a/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java\n+++ b/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java\n@@ -27,11 +27,9 @@ import org.apache.lucene.util.RamUsageEstimator;\n import org.elasticsearch.cache.recycler.CacheRecycler;\n import org.elasticsearch.common.lucene.ReaderContextAware;\n import org.elasticsearch.common.lucene.ScorerAware;\n-import org.elasticsearch.index.fielddata.IndexFieldData;\n import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;\n import org.elasticsearch.index.fielddata.IndexNumericFieldData;\n import org.elasticsearch.search.aggregations.AggregationExecutionException;\n-import org.elasticsearch.search.aggregations.support.FieldDataSource.Uniqueness;\n import org.elasticsearch.search.aggregations.support.bytes.BytesValuesSource;\n import org.elasticsearch.search.aggregations.support.geopoints.GeoPointValuesSource;\n import org.elasticsearch.search.aggregations.support.numeric.NumericValuesSource;\n@@ -162,15 +160,10 @@ public class AggregationContext implements ReaderContextAware, ScorerAware {\n         return new NumericValuesSource(dataSource, config.formatter(), config.parser());\n     }\n \n-    private ValuesSource bytesField(ObjectObjectOpenHashMap<String, FieldDataSource> fieldDataSources, ValuesSourceConfig<?> config) {\n+    private BytesValuesSource bytesField(ObjectObjectOpenHashMap<String, FieldDataSource> fieldDataSources, ValuesSourceConfig<?> config) {\n         FieldDataSource dataSource = fieldDataSources.get(config.fieldContext.field());\n         if (dataSource == null) {\n-            final IndexFieldData<?> indexFieldData = config.fieldContext.indexFieldData();\n-            if (indexFieldData instanceof IndexFieldData.WithOrdinals<?>) {\n-                dataSource = new FieldDataSource.Bytes.WithOrdinals.FieldData((IndexFieldData.WithOrdinals<?>) indexFieldData);\n-            } else {\n-                dataSource = new FieldDataSource.Bytes.FieldData(indexFieldData);\n-            }\n+            dataSource = new FieldDataSource.Bytes.FieldData(config.fieldContext.indexFieldData());\n             setReaderIfNeeded((ReaderContextAware) dataSource);\n             readerAwares.add((ReaderContextAware) dataSource);\n             fieldDataSources.put(config.fieldContext.field(), dataSource);\n@@ -185,19 +178,14 @@ public class AggregationContext implements ReaderContextAware, ScorerAware {\n         // Even in case we wrap field data, we might still need to wrap for sorting, because the wrapped field data might be\n         // eg. a numeric field data that doesn't sort according to the byte order. However field data values are unique so no\n         // need to wrap for uniqueness\n-        if ((config.ensureUnique && dataSource.getUniqueness() != Uniqueness.UNIQUE) || config.ensureSorted) {\n+        if ((config.ensureUnique && !(dataSource instanceof FieldDataSource.Bytes.FieldData)) || config.ensureSorted) {\n             dataSource = new FieldDataSource.Bytes.SortedAndUnique(dataSource);\n             readerAwares.add((ReaderContextAware) dataSource);\n         }\n-\n         if (config.needsHashes) { // the data source needs hash if at least one consumer needs hashes\n             dataSource.setNeedsHashes(true);\n         }\n-        if (dataSource instanceof FieldDataSource.Bytes.WithOrdinals) {\n-            return new BytesValuesSource.WithOrdinals((FieldDataSource.Bytes.WithOrdinals) dataSource);\n-        } else {\n-            return new BytesValuesSource(dataSource);\n-        }\n+        return new BytesValuesSource(dataSource);\n     }\n \n     private BytesValuesSource bytesScript(ValuesSourceConfig<?> config) {\ndiff --git a/src/main/java/org/elasticsearch/search/aggregations/support/FieldDataSource.java b/src/main/java/org/elasticsearch/search/aggregations/support/FieldDataSource.java\nindex ac7c121ea55..dc87f3922d2 100644\n--- a/src/main/java/org/elasticsearch/search/aggregations/support/FieldDataSource.java\n+++ b/src/main/java/org/elasticsearch/search/aggregations/support/FieldDataSource.java\n@@ -58,51 +58,6 @@ public abstract class FieldDataSource {\n \n     public static abstract class Bytes extends FieldDataSource {\n \n-        public static abstract class WithOrdinals extends Bytes {\n-\n-            public abstract BytesValues.WithOrdinals bytesValues();\n-\n-            public static class FieldData extends WithOrdinals implements ReaderContextAware {\n-\n-                protected boolean needsHashes;\n-                protected final IndexFieldData.WithOrdinals<?> indexFieldData;\n-                protected AtomicFieldData.WithOrdinals<?> atomicFieldData;\n-                private BytesValues.WithOrdinals bytesValues;\n-\n-                public FieldData(IndexFieldData.WithOrdinals<?> indexFieldData) {\n-                    this.indexFieldData = indexFieldData;\n-                    needsHashes = false;\n-                }\n-\n-                @Override\n-                public Uniqueness getUniqueness() {\n-                    return Uniqueness.UNIQUE;\n-                }\n-\n-                public final void setNeedsHashes(boolean needsHashes) {\n-                    this.needsHashes = needsHashes;\n-                }\n-\n-                @Override\n-                public void setNextReader(AtomicReaderContext reader) {\n-                    atomicFieldData = indexFieldData.load(reader);\n-                    if (bytesValues != null) {\n-                        bytesValues = atomicFieldData.getBytesValues(needsHashes);\n-                    }\n-                }\n-\n-                @Override\n-                public BytesValues.WithOrdinals bytesValues() {\n-                    if (bytesValues == null) {\n-                        bytesValues = atomicFieldData.getBytesValues(needsHashes);\n-                    }\n-                    return bytesValues;\n-                }\n-\n-            }\n-\n-        }\n-\n         public static class FieldData extends Bytes implements ReaderContextAware {\n \n             protected boolean needsHashes;\ndiff --git a/src/main/java/org/elasticsearch/search/aggregations/support/bytes/BytesValuesSource.java b/src/main/java/org/elasticsearch/search/aggregations/support/bytes/BytesValuesSource.java\nindex 7f47c703a5e..ca159298019 100644\n--- a/src/main/java/org/elasticsearch/search/aggregations/support/bytes/BytesValuesSource.java\n+++ b/src/main/java/org/elasticsearch/search/aggregations/support/bytes/BytesValuesSource.java\n@@ -26,7 +26,7 @@ import org.elasticsearch.search.aggregations.support.ValuesSource;\n /**\n  *\n  */\n-public class BytesValuesSource implements ValuesSource {\n+public final class BytesValuesSource implements ValuesSource {\n \n     private final FieldDataSource source;\n \n@@ -39,20 +39,4 @@ public class BytesValuesSource implements ValuesSource {\n         return source.bytesValues();\n     }\n \n-    public static final class WithOrdinals extends BytesValuesSource {\n-\n-        private final FieldDataSource.Bytes.WithOrdinals source;\n-\n-        public WithOrdinals(FieldDataSource.Bytes.WithOrdinals source) {\n-            super(source);\n-            this.source = source;\n-        }\n-\n-        @Override\n-        public BytesValues.WithOrdinals bytesValues() {\n-            return source.bytesValues();\n-        }\n-\n-    }\n-\n }\ndiff --git a/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java\nindex fbf863811d7..5967e59f523 100644\n--- a/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java\n+++ b/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java\n@@ -77,7 +77,7 @@ public class SearchSourceBuilder implements ToXContent {\n \n     private BytesReference queryBinary;\n \n-    private FilterBuilder postFilterBuilder;\n+    private FilterBuilder filterBuilder;\n \n     private BytesReference filterBinary;\n \n@@ -186,11 +186,11 @@ public class SearchSourceBuilder implements ToXContent {\n     }\n \n     /**\n-     * Sets a filter that will be executed after the query has been executed and only has affect on the search hits\n-     * (not aggregations or facets). This filter is always executed as last filtering mechanism.\n+     * Sets a filter on the query executed that only applies to the search query\n+     * (and not facets for example).\n      */\n-    public SearchSourceBuilder postFilter(FilterBuilder postFilter) {\n-        this.postFilterBuilder = postFilter;\n+    public SearchSourceBuilder filter(FilterBuilder filter) {\n+        this.filterBuilder = filter;\n         return this;\n     }\n \n@@ -198,52 +198,52 @@ public class SearchSourceBuilder implements ToXContent {\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchSourceBuilder postFilter(String postFilterString) {\n-        return postFilter(postFilterString.getBytes(Charsets.UTF_8));\n+    public SearchSourceBuilder filter(String filterString) {\n+        return filter(filterString.getBytes(Charsets.UTF_8));\n     }\n \n     /**\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchSourceBuilder postFilter(byte[] postFilter) {\n-        return postFilter(postFilter, 0, postFilter.length);\n+    public SearchSourceBuilder filter(byte[] filter) {\n+        return filter(filter, 0, filter.length);\n     }\n \n     /**\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchSourceBuilder postFilter(byte[] postFilterBinary, int postFilterBinaryOffset, int postFilterBinaryLength) {\n-        return postFilter(new BytesArray(postFilterBinary, postFilterBinaryOffset, postFilterBinaryLength));\n+    public SearchSourceBuilder filter(byte[] filterBinary, int filterBinaryOffset, int filterBinaryLength) {\n+        return filter(new BytesArray(filterBinary, filterBinaryOffset, filterBinaryLength));\n     }\n \n     /**\n      * Sets a filter on the query executed that only applies to the search query\n      * (and not facets for example).\n      */\n-    public SearchSourceBuilder postFilter(BytesReference postFilterBinary) {\n-        this.filterBinary = postFilterBinary;\n+    public SearchSourceBuilder filter(BytesReference filterBinary) {\n+        this.filterBinary = filterBinary;\n         return this;\n     }\n \n     /**\n      * Constructs a new search source builder with a query from a builder.\n      */\n-    public SearchSourceBuilder postFilter(XContentBuilder postFilter) {\n-        return postFilter(postFilter.bytes());\n+    public SearchSourceBuilder filter(XContentBuilder filter) {\n+        return filter(filter.bytes());\n     }\n \n     /**\n      * Constructs a new search source builder with a query from a map.\n      */\n-    public SearchSourceBuilder postFilter(Map postFilter) {\n+    public SearchSourceBuilder filter(Map filter) {\n         try {\n             XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);\n-            builder.map(postFilter);\n-            return postFilter(builder);\n+            builder.map(filter);\n+            return filter(builder);\n         } catch (IOException e) {\n-            throw new ElasticSearchGenerationException(\"Failed to generate [\" + postFilter + \"]\", e);\n+            throw new ElasticSearchGenerationException(\"Failed to generate [\" + filter + \"]\", e);\n         }\n     }\n \n@@ -714,9 +714,9 @@ public class SearchSourceBuilder implements ToXContent {\n             }\n         }\n \n-        if (postFilterBuilder != null) {\n-            builder.field(\"post_filter\");\n-            postFilterBuilder.toXContent(builder, params);\n+        if (filterBuilder != null) {\n+            builder.field(\"filter\");\n+            filterBuilder.toXContent(builder, params);\n         }\n \n         if (filterBinary != null) {\ndiff --git a/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java b/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java\nindex 3175ae69f84..c2075d52d3c 100644\n--- a/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java\n+++ b/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java\n@@ -58,7 +58,7 @@ public class MatchedQueriesFetchSubPhase implements FetchSubPhase {\n     @Override\n     public boolean hitExecutionNeeded(SearchContext context) {\n         return !context.parsedQuery().namedFilters().isEmpty()\n-                || (context.parsedPostFilter() !=null && !context.parsedPostFilter().namedFilters().isEmpty());\n+                || (context.parsedFilter() !=null && !context.parsedFilter().namedFilters().isEmpty());\n     }\n \n     @Override\n@@ -67,8 +67,8 @@ public class MatchedQueriesFetchSubPhase implements FetchSubPhase {\n \n         addMatchedQueries(hitContext, context.parsedQuery().namedFilters(), matchedQueries);\n \n-        if (context.parsedPostFilter() != null) {\n-            addMatchedQueries(hitContext, context.parsedPostFilter().namedFilters(), matchedQueries);\n+        if (context.parsedFilter() != null) {\n+            addMatchedQueries(hitContext, context.parsedFilter().namedFilters(), matchedQueries);\n         }\n \n         hitContext.hit().matchedQueries(matchedQueries.toArray(new String[matchedQueries.size()]));\ndiff --git a/src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java b/src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java\nindex b6de209405d..ca2d3a56feb 100644\n--- a/src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java\n+++ b/src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java\n@@ -104,7 +104,7 @@ public class FastVectorHighlighter implements Highlighter {\n                 if (field.numberOfFragments() == 0) {\n                     fragListBuilder = new SingleFragListBuilder();\n \n-                    if (!field.forceSource() && mapper.fieldType().stored()) {\n+                    if (mapper.fieldType().stored()) {\n                         fragmentsBuilder = new SimpleFragmentsBuilder(mapper, field.preTags(), field.postTags(), boundaryScanner);\n                     } else {\n                         fragmentsBuilder = new SourceSimpleFragmentsBuilder(mapper, context, field.preTags(), field.postTags(), boundaryScanner);\n@@ -112,13 +112,13 @@ public class FastVectorHighlighter implements Highlighter {\n                 } else {\n                     fragListBuilder = field.fragmentOffset() == -1 ? new SimpleFragListBuilder() : new SimpleFragListBuilder(field.fragmentOffset());\n                     if (field.scoreOrdered()) {\n-                        if (!field.forceSource() && mapper.fieldType().stored()) {\n+                        if (mapper.fieldType().stored()) {\n                             fragmentsBuilder = new ScoreOrderFragmentsBuilder(field.preTags(), field.postTags(), boundaryScanner);\n                         } else {\n                             fragmentsBuilder = new SourceScoreOrderFragmentsBuilder(mapper, context, field.preTags(), field.postTags(), boundaryScanner);\n                         }\n                     } else {\n-                        if (!field.forceSource() && mapper.fieldType().stored()) {\n+                        if (mapper.fieldType().stored()) {\n                             fragmentsBuilder = new SimpleFragmentsBuilder(mapper, field.preTags(), field.postTags(), boundaryScanner);\n                         } else {\n                             fragmentsBuilder = new SourceSimpleFragmentsBuilder(mapper, context, field.preTags(), field.postTags(), boundaryScanner);\ndiff --git a/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java b/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java\nindex 1946fdbd696..42099f79e77 100644\n--- a/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java\n+++ b/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java\n@@ -60,8 +60,6 @@ public class HighlightBuilder implements ToXContent {\n \n     private Map<String, Object> options;\n \n-    private Boolean forceSource;\n-\n     /**\n      * Adds a field to be highlighted with default fragment size of 100 characters, and\n      * default number of fragments of 5 using the default encoder\n@@ -235,14 +233,6 @@ public class HighlightBuilder implements ToXContent {\n         return this;\n     }\n \n-    /**\n-     * Forces the highlighting to highlight fields based on the source even if fields are stored separately.\n-     */\n-    public HighlightBuilder forceSource(boolean forceSource) {\n-        this.forceSource = forceSource;\n-        return this;\n-    }\n-\n     @Override\n     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n         builder.startObject(\"highlight\");\n@@ -279,9 +269,6 @@ public class HighlightBuilder implements ToXContent {\n         if (options != null && options.size() > 0) {\n             builder.field(\"options\", options);\n         }\n-        if (forceSource != null) {\n-            builder.field(\"force_source\", forceSource);\n-        }\n         if (fields != null) {\n             builder.startObject(\"fields\");\n             for (Field field : fields) {\n@@ -334,9 +321,6 @@ public class HighlightBuilder implements ToXContent {\n                 if (field.options != null && field.options.size() > 0) {\n                     builder.field(\"options\", field.options);\n                 }\n-                if (field.forceSource != null) {\n-                    builder.field(\"force_source\", forceSource);\n-                }\n \n                 builder.endObject();\n             }\n@@ -365,7 +349,6 @@ public class HighlightBuilder implements ToXContent {\n         Integer noMatchSize;\n         String[] matchedFields;\n         Map<String, Object> options;\n-        Boolean forceSource;\n \n         public Field(String name) {\n             this.name = name;\n@@ -496,14 +479,5 @@ public class HighlightBuilder implements ToXContent {\n             this.matchedFields = matchedFields;\n             return this;\n         }\n-\n-        /**\n-         * Forces the highlighting to highlight this field based on the source even if this field is stored separately.\n-         */\n-        public Field forceSource(boolean forceSource) {\n-            this.forceSource = forceSource;\n-            return this;\n-        }\n-\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java b/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java\nindex 881c4702b50..8804733a7fe 100644\n--- a/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java\n+++ b/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java\n@@ -31,7 +31,6 @@ import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.index.mapper.DocumentMapper;\n import org.elasticsearch.index.mapper.FieldMapper;\n import org.elasticsearch.index.mapper.MapperService;\n-import org.elasticsearch.index.mapper.internal.SourceFieldMapper;\n import org.elasticsearch.search.SearchParseElement;\n import org.elasticsearch.search.fetch.FetchSubPhase;\n import org.elasticsearch.search.internal.InternalSearchHit;\n@@ -86,13 +85,6 @@ public class HighlightPhase extends AbstractComponent implements FetchSubPhase {\n                 fieldNamesToHighlight = ImmutableSet.of(field.field());\n             }\n \n-            if (field.forceSource()) {\n-                SourceFieldMapper sourceFieldMapper = context.mapperService().documentMapper(hitContext.hit().type()).sourceMapper();\n-                if (!sourceFieldMapper.enabled()) {\n-                    throw new ElasticSearchIllegalArgumentException(\"source is forced for field [\" +  field.field() + \"] but type [\" + hitContext.hit().type() + \"] has disabled _source\");\n-                }\n-            }\n-\n             for (String fieldName : fieldNamesToHighlight) {\n                 FieldMapper<?> fieldMapper = getMapperForField(fieldName, context, hitContext);\n                 if (fieldMapper == null) {\ndiff --git a/src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java b/src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java\nindex 963691189cb..4fbab0c34ca 100644\n--- a/src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java\n+++ b/src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java\n@@ -41,9 +41,9 @@ public final class HighlightUtils {\n \n     }\n \n-    static List<Object> loadFieldValues(FieldMapper<?> mapper, SearchContext searchContext, FetchSubPhase.HitContext hitContext, boolean forceSource) throws IOException {\n+    static List<Object> loadFieldValues(FieldMapper<?> mapper, SearchContext searchContext, FetchSubPhase.HitContext hitContext) throws IOException {\n         List<Object> textsToHighlight;\n-        if (!forceSource && mapper.fieldType().stored()) {\n+        if (mapper.fieldType().stored()) {\n             CustomFieldsVisitor fieldVisitor = new CustomFieldsVisitor(ImmutableSet.of(mapper.names().indexName()), false);\n             hitContext.reader().document(hitContext.docId(), fieldVisitor);\n             textsToHighlight = fieldVisitor.fields().get(mapper.names().indexName());\ndiff --git a/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java b/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java\nindex c50933c1c40..c022fb4da77 100644\n--- a/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java\n+++ b/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java\n@@ -75,7 +75,6 @@ public class HighlighterParseElement implements SearchParseElement {\n         boolean globalScoreOrdered = false;\n         boolean globalHighlightFilter = false;\n         boolean globalRequireFieldMatch = false;\n-        boolean globalForceSource = false;\n         int globalFragmentSize = 100;\n         int globalNumOfFragments = 5;\n         String globalEncoder = \"default\";\n@@ -137,8 +136,6 @@ public class HighlighterParseElement implements SearchParseElement {\n                     globalFragmenter = parser.text();\n                 } else if (\"no_match_size\".equals(topLevelFieldName) || \"noMatchSize\".equals(topLevelFieldName)) {\n                     globalNoMatchSize = parser.intValue();\n-                } else if (\"force_source\".equals(topLevelFieldName) || \"forceSource\".equals(topLevelFieldName)) {\n-                    globalForceSource = parser.booleanValue();\n                 }\n             } else if (token == XContentParser.Token.START_OBJECT && \"options\".equals(topLevelFieldName)) {\n                 globalOptions = parser.map();\n@@ -202,8 +199,6 @@ public class HighlighterParseElement implements SearchParseElement {\n                                         field.fragmenter(parser.text());\n                                     } else if (\"no_match_size\".equals(fieldName) || \"noMatchSize\".equals(fieldName)) {\n                                         field.noMatchSize(parser.intValue());\n-                                    } else if (\"force_source\".equals(fieldName) || \"forceSource\".equals(fieldName)) {\n-                                        field.forceSource(parser.booleanValue());\n                                     }\n                                 } else if (token == XContentParser.Token.START_OBJECT) {\n                                     if (\"highlight_query\".equals(fieldName) || \"highlightQuery\".equals(fieldName)) {\n@@ -272,9 +267,6 @@ public class HighlighterParseElement implements SearchParseElement {\n             if (field.noMatchSize() == -1) {\n                 field.noMatchSize(globalNoMatchSize);\n             }\n-            if (field.forceSource() == null) {\n-                field.forceSource(globalForceSource);\n-            }\n         }\n \n         context.highlight(new SearchContextHighlight(fields));\ndiff --git a/src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java b/src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java\nindex f67665ce422..f6596f4a1a3 100644\n--- a/src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java\n+++ b/src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java\n@@ -99,7 +99,7 @@ public class PlainHighlighter implements Highlighter {\n         List<Object> textsToHighlight;\n \n         try {\n-            textsToHighlight = HighlightUtils.loadFieldValues(mapper, context, hitContext, field.forceSource());\n+            textsToHighlight = HighlightUtils.loadFieldValues(mapper, context, hitContext);\n \n             for (Object textToHighlight : textsToHighlight) {\n                 String text = textToHighlight.toString();\ndiff --git a/src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java b/src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java\nindex b850a1bbeeb..bbf3a17b944 100644\n--- a/src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java\n+++ b/src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java\n@@ -93,7 +93,7 @@ public class PostingsHighlighter implements Highlighter {\n \n         try {\n             //we manually load the field values (from source if needed)\n-            List<Object> textsToHighlight = HighlightUtils.loadFieldValues(fieldMapper, context, hitContext, field.forceSource());\n+            List<Object> textsToHighlight = HighlightUtils.loadFieldValues(fieldMapper, context, hitContext);\n             CustomPostingsHighlighter highlighter = new CustomPostingsHighlighter(mapperHighlighterEntry.passageFormatter, textsToHighlight, mergeValues, Integer.MAX_VALUE-1, field.noMatchSize());\n \n              if (field.numberOfFragments() == 0) {\ndiff --git a/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java b/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java\nindex d43b0552d50..157f01251a2 100644\n--- a/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java\n+++ b/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java\n@@ -64,8 +64,6 @@ public class SearchContextHighlight {\n \n         private String highlighterType;\n \n-        private Boolean forceSource;\n-\n         private String fragmenter;\n \n         private int boundaryMaxScan = -1;\n@@ -168,14 +166,6 @@ public class SearchContextHighlight {\n             this.highlighterType = type;\n         }\n \n-        public Boolean forceSource() {\n-            return forceSource;\n-        }\n-\n-        public void forceSource(boolean forceSource) {\n-            this.forceSource = forceSource;\n-        }\n-\n         public String fragmenter() {\n             return fragmenter;\n         }\ndiff --git a/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java b/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java\nindex 7b97c358989..02438242943 100644\n--- a/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java\n+++ b/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java\n@@ -145,11 +145,11 @@ public class ContextIndexSearcher extends IndexSearcher {\n                 // TODO should we create a cache of segment->docIdSets so we won't create one each time?\n                 collector = this.mainDocIdSetCollector = new DocIdSetCollector(searchContext.docSetCache(), collector);\n             }\n-            if (searchContext.parsedPostFilter() != null) {\n+            if (searchContext.parsedFilter() != null) {\n                 // this will only get applied to the actual search collector and not\n                 // to any scoped collectors, also, it will only be applied to the main collector\n                 // since that is where the filter should only work\n-                collector = new FilteredCollector(collector, searchContext.parsedPostFilter().filter());\n+                collector = new FilteredCollector(collector, searchContext.parsedFilter().filter());\n             }\n             if (queryCollectors != null && !queryCollectors.isEmpty()) {\n                 collector = new MultiCollector(collector, queryCollectors.toArray(new Collector[queryCollectors.size()]));\ndiff --git a/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java\nindex 5695446f96c..5640d42bd05 100644\n--- a/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java\n+++ b/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java\n@@ -137,7 +137,7 @@ public class DefaultSearchContext extends SearchContext {\n \n     private Query query;\n \n-    private ParsedFilter postFilter;\n+    private ParsedFilter filter;\n \n     private Filter aliasFilter;\n \n@@ -471,13 +471,13 @@ public class DefaultSearchContext extends SearchContext {\n         return this.trackScores;\n     }\n \n-    public SearchContext parsedPostFilter(ParsedFilter postFilter) {\n-        this.postFilter = postFilter;\n+    public SearchContext parsedFilter(ParsedFilter filter) {\n+        this.filter = filter;\n         return this;\n     }\n \n-    public ParsedFilter parsedPostFilter() {\n-        return this.postFilter;\n+    public ParsedFilter parsedFilter() {\n+        return this.filter;\n     }\n \n     public Filter aliasFilter() {\ndiff --git a/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/src/main/java/org/elasticsearch/search/internal/SearchContext.java\nindex 3b82d3382e2..ce48be5e1c1 100644\n--- a/src/main/java/org/elasticsearch/search/internal/SearchContext.java\n+++ b/src/main/java/org/elasticsearch/search/internal/SearchContext.java\n@@ -199,9 +199,9 @@ public abstract class SearchContext implements Releasable {\n \n     public abstract boolean trackScores();\n \n-    public abstract SearchContext parsedPostFilter(ParsedFilter postFilter);\n+    public abstract SearchContext parsedFilter(ParsedFilter filter);\n \n-    public abstract ParsedFilter parsedPostFilter();\n+    public abstract ParsedFilter parsedFilter();\n \n     public abstract Filter aliasFilter();\n \ndiff --git a/src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java b/src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java\nindex 6de16603691..f574bbfd5a9 100644\n--- a/src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java\n+++ b/src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java\n@@ -37,7 +37,7 @@ public class FilterBinaryParseElement implements SearchParseElement {\n         try {\n             ParsedFilter filter = context.queryParserService().parseInnerFilter(fSourceParser);\n             if (filter != null) {\n-                context.parsedPostFilter(filter);\n+                context.parsedFilter(filter);\n             }\n         } finally {\n             fSourceParser.close();\ndiff --git a/src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java b/src/main/java/org/elasticsearch/search/query/FilterParseElement.java\nsimilarity index 83%\nrename from src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java\nrename to src/main/java/org/elasticsearch/search/query/FilterParseElement.java\nindex 5af05e135a2..66e2967cb20 100644\n--- a/src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java\n+++ b/src/main/java/org/elasticsearch/search/query/FilterParseElement.java\n@@ -27,13 +27,13 @@ import org.elasticsearch.search.internal.SearchContext;\n /**\n  *\n  */\n-public class PostFilterParseElement implements SearchParseElement {\n+public class FilterParseElement implements SearchParseElement {\n \n     @Override\n     public void parse(XContentParser parser, SearchContext context) throws Exception {\n-        ParsedFilter postFilter = context.queryParserService().parseInnerFilter(parser);\n-        if (postFilter != null) {\n-            context.parsedPostFilter(postFilter);\n+        ParsedFilter filter = context.queryParserService().parseInnerFilter(parser);\n+        if (filter != null) {\n+            context.parsedFilter(filter);\n         }\n     }\n }\n\\ No newline at end of file\ndiff --git a/src/main/java/org/elasticsearch/search/query/QueryPhase.java b/src/main/java/org/elasticsearch/search/query/QueryPhase.java\nindex 81408f6fdd7..276f241ca35 100644\n--- a/src/main/java/org/elasticsearch/search/query/QueryPhase.java\n+++ b/src/main/java/org/elasticsearch/search/query/QueryPhase.java\n@@ -66,9 +66,7 @@ public class QueryPhase implements SearchPhase {\n                 .put(\"query\", new QueryParseElement())\n                 .put(\"queryBinary\", new QueryBinaryParseElement())\n                 .put(\"query_binary\", new QueryBinaryParseElement())\n-                .put(\"filter\", new PostFilterParseElement()) // For bw comp reason, should be removed in version 1.1\n-                .put(\"post_filter\", new PostFilterParseElement())\n-                .put(\"postFilter\", new PostFilterParseElement())\n+                .put(\"filter\", new FilterParseElement())\n                 .put(\"filterBinary\", new FilterBinaryParseElement())\n                 .put(\"filter_binary\", new FilterBinaryParseElement())\n                 .put(\"sort\", new SortParseElement())\ndiff --git a/src/main/java/org/elasticsearch/search/suggest/Suggest.java b/src/main/java/org/elasticsearch/search/suggest/Suggest.java\nindex 1a5353e2532..1776dadd4c5 100644\n--- a/src/main/java/org/elasticsearch/search/suggest/Suggest.java\n+++ b/src/main/java/org/elasticsearch/search/suggest/Suggest.java\n@@ -20,7 +20,6 @@ package org.elasticsearch.search.suggest;\n \n import org.apache.lucene.util.CollectionUtil;\n import org.elasticsearch.ElasticSearchException;\n-import org.elasticsearch.ElasticSearchIllegalStateException;\n import org.elasticsearch.Version;\n import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n@@ -251,11 +250,7 @@ public class Suggest implements Iterable<Suggest.Suggestion<? extends Entry<? ex\n             List<T> currentEntries = new ArrayList<T>();\n             for (int i = 0; i < size; i++) {\n                 for (Suggestion<T> suggestion : toReduce) {\n-                    if(suggestion.entries.size() != size) {\n-                        throw new ElasticSearchIllegalStateException(\"Can't merge suggest result, this might be caused by suggest calls \" +\n-                                \"across multiple indices with different analysis chains. Suggest entries have different sizes actual [\" +\n-                                suggestion.entries.size() + \"] expected [\" + size +\"]\");\n-                    }\n+                    assert suggestion.entries.size() == size;\n                     assert suggestion.name.equals(leader.name);\n                     currentEntries.add(suggestion.entries.get(i));\n                 }\n@@ -366,18 +361,14 @@ public class Suggest implements Iterable<Suggest.Suggestion<? extends Entry<? ex\n                 CollectionUtil.timSort(options, comparator);\n             }\n \n-            protected <T extends Entry<O>> Entry<O> reduce(List<T> toReduce) {\n+            protected Entry<O> reduce(List<? extends Entry<O>> toReduce) {\n                 if (toReduce.size() == 1) {\n                     return toReduce.get(0);\n                 }\n                 final Map<O, O> entries = new HashMap<O, O>();\n                 Entry<O> leader = toReduce.get(0);\n                 for (Entry<O> entry : toReduce) {\n-                    if (!leader.text.equals(entry.text)) {\n-                        throw new ElasticSearchIllegalStateException(\"Can't merge suggest entries, this might be caused by suggest calls \" +\n-                                \"across multiple indices with different analysis chains. Suggest entries have different text actual [\" +\n-                                entry.text + \"] expected [\" + leader.text +\"]\");\n-                    }\n+                    assert leader.text.equals(entry.text);\n                     assert leader.offset == entry.offset;\n                     assert leader.length == entry.length;\n                     leader.merge(entry);\ndiff --git a/src/main/java/org/elasticsearch/transport/Transport.java b/src/main/java/org/elasticsearch/transport/Transport.java\nindex e377252d853..741a34cf2b1 100644\n--- a/src/main/java/org/elasticsearch/transport/Transport.java\n+++ b/src/main/java/org/elasticsearch/transport/Transport.java\n@@ -31,10 +31,6 @@ import java.io.IOException;\n  */\n public interface Transport extends LifecycleComponent<Transport> {\n \n-    public static class TransportSettings {\n-        public static final String TRANSPORT_TCP_COMPRESS = \"transport.tcp.compress\";\n-    }\n-\n     void transportServiceAdapter(TransportServiceAdapter service);\n \n     /**\ndiff --git a/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java\nindex 95e8e534a0f..1a50c689e10 100644\n--- a/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java\n+++ b/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java\n@@ -179,7 +179,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem\n         this.port = componentSettings.get(\"port\", settings.get(\"transport.tcp.port\", \"9300-9400\"));\n         this.bindHost = componentSettings.get(\"bind_host\", settings.get(\"transport.bind_host\", settings.get(\"transport.host\")));\n         this.publishHost = componentSettings.get(\"publish_host\", settings.get(\"transport.publish_host\", settings.get(\"transport.host\")));\n-        this.compress = settings.getAsBoolean(TransportSettings.TRANSPORT_TCP_COMPRESS, false);\n+        this.compress = settings.getAsBoolean(\"transport.tcp.compress\", false);\n         this.connectTimeout = componentSettings.getAsTime(\"connect_timeout\", settings.getAsTime(\"transport.tcp.connect_timeout\", settings.getAsTime(TCP_CONNECT_TIMEOUT, TCP_DEFAULT_CONNECT_TIMEOUT)));\n         this.tcpNoDelay = componentSettings.getAsBoolean(\"tcp_no_delay\", settings.getAsBoolean(TCP_NO_DELAY, true));\n         this.tcpKeepAlive = componentSettings.getAsBoolean(\"tcp_keep_alive\", settings.getAsBoolean(TCP_KEEP_ALIVE, true));\n@@ -542,9 +542,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem\n         BytesStreamOutput bStream = new BytesStreamOutput();\n         bStream.skip(NettyHeader.HEADER_SIZE);\n         StreamOutput stream = bStream;\n-        // only compress if asked, and, the request is not bytes, since then only\n-        // the header part is compressed, and the \"body\" can't be extracted as compressed\n-        if (options.compress() && (!(request instanceof BytesTransportRequest))) {\n+        if (options.compress()) {\n             status = TransportStatus.setCompress(status);\n             stream = CompressorFactory.defaultCompressor().streamOutput(stream);\n         }\ndiff --git a/src/rpm/init.d/elasticsearch b/src/rpm/init.d/elasticsearch\nindex f1a724fb7c6..c26b8af2a91 100644\n--- a/src/rpm/init.d/elasticsearch\n+++ b/src/rpm/init.d/elasticsearch\n@@ -85,7 +85,7 @@ start() {\n     fi\n     echo -n $\"Starting $prog: \"\n     # if not running, start it up here, usually something like \"daemon $exec\"\n-    daemon --user $ES_USER --pidfile $pidfile $exec -p $pidfile -d -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\n+    daemon --user $ES_USER --pidfile $pidfile $exec -p $pidfile -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\n     retval=$?\n     echo\n     [ $retval -eq 0 ] && touch $lockfile\ndiff --git a/src/rpm/systemd/elasticsearch.service b/src/rpm/systemd/elasticsearch.service\nindex a8547a24e78..740639ef74e 100644\n--- a/src/rpm/systemd/elasticsearch.service\n+++ b/src/rpm/systemd/elasticsearch.service\n@@ -8,7 +8,7 @@ EnvironmentFile=/etc/sysconfig/elasticsearch\n User=elasticsearch\n Group=elasticsearch\n PIDFile=/var/run/elasticsearch/elasticsearch.pid\n-ExecStart=/usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch/elasticsearch.pid -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\n+ExecStart=/usr/share/elasticsearch/bin/elasticsearch -p /var/run/elasticsearch/elasticsearch.pid -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\n # See MAX_OPEN_FILES in sysconfig\n LimitNOFILE=65535\n # See MAX_LOCKED_MEMORY in sysconfig, use \"infinity\" when MAX_LOCKED_MEMORY=unlimited and using bootstrap.mlockall: true\ndiff --git a/src/test/java/com/carrotsearch/randomizedtesting/StandaloneRandomizedContext.java b/src/test/java/com/carrotsearch/randomizedtesting/StandaloneRandomizedContext.java\ndeleted file mode 100644\nindex 8816f6e553e..00000000000\n--- a/src/test/java/com/carrotsearch/randomizedtesting/StandaloneRandomizedContext.java\n+++ /dev/null\n@@ -1,68 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package com.carrotsearch.randomizedtesting;\n-\n-/**\n- * Exposes methods that allow to use a {@link RandomizedContext} without using a {@link RandomizedRunner}\n- * This was specifically needed by the REST tests since they run with a custom junit runner ({@link org.elasticsearch.test.rest.junit.RestTestSuiteRunner})\n- */\n-public final class StandaloneRandomizedContext {\n-\n-    private StandaloneRandomizedContext() {\n-\n-    }\n-\n-    /**\n-     * Creates a new {@link RandomizedContext} associated to the current thread\n-     */\n-    public static void createRandomizedContext(Class<?> testClass, Randomness runnerRandomness) {\n-        //the randomized runner is passed in as null, which is fine as long as we don't try to access it afterwards\n-        RandomizedContext randomizedContext = RandomizedContext.create(Thread.currentThread().getThreadGroup(), testClass, null);\n-        randomizedContext.push(runnerRandomness.clone(Thread.currentThread()));\n-    }\n-\n-    /**\n-     * Destroys the {@link RandomizedContext} associated to the current thread\n-     */\n-    public static void disposeRandomizedContext() {\n-        RandomizedContext.current().dispose();\n-    }\n-\n-    public static void pushRandomness(Randomness randomness) {\n-        RandomizedContext.current().push(randomness);\n-    }\n-\n-    public static void popAndDestroy() {\n-        RandomizedContext.current().popAndDestroy();\n-    }\n-\n-    /**\n-     * Returns the string formatted seed associated to the current thread's randomized context\n-     */\n-    public static String getSeedAsString() {\n-        return SeedUtils.formatSeed(RandomizedContext.current().getRandomness().getSeed());\n-    }\n-\n-    /**\n-     * Util method to extract the seed out of a {@link Randomness} instance\n-     */\n-    public static long getSeed(Randomness randomness) {\n-        return randomness.getSeed();\n-    }\n-}\ndiff --git a/src/test/java/org/apache/lucene/util/SloppyMathTests.java b/src/test/java/org/apache/lucene/util/SloppyMathTests.java\nindex 798b0ff7a6b..dbead96c0cc 100644\n--- a/src/test/java/org/apache/lucene/util/SloppyMathTests.java\n+++ b/src/test/java/org/apache/lucene/util/SloppyMathTests.java\n@@ -58,7 +58,7 @@ public class SloppyMathTests extends ElasticsearchTestCase {\n     }\n \n     private static double maxError(double distance) {\n-        return distance / 1000.0;\n+        return distance / 10000.0;\n     }\n     \n     private void testSloppyMath(DistanceUnit unit, double...deltaDeg) {\ndiff --git a/src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsTests.java b/src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsTests.java\ndeleted file mode 100644\nindex 6d5d5d51a7a..00000000000\n--- a/src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsTests.java\n+++ /dev/null\n@@ -1,131 +0,0 @@\n-package org.elasticsearch.action.admin.cluster.stats;\n-/*\n- * Licensed to ElasticSearch under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-\n-import org.elasticsearch.Version;\n-import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.monitor.sigar.SigarService;\n-import org.elasticsearch.test.ElasticsearchIntegrationTest;\n-import org.elasticsearch.test.ElasticsearchIntegrationTest.ClusterScope;\n-import org.hamcrest.Matchers;\n-import org.junit.Test;\n-\n-@ClusterScope(scope = ElasticsearchIntegrationTest.Scope.TEST, numNodes = 0)\n-public class ClusterStatsTests extends ElasticsearchIntegrationTest {\n-\n-    private void assertCounts(ClusterStatsNodes.Counts counts, int total, int masterOnly, int dataOnly, int masterData, int client) {\n-        assertThat(counts.getTotal(), Matchers.equalTo(total));\n-        assertThat(counts.getMasterOnly(), Matchers.equalTo(masterOnly));\n-        assertThat(counts.getDataOnly(), Matchers.equalTo(dataOnly));\n-        assertThat(counts.getMasterData(), Matchers.equalTo(masterData));\n-        assertThat(counts.getClient(), Matchers.equalTo(client));\n-    }\n-\n-    @Test\n-    public void testNodeCounts() {\n-        cluster().startNode();\n-        ClusterStatsResponse response = client().admin().cluster().prepareClusterStats().get();\n-        assertCounts(response.getNodesStats().getCounts(), 1, 0, 0, 1, 0);\n-\n-        cluster().startNode(ImmutableSettings.builder().put(\"node.data\", false));\n-        response = client().admin().cluster().prepareClusterStats().get();\n-        assertCounts(response.getNodesStats().getCounts(), 2, 1, 0, 1, 0);\n-\n-        cluster().startNode(ImmutableSettings.builder().put(\"node.master\", false));\n-        response = client().admin().cluster().prepareClusterStats().get();\n-        assertCounts(response.getNodesStats().getCounts(), 3, 1, 1, 1, 0);\n-\n-        cluster().startNode(ImmutableSettings.builder().put(\"node.client\", true));\n-        response = client().admin().cluster().prepareClusterStats().get();\n-        assertCounts(response.getNodesStats().getCounts(), 4, 1, 1, 1, 1);\n-    }\n-\n-\n-    private void assertShardStats(ClusterStatsIndices.ShardStats stats, int indices, int total, int primaries, double replicationFactor) {\n-        assertThat(stats.getIndices(), Matchers.equalTo(indices));\n-        assertThat(stats.getTotal(), Matchers.equalTo(total));\n-        assertThat(stats.getPrimaries(), Matchers.equalTo(primaries));\n-        assertThat(stats.getReplication(), Matchers.equalTo(replicationFactor));\n-    }\n-\n-    @Test\n-    public void testIndicesShardStats() {\n-        cluster().startNode();\n-        prepareCreate(\"test1\").setSettings(\"number_of_shards\", 2, \"number_of_replicas\", 1).get();\n-        ensureYellow();\n-        ClusterStatsResponse response = client().admin().cluster().prepareClusterStats().get();\n-        assertThat(response.indicesStats.getDocs().getCount(), Matchers.equalTo(0l));\n-        assertThat(response.indicesStats.getIndexCount(), Matchers.equalTo(1));\n-        assertShardStats(response.getIndicesStats().getShards(), 1, 2, 2, 0.0);\n-\n-        // add another node, replicas should get assigned\n-        cluster().startNode();\n-        ensureGreen();\n-        index(\"test1\", \"type\", \"1\", \"f\", \"f\");\n-        refresh(); // make the doc visible\n-        response = client().admin().cluster().prepareClusterStats().get();\n-        assertThat(response.indicesStats.getDocs().getCount(), Matchers.equalTo(1l));\n-        assertShardStats(response.getIndicesStats().getShards(), 1, 4, 2, 1.0);\n-\n-        prepareCreate(\"test2\").setSettings(\"number_of_shards\", 3, \"number_of_replicas\", 0).get();\n-        ensureGreen();\n-        response = client().admin().cluster().prepareClusterStats().get();\n-        assertThat(response.indicesStats.getIndexCount(), Matchers.equalTo(2));\n-        assertShardStats(response.getIndicesStats().getShards(), 2, 7, 5, 2.0 / 5);\n-\n-        assertThat(response.getIndicesStats().getShards().getAvgIndexPrimaryShards(), Matchers.equalTo(2.5));\n-        assertThat(response.getIndicesStats().getShards().getMinIndexPrimaryShards(), Matchers.equalTo(2));\n-        assertThat(response.getIndicesStats().getShards().getMaxIndexPrimaryShards(), Matchers.equalTo(3));\n-\n-        assertThat(response.getIndicesStats().getShards().getAvgIndexShards(), Matchers.equalTo(3.5));\n-        assertThat(response.getIndicesStats().getShards().getMinIndexShards(), Matchers.equalTo(3));\n-        assertThat(response.getIndicesStats().getShards().getMaxIndexShards(), Matchers.equalTo(4));\n-\n-        assertThat(response.getIndicesStats().getShards().getAvgIndexReplication(), Matchers.equalTo(0.5));\n-        assertThat(response.getIndicesStats().getShards().getMinIndexReplication(), Matchers.equalTo(0.0));\n-        assertThat(response.getIndicesStats().getShards().getMaxIndexReplication(), Matchers.equalTo(1.0));\n-\n-    }\n-\n-    @Test\n-    public void testValuesSmokeScreen() {\n-        cluster().ensureAtMostNumNodes(5);\n-        cluster().ensureAtLeastNumNodes(1);\n-        SigarService sigarService = cluster().getInstance(SigarService.class);\n-        index(\"test1\", \"type\", \"1\", \"f\", \"f\");\n-\n-        ClusterStatsResponse response = client().admin().cluster().prepareClusterStats().get();\n-        assertThat(response.getTimestamp(), Matchers.greaterThan(946681200000l)); // 1 Jan 2000\n-        assertThat(response.indicesStats.getStore().getSizeInBytes(), Matchers.greaterThan(0l));\n-\n-        assertThat(response.nodesStats.getFs().getTotal().bytes(), Matchers.greaterThan(0l));\n-        assertThat(response.nodesStats.getJvm().getVersions().size(), Matchers.greaterThan(0));\n-        if (sigarService.sigarAvailable()) {\n-            // We only get those if we have sigar\n-            assertThat(response.nodesStats.getOs().getAvailableProcessors(), Matchers.greaterThan(0));\n-            assertThat(response.nodesStats.getOs().getAvailableMemory().bytes(), Matchers.greaterThan(0l));\n-            assertThat(response.nodesStats.getOs().getCpus().size(), Matchers.greaterThan(0));\n-        }\n-        assertThat(response.nodesStats.getVersions().size(), Matchers.greaterThan(0));\n-        assertThat(response.nodesStats.getVersions().contains(Version.CURRENT), Matchers.equalTo(true));\n-        assertThat(response.nodesStats.getPlugins().size(), Matchers.greaterThanOrEqualTo(0));\n-\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java b/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java\ndeleted file mode 100644\nindex f97479608f2..00000000000\n--- a/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java\n+++ /dev/null\n@@ -1,86 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.elasticsearch.benchmark.cluster;\n-\n-import com.google.common.collect.ImmutableMap;\n-import org.elasticsearch.cluster.ClusterState;\n-import org.elasticsearch.cluster.metadata.IndexMetaData;\n-import org.elasticsearch.cluster.metadata.MetaData;\n-import org.elasticsearch.cluster.node.DiscoveryNodes;\n-import org.elasticsearch.cluster.routing.RoutingTable;\n-import org.elasticsearch.cluster.routing.allocation.AllocationService;\n-import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.common.unit.TimeValue;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n-\n-import java.util.Random;\n-\n-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n-import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n-\n-public class ClusterAllocationRerouteBenchmark {\n-\n-    private static final ESLogger logger = Loggers.getLogger(ClusterAllocationRerouteBenchmark.class);\n-\n-    public static void main(String[] args) {\n-        final int numberOfRuns = 1;\n-        final int numIndices = 5 * 365; // five years\n-        final int numShards = 6;\n-        final int numReplicas = 2;\n-        final int numberOfNodes = 30;\n-        final int numberOfTags = 2;\n-        AllocationService strategy = ElasticsearchAllocationTestCase.createAllocationService(ImmutableSettings.EMPTY, new Random(1));\n-\n-        MetaData.Builder mb = MetaData.builder();\n-        for (int i = 1; i <= numIndices; i++) {\n-            mb.put(IndexMetaData.builder(\"test_\" + i).numberOfShards(numShards).numberOfReplicas(numReplicas));\n-        }\n-        MetaData metaData = mb.build();\n-        RoutingTable.Builder rb = RoutingTable.builder();\n-        for (int i = 1; i <= numIndices; i++) {\n-            rb.addAsNew(metaData.index(\"test_\" + i));\n-        }\n-        RoutingTable routingTable = rb.build();\n-        DiscoveryNodes.Builder nb = DiscoveryNodes.builder();\n-        for (int i = 1; i <= numberOfNodes; i++) {\n-            nb.put(newNode(\"node\" + i, ImmutableMap.of(\"tag\", \"tag_\" + (i % numberOfTags))));\n-        }\n-        ClusterState initialClusterState = ClusterState.builder().metaData(metaData).routingTable(routingTable).nodes(nb).build();\n-\n-        long start = System.currentTimeMillis();\n-        for (int i = 0; i < numberOfRuns; i++) {\n-            logger.info(\"[{}] starting... \", i);\n-            long runStart = System.currentTimeMillis();\n-            ClusterState clusterState = initialClusterState;\n-            while (clusterState.readOnlyRoutingNodes().hasUnassignedShards()) {\n-                logger.info(\"[{}] remaining unassigned {}\", i, clusterState.readOnlyRoutingNodes().unassigned().size());\n-                RoutingAllocation.Result result = strategy.applyStartedShards(clusterState, clusterState.readOnlyRoutingNodes().shardsWithState(INITIALIZING));\n-                clusterState = ClusterState.builder(clusterState).routingResult(result).build();\n-                result = strategy.reroute(clusterState);\n-                clusterState = ClusterState.builder(clusterState).routingResult(result).build();\n-            }\n-            logger.info(\"[{}] took {}\", i, TimeValue.timeValueMillis(System.currentTimeMillis() - runStart));\n-        }\n-        long took = System.currentTimeMillis() - start;\n-        logger.info(\"total took {}, AVG {}\", TimeValue.timeValueMillis(took), TimeValue.timeValueMillis(took / numberOfRuns));\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java b/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java\nindex 6e15f40b439..3b501711ea8 100644\n--- a/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java\n+++ b/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java\n@@ -81,7 +81,7 @@ public class TermsAggregationSearchBenchmark {\n         AGGREGATION {\n             @Override\n             SearchRequestBuilder addTermsAgg(SearchRequestBuilder builder, String name, String field, String executionHint) {\n-                return builder.addAggregation(AggregationBuilders.terms(name).executionHint(executionHint).field(field));\n+                return builder.addAggregation(AggregationBuilders.terms(name).field(field));\n             }\n \n             @Override\n@@ -234,8 +234,6 @@ public class TermsAggregationSearchBenchmark {\n         stats.add(terms(\"terms_facet_map_s_dv\", Method.FACET, \"s_value_dv\", \"map\"));\n         stats.add(terms(\"terms_agg_s\", Method.AGGREGATION, \"s_value\", null));\n         stats.add(terms(\"terms_agg_s_dv\", Method.AGGREGATION, \"s_value_dv\", null));\n-        stats.add(terms(\"terms_agg_map_s\", Method.AGGREGATION, \"s_value\", \"map\"));\n-        stats.add(terms(\"terms_agg_map_s_dv\", Method.AGGREGATION, \"s_value_dv\", \"map\"));\n         stats.add(terms(\"terms_facet_l\", Method.FACET, \"l_value\", null));\n         stats.add(terms(\"terms_facet_l_dv\", Method.FACET, \"l_value_dv\", null));\n         stats.add(terms(\"terms_agg_l\", Method.AGGREGATION, \"l_value\", null));\n@@ -246,8 +244,6 @@ public class TermsAggregationSearchBenchmark {\n         stats.add(terms(\"terms_facet_map_sm_dv\", Method.FACET, \"sm_value_dv\", \"map\"));\n         stats.add(terms(\"terms_agg_sm\", Method.AGGREGATION, \"sm_value\", null));\n         stats.add(terms(\"terms_agg_sm_dv\", Method.AGGREGATION, \"sm_value_dv\", null));\n-        stats.add(terms(\"terms_agg_map_sm\", Method.AGGREGATION, \"sm_value\", \"map\"));\n-        stats.add(terms(\"terms_agg_map_sm_dv\", Method.AGGREGATION, \"sm_value_dv\", \"map\"));\n         stats.add(terms(\"terms_facet_lm\", Method.FACET, \"lm_value\", null));\n         stats.add(terms(\"terms_facet_lm_dv\", Method.FACET, \"lm_value_dv\", null));\n         stats.add(terms(\"terms_agg_lm\", Method.AGGREGATION, \"lm_value\", null));\ndiff --git a/src/test/java/org/elasticsearch/cluster/ClusterServiceTests.java b/src/test/java/org/elasticsearch/cluster/ClusterServiceTests.java\nindex b93f7a1797a..160cf254944 100644\n--- a/src/test/java/org/elasticsearch/cluster/ClusterServiceTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/ClusterServiceTests.java\n@@ -83,7 +83,7 @@ public class ClusterServiceTests extends ElasticsearchIntegrationTest {\n         clusterService1.submitStateUpdateTask(\"test2\", new TimeoutClusterStateUpdateTask() {\n             @Override\n             public TimeValue timeout() {\n-                return TimeValue.timeValueMillis(100);\n+                return TimeValue.timeValueMillis(2);\n             }\n \n             @Override\n@@ -102,7 +102,7 @@ public class ClusterServiceTests extends ElasticsearchIntegrationTest {\n             }\n         });\n \n-        assertThat(timedOut.await(1000, TimeUnit.MILLISECONDS), equalTo(true));\n+        assertThat(timedOut.await(500, TimeUnit.MILLISECONDS), equalTo(true));\n         block.countDown();\n         Thread.sleep(100); // sleep a bit to double check that execute on the timed out update task is not called...\n         assertThat(executeCalled.get(), equalTo(false));\ndiff --git a/src/test/java/org/elasticsearch/cluster/ack/AckTests.java b/src/test/java/org/elasticsearch/cluster/ack/AckTests.java\nindex 841e6ca7e02..516f68e9df0 100644\n--- a/src/test/java/org/elasticsearch/cluster/ack/AckTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/ack/AckTests.java\n@@ -188,7 +188,8 @@ public class AckTests extends ElasticsearchIntegrationTest {\n \n         for (Client client : clients()) {\n             ClusterState clusterState = getLocalClusterState(client);\n-            for (MutableShardRouting mutableShardRouting : clusterState.routingNodes().routingNodeIter(moveAllocationCommand.fromNode())) {\n+            RoutingNode routingNode = clusterState.routingNodes().nodesToShards().get(moveAllocationCommand.fromNode());\n+            for (MutableShardRouting mutableShardRouting : routingNode) {\n                 //if the shard that we wanted to move is still on the same node, it must be relocating\n                 if (mutableShardRouting.shardId().equals(moveAllocationCommand.shardId())) {\n                     assertThat(mutableShardRouting.relocating(), equalTo(true));\n@@ -196,8 +197,9 @@ public class AckTests extends ElasticsearchIntegrationTest {\n \n             }\n \n+            routingNode = clusterState.routingNodes().nodesToShards().get(moveAllocationCommand.toNode());\n             boolean found = false;\n-            for (MutableShardRouting mutableShardRouting :  clusterState.routingNodes().routingNodeIter(moveAllocationCommand.toNode())) {\n+            for (MutableShardRouting mutableShardRouting : routingNode) {\n                 if (mutableShardRouting.shardId().equals(moveAllocationCommand.shardId())) {\n                     assertThat(mutableShardRouting.state(), anyOf(equalTo(ShardRoutingState.INITIALIZING), equalTo(ShardRoutingState.STARTED)));\n                     found = true;\n@@ -239,8 +241,9 @@ public class AckTests extends ElasticsearchIntegrationTest {\n         //testing only on master with the latest cluster state as we didn't make any change thus we cannot guarantee that\n         //all nodes hold the same cluster state version. We only know there was no need to change anything, thus no need for ack on this update.\n         ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().get();\n+        RoutingNode routingNode = clusterStateResponse.getState().routingNodes().nodesToShards().get(moveAllocationCommand.fromNode());\n         boolean found = false;\n-        for (MutableShardRouting mutableShardRouting :  clusterStateResponse.getState().routingNodes().routingNodeIter(moveAllocationCommand.fromNode())) {\n+        for (MutableShardRouting mutableShardRouting : routingNode) {\n             //the shard that we wanted to move is still on the same node, as we had dryRun flag\n             if (mutableShardRouting.shardId().equals(moveAllocationCommand.shardId())) {\n                 assertThat(mutableShardRouting.started(), equalTo(true));\n@@ -250,7 +253,8 @@ public class AckTests extends ElasticsearchIntegrationTest {\n         }\n         assertThat(found, equalTo(true));\n \n-        for (MutableShardRouting mutableShardRouting : clusterStateResponse.getState().routingNodes().routingNodeIter(moveAllocationCommand.toNode())) {\n+        routingNode = clusterStateResponse.getState().routingNodes().nodesToShards().get(moveAllocationCommand.toNode());\n+        for (MutableShardRouting mutableShardRouting : routingNode) {\n             if (mutableShardRouting.shardId().equals(moveAllocationCommand.shardId())) {\n                 fail(\"shard [\" + mutableShardRouting + \"] shouldn't be on node [\" + moveAllocationCommand.toString() + \"]\");\n             }\n@@ -277,11 +281,11 @@ public class AckTests extends ElasticsearchIntegrationTest {\n         String toNodeId = null;\n         MutableShardRouting shardToBeMoved = null;\n         ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().get();\n-        for (RoutingNode routingNode : clusterStateResponse.getState().routingNodes()) {\n+        for (RoutingNode routingNode : clusterStateResponse.getState().routingNodes().nodesToShards().values()) {\n             if (routingNode.node().isDataNode()) {\n                 if (fromNodeId == null && routingNode.numberOfOwningShards() > 0) {\n                     fromNodeId = routingNode.nodeId();\n-                    shardToBeMoved = routingNode.get(randomInt(routingNode.size()-1));\n+                    shardToBeMoved = routingNode.shards().get(randomInt(routingNode.shards().size()-1));\n                 } else {\n                     toNodeId = routingNode.nodeId();\n                 }\ndiff --git a/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java b/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java\nindex f5c49d7f657..e04395baa54 100644\n--- a/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java\n@@ -76,7 +76,7 @@ public class ClusterRerouteTests extends ElasticsearchIntegrationTest {\n                 .setDryRun(true)\n                 .execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(1));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).shards().get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n \n         logger.info(\"--> get the state, verify nothing changed because of the dry run\");\n         state = client().admin().cluster().prepareState().execute().actionGet().getState();\n@@ -87,7 +87,7 @@ public class ClusterRerouteTests extends ElasticsearchIntegrationTest {\n                 .add(new AllocateAllocationCommand(new ShardId(\"test\", 0), node_1, true))\n                 .execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(1));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).shards().get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n \n         ClusterHealthResponse healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();\n         assertThat(healthResponse.isTimedOut(), equalTo(false));\n@@ -95,15 +95,15 @@ public class ClusterRerouteTests extends ElasticsearchIntegrationTest {\n         logger.info(\"--> get the state, verify shard 1 primary allocated\");\n         state = client().admin().cluster().prepareState().execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(1));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).shards().get(0).state(), equalTo(ShardRoutingState.STARTED));\n \n         logger.info(\"--> move shard 1 primary from node1 to node2\");\n         state = client().admin().cluster().prepareReroute()\n                 .add(new MoveAllocationCommand(new ShardId(\"test\", 0), node_1, node_2))\n                 .execute().actionGet().getState();\n \n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.RELOCATING));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_2).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).shards().get(0).state(), equalTo(ShardRoutingState.RELOCATING));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_2).id()).shards().get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n \n \n         healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().setWaitForRelocatingShards(0).execute().actionGet();\n@@ -112,7 +112,7 @@ public class ClusterRerouteTests extends ElasticsearchIntegrationTest {\n         logger.info(\"--> get the state, verify shard 1 primary moved from node1 to node2\");\n         state = client().admin().cluster().prepareState().execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(1));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_2).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_2).id()).shards().get(0).state(), equalTo(ShardRoutingState.STARTED));\n     }\n \n     @Test\n@@ -143,7 +143,7 @@ public class ClusterRerouteTests extends ElasticsearchIntegrationTest {\n                 .add(new AllocateAllocationCommand(new ShardId(\"test\", 0), node_1, true))\n                 .execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(1));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).shards().get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n \n         healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();\n         assertThat(healthResponse.isTimedOut(), equalTo(false));\n@@ -151,7 +151,7 @@ public class ClusterRerouteTests extends ElasticsearchIntegrationTest {\n         logger.info(\"--> get the state, verify shard 1 primary allocated\");\n         state = client().admin().cluster().prepareState().execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(1));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).shards().get(0).state(), equalTo(ShardRoutingState.STARTED));\n \n         client().prepareIndex(\"test\", \"type\", \"1\").setSource(\"field\", \"value\").setRefresh(true).execute().actionGet();\n \n@@ -176,7 +176,7 @@ public class ClusterRerouteTests extends ElasticsearchIntegrationTest {\n                 .add(new AllocateAllocationCommand(new ShardId(\"test\", 0), node_1, true))\n                 .execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(1));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).shards().get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n \n         healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();\n         assertThat(healthResponse.isTimedOut(), equalTo(false));\n@@ -184,7 +184,7 @@ public class ClusterRerouteTests extends ElasticsearchIntegrationTest {\n         logger.info(\"--> get the state, verify shard 1 primary allocated\");\n         state = client().admin().cluster().prepareState().execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(1));\n-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));\n+        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).shards().get(0).state(), equalTo(ShardRoutingState.STARTED));\n \n     }\n \ndiff --git a/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationTests.java b/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationTests.java\nindex b583b6ef824..256bd0ea3ed 100644\n--- a/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationTests.java\n@@ -24,6 +24,9 @@ import org.elasticsearch.common.Priority;\n import org.elasticsearch.test.ElasticsearchIntegrationTest;\n import org.junit.Test;\n \n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n \n@@ -42,9 +45,10 @@ public class SimpleAllocationTests extends ElasticsearchIntegrationTest {\n         ensureGreen();            \n         ClusterState state = client().admin().cluster().prepareState().execute().actionGet().getState();\n         assertThat(state.routingNodes().unassigned().size(), equalTo(0));\n-        for (RoutingNode node : state.routingNodes()) {\n-            if (!node.isEmpty()) {\n-                assertThat(node.size(), equalTo(2));\n+        Map<String, RoutingNode> nodesToShards = state.routingNodes().getNodesToShards();\n+        for (Entry<String, RoutingNode> entry : nodesToShards.entrySet()) {\n+            if (!entry.getValue().shards().isEmpty()) { \n+                assertThat(entry.getValue().shards().size(), equalTo(2));\n             }\n         }\n         client().admin().indices().prepareUpdateSettings(\"test\").setSettings(settingsBuilder().put(\"index.number_of_replicas\", 0)).execute().actionGet();\n@@ -52,9 +56,10 @@ public class SimpleAllocationTests extends ElasticsearchIntegrationTest {\n         state = client().admin().cluster().prepareState().execute().actionGet().getState();\n \n         assertThat(state.routingNodes().unassigned().size(), equalTo(0));\n-        for (RoutingNode node : state.routingNodes()) {\n-            if (!node.isEmpty()) {\n-                assertThat(node.size(), equalTo(1));\n+        nodesToShards = state.routingNodes().getNodesToShards();\n+        for (Entry<String, RoutingNode> entry : nodesToShards.entrySet()) {\n+            if (!entry.getValue().shards().isEmpty()) {\n+                assertThat(entry.getValue().shards().size(), equalTo(1));\n             }\n         }\n         \n@@ -71,9 +76,10 @@ public class SimpleAllocationTests extends ElasticsearchIntegrationTest {\n         state = client().admin().cluster().prepareState().execute().actionGet().getState();\n \n         assertThat(state.routingNodes().unassigned().size(), equalTo(0));\n-        for (RoutingNode node : state.routingNodes()) {\n-            if (!node.isEmpty()) {\n-                assertThat(node.size(), equalTo(4));\n+        nodesToShards = state.routingNodes().getNodesToShards();\n+        for (Entry<String, RoutingNode> entry : nodesToShards.entrySet()) {\n+            if (!entry.getValue().shards().isEmpty()) {\n+                assertThat(entry.getValue().shards().size(), equalTo(4));\n             }\n         }\n     }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java\nindex 3e9bf274fff..edef45b3379 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java\n@@ -13,7 +13,7 @@ import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllo\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.hamcrest.Matcher;\n import org.hamcrest.Matchers;\n import org.junit.Test;\n@@ -23,23 +23,23 @@ import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;\n import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n \n-public class AddIncrementallyTests extends ElasticsearchAllocationTestCase {\n+public class AddIncrementallyTests extends ElasticsearchTestCase {\n     private final ESLogger logger = Loggers.getLogger(AddIncrementallyTests.class);\n \n     @Test\n     public void testAddNodesAndIndices() {\n         ImmutableSettings.Builder settings = settingsBuilder();\n         settings.put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());\n-        AllocationService service = createAllocationService(settings.build());\n+        AllocationService service = new AllocationService(settings.build());\n \n         ClusterState clusterState = initCluster(service, 1, 3, 3, 1);\n         assertThat(clusterState.routingNodes().node(\"node0\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(9));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(9));\n         int nodeOffset = 1;\n         clusterState = addNodes(clusterState, service, 1, nodeOffset++);\n         assertThat(clusterState.routingNodes().node(\"node0\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(0));\n         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(3));\n         clusterState = addNodes(clusterState, service, 1, nodeOffset++);\n         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(2));\n@@ -50,23 +50,23 @@ public class AddIncrementallyTests extends ElasticsearchAllocationTestCase {\n         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(2));\n \n         clusterState = addIndex(clusterState, service, 3, 2, 3);\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(2));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(2));\n         assertNumIndexShardsPerNode(clusterState, \"test3\", Matchers.equalTo(2));\n         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));\n \n         clusterState = addIndex(clusterState, service, 4, 2, 3);\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(4));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(4));\n         assertNumIndexShardsPerNode(clusterState, \"test4\", Matchers.equalTo(2));\n         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));\n         clusterState = addNodes(clusterState, service, 1, nodeOffset++);\n         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(0));\n         clusterState = removeNodes(clusterState, service, 1);\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(4));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(4));\n         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));\n         clusterState = addNodes(clusterState, service, 1, nodeOffset++);\n         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(0));\n         logger.debug(\"ClusterState: {}\", clusterState.getRoutingNodes().prettyPrint());\n     }\n \n@@ -75,16 +75,16 @@ public class AddIncrementallyTests extends ElasticsearchAllocationTestCase {\n         ImmutableSettings.Builder settings = settingsBuilder();\n         settings.put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString())\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 2);\n-        AllocationService service = createAllocationService(settings.build());\n+        AllocationService service = new AllocationService(settings.build());\n \n         ClusterState clusterState = initCluster(service, 1, 3, 3, 1);\n         assertThat(clusterState.routingNodes().node(\"node0\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(9));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(9));\n         int nodeOffset = 1;\n         clusterState = addNodes(clusterState, service, 1, nodeOffset++);\n         assertThat(clusterState.routingNodes().node(\"node0\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(0));\n         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(3));\n \n         logger.info(\"now, start one more node, check that rebalancing will happen because we set it to always\");\n@@ -146,16 +146,16 @@ public class AddIncrementallyTests extends ElasticsearchAllocationTestCase {\n         settings.put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString())\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 100)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 100);\n-        AllocationService service = createAllocationService(settings.build());\n+        AllocationService service = new AllocationService(settings.build());\n \n         ClusterState clusterState = initCluster(service, 1, 3, 3, 1);\n         assertThat(clusterState.routingNodes().node(\"node0\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(9));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(9));\n         int nodeOffset = 1;\n         clusterState = addNodes(clusterState, service, 1, nodeOffset++);\n         assertThat(clusterState.routingNodes().node(\"node0\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), Matchers.equalTo(9));\n-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));\n+        assertThat(clusterState.routingNodes().getUnassigned().size(), Matchers.equalTo(0));\n         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(3));\n \n         logger.info(\"now, start one more node, check that rebalancing will happen because we set it to always\");\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocatePostApiFlagTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocatePostApiFlagTests.java\nindex df7f0999299..fddd2884072 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocatePostApiFlagTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocatePostApiFlagTests.java\n@@ -26,7 +26,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n@@ -36,13 +36,13 @@ import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class AllocatePostApiFlagTests extends ElasticsearchAllocationTestCase {\n+public class AllocatePostApiFlagTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(AllocatePostApiFlagTests.class);\n \n     @Test\n     public void simpleFlagTests() {\n-        AllocationService allocation = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService allocation = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"creating an index with 1 shard, no replica\");\n         MetaData metaData = MetaData.builder()\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java\nindex 643622384ec..0270b14cbf6 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java\n@@ -39,11 +39,10 @@ import org.elasticsearch.common.xcontent.XContentFactory;\n import org.elasticsearch.common.xcontent.XContentParser;\n import org.elasticsearch.common.xcontent.XContentType;\n import org.elasticsearch.index.shard.ShardId;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n-import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;\n import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;\n import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n@@ -51,13 +50,13 @@ import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n+public class AllocationCommandsTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(AllocationCommandsTests.class);\n \n     @Test\n     public void moveShardCommand() {\n-        AllocationService allocation = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService allocation = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"creating an index with 1 shard, no replica\");\n         MetaData metaData = MetaData.builder()\n@@ -88,20 +87,20 @@ public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new MoveAllocationCommand(new ShardId(\"test\", 0), existingNodeId, toNodeId)));\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(existingNodeId).get(0).state(), equalTo(ShardRoutingState.RELOCATING));\n-        assertThat(clusterState.routingNodes().node(toNodeId).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n+        assertThat(clusterState.routingNodes().node(existingNodeId).shards().get(0).state(), equalTo(ShardRoutingState.RELOCATING));\n+        assertThat(clusterState.routingNodes().node(toNodeId).shards().get(0).state(), equalTo(ShardRoutingState.INITIALIZING));\n \n         logger.info(\"finish moving the shard\");\n         rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n-        assertThat(clusterState.routingNodes().node(existingNodeId).isEmpty(), equalTo(true));\n-        assertThat(clusterState.routingNodes().node(toNodeId).get(0).state(), equalTo(ShardRoutingState.STARTED));\n+        assertThat(clusterState.routingNodes().node(existingNodeId).shards().isEmpty(), equalTo(true));\n+        assertThat(clusterState.routingNodes().node(toNodeId).shards().get(0).state(), equalTo(ShardRoutingState.STARTED));\n     }\n \n     @Test\n     public void allocateCommand() {\n-        AllocationService allocation = createAllocationService(settingsBuilder()\n+        AllocationService allocation = new AllocationService(settingsBuilder()\n                 .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)\n                 .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)\n                 .build());\n@@ -136,16 +135,16 @@ public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId(\"test\", 0), \"node1\", true)));\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(INITIALIZING).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(0));\n \n         logger.info(\"--> start the primary shard\");\n         rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(0));\n \n         logger.info(\"--> allocate the replica shard on the primary shard node, should fail\");\n         try {\n@@ -158,18 +157,18 @@ public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId(\"test\", 0), \"node2\", false)));\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(INITIALIZING).size(), equalTo(1));\n \n \n         logger.info(\"--> start the replica shard\");\n         rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(STARTED).size(), equalTo(1));\n \n         logger.info(\"--> verify that we fail when there are no unassigned shards\");\n@@ -182,7 +181,7 @@ public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void cancelCommand() {\n-        AllocationService allocation = createAllocationService(settingsBuilder()\n+        AllocationService allocation = new AllocationService(settingsBuilder()\n                 .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)\n                 .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)\n                 .build());\n@@ -210,9 +209,9 @@ public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId(\"test\", 0), \"node1\", true)));\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(INITIALIZING).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(0));\n \n         logger.info(\"--> cancel primary allocation, make sure it fails...\");\n         try {\n@@ -224,9 +223,9 @@ public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n         logger.info(\"--> start the primary shard\");\n         rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(0));\n \n         logger.info(\"--> cancel primary allocation, make sure it fails...\");\n         try {\n@@ -239,27 +238,27 @@ public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId(\"test\", 0), \"node2\", false)));\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(INITIALIZING).size(), equalTo(1));\n \n         logger.info(\"--> cancel the relocation allocation\");\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new CancelAllocationCommand(new ShardId(\"test\", 0), \"node2\", false)));\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(0));\n-        assertThat(clusterState.routingNodes().node(\"node3\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().size(), equalTo(0));\n \n         logger.info(\"--> allocate the replica shard on on the second node\");\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId(\"test\", 0), \"node2\", false)));\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(INITIALIZING).size(), equalTo(1));\n \n         logger.info(\"--> cancel the primary being replicated, make sure it fails\");\n@@ -272,62 +271,43 @@ public class AllocationCommandsTests extends ElasticsearchAllocationTestCase {\n         logger.info(\"--> start the replica shard\");\n         rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(STARTED).size(), equalTo(1));\n \n         logger.info(\"--> cancel allocation of the replica shard\");\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new CancelAllocationCommand(new ShardId(\"test\", 0), \"node2\", false)));\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(0));\n-        assertThat(clusterState.routingNodes().node(\"node3\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().size(), equalTo(0));\n \n         logger.info(\"--> allocate the replica shard on on the second node\");\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId(\"test\", 0), \"node2\", false)));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n         assertThat(rerouteResult.changed(), equalTo(true));\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(INITIALIZING).size(), equalTo(1));\n         logger.info(\"--> start the replica shard\");\n         rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n         assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(STARTED).size(), equalTo(1));\n \n-        logger.info(\"--> move the replica shard\");\n-        rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new MoveAllocationCommand(new ShardId(\"test\", 0), \"node2\", \"node3\")));\n-        clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(RELOCATING).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node3\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node3\").shardsWithState(INITIALIZING).size(), equalTo(1));\n-\n-        logger.info(\"--> cancel the move of the replica shard\");\n-        rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new CancelAllocationCommand(new ShardId(\"test\", 0), \"node3\", false)));\n-        clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").shardsWithState(STARTED).size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(STARTED).size(), equalTo(1));\n-\n-\n         logger.info(\"--> cancel the primary allocation (with allow_primary set to true)\");\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new CancelAllocationCommand(new ShardId(\"test\", 0), \"node1\", true)));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n         assertThat(rerouteResult.changed(), equalTo(true));\n         assertThat(clusterState.routingNodes().node(\"node2\").shardsWithState(STARTED).get(0).primary(), equalTo(true));\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(0));\n-        assertThat(clusterState.routingNodes().node(\"node3\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().size(), equalTo(0));\n     }\n \n     @Test\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java\nindex 9785ae94936..fd55bb693fa 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java\n@@ -29,7 +29,7 @@ import org.elasticsearch.cluster.routing.ShardRouting;\n import org.elasticsearch.cluster.routing.ShardRoutingState;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -39,13 +39,13 @@ import static org.hamcrest.Matchers.*;\n \n /**\n  */\n-public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n+public class AwarenessAllocationTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(AwarenessAllocationTests.class);\n \n     @Test\n     public void moveShardOnceNewNodeWithAttributeAdded1() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.awareness.attributes\", \"rack_id\")\n@@ -114,7 +114,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void moveShardOnceNewNodeWithAttributeAdded2() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.awareness.attributes\", \"rack_id\")\n@@ -184,7 +184,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void moveShardOnceNewNodeWithAttributeAdded3() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n@@ -285,7 +285,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void moveShardOnceNewNodeWithAttributeAdded4() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n@@ -348,9 +348,9 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n         assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(10));\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(5));\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(5));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(10));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(5));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(5));\n \n         logger.info(\"--> do another reroute, make sure nothing moves\");\n         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));\n@@ -370,10 +370,10 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n         }\n         assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(5));\n-        assertThat(clusterState.getRoutingNodes().node(\"node4\").size(), equalTo(5));\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(5));\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(5));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(5));\n+        assertThat(clusterState.getRoutingNodes().node(\"node4\").shards().size(), equalTo(5));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(5));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(5));\n \n         logger.info(\"--> do another reroute, make sure nothing moves\");\n         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));\n@@ -381,7 +381,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void moveShardOnceNewNodeWithAttributeAdded5() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.awareness.attributes\", \"rack_id\")\n@@ -460,7 +460,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void moveShardOnceNewNodeWithAttributeAdded6() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.awareness.attributes\", \"rack_id\")\n@@ -541,7 +541,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void fullAwareness1() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.awareness.force.rack_id.values\", \"1,2\")\n@@ -609,7 +609,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void fullAwareness2() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.awareness.force.rack_id.values\", \"1,2\")\n@@ -678,7 +678,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void fullAwareness3() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n@@ -763,7 +763,7 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testUnbalancedZones() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.awareness.force.zone.values\", \"a,b\")\n                 .put(\"cluster.routing.allocation.awareness.attributes\", \"zone\")\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n@@ -821,8 +821,8 @@ public class AwarenessAllocationTests extends ElasticsearchAllocationTestCase {\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n         assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));\n-        assertThat(clusterState.getRoutingNodes().node(\"A-1\").size(), equalTo(2));\n-        assertThat(clusterState.getRoutingNodes().node(\"A-0\").size(), equalTo(3));\n-        assertThat(clusterState.getRoutingNodes().node(\"B-0\").size(), equalTo(5));\n+        assertThat(clusterState.getRoutingNodes().node(\"A-1\").shards().size(), equalTo(2));\n+        assertThat(clusterState.getRoutingNodes().node(\"A-0\").shards().size(), equalTo(3));\n+        assertThat(clusterState.getRoutingNodes().node(\"B-0\").shards().size(), equalTo(5));\n     }\n }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java\nindex 97b3f75be33..80d24e4c891 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java\n@@ -30,22 +30,25 @@ import org.elasticsearch.cluster.routing.*;\n import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;\n import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;\n import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;\n import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n import org.elasticsearch.common.settings.ImmutableSettings;\n import org.elasticsearch.gateway.none.NoneGatewayAllocator;\n import org.elasticsearch.node.settings.NodeSettingsService;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.hamcrest.Matchers;\n import org.junit.Test;\n \n+import java.util.List;\n+\n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;\n import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n \n-public class BalanceConfigurationTests extends ElasticsearchAllocationTestCase {\n+public class BalanceConfigurationTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(BalanceConfigurationTests.class);\n     // TODO maybe we can randomize these numbers somehow\n@@ -69,7 +72,7 @@ public class BalanceConfigurationTests extends ElasticsearchAllocationTestCase {\n         settings.put(BalancedShardsAllocator.SETTING_PRIMARY_BALANCE_FACTOR, primaryBalance);\n         settings.put(BalancedShardsAllocator.SETTING_THRESHOLD, balanceTreshold);\n \n-        AllocationService strategy = createAllocationService(settings.build());\n+        AllocationService strategy = new AllocationService(settings.build());\n \n         ClusterState clusterState = initCluster(strategy);\n         assertIndexBalance(logger, clusterState.getRoutingNodes(), numberOfNodes, numberOfIndices, numberOfReplicas, numberOfShards, balanceTreshold);\n@@ -97,7 +100,7 @@ public class BalanceConfigurationTests extends ElasticsearchAllocationTestCase {\n         settings.put(BalancedShardsAllocator.SETTING_PRIMARY_BALANCE_FACTOR, primaryBalance);\n         settings.put(BalancedShardsAllocator.SETTING_THRESHOLD, balanceTreshold);\n \n-        AllocationService strategy = createAllocationService(settings.build());\n+        AllocationService strategy = new AllocationService(settings.build());\n \n         ClusterState clusterState = initCluster(strategy);\n         assertReplicaBalance(logger, clusterState.getRoutingNodes(), numberOfNodes, numberOfIndices, numberOfReplicas, numberOfShards, balanceTreshold);\n@@ -125,7 +128,7 @@ public class BalanceConfigurationTests extends ElasticsearchAllocationTestCase {\n         settings.put(BalancedShardsAllocator.SETTING_PRIMARY_BALANCE_FACTOR, primaryBalance);\n         settings.put(BalancedShardsAllocator.SETTING_THRESHOLD, balanceTreshold);\n \n-        AllocationService strategy = createAllocationService(settings.build());\n+        AllocationService strategy = new AllocationService(settings.build());\n \n         ClusterState clusterstate = initCluster(strategy);\n         assertPrimaryBalance(logger, clusterstate.getRoutingNodes(), numberOfNodes, numberOfIndices, numberOfReplicas, numberOfShards, balanceTreshold);\n@@ -351,9 +354,10 @@ public class BalanceConfigurationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testNoRebalanceOnPrimaryOverload() {\n+\n         ImmutableSettings.Builder settings = settingsBuilder();\n-        AllocationService strategy = new AllocationService(settings.build(), randomAllocationDeciders(settings.build(),\n-                new NodeSettingsService(ImmutableSettings.Builder.EMPTY_SETTINGS), getRandom()), new ShardsAllocators(settings.build(),\n+        AllocationService strategy = new AllocationService(settings.build(), new AllocationDeciders(settings.build(),\n+                new NodeSettingsService(ImmutableSettings.Builder.EMPTY_SETTINGS)), new ShardsAllocators(settings.build(),\n                 new NoneGatewayAllocator(), new ShardsAllocator() {\n \n             @Override\n@@ -399,43 +403,43 @@ public class BalanceConfigurationTests extends ElasticsearchAllocationTestCase {\n              */\n             @Override\n             public boolean allocateUnassigned(RoutingAllocation allocation) {\n-                RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned();\n+                List<MutableShardRouting> unassigned = allocation.routingNodes().unassigned();\n                 boolean changed = !unassigned.isEmpty();\n                 for (MutableShardRouting sr : unassigned) {\n                     switch (sr.id()) {\n                         case 0:\n                             if (sr.primary()) {\n-                                allocation.routingNodes().assign(sr, \"node1\");\n+                                allocation.routingNodes().node(\"node1\").add(sr);\n                             } else {\n-                                allocation.routingNodes().assign(sr, \"node0\");\n+                                allocation.routingNodes().node(\"node0\").add(sr);\n                             }\n                             break;\n                         case 1:\n                             if (sr.primary()) {\n-                                allocation.routingNodes().assign(sr, \"node1\");\n+                                allocation.routingNodes().node(\"node1\").add(sr);\n                             } else {\n-                                allocation.routingNodes().assign(sr, \"node2\");\n+                                allocation.routingNodes().node(\"node2\").add(sr);\n                             }\n                             break;\n                         case 2:\n                             if (sr.primary()) {\n-                                allocation.routingNodes().assign(sr, \"node3\");\n+                                allocation.routingNodes().node(\"node3\").add(sr);\n                             } else {\n-                                allocation.routingNodes().assign(sr, \"node2\");\n+                                allocation.routingNodes().node(\"node2\").add(sr);\n                             }\n                             break;\n                         case 3:\n                             if (sr.primary()) {\n-                                allocation.routingNodes().assign(sr, \"node3\");\n+                                allocation.routingNodes().node(\"node3\").add(sr);\n                             } else {\n-                                allocation.routingNodes().assign(sr, \"node1\");\n+                                allocation.routingNodes().node(\"node1\").add(sr);\n                             }\n                             break;\n                         case 4:\n                             if (sr.primary()) {\n-                                allocation.routingNodes().assign(sr, \"node2\");\n+                                allocation.routingNodes().node(\"node2\").add(sr);\n                             } else {\n-                                allocation.routingNodes().assign(sr, \"node0\");\n+                                allocation.routingNodes().node(\"node0\").add(sr);\n                             }\n                             break;\n                     }\n@@ -470,7 +474,7 @@ public class BalanceConfigurationTests extends ElasticsearchAllocationTestCase {\n                 assertThat(mutableShardRouting.state(), Matchers.equalTo(ShardRoutingState.INITIALIZING));\n             }\n         }\n-        strategy = createAllocationService(settings.build());\n+        strategy = new AllocationService(settings.build());\n \n         logger.info(\"use the new allocator and check if it moves shards\");\n         routingNodes = clusterState.routingNodes();\n@@ -507,4 +511,4 @@ public class BalanceConfigurationTests extends ElasticsearchAllocationTestCase {\n \n     }\n \n-}\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java\nindex 03f4907d7cf..120b60deeb1 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java\n@@ -28,7 +28,7 @@ import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -36,13 +36,13 @@ import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTest\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n \n-public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCase {\n+public class ClusterRebalanceRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(ClusterRebalanceRoutingTests.class);\n \n     @Test\n     public void testAlways() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\n@@ -121,14 +121,14 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n         routingNodes = clusterState.routingNodes();\n \n-        assertThat(routingNodes.node(\"node3\").size(), equalTo(1));\n-        assertThat(routingNodes.node(\"node3\").get(0).shardId().index().name(), equalTo(\"test1\"));\n+        assertThat(routingNodes.node(\"node3\").shards().size(), equalTo(1));\n+        assertThat(routingNodes.node(\"node3\").shards().get(0).shardId().index().name(), equalTo(\"test1\"));\n     }\n \n \n     @Test\n     public void testClusterPrimariesActive1() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_PRIMARIES_ACTIVE.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_PRIMARIES_ACTIVE.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\n@@ -149,13 +149,13 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n         for (int i = 0; i < routingTable.index(\"test1\").shards().size(); i++) {\n-            assertThat(routingTable.index(\"test1\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test1\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test1\").shard(i).primaryShard().state(), equalTo(INITIALIZING));\n             assertThat(routingTable.index(\"test1\").shard(i).replicaShards().get(0).state(), equalTo(UNASSIGNED));\n         }\n \n         for (int i = 0; i < routingTable.index(\"test2\").shards().size(); i++) {\n-            assertThat(routingTable.index(\"test2\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test2\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test2\").shard(i).primaryShard().state(), equalTo(INITIALIZING));\n             assertThat(routingTable.index(\"test2\").shard(i).replicaShards().get(0).state(), equalTo(UNASSIGNED));\n         }\n@@ -168,13 +168,13 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         routingNodes = clusterState.routingNodes();\n \n         for (int i = 0; i < routingTable.index(\"test1\").shards().size(); i++) {\n-            assertThat(routingTable.index(\"test1\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test1\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test1\").shard(i).primaryShard().state(), equalTo(STARTED));\n             assertThat(routingTable.index(\"test1\").shard(i).replicaShards().get(0).state(), equalTo(INITIALIZING));\n         }\n \n         for (int i = 0; i < routingTable.index(\"test2\").shards().size(); i++) {\n-            assertThat(routingTable.index(\"test2\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test2\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test2\").shard(i).primaryShard().state(), equalTo(INITIALIZING));\n             assertThat(routingTable.index(\"test2\").shard(i).replicaShards().get(0).state(), equalTo(UNASSIGNED));\n         }\n@@ -187,13 +187,13 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         routingNodes = clusterState.routingNodes();\n \n         for (int i = 0; i < routingTable.index(\"test1\").shards().size(); i++) {\n-            assertThat(routingTable.index(\"test1\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test1\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test1\").shard(i).primaryShard().state(), equalTo(STARTED));\n             assertThat(routingTable.index(\"test1\").shard(i).replicaShards().get(0).state(), equalTo(STARTED));\n         }\n \n         for (int i = 0; i < routingTable.index(\"test2\").shards().size(); i++) {\n-            assertThat(routingTable.index(\"test2\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test2\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test2\").shard(i).primaryShard().state(), equalTo(INITIALIZING));\n             assertThat(routingTable.index(\"test2\").shard(i).replicaShards().get(0).state(), equalTo(UNASSIGNED));\n         }\n@@ -206,13 +206,13 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         routingNodes = clusterState.routingNodes();\n \n         for (int i = 0; i < routingTable.index(\"test1\").shards().size(); i++) {\n-            assertThat(routingTable.index(\"test1\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test1\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test1\").shard(i).primaryShard().state(), equalTo(STARTED));\n             assertThat(routingTable.index(\"test1\").shard(i).replicaShards().get(0).state(), equalTo(STARTED));\n         }\n \n         for (int i = 0; i < routingTable.index(\"test2\").shards().size(); i++) {\n-            assertThat(routingTable.index(\"test2\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test2\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test2\").shard(i).primaryShard().state(), equalTo(STARTED));\n             assertThat(routingTable.index(\"test2\").shard(i).replicaShards().get(0).state(), equalTo(INITIALIZING));\n         }\n@@ -226,13 +226,13 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n         routingNodes = clusterState.routingNodes();\n \n-        assertThat(routingNodes.node(\"node3\").size(), equalTo(1));\n-        assertThat(routingNodes.node(\"node3\").get(0).shardId().index().name(), equalTo(\"test1\"));\n+        assertThat(routingNodes.node(\"node3\").shards().size(), equalTo(1));\n+        assertThat(routingNodes.node(\"node3\").shards().get(0).shardId().index().name(), equalTo(\"test1\"));\n     }\n \n     @Test\n     public void testClusterPrimariesActive2() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_PRIMARIES_ACTIVE.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_PRIMARIES_ACTIVE.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\n@@ -311,12 +311,12 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n         routingNodes = clusterState.routingNodes();\n \n-        assertThat(routingNodes.node(\"node3\").isEmpty(), equalTo(true));\n+        assertThat(routingNodes.node(\"node3\").shards().isEmpty(), equalTo(true));\n     }\n \n     @Test\n     public void testClusterAllActive1() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\n@@ -433,13 +433,13 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n         routingNodes = clusterState.routingNodes();\n \n-        assertThat(routingNodes.node(\"node3\").size(), equalTo(1));\n-        assertThat(routingNodes.node(\"node3\").get(0).shardId().index().name(), equalTo(\"test1\"));\n+        assertThat(routingNodes.node(\"node3\").shards().size(), equalTo(1));\n+        assertThat(routingNodes.node(\"node3\").shards().get(0).shardId().index().name(), equalTo(\"test1\"));\n     }\n \n     @Test\n     public void testClusterAllActive2() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\n@@ -518,12 +518,12 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n         routingNodes = clusterState.routingNodes();\n \n-        assertThat(routingNodes.node(\"node3\").isEmpty(), equalTo(true));\n+        assertThat(routingNodes.node(\"node3\").shards().isEmpty(), equalTo(true));\n     }\n \n     @Test\n     public void testClusterAllActive3() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\n@@ -621,6 +621,6 @@ public class ClusterRebalanceRoutingTests extends ElasticsearchAllocationTestCas\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n         routingNodes = clusterState.routingNodes();\n \n-        assertThat(routingNodes.node(\"node3\").isEmpty(), equalTo(true));\n+        assertThat(routingNodes.node(\"node3\").shards().isEmpty(), equalTo(true));\n     }\n }\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java\nindex 1afa702c1cd..3eb108ba015 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java\n@@ -27,7 +27,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -36,13 +36,13 @@ import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilde\n import static org.hamcrest.Matchers.equalTo;\n import static org.hamcrest.Matchers.nullValue;\n \n-public class ConcurrentRebalanceRoutingTests extends ElasticsearchAllocationTestCase {\n+public class ConcurrentRebalanceRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(ConcurrentRebalanceRoutingTests.class);\n \n     @Test\n     public void testClusterConcurrentRebalance() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.cluster_concurrent_rebalance\", 3)\n                 .build());\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java\nindex 4410382be05..5b8e8bbcf83 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java\n@@ -28,7 +28,7 @@ import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;\n import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -38,13 +38,13 @@ import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n+public class DeadNodesAllocationTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(DeadNodesAllocationTests.class);\n \n     @Test\n     public void simpleDeadNodeOnStartedPrimaryShard() {\n-        AllocationService allocation = createAllocationService(settingsBuilder()\n+        AllocationService allocation = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .build());\n@@ -75,10 +75,10 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n         logger.info(\"--> verifying all is allocated\");\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().get(0).state(), equalTo(STARTED));\n \n         logger.info(\"--> fail node with primary\");\n         String nodeIdToFail = clusterState.routingTable().index(\"test\").shard(0).primaryShard().currentNodeId();\n@@ -90,13 +90,13 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState);\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n-        assertThat(clusterState.routingNodes().node(nodeIdRemaining).get(0).primary(), equalTo(true));\n-        assertThat(clusterState.routingNodes().node(nodeIdRemaining).get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(nodeIdRemaining).shards().get(0).primary(), equalTo(true));\n+        assertThat(clusterState.routingNodes().node(nodeIdRemaining).shards().get(0).state(), equalTo(STARTED));\n     }\n \n     @Test\n     public void deadNodeWhileRelocatingOnToNode() {\n-        AllocationService allocation = createAllocationService(settingsBuilder()\n+        AllocationService allocation = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .build());\n@@ -127,10 +127,10 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n         logger.info(\"--> verifying all is allocated\");\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().get(0).state(), equalTo(STARTED));\n \n         logger.info(\"--> adding additional node\");\n         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())\n@@ -139,11 +139,11 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState);\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node3\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().size(), equalTo(0));\n \n         String origPrimaryNodeId = clusterState.routingTable().index(\"test\").shard(0).primaryShard().currentNodeId();\n         String origReplicaNodeId = clusterState.routingTable().index(\"test\").shard(0).replicaShards().get(0).currentNodeId();\n@@ -154,8 +154,8 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         );\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));\n-        assertThat(clusterState.routingNodes().node(\"node3\").get(0).state(), equalTo(INITIALIZING));\n+        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).shards().get(0).state(), equalTo(RELOCATING));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().get(0).state(), equalTo(INITIALIZING));\n \n         logger.info(\"--> fail primary shard recovering instance on node3 being initialized by killing node3\");\n         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()\n@@ -165,13 +165,13 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState);\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(origReplicaNodeId).shards().get(0).state(), equalTo(STARTED));\n     }\n \n     @Test\n     public void deadNodeWhileRelocatingOnFromNode() {\n-        AllocationService allocation = createAllocationService(settingsBuilder()\n+        AllocationService allocation = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .build());\n@@ -202,10 +202,10 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n         logger.info(\"--> verifying all is allocated\");\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().get(0).state(), equalTo(STARTED));\n \n         logger.info(\"--> adding additional node\");\n         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())\n@@ -214,11 +214,11 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState);\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node3\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().size(), equalTo(0));\n \n         String origPrimaryNodeId = clusterState.routingTable().index(\"test\").shard(0).primaryShard().currentNodeId();\n         String origReplicaNodeId = clusterState.routingTable().index(\"test\").shard(0).replicaShards().get(0).currentNodeId();\n@@ -229,8 +229,8 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         );\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));\n-        assertThat(clusterState.routingNodes().node(\"node3\").get(0).state(), equalTo(INITIALIZING));\n+        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).shards().get(0).state(), equalTo(RELOCATING));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().get(0).state(), equalTo(INITIALIZING));\n \n         logger.info(\"--> fail primary shard recovering instance on 'origPrimaryNodeId' being relocated\");\n         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()\n@@ -240,7 +240,7 @@ public class DeadNodesAllocationTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState);\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n-        assertThat(clusterState.routingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node3\").get(0).state(), equalTo(INITIALIZING));\n+        assertThat(clusterState.routingNodes().node(origReplicaNodeId).shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().get(0).state(), equalTo(INITIALIZING));\n     }\n }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java\nindex a3121bc9941..2c6c0d25853 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java\n@@ -28,7 +28,7 @@ import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDec\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n@@ -39,13 +39,13 @@ import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class DisableAllocationTests extends ElasticsearchAllocationTestCase {\n+public class DisableAllocationTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(DisableAllocationTests.class);\n \n     @Test\n     public void testClusterDisableAllocation() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)\n                 .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)\n                 .build());\n@@ -75,7 +75,7 @@ public class DisableAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testClusterDisableReplicaAllocation() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.disable_replica_allocation\", true)\n                 .build());\n \n@@ -109,7 +109,7 @@ public class DisableAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testIndexDisableAllocation() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .build());\n \n         MetaData metaData = MetaData.builder()\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java\nindex 1a0c27de5e5..c82f7efbfa6 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java\n@@ -28,7 +28,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n@@ -40,13 +40,13 @@ import static org.hamcrest.Matchers.equalTo;\n /**\n  *\n  */\n-public class ElectReplicaAsPrimaryDuringRelocationTests extends ElasticsearchAllocationTestCase {\n+public class ElectReplicaAsPrimaryDuringRelocationTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(ElectReplicaAsPrimaryDuringRelocationTests.class);\n \n     @Test\n     public void testElectReplicaAsPrimaryDuringRelocation() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java\nindex b92bb7ead60..cc265db2255 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java\n@@ -29,7 +29,7 @@ import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n@@ -38,13 +38,13 @@ import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTest\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n \n-public class FailedNodeRoutingTests extends ElasticsearchAllocationTestCase {\n+public class FailedNodeRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(FailedNodeRoutingTests.class);\n \n     @Test\n     public void simpleFailedNodeTest() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\n@@ -104,7 +104,7 @@ public class FailedNodeRoutingTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void simpleFailedNodeTestNoReassign() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java\nindex 1295dc68501..fc703195939 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java\n@@ -31,7 +31,7 @@ import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;\n import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -42,13 +42,13 @@ import static org.hamcrest.Matchers.*;\n /**\n  *\n  */\n-public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n+public class FailedShardsRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(FailedShardsRoutingTests.class);\n \n     @Test\n     public void testFailedShardPrimaryRelocatingToAndFrom() {\n-        AllocationService allocation = createAllocationService(settingsBuilder()\n+        AllocationService allocation = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .build());\n@@ -79,10 +79,10 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n         logger.info(\"--> verifying all is allocated\");\n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().get(0).state(), equalTo(STARTED));\n \n         logger.info(\"--> adding additional node\");\n         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())\n@@ -91,11 +91,11 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n         rerouteResult = allocation.reroute(clusterState);\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n-        assertThat(clusterState.routingNodes().node(\"node1\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node1\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.routingNodes().node(\"node2\").get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node3\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node1\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.routingNodes().node(\"node2\").shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().size(), equalTo(0));\n \n         String origPrimaryNodeId = clusterState.routingTable().index(\"test\").shard(0).primaryShard().currentNodeId();\n         String origReplicaNodeId = clusterState.routingTable().index(\"test\").shard(0).replicaShards().get(0).currentNodeId();\n@@ -106,15 +106,15 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n         );\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));\n-        assertThat(clusterState.routingNodes().node(\"node3\").get(0).state(), equalTo(INITIALIZING));\n+        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).shards().get(0).state(), equalTo(RELOCATING));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().get(0).state(), equalTo(INITIALIZING));\n \n         logger.info(\"--> fail primary shard recovering instance on node3 being initialized\");\n-        rerouteResult = allocation.applyFailedShard(clusterState, new ImmutableShardRouting(clusterState.routingNodes().node(\"node3\").get(0)));\n+        rerouteResult = allocation.applyFailedShard(clusterState, new ImmutableShardRouting(clusterState.routingNodes().node(\"node3\").shards().get(0)));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(STARTED));\n-        assertThat(clusterState.routingNodes().node(\"node3\").size(), equalTo(0));\n+        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).shards().get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().size(), equalTo(0));\n \n         logger.info(\"--> moving primary shard to node3\");\n         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(\n@@ -122,22 +122,22 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n         );\n         assertThat(rerouteResult.changed(), equalTo(true));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));\n-        assertThat(clusterState.routingNodes().node(\"node3\").get(0).state(), equalTo(INITIALIZING));\n+        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).shards().get(0).state(), equalTo(RELOCATING));\n+        assertThat(clusterState.routingNodes().node(\"node3\").shards().get(0).state(), equalTo(INITIALIZING));\n \n         logger.info(\"--> fail primary shard recovering instance on node1 being relocated\");\n-        rerouteResult = allocation.applyFailedShard(clusterState, new ImmutableShardRouting(clusterState.routingNodes().node(origPrimaryNodeId).get(0)));\n+        rerouteResult = allocation.applyFailedShard(clusterState, new ImmutableShardRouting(clusterState.routingNodes().node(origPrimaryNodeId).shards().get(0)));\n         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();\n \n         // check promotion of replica to primary\n-        assertThat(clusterState.routingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));\n+        assertThat(clusterState.routingNodes().node(origReplicaNodeId).shards().get(0).state(), equalTo(STARTED));\n         assertThat(clusterState.routingTable().index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(origReplicaNodeId));\n         assertThat(clusterState.routingTable().index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), anyOf(equalTo(origPrimaryNodeId), equalTo(\"node3\")));\n     }\n \n     @Test\n     public void failPrimaryStartedCheckReplicaElected() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .build());\n@@ -162,7 +162,6 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n \n         logger.info(\"Start the shards (primaries)\");\n         RoutingNodes routingNodes = clusterState.routingNodes();\n-\n         prevRoutingTable = routingTable;\n         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n@@ -171,7 +170,7 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         for (int i = 0; i < routingTable.index(\"test\").shards().size(); i++) {\n             assertThat(routingTable.index(\"test\").shard(i).size(), equalTo(2));\n-            assertThat(routingTable.index(\"test\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test\").shard(i).primaryShard().state(), equalTo(STARTED));\n             assertThat(routingTable.index(\"test\").shard(i).primaryShard().currentNodeId(), anyOf(equalTo(\"node1\"), equalTo(\"node2\")));\n             assertThat(routingTable.index(\"test\").shard(i).replicaShards().size(), equalTo(1));\n@@ -189,7 +188,7 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         for (int i = 0; i < routingTable.index(\"test\").shards().size(); i++) {\n             assertThat(routingTable.index(\"test\").shard(i).size(), equalTo(2));\n-            assertThat(routingTable.index(\"test\").shard(i).size(), equalTo(2));\n+            assertThat(routingTable.index(\"test\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test\").shard(i).primaryShard().state(), equalTo(STARTED));\n             assertThat(routingTable.index(\"test\").shard(i).primaryShard().currentNodeId(), anyOf(equalTo(\"node1\"), equalTo(\"node2\")));\n             assertThat(routingTable.index(\"test\").shard(i).replicaShards().size(), equalTo(1));\n@@ -206,7 +205,7 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n \n         assertThat(prevRoutingTable != routingTable, equalTo(true));\n         assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(2));\n-        assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(2));\n+        assertThat(routingTable.index(\"test\").shard(0).shards().size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), not(equalTo(shardToFail.currentNodeId())));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().state(), equalTo(STARTED));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), anyOf(equalTo(\"node1\"), equalTo(\"node2\")));\n@@ -219,7 +218,7 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void firstAllocationFailureSingleNode() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .build());\n@@ -276,7 +275,7 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void firstAllocationFailureTwoNodes() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .build());\n@@ -298,7 +297,6 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n         RoutingTable prevRoutingTable = routingTable;\n         routingTable = strategy.reroute(clusterState).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        final String nodeHoldingPrimary = routingTable.index(\"test\").shard(0).primaryShard().currentNodeId();\n \n         assertThat(prevRoutingTable != routingTable, equalTo(true));\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n@@ -306,14 +304,14 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n             assertThat(routingTable.index(\"test\").shard(i).size(), equalTo(2));\n             assertThat(routingTable.index(\"test\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test\").shard(i).primaryShard().state(), equalTo(INITIALIZING));\n-            assertThat(routingTable.index(\"test\").shard(i).primaryShard().currentNodeId(), equalTo(nodeHoldingPrimary));\n+            assertThat(routingTable.index(\"test\").shard(i).primaryShard().currentNodeId(), equalTo(\"node1\"));\n             assertThat(routingTable.index(\"test\").shard(i).replicaShards().size(), equalTo(1));\n             assertThat(routingTable.index(\"test\").shard(i).replicaShards().get(0).state(), equalTo(UNASSIGNED));\n         }\n \n         logger.info(\"fail the first shard, will start INITIALIZING on the second node\");\n         prevRoutingTable = routingTable;\n-        routingTable = strategy.applyFailedShard(clusterState, new ImmutableShardRouting(\"test\", 0, nodeHoldingPrimary, true, INITIALIZING, 0)).routingTable();\n+        routingTable = strategy.applyFailedShard(clusterState, new ImmutableShardRouting(\"test\", 0, \"node1\", true, INITIALIZING, 0)).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n         RoutingNodes routingNodes = clusterState.routingNodes();\n \n@@ -323,18 +321,18 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n             assertThat(routingTable.index(\"test\").shard(i).size(), equalTo(2));\n             assertThat(routingTable.index(\"test\").shard(i).shards().size(), equalTo(2));\n             assertThat(routingTable.index(\"test\").shard(i).primaryShard().state(), equalTo(INITIALIZING));\n-            assertThat(routingTable.index(\"test\").shard(i).primaryShard().currentNodeId(), not(equalTo(nodeHoldingPrimary)));\n+            assertThat(routingTable.index(\"test\").shard(i).primaryShard().currentNodeId(), equalTo(\"node2\"));\n             assertThat(routingTable.index(\"test\").shard(i).replicaShards().size(), equalTo(1));\n             assertThat(routingTable.index(\"test\").shard(i).replicaShards().get(0).state(), equalTo(UNASSIGNED));\n         }\n \n         logger.info(\"fail the shard again, see that nothing happens\");\n-        assertThat(strategy.applyFailedShard(clusterState, new ImmutableShardRouting(\"test\", 0, nodeHoldingPrimary, true, INITIALIZING, 0)).changed(), equalTo(false));\n+        assertThat(strategy.applyFailedShard(clusterState, new ImmutableShardRouting(\"test\", 0, \"node1\", true, INITIALIZING, 0)).changed(), equalTo(false));\n     }\n \n     @Test\n     public void rebalanceFailure() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .build());\n@@ -410,7 +408,7 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n \n \n         logger.info(\"Fail the shards on node 3\");\n-        ShardRouting shardToFail = routingNodes.node(\"node3\").get(0);\n+        ShardRouting shardToFail = routingNodes.node(\"node3\").shards().get(0);\n         routingNodes = clusterState.routingNodes();\n         prevRoutingTable = routingTable;\n         routingTable = strategy.applyFailedShard(clusterState, new ImmutableShardRouting(shardToFail)).routingTable();\n@@ -425,6 +423,6 @@ public class FailedShardsRoutingTests extends ElasticsearchAllocationTestCase {\n         assertThat(routingNodes.node(\"node2\").numberOfShardsWithState(STARTED), lessThan(3));\n         assertThat(routingNodes.node(\"node3\").numberOfShardsWithState(INITIALIZING), equalTo(1));\n         // make sure the failedShard is not INITIALIZING again on node3\n-        assertThat(routingNodes.node(\"node3\").get(0).shardId(), not(equalTo(shardToFail.shardId())));\n+        assertThat(routingNodes.node(\"node3\").shards().get(0).shardId(), not(equalTo(shardToFail.shardId())));\n     }\n }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java\nindex 0989b2b09f3..b2229354d3b 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java\n@@ -29,7 +29,7 @@ import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.cluster.routing.ShardRoutingState;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.hamcrest.Matchers;\n import org.junit.Test;\n \n@@ -42,13 +42,13 @@ import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class FilterRoutingTests extends ElasticsearchAllocationTestCase {\n+public class FilterRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(FilterRoutingTests.class);\n \n     @Test\n     public void testClusterFilters() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.include.tag1\", \"value1,value2\")\n                 .put(\"cluster.routing.allocation.exclude.tag1\", \"value3,value4\")\n                 .build());\n@@ -94,7 +94,7 @@ public class FilterRoutingTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testIndexFilters() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .build());\n \n         logger.info(\"Building initial routing table\");\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java\nindex dacc28f33a4..d3aa86266cd 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java\n@@ -28,7 +28,7 @@ import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -40,13 +40,13 @@ import static org.hamcrest.Matchers.nullValue;\n /**\n  *\n  */\n-public class IndexBalanceTests extends ElasticsearchAllocationTestCase {\n+public class IndexBalanceTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(IndexBalanceTests.class);\n \n     @Test\n     public void testBalanceAllNodesStarted() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n@@ -177,7 +177,7 @@ public class IndexBalanceTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testBalanceIncrementallyStartNodes() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n@@ -340,7 +340,7 @@ public class IndexBalanceTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testBalanceAllNodesStartedAddIndex() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java\nindex 82fc1e58a06..206fa7bdeae 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java\n@@ -25,7 +25,7 @@ import org.elasticsearch.cluster.metadata.MetaData;\n import org.elasticsearch.cluster.node.DiscoveryNodes;\n import org.elasticsearch.cluster.routing.MutableShardRouting;\n import org.elasticsearch.cluster.routing.RoutingTable;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -36,7 +36,7 @@ import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class PreferLocalPrimariesToRelocatingPrimariesTests extends ElasticsearchAllocationTestCase {\n+public class PreferLocalPrimariesToRelocatingPrimariesTests extends ElasticsearchTestCase {\n     @Test\n     public void testPreferLocalPrimaryAllocationOverFiltered() {\n         int concurrentRecoveries = randomIntBetween(1, 10);\n@@ -45,7 +45,7 @@ public class PreferLocalPrimariesToRelocatingPrimariesTests extends Elasticsearc\n         int totalNumberOfShards = numberOfShards * 2;\n \n         logger.info(\"create an allocation with [{}] initial primary recoveries and [{}] concurrent recoveries\", primaryRecoveries, concurrentRecoveries);\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", concurrentRecoveries)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", primaryRecoveries)\n                 .build());\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java\nindex 87be5458997..36a20bc3f0f 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java\n@@ -26,7 +26,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n@@ -36,14 +36,14 @@ import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class PreferPrimaryAllocationTests extends ElasticsearchAllocationTestCase {\n+public class PreferPrimaryAllocationTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(PreferPrimaryAllocationTests.class);\n \n     @Test\n     public void testPreferPrimaryAllocationOverReplicas() {\n         logger.info(\"create an allocation with 1 initial recoveries\");\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 1)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 1)\n                 .build());\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java\nindex edf53c2a763..e10a44c1818 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java\n@@ -27,7 +27,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n@@ -40,13 +40,13 @@ import static org.hamcrest.Matchers.nullValue;\n /**\n  *\n  */\n-public class PrimaryElectionRoutingTests extends ElasticsearchAllocationTestCase {\n+public class PrimaryElectionRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(PrimaryElectionRoutingTests.class);\n \n     @Test\n     public void testBackupElectionToPrimaryWhenPrimaryCanBeAllocatedToAnotherNode() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \n@@ -97,7 +97,7 @@ public class PrimaryElectionRoutingTests extends ElasticsearchAllocationTestCase\n \n     @Test\n     public void testRemovingInitializingReplicasIfPrimariesFails() {\n-        AllocationService allocation = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService allocation = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java\nindex ad5d5a247ec..bbb83e78750 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java\n@@ -27,7 +27,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n@@ -39,14 +39,14 @@ import static org.hamcrest.Matchers.equalTo;\n /**\n  *\n  */\n-public class PrimaryNotRelocatedWhileBeingRecoveredTests extends ElasticsearchAllocationTestCase {\n+public class PrimaryNotRelocatedWhileBeingRecoveredTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(PrimaryNotRelocatedWhileBeingRecoveredTests.class);\n \n \n     @Test\n     public void testPrimaryNotRelocatedWhileBeingRecoveredFrom() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .build());\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java\nindex 1deb7433633..c08a63d3258 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java\n@@ -35,7 +35,7 @@ import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;\n import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n import org.elasticsearch.cluster.routing.allocation.decider.SameShardAllocationDecider;\n import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.hamcrest.Matchers;\n import org.junit.Test;\n \n@@ -48,7 +48,7 @@ import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTest\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n \n-public class RandomAllocationDeciderTests extends ElasticsearchAllocationTestCase {\n+public class RandomAllocationDeciderTests extends ElasticsearchTestCase {\n \n     /* This test will make random allocation decision on a growing and shrinking\n      * cluster leading to a random distribution of the shards. After a certain\n@@ -156,7 +156,7 @@ public class RandomAllocationDeciderTests extends ElasticsearchAllocationTestCas\n             if (clusterState.getRoutingNodes().node(\"NODE_\" + i) == null) {\n                 continue;\n             }\n-            assertThat(clusterState.getRoutingNodes().node(\"NODE_\" + i).size(), Matchers.anyOf(\n+            assertThat(clusterState.getRoutingNodes().node(\"NODE_\" + i).shards().size(), Matchers.anyOf(\n                     Matchers.anyOf(equalTo((shards / numNodes) + 1), equalTo((shards / numNodes) - 1), equalTo((shards / numNodes))),\n                     Matchers.allOf(Matchers.greaterThanOrEqualTo(lowerBound), Matchers.lessThanOrEqualTo(upperBound))));\n         }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java\nindex a58353f2158..efb2d0e1651 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java\n@@ -28,7 +28,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -40,13 +40,13 @@ import static org.hamcrest.Matchers.nullValue;\n /**\n  *\n  */\n-public class RebalanceAfterActiveTests extends ElasticsearchAllocationTestCase {\n+public class RebalanceAfterActiveTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(RebalanceAfterActiveTests.class);\n \n     @Test\n     public void testRebalanceOnlyAfterAllShardsAreActive() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.cluster_concurrent_rebalance\", -1)\n@@ -145,7 +145,7 @@ public class RebalanceAfterActiveTests extends ElasticsearchAllocationTestCase {\n         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(10));\n         // make sure we have an even relocation\n         for (RoutingNode routingNode : routingNodes) {\n-            assertThat(routingNode.size(), equalTo(1));\n+            assertThat(routingNode.shards().size(), equalTo(1));\n         }\n     }\n }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java\nindex f6e40fe3148..ed6d3755b8f 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java\n@@ -27,26 +27,25 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.not;\n import static org.hamcrest.Matchers.nullValue;\n \n /**\n  *\n  */\n-public class ReplicaAllocatedAfterPrimaryTests extends ElasticsearchAllocationTestCase {\n+public class ReplicaAllocatedAfterPrimaryTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(ReplicaAllocatedAfterPrimaryTests.class);\n \n     @Test\n     public void testBackupIsAllocatedAfterPrimary() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \n@@ -74,15 +73,13 @@ public class ReplicaAllocatedAfterPrimaryTests extends ElasticsearchAllocationTe\n         RoutingTable prevRoutingTable = routingTable;\n         routingTable = strategy.reroute(clusterState).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        final String nodeHoldingPrimary = routingTable.index(\"test\").shard(0).primaryShard().currentNodeId();\n-\n \n         assertThat(prevRoutingTable != routingTable, equalTo(true));\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).shards().size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().state(), equalTo(INITIALIZING));\n-        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(nodeHoldingPrimary));\n+        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(\"node1\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).state(), equalTo(UNASSIGNED));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), nullValue());\n@@ -90,19 +87,18 @@ public class ReplicaAllocatedAfterPrimaryTests extends ElasticsearchAllocationTe\n         logger.info(\"Start all the primary shards\");\n         RoutingNodes routingNodes = clusterState.routingNodes();\n         prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.node(nodeHoldingPrimary).shardsWithState(INITIALIZING)).routingTable();\n+        routingTable = strategy.applyStartedShards(clusterState, routingNodes.node(\"node1\").shardsWithState(INITIALIZING)).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        final String nodeHoldingReplica = routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId();\n-        assertThat(nodeHoldingPrimary, not(equalTo(nodeHoldingReplica)));\n+\n         assertThat(prevRoutingTable != routingTable, equalTo(true));\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).shards().size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(nodeHoldingPrimary));\n+        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(\"node1\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).state(), equalTo(INITIALIZING));\n-        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(nodeHoldingReplica));\n+        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(\"node2\"));\n \n     }\n }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocationTests.java\nindex dd9cbc1d3b5..f972a39dc95 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocationTests.java\n@@ -26,7 +26,7 @@ import org.elasticsearch.cluster.routing.MutableShardRouting;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.transport.DummyTransportAddress;\n import org.elasticsearch.common.transport.TransportAddress;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Ignore;\n \n import java.util.List;\n@@ -36,7 +36,7 @@ import static com.google.common.collect.Lists.newArrayList;\n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n \n @Ignore(\"Not a test\")\n-public class RoutingAllocationTests extends ElasticsearchAllocationTestCase {\n+public class RoutingAllocationTests extends ElasticsearchTestCase {\n \n     public static DiscoveryNode newNode(String nodeId) {\n         return new DiscoveryNode(nodeId, DummyTransportAddress.INSTANCE, Version.CURRENT);\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java\ndeleted file mode 100644\nindex 9a619f15e0a..00000000000\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java\n+++ /dev/null\n@@ -1,416 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.cluster.routing.allocation;\n-\n-import org.elasticsearch.cluster.ClusterState;\n-import org.elasticsearch.cluster.metadata.IndexMetaData;\n-import org.elasticsearch.cluster.metadata.MetaData;\n-import org.elasticsearch.cluster.node.DiscoveryNodes;\n-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n-import org.elasticsearch.cluster.routing.RoutingNodes;\n-import org.elasticsearch.cluster.routing.RoutingTable;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n-import org.junit.Test;\n-\n-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n-import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;\n-import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n-import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n-import static org.hamcrest.Matchers.equalTo;\n-\n-/**\n- *\n- */\n-public class RoutingNodesIntegrityTests extends ElasticsearchAllocationTestCase {\n-\n-    private final ESLogger logger = Loggers.getLogger(IndexBalanceTests.class);\n-\n-    @Test\n-    public void testBalanceAllNodesStarted() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n-                .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n-                .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n-                .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n-                .put(\"cluster.routing.allocation.cluster_concurrent_rebalance\", -1).build());\n-\n-        logger.info(\"Building initial routing table\");\n-\n-        MetaData metaData = MetaData.builder().put(IndexMetaData.builder(\"test\").numberOfShards(3).numberOfReplicas(1))\n-                .put(IndexMetaData.builder(\"test1\").numberOfShards(3).numberOfReplicas(1)).build();\n-\n-        RoutingTable routingTable = RoutingTable.builder().addAsNew(metaData.index(\"test\")).addAsNew(metaData.index(\"test1\")).build();\n-\n-        ClusterState clusterState = ClusterState.builder().metaData(metaData).routingTable(routingTable).build();\n-        RoutingNodes routingNodes = clusterState.routingNodes();\n-\n-        logger.info(\"Adding three node and performing rerouting\");\n-        clusterState = ClusterState.builder(clusterState)\n-                .nodes(DiscoveryNodes.builder().put(newNode(\"node1\")).put(newNode(\"node2\")).put(newNode(\"node3\"))).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        // all shards are unassigned. so no inactive shards or primaries.\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(false));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(true));\n-\n-        RoutingTable prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(true));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(true));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-        logger.info(\"Another round of rebalancing\");\n-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())).build();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        logger.info(\"Reroute, nothing should change\");\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-\n-        logger.info(\"Start the more shards\");\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(false));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-    }\n-\n-    @Test\n-    public void testBalanceIncrementallyStartNodes() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n-                .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n-                .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n-                .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n-                .put(\"cluster.routing.allocation.cluster_concurrent_rebalance\", -1).build());\n-\n-        logger.info(\"Building initial routing table\");\n-\n-        MetaData metaData = MetaData.builder().put(IndexMetaData.builder(\"test\").numberOfShards(3).numberOfReplicas(1))\n-                .put(IndexMetaData.builder(\"test1\").numberOfShards(3).numberOfReplicas(1)).build();\n-\n-        RoutingTable routingTable = RoutingTable.builder().addAsNew(metaData.index(\"test\")).addAsNew(metaData.index(\"test1\")).build();\n-\n-        ClusterState clusterState = ClusterState.builder().metaData(metaData).routingTable(routingTable).build();\n-\n-        logger.info(\"Adding one node and performing rerouting\");\n-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode(\"node1\"))).build();\n-\n-        RoutingTable prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-\n-        logger.info(\"Add another node and perform rerouting, nothing will happen since primary not started\");\n-        clusterState = ClusterState.builder(clusterState)\n-                .nodes(DiscoveryNodes.builder(clusterState.nodes()).put(newNode(\"node2\"))).build();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-\n-        logger.info(\"Start the primary shard\");\n-        RoutingNodes routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-\n-        logger.info(\"Reroute, nothing should change\");\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-\n-        logger.info(\"Start the backup shard\");\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        logger.info(\"Add another node and perform rerouting, nothing will happen since primary not started\");\n-        clusterState = ClusterState.builder(clusterState)\n-                .nodes(DiscoveryNodes.builder(clusterState.nodes()).put(newNode(\"node3\"))).build();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-\n-        logger.info(\"Reroute, nothing should change\");\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-\n-        logger.info(\"Start the backup shard\");\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(prevRoutingTable != routingTable, equalTo(true));\n-        assertThat(routingTable.index(\"test\").shards().size(), equalTo(3));\n-\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(prevRoutingTable != routingTable, equalTo(true));\n-        assertThat(routingTable.index(\"test1\").shards().size(), equalTo(3));\n-\n-        assertThat(routingNodes.node(\"node1\").numberOfShardsWithState(STARTED), equalTo(4));\n-        assertThat(routingNodes.node(\"node2\").numberOfShardsWithState(STARTED), equalTo(4));\n-        assertThat(routingNodes.node(\"node3\").numberOfShardsWithState(STARTED), equalTo(4));\n-\n-        assertThat(routingNodes.node(\"node1\").shardsWithState(\"test\", STARTED).size(), equalTo(2));\n-        assertThat(routingNodes.node(\"node2\").shardsWithState(\"test\", STARTED).size(), equalTo(2));\n-        assertThat(routingNodes.node(\"node3\").shardsWithState(\"test\", STARTED).size(), equalTo(2));\n-\n-        assertThat(routingNodes.node(\"node1\").shardsWithState(\"test1\", STARTED).size(), equalTo(2));\n-        assertThat(routingNodes.node(\"node2\").shardsWithState(\"test1\", STARTED).size(), equalTo(2));\n-        assertThat(routingNodes.node(\"node3\").shardsWithState(\"test1\", STARTED).size(), equalTo(2));\n-    }\n-\n-    @Test\n-    public void testBalanceAllNodesStartedAddIndex() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n-                .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 1)\n-                .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 3)\n-                .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n-                .put(\"cluster.routing.allocation.cluster_concurrent_rebalance\", -1).build());\n-\n-        logger.info(\"Building initial routing table\");\n-\n-        MetaData metaData = MetaData.builder().put(IndexMetaData.builder(\"test\").numberOfShards(3).numberOfReplicas(1)).build();\n-\n-        RoutingTable routingTable = RoutingTable.builder().addAsNew(metaData.index(\"test\")).build();\n-\n-        ClusterState clusterState = ClusterState.builder().metaData(metaData).routingTable(routingTable).build();\n-\n-        logger.info(\"Adding three node and performing rerouting\");\n-        clusterState = ClusterState.builder(clusterState)\n-                .nodes(DiscoveryNodes.builder().put(newNode(\"node1\")).put(newNode(\"node2\")).put(newNode(\"node3\"))).build();\n-\n-        RoutingNodes routingNodes = clusterState.routingNodes();\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(false));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(true));\n-\n-        RoutingTable prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(true));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(true));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-        logger.info(\"Another round of rebalancing\");\n-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())).build();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-\n-        assertThat(prevRoutingTable == routingTable, equalTo(true));\n-\n-        routingNodes = clusterState.routingNodes();\n-        assertThat(routingNodes.node(\"node1\").numberOfShardsWithState(INITIALIZING), equalTo(1));\n-        assertThat(routingNodes.node(\"node2\").numberOfShardsWithState(INITIALIZING), equalTo(1));\n-        assertThat(routingNodes.node(\"node3\").numberOfShardsWithState(INITIALIZING), equalTo(1));\n-\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(true));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-        assertThat(routingNodes.node(\"node1\").numberOfShardsWithState(STARTED), equalTo(1));\n-        assertThat(routingNodes.node(\"node2\").numberOfShardsWithState(STARTED), equalTo(1));\n-        assertThat(routingNodes.node(\"node3\").numberOfShardsWithState(STARTED), equalTo(1));\n-\n-        logger.info(\"Reroute, nothing should change\");\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        assertThat(prevRoutingTable == routingTable, equalTo(true));\n-\n-        logger.info(\"Start the more shards\");\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(false));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-        assertThat(routingNodes.node(\"node1\").numberOfShardsWithState(STARTED), equalTo(2));\n-        assertThat(routingNodes.node(\"node2\").numberOfShardsWithState(STARTED), equalTo(2));\n-        assertThat(routingNodes.node(\"node3\").numberOfShardsWithState(STARTED), equalTo(2));\n-\n-        assertThat(routingNodes.node(\"node1\").shardsWithState(\"test\", STARTED).size(), equalTo(2));\n-        assertThat(routingNodes.node(\"node2\").shardsWithState(\"test\", STARTED).size(), equalTo(2));\n-        assertThat(routingNodes.node(\"node3\").shardsWithState(\"test\", STARTED).size(), equalTo(2));\n-\n-        logger.info(\"Add new index 3 shards 1 replica\");\n-\n-        prevRoutingTable = routingTable;\n-        metaData = MetaData.builder(metaData)\n-                .put(IndexMetaData.builder(\"test1\").settings(ImmutableSettings.settingsBuilder()\n-                        .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 3)\n-                        .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)\n-                ))\n-                .build();\n-        routingTable = RoutingTable.builder(routingTable)\n-                .addAsNew(metaData.index(\"test1\"))\n-                .build();\n-        clusterState = ClusterState.builder(clusterState).metaData(metaData).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(false));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(true));\n-\n-        assertThat(routingTable.index(\"test1\").shards().size(), equalTo(3));\n-\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-\n-        logger.info(\"Reroute, assign\");\n-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())).build();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(true));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(true));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-        assertThat(prevRoutingTable == routingTable, equalTo(true));\n-\n-        logger.info(\"Reroute, start the primaries\");\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(true));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-        logger.info(\"Reroute, start the replicas\");\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(false));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-\n-        assertThat(routingNodes.node(\"node1\").numberOfShardsWithState(STARTED), equalTo(4));\n-        assertThat(routingNodes.node(\"node2\").numberOfShardsWithState(STARTED), equalTo(4));\n-        assertThat(routingNodes.node(\"node3\").numberOfShardsWithState(STARTED), equalTo(4));\n-\n-        assertThat(routingNodes.node(\"node1\").shardsWithState(\"test1\", STARTED).size(), equalTo(2));\n-        assertThat(routingNodes.node(\"node2\").shardsWithState(\"test1\", STARTED).size(), equalTo(2));\n-        assertThat(routingNodes.node(\"node3\").shardsWithState(\"test1\", STARTED).size(), equalTo(2));\n-\n-        logger.info(\"kill one node\");\n-        IndexShardRoutingTable indexShardRoutingTable = routingTable.index(\"test\").shard(0);\n-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove(indexShardRoutingTable.primaryShard().currentNodeId())).build();\n-        routingTable = strategy.reroute(clusterState).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(true));\n-        // replica got promoted to primary\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-        logger.info(\"Start Recovering shards round 1\");\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(true));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-        logger.info(\"Start Recovering shards round 2\");\n-        routingNodes = clusterState.routingNodes();\n-        prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        routingNodes = clusterState.routingNodes();\n-\n-        assertThat(assertShardStats(routingNodes), equalTo(true));\n-        assertThat(routingNodes.hasInactiveShards(), equalTo(false));\n-        assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));\n-        assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));\n-\n-    }\n-\n-    private boolean assertShardStats(RoutingNodes routingNodes) {\n-        return RoutingNodes.assertShardStats(routingNodes);\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java\nindex 8fce31ed215..5ae5de603ac 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java\n@@ -11,26 +11,23 @@ import org.elasticsearch.cluster.routing.allocation.decider.SameShardAllocationD\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n import org.elasticsearch.common.transport.InetSocketTransportAddress;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n-import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n-import static org.elasticsearch.cluster.routing.allocation.RoutingNodesUtils.numberOfShardsOfType;\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class SameShardRoutingTests extends ElasticsearchAllocationTestCase {\n+public class SameShardRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(SameShardRoutingTests.class);\n \n     @Test\n-    @TestLogging(\"cluster.routing.allocation:TRACE\")\n     public void sameHost() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(SameShardAllocationDecider.SAME_HOST_SETTING, true).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(SameShardAllocationDecider.SAME_HOST_SETTING, true).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test\").numberOfShards(2).numberOfReplicas(1))\n@@ -48,14 +45,14 @@ public class SameShardRoutingTests extends ElasticsearchAllocationTestCase {\n         routingTable = strategy.reroute(clusterState).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(2));\n+        assertThat(clusterState.readOnlyRoutingNodes().numberOfShardsOfType(ShardRoutingState.INITIALIZING), equalTo(2));\n \n         logger.info(\"--> start all primary shards, no replica will be started since its on the same host\");\n         routingTable = strategy.applyStartedShards(clusterState, clusterState.readOnlyRoutingNodes().shardsWithState(INITIALIZING)).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.STARTED), equalTo(2));\n-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(0));\n+        assertThat(clusterState.readOnlyRoutingNodes().numberOfShardsOfType(ShardRoutingState.STARTED), equalTo(2));\n+        assertThat(clusterState.readOnlyRoutingNodes().numberOfShardsOfType(ShardRoutingState.INITIALIZING), equalTo(0));\n \n         logger.info(\"--> add another node, with a different host, replicas will be allocating\");\n         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())\n@@ -63,8 +60,8 @@ public class SameShardRoutingTests extends ElasticsearchAllocationTestCase {\n         routingTable = strategy.reroute(clusterState).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.STARTED), equalTo(2));\n-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(2));\n+        assertThat(clusterState.readOnlyRoutingNodes().numberOfShardsOfType(ShardRoutingState.STARTED), equalTo(2));\n+        assertThat(clusterState.readOnlyRoutingNodes().numberOfShardsOfType(ShardRoutingState.INITIALIZING), equalTo(2));\n         for (MutableShardRouting shardRouting : clusterState.readOnlyRoutingNodes().shardsWithState(INITIALIZING)) {\n             assertThat(shardRouting.currentNodeId(), equalTo(\"node3\"));\n         }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java\nindex 52b77893175..ba673dfbee9 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java\n@@ -28,7 +28,7 @@ import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -36,13 +36,13 @@ import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTest\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n \n-public class ShardVersioningTests extends ElasticsearchAllocationTestCase {\n+public class ShardVersioningTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(ShardVersioningTests.class);\n \n     @Test\n     public void simple() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.allow_rebalance\", ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());\n \n         MetaData metaData = MetaData.builder()\n                 .put(IndexMetaData.builder(\"test1\").numberOfShards(1).numberOfReplicas(1))\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java\nindex fb4770c9d9e..991fd6d7528 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java\n@@ -31,24 +31,23 @@ import org.elasticsearch.cluster.routing.allocation.decider.ShardsLimitAllocatio\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n-import static org.elasticsearch.cluster.routing.allocation.RoutingNodesUtils.numberOfShardsOfType;\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n \n /**\n  */\n-public class ShardsLimitAllocationTests extends ElasticsearchAllocationTestCase {\n+public class ShardsLimitAllocationTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(ShardsLimitAllocationTests.class);\n \n     @Test\n     public void indexLevelShardsLimitAllocate() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \n@@ -91,7 +90,7 @@ public class ShardsLimitAllocationTests extends ElasticsearchAllocationTestCase\n \n     @Test\n     public void indexLevelShardsLimitRemain() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.cluster_concurrent_rebalance\", -1)\n@@ -124,7 +123,7 @@ public class ShardsLimitAllocationTests extends ElasticsearchAllocationTestCase\n         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), STARTED), equalTo(5));\n+        assertThat(clusterState.readOnlyRoutingNodes().numberOfShardsOfType(STARTED), equalTo(5));\n \n         logger.info(\"add another index with 5 shards\");\n         metaData = MetaData.builder(metaData)\n@@ -148,7 +147,7 @@ public class ShardsLimitAllocationTests extends ElasticsearchAllocationTestCase\n         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), STARTED), equalTo(10));\n+        assertThat(clusterState.readOnlyRoutingNodes().numberOfShardsOfType(STARTED), equalTo(10));\n \n         for (MutableShardRouting shardRouting : clusterState.readOnlyRoutingNodes().node(\"node1\")) {\n             assertThat(shardRouting.index(), equalTo(\"test\"));\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java\nindex 5da6d06b652..ee55f9e7e45 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java\n@@ -24,10 +24,13 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;\n import org.elasticsearch.cluster.metadata.MetaData;\n import org.elasticsearch.cluster.node.DiscoveryNode;\n import org.elasticsearch.cluster.node.DiscoveryNodes;\n-import org.elasticsearch.cluster.routing.*;\n+import org.elasticsearch.cluster.routing.MutableShardRouting;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.RoutingNodes;\n+import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import java.util.List;\n@@ -37,20 +40,19 @@ import static com.google.common.collect.Lists.newArrayList;\n import static com.google.common.collect.Sets.newHashSet;\n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTests.newNode;\n-import static org.elasticsearch.cluster.routing.allocation.RoutingNodesUtils.numberOfShardsOfType;\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.*;\n \n /**\n  *\n  */\n-public class SingleShardNoReplicasRoutingTests extends ElasticsearchAllocationTestCase {\n+public class SingleShardNoReplicasRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(SingleShardNoReplicasRoutingTests.class);\n \n     @Test\n     public void testSingleIndexStartedShard() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \n@@ -152,7 +154,7 @@ public class SingleShardNoReplicasRoutingTests extends ElasticsearchAllocationTe\n \n     @Test\n     public void testSingleIndexShardFailed() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \n@@ -202,7 +204,7 @@ public class SingleShardNoReplicasRoutingTests extends ElasticsearchAllocationTe\n \n     @Test\n     public void testMultiIndexEvenDistribution() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.cluster_concurrent_rebalance\", -1)\n@@ -261,7 +263,7 @@ public class SingleShardNoReplicasRoutingTests extends ElasticsearchAllocationTe\n         Set<String> encounteredIndices = newHashSet();\n         for (RoutingNode routingNode : routingNodes) {\n             assertThat(routingNode.numberOfShardsWithState(STARTED), equalTo(0));\n-            assertThat(routingNode.size(), equalTo(2));\n+            assertThat(routingNode.shards().size(), equalTo(2));\n             // make sure we still have 2 shards initializing per node on the only 25 nodes\n             int nodeIndex = Integer.parseInt(routingNode.nodeId().substring(\"node\".length()));\n             assertThat(nodeIndex, lessThan(25));\n@@ -315,7 +317,7 @@ public class SingleShardNoReplicasRoutingTests extends ElasticsearchAllocationTe\n \n     @Test\n     public void testMultiIndexUnevenNodes() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.cluster_concurrent_rebalance\", -1)\n@@ -356,7 +358,7 @@ public class SingleShardNoReplicasRoutingTests extends ElasticsearchAllocationTe\n             assertThat(routingTable.index(\"test\" + i).shard(0).shards().get(0).state(), equalTo(INITIALIZING));\n         }\n         RoutingNodes routingNodes = clusterState.routingNodes();\n-        assertThat(numberOfShardsOfType(routingNodes, INITIALIZING), equalTo(numberOfIndices));\n+        assertThat(routingNodes.numberOfShardsOfType(INITIALIZING), equalTo(numberOfIndices));\n         assertThat(routingNodes.node(\"node1\").numberOfShardsWithState(INITIALIZING), anyOf(equalTo(3), equalTo(4)));\n         assertThat(routingNodes.node(\"node2\").numberOfShardsWithState(INITIALIZING), anyOf(equalTo(3), equalTo(4)));\n         assertThat(routingNodes.node(\"node2\").numberOfShardsWithState(INITIALIZING), anyOf(equalTo(3), equalTo(4)));\n@@ -385,8 +387,8 @@ public class SingleShardNoReplicasRoutingTests extends ElasticsearchAllocationTe\n             assertThat(routingTable.index(\"test\" + i).shard(0).shards().get(0).state(), anyOf(equalTo(RELOCATING), equalTo(STARTED)));\n         }\n         routingNodes = clusterState.routingNodes();\n-        assertThat(\"4 source shard routing are relocating\", numberOfShardsOfType(routingNodes, RELOCATING), equalTo(4));\n-        assertThat(\"4 target shard routing are initializing\", numberOfShardsOfType(routingNodes, INITIALIZING), equalTo(4));\n+        assertThat(\"4 source shard routing are relocating\", routingNodes.numberOfShardsOfType(RELOCATING), equalTo(4));\n+        assertThat(\"4 target shard routing are initializing\", routingNodes.numberOfShardsOfType(INITIALIZING), equalTo(4));\n \n         logger.info(\"Now, mark the relocated as started\");\n         prevRoutingTable = routingTable;\n@@ -402,7 +404,7 @@ public class SingleShardNoReplicasRoutingTests extends ElasticsearchAllocationTe\n             assertThat(routingTable.index(\"test\" + i).shard(0).shards().get(0).state(), anyOf(equalTo(RELOCATING), equalTo(STARTED)));\n         }\n         routingNodes = clusterState.routingNodes();\n-        assertThat(numberOfShardsOfType(routingNodes, STARTED), equalTo(numberOfIndices));\n+        assertThat(routingNodes.numberOfShardsOfType(STARTED), equalTo(numberOfIndices));\n         for (RoutingNode routingNode : routingNodes) {\n             assertThat(routingNode.numberOfShardsWithState(STARTED), equalTo(2));\n         }\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java\nindex bc9162058c2..1eb5ec56424 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java\n@@ -27,7 +27,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -39,13 +39,13 @@ import static org.hamcrest.Matchers.nullValue;\n /**\n  *\n  */\n-public class SingleShardOneReplicaRoutingTests extends ElasticsearchAllocationTestCase {\n+public class SingleShardOneReplicaRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(SingleShardOneReplicaRoutingTests.class);\n \n     @Test\n     public void testSingleIndexFirstStartPrimaryThenBackups() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java\nindex b1ec6e0a154..0e61d336586 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java\n@@ -27,7 +27,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -38,13 +38,13 @@ import static org.hamcrest.Matchers.*;\n /**\n  *\n  */\n-public class TenShardsOneReplicaRoutingTests extends ElasticsearchAllocationTestCase {\n+public class TenShardsOneReplicaRoutingTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(TenShardsOneReplicaRoutingTests.class);\n \n     @Test\n     public void testSingleIndexFirstStartPrimaryThenBackups() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java\nindex 38adb3a786f..da89e111da4 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java\n@@ -26,7 +26,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -37,13 +37,13 @@ import static org.hamcrest.Matchers.equalTo;\n /**\n  *\n  */\n-public class ThrottlingAllocationTests extends ElasticsearchAllocationTestCase {\n+public class ThrottlingAllocationTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(ThrottlingAllocationTests.class);\n \n     @Test\n     public void testPrimaryRecoveryThrottling() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.node_concurrent_recoveries\", 3)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 3)\n                 .build());\n@@ -104,7 +104,7 @@ public class ThrottlingAllocationTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testReplicaAndPrimaryRecoveryThrottling() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 3)\n                 .put(\"cluster.routing.allocation.node_initial_primaries_recoveries\", 3)\n                 .build());\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java\nindex 00482dfcc8a..8ba7edbe560 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java\n@@ -8,7 +8,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.*;\n@@ -19,13 +19,13 @@ import static org.hamcrest.Matchers.*;\n /**\n  *\n  */\n-public class UpdateNumberOfReplicasTests extends ElasticsearchAllocationTestCase {\n+public class UpdateNumberOfReplicasTests extends ElasticsearchTestCase {\n \n     private final ESLogger logger = Loggers.getLogger(UpdateNumberOfReplicasTests.class);\n \n     @Test\n     public void testUpdateNumberOfReplicas() {\n-        AllocationService strategy = createAllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n+        AllocationService strategy = new AllocationService(settingsBuilder().put(\"cluster.routing.allocation.concurrent_recoveries\", 10).build());\n \n         logger.info(\"Building initial routing table\");\n \n@@ -58,7 +58,7 @@ public class UpdateNumberOfReplicasTests extends ElasticsearchAllocationTestCase\n         logger.info(\"Start all the primary shards\");\n         RoutingNodes routingNodes = clusterState.routingNodes();\n         prevRoutingTable = routingTable;\n-        routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n+        routingTable = strategy.applyStartedShards(clusterState, routingNodes.node(\"node1\").shardsWithState(INITIALIZING)).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n         logger.info(\"Start all the replica shards\");\n@@ -66,19 +66,16 @@ public class UpdateNumberOfReplicasTests extends ElasticsearchAllocationTestCase\n         prevRoutingTable = routingTable;\n         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n-        final String nodeHoldingPrimary = routingTable.index(\"test\").shard(0).primaryShard().currentNodeId();\n-        final String nodeHoldingReplica = routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId();\n-        assertThat(nodeHoldingPrimary, not(equalTo(nodeHoldingReplica)));\n+\n         assertThat(prevRoutingTable != routingTable, equalTo(true));\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).shards().size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(nodeHoldingPrimary));\n+        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(\"node1\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(nodeHoldingReplica));\n-\n+        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(\"node2\"));\n \n         logger.info(\"add another replica\");\n         routingNodes = clusterState.routingNodes();\n@@ -93,10 +90,10 @@ public class UpdateNumberOfReplicasTests extends ElasticsearchAllocationTestCase\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(3));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(nodeHoldingPrimary));\n+        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(\"node1\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(nodeHoldingReplica));\n+        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(\"node2\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(1).state(), equalTo(UNASSIGNED));\n \n         logger.info(\"Add another node and start the added replica\");\n@@ -109,10 +106,10 @@ public class UpdateNumberOfReplicasTests extends ElasticsearchAllocationTestCase\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(3));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(nodeHoldingPrimary));\n+        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(\"node1\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(nodeHoldingReplica));\n+        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(\"node2\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(1).state(), equalTo(INITIALIZING));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(1).currentNodeId(), equalTo(\"node3\"));\n \n@@ -125,10 +122,10 @@ public class UpdateNumberOfReplicasTests extends ElasticsearchAllocationTestCase\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(3));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(nodeHoldingPrimary));\n+        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(\"node1\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(nodeHoldingReplica));\n+        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), equalTo(\"node2\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(1).state(), equalTo(STARTED));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(1).currentNodeId(), equalTo(\"node3\"));\n \n@@ -145,10 +142,10 @@ public class UpdateNumberOfReplicasTests extends ElasticsearchAllocationTestCase\n         assertThat(routingTable.index(\"test\").shards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).size(), equalTo(2));\n         assertThat(routingTable.index(\"test\").shard(0).primaryShard().state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(nodeHoldingPrimary));\n+        assertThat(routingTable.index(\"test\").shard(0).primaryShard().currentNodeId(), equalTo(\"node1\"));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().size(), equalTo(1));\n         assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).state(), equalTo(STARTED));\n-        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), anyOf(equalTo(nodeHoldingReplica), equalTo(\"node3\")));\n+        assertThat(routingTable.index(\"test\").shard(0).replicaShards().get(0).currentNodeId(), anyOf(equalTo(\"node2\"), equalTo(\"node3\")));\n \n         logger.info(\"do a reroute, should remain the same\");\n         prevRoutingTable = routingTable;\ndiff --git a/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java b/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java\nindex 05071bda4f8..c443df5988e 100644\n--- a/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java\n@@ -33,7 +33,7 @@ import org.elasticsearch.cluster.routing.allocation.AllocationService;\n import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;\n import org.elasticsearch.common.settings.ImmutableSettings;\n import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import java.util.Arrays;\n@@ -46,7 +46,7 @@ import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTest\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.equalTo;\n \n-public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n+public class DiskThresholdDeciderTests extends ElasticsearchTestCase {\n \n     @Test\n     public void diskThresholdTest() {\n@@ -115,7 +115,7 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n         // Assert that we're able to start the primary\n         assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));\n         // Assert that node1 didn't get any shards because its disk usage is too high\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n \n         logger.info(\"--> start the shards (replicas)\");\n         routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();\n@@ -145,9 +145,9 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n         logShardStates(clusterState);\n         // Assert that the replica couldn't be started since node1 doesn't have enough space\n         assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(1));\n \n         logger.info(\"--> changing decider settings\");\n \n@@ -176,9 +176,9 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n \n         // Shards remain started\n         assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(1));\n \n         logger.info(\"--> changing settings again\");\n \n@@ -207,10 +207,10 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n         logShardStates(clusterState);\n         // Shards remain started\n         assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n         // Shard hasn't been moved off of node2 yet because there's nowhere for it to go\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(1));\n \n         logger.info(\"--> adding node4\");\n \n@@ -230,11 +230,11 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n         logShardStates(clusterState);\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n         // Node4 is available now, so the shard is moved off of node2\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(0));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(1));\n-        assertThat(clusterState.getRoutingNodes().node(\"node4\").size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node4\").shards().size(), equalTo(1));\n     }\n \n     @Test\n@@ -304,7 +304,7 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n         // Assert that we're able to start the primary\n         assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));\n         // Assert that node1 didn't get any shards because its disk usage is too high\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n \n         logger.info(\"--> start the shards (replicas)\");\n         routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();\n@@ -334,9 +334,9 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n         logShardStates(clusterState);\n         // Assert that the replica couldn't be started since node1 doesn't have enough space\n         assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(1));\n \n         logger.info(\"--> changing decider settings\");\n \n@@ -365,9 +365,9 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n \n         // Shards remain started\n         assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(1));\n \n         logger.info(\"--> changing settings again\");\n \n@@ -396,10 +396,10 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n         logShardStates(clusterState);\n         // Shards remain started\n         assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n         // Shard hasn't been moved off of node2 yet because there's nowhere for it to go\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(1));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(1));\n \n         logger.info(\"--> adding node4\");\n \n@@ -419,11 +419,11 @@ public class DiskThresholdDeciderTests extends ElasticsearchAllocationTestCase {\n         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();\n \n         logShardStates(clusterState);\n-        assertThat(clusterState.getRoutingNodes().node(\"node1\").size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node1\").shards().size(), equalTo(0));\n         // Node4 is available now, so the shard is moved off of node2\n-        assertThat(clusterState.getRoutingNodes().node(\"node2\").size(), equalTo(0));\n-        assertThat(clusterState.getRoutingNodes().node(\"node3\").size(), equalTo(1));\n-        assertThat(clusterState.getRoutingNodes().node(\"node4\").size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node2\").shards().size(), equalTo(0));\n+        assertThat(clusterState.getRoutingNodes().node(\"node3\").shards().size(), equalTo(1));\n+        assertThat(clusterState.getRoutingNodes().node(\"node4\").shards().size(), equalTo(1));\n     }\n \n     @Test\ndiff --git a/src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java b/src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java\nindex abda107860b..9e9fde6d683 100644\n--- a/src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java\n@@ -30,7 +30,7 @@ import org.elasticsearch.cluster.routing.allocation.AllocationService;\n import org.elasticsearch.common.io.stream.BytesStreamInput;\n import org.elasticsearch.common.io.stream.BytesStreamOutput;\n import org.elasticsearch.common.transport.DummyTransportAddress;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.hamcrest.Matchers.equalTo;\n@@ -38,7 +38,7 @@ import static org.hamcrest.Matchers.equalTo;\n /**\n  *\n  */\n-public class ClusterSerializationTests extends ElasticsearchAllocationTestCase {\n+public class ClusterSerializationTests extends ElasticsearchTestCase {\n \n     @Test\n     public void testClusterStateSerialization() throws Exception {\n@@ -54,7 +54,7 @@ public class ClusterSerializationTests extends ElasticsearchAllocationTestCase {\n \n         ClusterState clusterState = ClusterState.builder().nodes(nodes).metaData(metaData).routingTable(routingTable).build();\n \n-        AllocationService strategy = createAllocationService();\n+        AllocationService strategy = new AllocationService();\n         clusterState = ClusterState.builder(clusterState).routingTable(strategy.reroute(clusterState).routingTable()).build();\n \n         ClusterState serializedClusterState = ClusterState.Builder.fromBytes(ClusterState.Builder.toBytes(clusterState), newNode(\"node1\"));\n@@ -77,7 +77,7 @@ public class ClusterSerializationTests extends ElasticsearchAllocationTestCase {\n \n         ClusterState clusterState = ClusterState.builder().nodes(nodes).metaData(metaData).routingTable(routingTable).build();\n \n-        AllocationService strategy = createAllocationService();\n+        AllocationService strategy = new AllocationService();\n         RoutingTable source = strategy.reroute(clusterState).routingTable();\n \n         BytesStreamOutput outStream = new BytesStreamOutput();\ndiff --git a/src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java b/src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java\nindex 0d04a7a27bd..fc33d6f48cf 100644\n--- a/src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java\n@@ -10,7 +10,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;\n import org.elasticsearch.cluster.routing.RoutingTable;\n import org.elasticsearch.cluster.routing.allocation.AllocationService;\n import org.elasticsearch.common.transport.DummyTransportAddress;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.hamcrest.Matchers.containsString;\n@@ -18,7 +18,7 @@ import static org.hamcrest.Matchers.containsString;\n /**\n  *\n  */\n-public class ClusterStateToStringTests extends ElasticsearchAllocationTestCase {\n+public class ClusterStateToStringTests extends ElasticsearchTestCase {\n     @Test\n     public void testClusterStateSerialization() throws Exception {\n         MetaData metaData = MetaData.builder()\n@@ -34,7 +34,7 @@ public class ClusterStateToStringTests extends ElasticsearchAllocationTestCase {\n \n         ClusterState clusterState = ClusterState.builder().nodes(nodes).metaData(metaData).routingTable(routingTable).build();\n \n-        AllocationService strategy = createAllocationService();\n+        AllocationService strategy = new AllocationService();\n         clusterState = ClusterState.builder(clusterState).routingTable(strategy.reroute(clusterState).routingTable()).build();\n \n         String clusterStateString = clusterState.toString();\ndiff --git a/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java b/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java\nindex f504dc5f4a4..a0721e4f86d 100644\n--- a/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java\n+++ b/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java\n@@ -32,7 +32,7 @@ import org.elasticsearch.cluster.routing.operation.hash.djb.DjbHashFunction;\n import org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting;\n import org.elasticsearch.common.settings.ImmutableSettings;\n import org.elasticsearch.index.shard.ShardId;\n-import org.elasticsearch.test.ElasticsearchAllocationTestCase;\n+import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;\n@@ -40,7 +40,7 @@ import static org.elasticsearch.cluster.routing.allocation.RoutingAllocationTest\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.hamcrest.Matchers.*;\n \n-public class RoutingIteratorTests extends ElasticsearchAllocationTestCase {\n+public class RoutingIteratorTests extends ElasticsearchTestCase {\n \n     @Test\n     public void testEmptyIterator() {\n@@ -248,7 +248,7 @@ public class RoutingIteratorTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testAttributePreferenceRouting() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .put(\"cluster.routing.allocation.allow_rebalance\", \"always\")\n                 .put(\"cluster.routing.allocation.awareness.attributes\", \"rack_id,zone\")\n@@ -299,7 +299,7 @@ public class RoutingIteratorTests extends ElasticsearchAllocationTestCase {\n \n     @Test\n     public void testShardsAndPreferNodeRouting() {\n-        AllocationService strategy = createAllocationService(settingsBuilder()\n+        AllocationService strategy = new AllocationService(settingsBuilder()\n                 .put(\"cluster.routing.allocation.concurrent_recoveries\", 10)\n                 .build());\n \ndiff --git a/src/test/java/org/elasticsearch/common/util/BigArraysTests.java b/src/test/java/org/elasticsearch/common/util/BigArraysTests.java\nindex 0517299703c..ba1825897a0 100644\n--- a/src/test/java/org/elasticsearch/common/util/BigArraysTests.java\n+++ b/src/test/java/org/elasticsearch/common/util/BigArraysTests.java\n@@ -109,24 +109,4 @@ public class BigArraysTests extends ElasticsearchTestCase {\n         }\n     }\n \n-    public void testLongArrayFill() {\n-        final int len = randomIntBetween(1, 100000);\n-        final int fromIndex = randomIntBetween(0, len - 1);\n-        final int toIndex = randomBoolean()\n-            ? Math.min(fromIndex + randomInt(100), len) // single page\n-            : randomIntBetween(fromIndex, len); // likely multiple pages\n-        final LongArray array2 = BigArrays.newLongArray(len);\n-        final long[] array1 = new long[len];\n-        for (int i = 0; i < len; ++i) {\n-            array1[i] = randomLong();\n-            array2.set(i, array1[i]);\n-        }\n-        final long rand = randomLong();\n-        Arrays.fill(array1, fromIndex, toIndex, rand);\n-        array2.fill(fromIndex, toIndex, rand);\n-        for (int i = 0; i < len; ++i) {\n-            assertEquals(array1[i], array2.get(i));\n-        }\n-    }\n-\n }\ndiff --git a/src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java b/src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java\nindex 35dd2e1b6dd..59e0432b32f 100644\n--- a/src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java\n+++ b/src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java\n@@ -120,10 +120,10 @@ public class DeleteByQueryTests extends ElasticsearchIntegrationTest {\n                     .setSource(\"foo\", \"bar\").get();\n         }\n         refresh();\n-        assertHitCount(client().prepareCount(\"test\").setQuery(QueryBuilders.matchQuery(\"_id\", Integer.toString(between(0, numDocs - 1)))).get(), 1);\n+        assertHitCount(client().prepareCount(\"test\").setQuery(QueryBuilders.fieldQuery(\"_id\", Integer.toString(between(0, numDocs - 1)))).get(), 1);\n         assertHitCount(client().prepareCount(\"test\").setQuery(QueryBuilders.matchAllQuery()).get(), numDocs);\n         client().prepareDeleteByQuery(\"test\")\n-                .setQuery(QueryBuilders.matchQuery(\"_id\", Integer.toString(between(0, numDocs - 1))))\n+                .setQuery(QueryBuilders.fieldQuery(\"_id\", Integer.toString(between(0, numDocs - 1))))\n                 .execute().actionGet();\n         refresh();\n         assertHitCount(client().prepareCount(\"test\").setQuery(QueryBuilders.matchAllQuery()).get(), numDocs - 1);\ndiff --git a/src/test/java/org/elasticsearch/gateway/local/QuorumLocalGatewayTests.java b/src/test/java/org/elasticsearch/gateway/local/QuorumLocalGatewayTests.java\nindex 975c2d19142..dc0463e924b 100644\n--- a/src/test/java/org/elasticsearch/gateway/local/QuorumLocalGatewayTests.java\n+++ b/src/test/java/org/elasticsearch/gateway/local/QuorumLocalGatewayTests.java\n@@ -41,8 +41,6 @@ import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;\n import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;\n import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.is;\n-import static org.hamcrest.Matchers.notNullValue;\n \n /**\n  *\n@@ -78,7 +76,7 @@ public class QuorumLocalGatewayTests extends ElasticsearchIntegrationTest {\n         }\n         \n         final String nodeToRemove = nodes[between(0,2)];\n-        logger.info(\"--> restarting 1 nodes -- kill 2\");\n+        logger.info(\"--> restarting 2 nodes -- kill 1\");\n         cluster().fullRestart(new RestartCallback() {\n             @Override\n             public Settings onNodeStopped(String nodeName) throws Exception {\n@@ -87,30 +85,23 @@ public class QuorumLocalGatewayTests extends ElasticsearchIntegrationTest {\n             \n             @Override\n             public boolean doRestart(String nodeName) {\n-                return nodeToRemove.equals(nodeName);\n+                return !nodeToRemove.equals(nodeName);\n             }\n         });\n-        if (randomBoolean()) {\n-            Thread.sleep(between(1, 400)); // wait a bit and give is a chance to try to allocate\n-        }\n-        clusterHealth = client().admin().cluster().health(clusterHealthRequest().waitForNodes(\"1\")).actionGet();\n-        assertThat(clusterHealth.isTimedOut(), equalTo(false));\n-        assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.RED));  // nothing allocated yet\n+\n         assertThat(awaitBusy(new Predicate<Object>() {\n             @Override\n             public boolean apply(Object input) {\n                 ClusterStateResponse clusterStateResponse = cluster().smartClient().admin().cluster().prepareState().setMasterNodeTimeout(\"500ms\").get();\n-                return clusterStateResponse.getState() != null && clusterStateResponse.getState().routingTable().index(\"test\") != null;\n-            }}), equalTo(true)); // wait until we get a cluster state - could be null if we quick enough.\n-        final ClusterStateResponse clusterStateResponse = cluster().smartClient().admin().cluster().prepareState().setMasterNodeTimeout(\"500ms\").get();\n-        assertThat(clusterStateResponse.getState(), notNullValue());\n-        assertThat(clusterStateResponse.getState().routingTable().index(\"test\"), notNullValue());\n-        assertThat(clusterStateResponse.getState().routingTable().index(\"test\").allPrimaryShardsActive(), is(false));\n+                return !clusterStateResponse.getState().routingTable().index(\"test\").allPrimaryShardsActive();\n+            }\n+        }, 30, TimeUnit.SECONDS), equalTo(true));\n+\n         logger.info(\"--> change the recovery.initial_shards setting, and make sure its recovered\");\n         client().admin().indices().prepareUpdateSettings(\"test\").setSettings(settingsBuilder().put(\"recovery.initial_shards\", 1)).get();\n \n-        logger.info(\"--> running cluster_health (wait for the shards to startup), 2 shards since we only have 1 node\");\n-        clusterHealth = client().admin().cluster().health(clusterHealthRequest().waitForYellowStatus().waitForActiveShards(2)).actionGet();\n+        logger.info(\"--> running cluster_health (wait for the shards to startup), 4 shards since we only have 2 nodes\");\n+        clusterHealth = client().admin().cluster().health(clusterHealthRequest().waitForYellowStatus().waitForActiveShards(4)).actionGet();\n         logger.info(\"--> done cluster_health, status \" + clusterHealth.getStatus());\n         assertThat(clusterHealth.isTimedOut(), equalTo(false));\n         assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.YELLOW));\ndiff --git a/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java b/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java\nindex 6e71a9e524a..d97596396ac 100644\n--- a/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java\n+++ b/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java\n@@ -66,8 +66,6 @@ public abstract class AbstractFieldDataTests extends ElasticsearchTestCase {\n             mapper = MapperBuilders.shortField(fieldName).fieldDataSettings(type.getSettings()).build(context);\n         } else if (type.getType().equals(\"byte\")) {\n             mapper = MapperBuilders.byteField(fieldName).fieldDataSettings(type.getSettings()).build(context);\n-        } else if (type.getType().equals(\"geo_point\")) {\n-            mapper = MapperBuilders.geoPointField(fieldName).fieldDataSettings(type.getSettings()).build(context);\n         } else {\n             throw new UnsupportedOperationException(type.getType());\n         }\ndiff --git a/src/test/java/org/elasticsearch/index/fielddata/DisabledFieldDataFormatTests.java b/src/test/java/org/elasticsearch/index/fielddata/DisabledFieldDataFormatTests.java\ndeleted file mode 100644\nindex b088035f112..00000000000\n--- a/src/test/java/org/elasticsearch/index/fielddata/DisabledFieldDataFormatTests.java\n+++ /dev/null\n@@ -1,93 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.index.fielddata;\n-\n-import org.elasticsearch.action.search.SearchPhaseExecutionException;\n-import org.elasticsearch.action.search.SearchResponse;\n-import org.elasticsearch.common.xcontent.XContentFactory;\n-import org.elasticsearch.search.aggregations.AggregationBuilders;\n-import org.elasticsearch.test.ElasticsearchIntegrationTest;\n-\n-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;\n-\n-public class DisabledFieldDataFormatTests extends ElasticsearchIntegrationTest {\n-\n-    public void test() throws Exception {\n-        createIndex(\"test\");\n-        ensureGreen();\n-        for (int i = 0; i < 10; ++i) {\n-            client().prepareIndex(\"test\", \"type\", Integer.toString(i)).setSource(\"s\", \"value\" + i).execute().actionGet();\n-        }\n-\n-        refresh();\n-\n-        // disable field data\n-        updateFormat(\"disabled\");\n-\n-        SearchResponse resp = null;\n-        // try to run something that relies on field data and make sure that it fails\n-        try {\n-            resp = client().prepareSearch(\"test\").addAggregation(AggregationBuilders.terms(\"t\").field(\"s\")).execute().actionGet();\n-            assertTrue(resp.toString(), resp.getFailedShards() > 0);\n-        } catch (SearchPhaseExecutionException e) {\n-            // expected\n-        }\n-\n-        // enable it again\n-        updateFormat(\"paged_bytes\");\n-\n-        // try to run something that relies on field data and make sure that it works\n-        resp = client().prepareSearch(\"test\").addAggregation(AggregationBuilders.terms(\"t\").field(\"s\")).execute().actionGet();\n-        assertNoFailures(resp);\n-\n-        // disable it again\n-        updateFormat(\"disabled\");\n-\n-        // this time, it should work because segments are already loaded\n-        resp = client().prepareSearch(\"test\").addAggregation(AggregationBuilders.terms(\"t\").field(\"s\")).execute().actionGet();\n-        assertNoFailures(resp);\n-\n-        // but add more docs and the new segment won't be loaded\n-        client().prepareIndex(\"test\", \"type\", \"-1\").setSource(\"s\", \"value\").execute().actionGet();\n-        refresh();\n-        try {\n-            resp = client().prepareSearch(\"test\").addAggregation(AggregationBuilders.terms(\"t\").field(\"s\")).execute().actionGet();\n-            assertTrue(resp.toString(), resp.getFailedShards() > 0);\n-        } catch (SearchPhaseExecutionException e) {\n-            // expected\n-        }\n-    }\n-\n-    private void updateFormat(String format) throws Exception {\n-        client().admin().indices().preparePutMapping(\"test\").setType(\"type\").setSource(\n-                XContentFactory.jsonBuilder().startObject().startObject(\"type\")\n-                    .startObject(\"properties\")\n-                        .startObject(\"s\")\n-                            .field(\"type\", \"string\")\n-                                .startObject(\"fielddata\")\n-                                    .field(\"format\", format)\n-                                .endObject()\n-                            .endObject()\n-                          .endObject()\n-                        .endObject()\n-                    .endObject()).execute().actionGet();\n-    }\n-\n-}\ndiff --git a/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java b/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java\nindex 7e973c3424f..3e4f54b6850 100644\n--- a/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java\n+++ b/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java\n@@ -18,7 +18,6 @@\n  */\n package org.elasticsearch.index.fielddata;\n \n-import com.google.common.collect.Lists;\n import org.apache.lucene.document.*;\n import org.apache.lucene.index.AtomicReaderContext;\n import org.apache.lucene.index.CompositeReaderContext;\n@@ -27,11 +26,7 @@ import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.English;\n import org.apache.lucene.util.LuceneTestCase;\n import org.apache.lucene.util.NumericUtils;\n-import org.elasticsearch.common.geo.GeoDistance;\n-import org.elasticsearch.common.geo.GeoPoint;\n import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.common.unit.DistanceUnit;\n-import org.elasticsearch.common.unit.DistanceUnit.Distance;\n import org.junit.Test;\n \n import java.util.*;\n@@ -342,59 +337,6 @@ public class DuelFieldDataTests extends AbstractFieldDataTests {\n \n     }\n \n-    public void testDuelGeoPoints() throws Exception {\n-        Random random = getRandom();\n-        int atLeast = atLeast(random, 1000);\n-        int maxValuesPerDoc = randomIntBetween(1, 3);\n-        for (int i = 0; i < atLeast; i++) {\n-            Document d = new Document();\n-            d.add(new StringField(\"_id\", \"\" + i, Field.Store.NO));\n-            final int numValues = randomInt(maxValuesPerDoc);\n-            for (int j = 0; j < numValues; ++j) {\n-                final double lat = randomDouble() * 180 - 90;\n-                final double lon = randomDouble() * 360 - 180;\n-                d.add(new StringField(\"geopoint\", lat + \",\" + lon, Field.Store.NO));\n-            }\n-            writer.addDocument(d);\n-            if (random.nextInt(10) == 0) {\n-                refreshReader();\n-            }\n-        }\n-        AtomicReaderContext context = refreshReader();\n-        Map<FieldDataType, Type> typeMap = new HashMap<FieldDataType, DuelFieldDataTests.Type>();\n-        final Distance precision = new Distance(1, randomFrom(DistanceUnit.values()));\n-        typeMap.put(new FieldDataType(\"geo_point\", ImmutableSettings.builder().put(\"format\", \"array\")), Type.GeoPoint);\n-        typeMap.put(new FieldDataType(\"geo_point\", ImmutableSettings.builder().put(\"format\", \"compressed\").put(\"precision\", precision)), Type.GeoPoint);\n-\n-        ArrayList<Entry<FieldDataType, Type>> list = new ArrayList<Entry<FieldDataType, Type>>(typeMap.entrySet());\n-        while (!list.isEmpty()) {\n-            Entry<FieldDataType, Type> left;\n-            Entry<FieldDataType, Type> right;\n-            if (list.size() > 1) {\n-                left = list.remove(random.nextInt(list.size()));\n-                right = list.remove(random.nextInt(list.size()));\n-            } else {\n-                right = left = list.remove(0);\n-            }\n-            ifdService.clear();\n-            IndexGeoPointFieldData<?> leftFieldData = getForField(left.getKey(), left.getValue().name().toLowerCase(Locale.ROOT));\n-\n-            ifdService.clear();\n-            IndexGeoPointFieldData<?> rightFieldData = getForField(right.getKey(), right.getValue().name().toLowerCase(Locale.ROOT));\n-\n-            duelFieldDataGeoPoint(random, context, leftFieldData, rightFieldData, precision);\n-            duelFieldDataGeoPoint(random, context, rightFieldData, leftFieldData, precision);\n-\n-            DirectoryReader perSegment = DirectoryReader.open(writer, true);\n-            CompositeReaderContext composite = perSegment.getContext();\n-            List<AtomicReaderContext> leaves = composite.leaves();\n-            for (AtomicReaderContext atomicReaderContext : leaves) {\n-                duelFieldDataGeoPoint(random, atomicReaderContext, leftFieldData, rightFieldData, precision);\n-            }\n-            perSegment.close();\n-        }\n-    }\n-\n     private void assertOrder(AtomicFieldData.Order order, IndexFieldData<?> data, AtomicReaderContext context) throws Exception {\n         AtomicFieldData<?> leftData = randomBoolean() ? data.load(context) : data.loadDirect(context);\n         assertThat(leftData.getBytesValues(randomBoolean()).getOrder(), is(order));\n@@ -496,44 +438,6 @@ public class DuelFieldDataTests extends AbstractFieldDataTests {\n         }\n     }\n \n-    private static void duelFieldDataGeoPoint(Random random, AtomicReaderContext context, IndexGeoPointFieldData<?> left, IndexGeoPointFieldData<?> right, Distance precision) throws Exception {\n-        AtomicGeoPointFieldData<?> leftData = random.nextBoolean() ? left.load(context) : left.loadDirect(context);\n-        AtomicGeoPointFieldData<?> rightData = random.nextBoolean() ? right.load(context) : right.loadDirect(context);\n-\n-        assertThat(leftData.getNumDocs(), equalTo(rightData.getNumDocs()));\n-\n-        int numDocs = leftData.getNumDocs();\n-        GeoPointValues leftValues = leftData.getGeoPointValues();\n-        GeoPointValues rightValues = rightData.getGeoPointValues();\n-        for (int i = 0; i < numDocs; ++i) {\n-            final int numValues = leftValues.setDocument(i);\n-            assertEquals(numValues, rightValues.setDocument(i));\n-            List<GeoPoint> leftPoints = Lists.newArrayList();\n-            List<GeoPoint> rightPoints = Lists.newArrayList();\n-            for (int j = 0; j < numValues; ++j) {\n-                GeoPoint l = leftValues.nextValue();\n-                leftPoints.add(new GeoPoint(l.getLat(), l.getLon()));\n-                GeoPoint r = rightValues.nextValue();\n-                rightPoints.add(new GeoPoint(r.getLat(), r.getLon()));\n-            }\n-            for (GeoPoint l : leftPoints) {\n-                assertTrue(\"Couldn't find \" + l + \" among \" + rightPoints, contains(l, rightPoints, precision));\n-            }\n-            for (GeoPoint r : rightPoints) {\n-                assertTrue(\"Couldn't find \" + r + \" among \" + leftPoints, contains(r, leftPoints, precision));\n-            }\n-        }\n-    }\n-\n-    private static boolean contains(GeoPoint point, List<GeoPoint> set, Distance precision) {\n-        for (GeoPoint r : set) {\n-            final double distance = GeoDistance.PLANE.calculate(point.getLat(), point.getLon(), r.getLat(), r.getLon(), DistanceUnit.METERS);\n-            if (new Distance(distance, DistanceUnit.METERS).compareTo(precision) <= 0) {\n-                return true;\n-            }\n-        }\n-        return false;\n-    }\n \n     private static class Preprocessor {\n \n@@ -563,7 +467,7 @@ public class DuelFieldDataTests extends AbstractFieldDataTests {\n \n \n     private static enum Type {\n-        Float(AtomicFieldData.Order.NUMERIC), Double(AtomicFieldData.Order.NUMERIC), Integer(AtomicFieldData.Order.NUMERIC), Long(AtomicFieldData.Order.NUMERIC), Bytes(AtomicFieldData.Order.BYTES), GeoPoint(AtomicFieldData.Order.NONE);\n+        Float(AtomicFieldData.Order.NUMERIC), Double(AtomicFieldData.Order.NUMERIC), Integer(AtomicFieldData.Order.NUMERIC), Long(AtomicFieldData.Order.NUMERIC), Bytes(AtomicFieldData.Order.BYTES);\n \n         private final AtomicFieldData.Order order;\n         Type(AtomicFieldData.Order order) {\ndiff --git a/src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java b/src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java\nindex bdd4347fc06..e94ccd7166d 100644\n--- a/src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java\n+++ b/src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java\n@@ -19,12 +19,6 @@\n \n package org.elasticsearch.index.fielddata;\n \n-import org.apache.lucene.analysis.core.KeywordAnalyzer;\n-import org.apache.lucene.document.Document;\n-import org.apache.lucene.document.Field.Store;\n-import org.apache.lucene.document.StringField;\n-import org.apache.lucene.index.*;\n-import org.apache.lucene.store.RAMDirectory;\n import org.elasticsearch.common.settings.ImmutableSettings;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.index.Index;\n@@ -32,16 +26,10 @@ import org.elasticsearch.index.fielddata.plain.*;\n import org.elasticsearch.index.mapper.ContentPath;\n import org.elasticsearch.index.mapper.FieldMapper;\n import org.elasticsearch.index.mapper.Mapper.BuilderContext;\n-import org.elasticsearch.index.mapper.MapperBuilders;\n import org.elasticsearch.index.mapper.core.*;\n import org.elasticsearch.test.ElasticsearchTestCase;\n \n import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.IdentityHashMap;\n-import java.util.Set;\n-\n-import static org.hamcrest.Matchers.instanceOf;\n \n public class IndexFieldDataServiceTests extends ElasticsearchTestCase {\n \n@@ -100,7 +88,7 @@ public class IndexFieldDataServiceTests extends ElasticsearchTestCase {\n     public void testByPassDocValues() {\n         final IndexFieldDataService ifdService = new IndexFieldDataService(new Index(\"test\"));\n         final BuilderContext ctx = new BuilderContext(null, new ContentPath(1));\n-        final StringFieldMapper stringMapper = MapperBuilders.stringField(\"string\").tokenized(false).fieldDataSettings(DOC_VALUES_SETTINGS).fieldDataSettings(ImmutableSettings.builder().put(\"format\", \"fst\").build()).build(ctx);\n+        final StringFieldMapper stringMapper = new StringFieldMapper.Builder(\"string\").tokenized(false).fieldDataSettings(DOC_VALUES_SETTINGS).fieldDataSettings(ImmutableSettings.builder().put(\"format\", \"fst\").build()).build(ctx);\n         ifdService.clear();\n         IndexFieldData<?> fd = ifdService.getForField(stringMapper);\n         assertTrue(fd instanceof FSTBytesIndexFieldData);\n@@ -117,53 +105,15 @@ public class IndexFieldDataServiceTests extends ElasticsearchTestCase {\n             assertTrue(fd instanceof PackedArrayIndexFieldData);\n         }\n \n-        final FloatFieldMapper floatMapper = MapperBuilders.floatField(\"float\").fieldDataSettings(DOC_VALUES_SETTINGS).fieldDataSettings(fdSettings).build(ctx);\n+        final FloatFieldMapper floatMapper = new FloatFieldMapper.Builder(\"float\").fieldDataSettings(DOC_VALUES_SETTINGS).fieldDataSettings(fdSettings).build(ctx);\n         ifdService.clear();\n         fd = ifdService.getForField(floatMapper);\n         assertTrue(fd instanceof FloatArrayIndexFieldData);\n \n-        final DoubleFieldMapper doubleMapper = MapperBuilders.doubleField(\"double\").fieldDataSettings(DOC_VALUES_SETTINGS).fieldDataSettings(fdSettings).build(ctx);\n+        final DoubleFieldMapper doubleMapper = new DoubleFieldMapper.Builder(\"double\").fieldDataSettings(DOC_VALUES_SETTINGS).fieldDataSettings(fdSettings).build(ctx);\n         ifdService.clear();\n         fd = ifdService.getForField(doubleMapper);\n         assertTrue(fd instanceof DoubleArrayIndexFieldData);\n     }\n \n-    public void testChangeFieldDataFormat() throws Exception {\n-        final IndexFieldDataService ifdService = new IndexFieldDataService(new Index(\"test\"));\n-        final BuilderContext ctx = new BuilderContext(null, new ContentPath(1));\n-        final StringFieldMapper mapper1 = MapperBuilders.stringField(\"s\").tokenized(false).fieldDataSettings(ImmutableSettings.builder().put(FieldDataType.FORMAT_KEY, \"paged_bytes\").build()).build(ctx);\n-        final IndexWriter writer = new IndexWriter(new RAMDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new KeywordAnalyzer()));\n-        Document doc = new Document();\n-        doc.add(new StringField(\"s\", \"thisisastring\", Store.NO));\n-        writer.addDocument(doc);\n-        final IndexReader reader1 = DirectoryReader.open(writer, true);\n-        IndexFieldData<?> ifd = ifdService.getForField(mapper1);\n-        assertThat(ifd, instanceOf(PagedBytesIndexFieldData.class));\n-        Set<AtomicReader> oldSegments = Collections.newSetFromMap(new IdentityHashMap<AtomicReader, Boolean>());\n-        for (AtomicReaderContext arc : reader1.leaves()) {\n-            oldSegments.add(arc.reader());\n-            AtomicFieldData<?> afd = ifd.load(arc);\n-            assertThat(afd, instanceOf(PagedBytesAtomicFieldData.class));\n-        }\n-        // write new segment\n-        writer.addDocument(doc);\n-        final IndexReader reader2 = DirectoryReader.open(writer, true);\n-        final StringFieldMapper mapper2 = MapperBuilders.stringField(\"s\").tokenized(false).fieldDataSettings(ImmutableSettings.builder().put(FieldDataType.FORMAT_KEY, \"fst\").build()).build(ctx);\n-        ifdService.onMappingUpdate();\n-        ifd = ifdService.getForField(mapper2);\n-        assertThat(ifd, instanceOf(FSTBytesIndexFieldData.class));\n-        for (AtomicReaderContext arc : reader2.leaves()) {\n-            AtomicFieldData<?> afd = ifd.load(arc);\n-            if (oldSegments.contains(arc.reader())) {\n-                assertThat(afd, instanceOf(PagedBytesAtomicFieldData.class));\n-            } else {\n-                assertThat(afd, instanceOf(FSTBytesAtomicFieldData.class));\n-            }\n-        }\n-        reader1.close();\n-        reader2.close();\n-        writer.close();\n-        writer.getDirectory().close();\n-    }\n-\n }\ndiff --git a/src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java b/src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java\nindex 786c00f2a28..a385c3607d4 100644\n--- a/src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java\n+++ b/src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java\n@@ -32,7 +32,6 @@ import org.elasticsearch.index.analysis.AnalysisModule;\n import org.elasticsearch.index.analysis.AnalysisService;\n import org.elasticsearch.index.codec.docvaluesformat.DocValuesFormatService;\n import org.elasticsearch.index.codec.postingsformat.PostingsFormatService;\n-import org.elasticsearch.index.fielddata.IndexFieldDataService;\n import org.elasticsearch.index.settings.IndexSettingsModule;\n import org.elasticsearch.index.similarity.SimilarityLookupService;\n import org.elasticsearch.indices.analysis.IndicesAnalysisModule;\n@@ -54,12 +53,8 @@ public class MapperTestUtils {\n     }\n \n     public static MapperService newMapperService() {\n-        return newMapperService(new Index(\"test\"), ImmutableSettings.Builder.EMPTY_SETTINGS);\n-    }\n-\n-    public static MapperService newMapperService(Index index, Settings indexSettings) {\n-        return new MapperService(index, indexSettings, new Environment(), newAnalysisService(), new IndexFieldDataService(index),\n-                new PostingsFormatService(index), new DocValuesFormatService(index), newSimilarityLookupService());\n+        return new MapperService(new Index(\"test\"), ImmutableSettings.Builder.EMPTY_SETTINGS, new Environment(), newAnalysisService(),\n+                new PostingsFormatService(new Index(\"test\")), new DocValuesFormatService(new Index(\"test\")), newSimilarityLookupService());\n     }\n \n     public static AnalysisService newAnalysisService() {\ndiff --git a/src/test/java/org/elasticsearch/index/mapper/geo/GeoEncodingTests.java b/src/test/java/org/elasticsearch/index/mapper/geo/GeoEncodingTests.java\ndeleted file mode 100644\nindex 933f9f67858..00000000000\n--- a/src/test/java/org/elasticsearch/index/mapper/geo/GeoEncodingTests.java\n+++ /dev/null\n@@ -1,48 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.index.mapper.geo;\n-\n-import org.elasticsearch.common.geo.GeoDistance;\n-import org.elasticsearch.common.geo.GeoPoint;\n-import org.elasticsearch.common.unit.DistanceUnit;\n-import org.elasticsearch.common.unit.DistanceUnit.Distance;\n-import org.elasticsearch.test.ElasticsearchTestCase;\n-\n-import java.util.Arrays;\n-\n-import static org.hamcrest.Matchers.lessThanOrEqualTo;\n-\n-\n-public class GeoEncodingTests extends ElasticsearchTestCase {\n-\n-    public void test() {\n-        for (int i = 0; i < 10000; ++i) {\n-            final double lat = randomDouble() * 180 - 90;\n-            final double lon = randomDouble() * 360 - 180;\n-            final Distance precision = new Distance(randomDouble() * 10, randomFrom(Arrays.asList(DistanceUnit.MILLIMETERS, DistanceUnit.METERS, DistanceUnit.KILOMETERS)));\n-            final GeoPointFieldMapper.Encoding encoding = GeoPointFieldMapper.Encoding.of(precision);\n-            assertThat(encoding.precision().convert(DistanceUnit.METERS).value, lessThanOrEqualTo(precision.convert(DistanceUnit.METERS).value));\n-            final GeoPoint geoPoint = encoding.decode(encoding.encodeCoordinate(lat), encoding.encodeCoordinate(lon), new GeoPoint());\n-            final double error = GeoDistance.PLANE.calculate(lat, lon, geoPoint.lat(), geoPoint.lon(), DistanceUnit.METERS);\n-            assertThat(error, lessThanOrEqualTo(precision.convert(DistanceUnit.METERS).value));\n-        }\n-    }\n-\n-}\ndiff --git a/src/test/java/org/elasticsearch/index/mapper/geo/GeoMappingTests.java b/src/test/java/org/elasticsearch/index/mapper/geo/GeoMappingTests.java\ndeleted file mode 100644\nindex 156f7f54e87..00000000000\n--- a/src/test/java/org/elasticsearch/index/mapper/geo/GeoMappingTests.java\n+++ /dev/null\n@@ -1,78 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.index.mapper.geo;\n-\n-import org.elasticsearch.action.admin.indices.mapping.get.GetMappingsRequest;\n-import org.elasticsearch.cluster.metadata.MappingMetaData;\n-import org.elasticsearch.common.collect.ImmutableOpenMap;\n-import org.elasticsearch.common.unit.DistanceUnit;\n-import org.elasticsearch.common.unit.DistanceUnit.Distance;\n-import org.elasticsearch.common.xcontent.XContentFactory;\n-import org.elasticsearch.test.ElasticsearchIntegrationTest;\n-\n-import java.util.Map;\n-\n-public class GeoMappingTests extends ElasticsearchIntegrationTest {\n-\n-    public void testUpdatePrecision() throws Exception {\n-        prepareCreate(\"test\").addMapping(\"type1\", XContentFactory.jsonBuilder().startObject()\n-                .startObject(\"type1\")\n-                    .startObject(\"properties\")\n-                        .startObject(\"pin\")\n-                            .field(\"type\", \"geo_point\")\n-                            .startObject(\"fielddata\")\n-                                .field(\"format\", \"compressed\")\n-                                .field(\"precision\", \"2mm\")\n-                            .endObject()\n-                        .endObject()\n-                    .endObject()\n-                .endObject()\n-                .endObject()).execute().actionGet();\n-\n-        assertPrecision(new Distance(2, DistanceUnit.MILLIMETERS));\n-\n-        client().admin().indices().preparePutMapping(\"test\").setType(\"type1\").setSource(XContentFactory.jsonBuilder().startObject()\n-                .startObject(\"type1\")\n-                .startObject(\"properties\")\n-                    .startObject(\"pin\")\n-                        .field(\"type\", \"geo_point\")\n-                            .startObject(\"fielddata\")\n-                                .field(\"format\", \"compressed\")\n-                                .field(\"precision\", \"11m\")\n-                            .endObject()\n-                    .endObject()\n-                .endObject()\n-            .endObject()\n-            .endObject()).execute().actionGet();\n-\n-        assertPrecision(new Distance(11, DistanceUnit.METERS));\n-    }\n-\n-    private void assertPrecision(Distance expected) throws Exception {\n-        ImmutableOpenMap<String, ImmutableOpenMap<String, MappingMetaData>> mappings = client().admin().indices().getMappings(new GetMappingsRequest().indices(\"test\").types(\"type1\")).actionGet().getMappings();\n-        assertNotNull(mappings);\n-        Map<String, ?> properties = (Map<String, ?>) mappings.get(\"test\").get(\"type1\").getSourceAsMap().get(\"properties\");\n-        Map<String, ?> pinProperties = (Map<String, ?>) properties.get(\"pin\");\n-        Map<String, ?> pinFieldData = (Map<String, ?>) pinProperties.get(\"fielddata\");\n-        Distance precision = Distance.parseDistance(pinFieldData.get(\"precision\").toString(), DistanceUnit.METERS);\n-        assertEquals(expected, precision);\n-    }\n-\n-}\ndiff --git a/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java b/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java\nindex bd9ac9cad2e..680402eac83 100644\n--- a/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java\n+++ b/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java\n@@ -462,6 +462,116 @@ public class SimpleIndexQueryParserTests extends ElasticsearchTestCase {\n         assertThat(fuzzyQuery.getMax().longValue(), equalTo(17l));\n     }\n \n+    @Test\n+    public void testFieldQueryBuilder1() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        Query parsedQuery = queryParser.parse(fieldQuery(\"age\", 34).buildAsBytes()).query();\n+        assertThat(parsedQuery, instanceOf(NumericRangeQuery.class));\n+        NumericRangeQuery fieldQuery = (NumericRangeQuery) parsedQuery;\n+        assertThat(fieldQuery.getMin().intValue(), equalTo(34));\n+        assertThat(fieldQuery.getMax().intValue(), equalTo(34));\n+        assertThat(fieldQuery.includesMax(), equalTo(true));\n+        assertThat(fieldQuery.includesMin(), equalTo(true));\n+    }\n+\n+    @Test\n+    public void testFieldQuery1() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        String query = copyToStringFromClasspath(\"/org/elasticsearch/index/query/field1.json\");\n+        Query parsedQuery = queryParser.parse(query).query();\n+        assertThat(parsedQuery, instanceOf(NumericRangeQuery.class));\n+        NumericRangeQuery fieldQuery = (NumericRangeQuery) parsedQuery;\n+        assertThat(fieldQuery.getMin().intValue(), equalTo(34));\n+        assertThat(fieldQuery.getMax().intValue(), equalTo(34));\n+        assertThat(fieldQuery.includesMax(), equalTo(true));\n+        assertThat(fieldQuery.includesMin(), equalTo(true));\n+    }\n+\n+    @Test\n+    public void testFieldQuery2() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        String query = copyToStringFromClasspath(\"/org/elasticsearch/index/query/field2.json\");\n+        Query parsedQuery = queryParser.parse(query).query();\n+        assertThat(parsedQuery, instanceOf(BooleanQuery.class));\n+        BooleanQuery bQuery = (BooleanQuery) parsedQuery;\n+        assertThat(bQuery.getClauses().length, equalTo(2));\n+        assertThat(((TermQuery) bQuery.getClauses()[0].getQuery()).getTerm().field(), equalTo(\"name.first\"));\n+        assertThat(((TermQuery) bQuery.getClauses()[0].getQuery()).getTerm().text(), equalTo(\"something\"));\n+        assertThat(((TermQuery) bQuery.getClauses()[1].getQuery()).getTerm().field(), equalTo(\"name.first\"));\n+        assertThat(((TermQuery) bQuery.getClauses()[1].getQuery()).getTerm().text(), equalTo(\"else\"));\n+    }\n+\n+    @Test\n+    public void testFieldQuery3() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        String query = copyToStringFromClasspath(\"/org/elasticsearch/index/query/field3.json\");\n+        Query parsedQuery = queryParser.parse(query).query();\n+        assertThat((double) parsedQuery.getBoost(), closeTo(2.0, 0.01));\n+        assertThat(parsedQuery, instanceOf(NumericRangeQuery.class));\n+        NumericRangeQuery fieldQuery = (NumericRangeQuery) parsedQuery;\n+        assertThat(fieldQuery.getMin().intValue(), equalTo(34));\n+        assertThat(fieldQuery.getMax().intValue(), equalTo(34));\n+        assertThat(fieldQuery.includesMax(), equalTo(true));\n+        assertThat(fieldQuery.includesMin(), equalTo(true));\n+    }\n+\n+    @Test\n+    public void testTextQuery1() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        String query = copyToStringFromClasspath(\"/org/elasticsearch/index/query/text1.json\");\n+        Query parsedQuery = queryParser.parse(query).query();\n+        assertThat(parsedQuery, instanceOf(BooleanQuery.class));\n+        BooleanQuery booleanQuery = (BooleanQuery) parsedQuery;\n+        assertThat((double) booleanQuery.getBoost(), closeTo(1.0d, 0.00001d));\n+        assertThat(((TermQuery) booleanQuery.getClauses()[0].getQuery()).getTerm(), equalTo(new Term(\"name.first\", \"aaa\")));\n+        assertThat(((TermQuery) booleanQuery.getClauses()[1].getQuery()).getTerm(), equalTo(new Term(\"name.first\", \"bbb\")));\n+    }\n+\n+    @Test\n+    public void testTextQuery2() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        String query = copyToStringFromClasspath(\"/org/elasticsearch/index/query/text2.json\");\n+        Query parsedQuery = queryParser.parse(query).query();\n+        assertThat(parsedQuery, instanceOf(BooleanQuery.class));\n+        BooleanQuery booleanQuery = (BooleanQuery) parsedQuery;\n+        assertThat((double) booleanQuery.getBoost(), closeTo(1.5d, 0.00001d));\n+        assertThat(((TermQuery) booleanQuery.getClauses()[0].getQuery()).getTerm(), equalTo(new Term(\"name.first\", \"aaa\")));\n+        assertThat(((TermQuery) booleanQuery.getClauses()[1].getQuery()).getTerm(), equalTo(new Term(\"name.first\", \"bbb\")));\n+    }\n+\n+    @Test\n+    public void testTextQuery3() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        String query = copyToStringFromClasspath(\"/org/elasticsearch/index/query/text3.json\");\n+        Query parsedQuery = queryParser.parse(query).query();\n+        assertThat(parsedQuery, instanceOf(PhraseQuery.class));\n+        PhraseQuery phraseQuery = (PhraseQuery) parsedQuery;\n+        assertThat(phraseQuery.getTerms()[0], equalTo(new Term(\"name.first\", \"aaa\")));\n+        assertThat(phraseQuery.getTerms()[1], equalTo(new Term(\"name.first\", \"bbb\")));\n+    }\n+\n+    @Test\n+    public void testTextQuery4() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        String query = copyToStringFromClasspath(\"/org/elasticsearch/index/query/text4.json\");\n+        Query parsedQuery = queryParser.parse(query).query();\n+        assertThat(parsedQuery, instanceOf(MultiPhrasePrefixQuery.class));\n+        MultiPhrasePrefixQuery phraseQuery = (MultiPhrasePrefixQuery) parsedQuery;\n+        assertThat(phraseQuery.getTermArrays().get(0)[0], equalTo(new Term(\"name.first\", \"aaa\")));\n+        assertThat(phraseQuery.getTermArrays().get(1)[0], equalTo(new Term(\"name.first\", \"bbb\")));\n+    }\n+\n+    @Test\n+    public void testTextQuery4_2() throws IOException {\n+        IndexQueryParserService queryParser = queryParser();\n+        String query = copyToStringFromClasspath(\"/org/elasticsearch/index/query/text4_2.json\");\n+        Query parsedQuery = queryParser.parse(query).query();\n+        assertThat(parsedQuery, instanceOf(MultiPhrasePrefixQuery.class));\n+        MultiPhrasePrefixQuery phraseQuery = (MultiPhrasePrefixQuery) parsedQuery;\n+        assertThat(phraseQuery.getTermArrays().get(0)[0], equalTo(new Term(\"name.first\", \"aaa\")));\n+        assertThat(phraseQuery.getTermArrays().get(1)[0], equalTo(new Term(\"name.first\", \"bbb\")));\n+    }\n+\n     @Test\n     public void testTermWithBoostQueryBuilder() throws IOException {\n         IndexQueryParserService queryParser = queryParser();\ndiff --git a/src/test/java/org/elasticsearch/index/query/field1.json b/src/test/java/org/elasticsearch/index/query/field1.json\nnew file mode 100644\nindex 00000000000..4fab5eeab44\n--- /dev/null\n+++ b/src/test/java/org/elasticsearch/index/query/field1.json\n@@ -0,0 +1,5 @@\n+{\n+    \"field\":{\n+        \"age\":34\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/index/query/field2.json b/src/test/java/org/elasticsearch/index/query/field2.json\nnew file mode 100644\nindex 00000000000..de78420b9e1\n--- /dev/null\n+++ b/src/test/java/org/elasticsearch/index/query/field2.json\n@@ -0,0 +1,5 @@\n+{\n+    field:{\n+        \"name.first\":\"something else\"\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/index/query/text1.json b/src/test/java/org/elasticsearch/index/query/text1.json\nnew file mode 100644\nindex 00000000000..454cd4d3a60\n--- /dev/null\n+++ b/src/test/java/org/elasticsearch/index/query/text1.json\n@@ -0,0 +1,5 @@\n+{\n+    \"text\":{\n+        \"name.first\":\"aaa bbb\"\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/index/query/text2.json b/src/test/java/org/elasticsearch/index/query/text2.json\nnew file mode 100644\nindex 00000000000..36517c4ce19\n--- /dev/null\n+++ b/src/test/java/org/elasticsearch/index/query/text2.json\n@@ -0,0 +1,8 @@\n+{\n+    \"text\":{\n+        \"name.first\":{\n+            \"query\":\"aaa bbb\",\n+            \"boost\":1.5\n+        }\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/index/query/text3.json b/src/test/java/org/elasticsearch/index/query/text3.json\nnew file mode 100644\nindex 00000000000..ebead81547e\n--- /dev/null\n+++ b/src/test/java/org/elasticsearch/index/query/text3.json\n@@ -0,0 +1,8 @@\n+{\n+    \"text\":{\n+        \"name.first\":{\n+            \"query\":\"aaa bbb\",\n+            \"type\":\"phrase\"\n+        }\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/index/query/text4.json b/src/test/java/org/elasticsearch/index/query/text4.json\nnew file mode 100644\nindex 00000000000..f95fc9caeaf\n--- /dev/null\n+++ b/src/test/java/org/elasticsearch/index/query/text4.json\n@@ -0,0 +1,8 @@\n+{\n+    \"text\":{\n+        \"name.first\":{\n+            \"query\":\"aaa bbb\",\n+            \"type\":\"phrase_prefix\"\n+        }\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/index/query/text4_2.json b/src/test/java/org/elasticsearch/index/query/text4_2.json\nnew file mode 100644\nindex 00000000000..ae947e400f0\n--- /dev/null\n+++ b/src/test/java/org/elasticsearch/index/query/text4_2.json\n@@ -0,0 +1,7 @@\n+{\n+    \"text_phrase_prefix\":{\n+        \"name.first\":{\n+            \"query\":\"aaa bbb\"\n+        }\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java b/src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java\nindex 5639045a949..d3101992411 100644\n--- a/src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java\n+++ b/src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java\n@@ -36,14 +36,15 @@ import org.elasticsearch.common.lucene.search.XConstantScoreQuery;\n import org.elasticsearch.common.lucene.search.XFilteredQuery;\n import org.elasticsearch.common.settings.ImmutableSettings;\n import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.env.Environment;\n import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.analysis.AnalysisService;\n import org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache;\n import org.elasticsearch.index.cache.id.IdCache;\n import org.elasticsearch.index.cache.id.SimpleIdCacheTests;\n import org.elasticsearch.index.cache.id.simple.SimpleIdCache;\n import org.elasticsearch.index.engine.Engine;\n import org.elasticsearch.index.mapper.MapperService;\n-import org.elasticsearch.index.mapper.MapperTestUtils;\n import org.elasticsearch.index.mapper.Uid;\n import org.elasticsearch.index.mapper.internal.ParentFieldMapper;\n import org.elasticsearch.index.mapper.internal.TypeFieldMapper;\n@@ -327,7 +328,9 @@ public class ChildrenConstantScoreQueryTests extends ElasticsearchLuceneTestCase\n         final IdCache idCache = new SimpleIdCache(index, ImmutableSettings.EMPTY);\n         final CacheRecycler cacheRecycler = new CacheRecycler(ImmutableSettings.EMPTY);\n         Settings settings = ImmutableSettings.EMPTY;\n-        MapperService mapperService = MapperTestUtils.newMapperService(index, settings);\n+        MapperService mapperService = new MapperService(\n+                index, settings, new Environment(), new AnalysisService(index), null, null, null\n+        );\n         mapperService.merge(\n                 childType, new CompressedString(PutMappingRequest.buildFromSimplifiedDef(childType, \"_parent\", \"type=\" + parentType).string()), true\n         );\ndiff --git a/src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java b/src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java\nindex d396f4d1919..290ba8edac8 100644\n--- a/src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java\n+++ b/src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java\n@@ -353,12 +353,12 @@ class TestSearchContext extends SearchContext {\n     }\n \n     @Override\n-    public SearchContext parsedPostFilter(ParsedFilter postFilter) {\n+    public SearchContext parsedFilter(ParsedFilter filter) {\n         return null;\n     }\n \n     @Override\n-    public ParsedFilter parsedPostFilter() {\n+    public ParsedFilter parsedFilter() {\n         return null;\n     }\n \ndiff --git a/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionTests.java b/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionTests.java\nindex 654835da883..095692f7208 100644\n--- a/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionTests.java\n+++ b/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionTests.java\n@@ -20,9 +20,6 @@\n package org.elasticsearch.indexlifecycle;\n \n import com.carrotsearch.randomizedtesting.annotations.Nightly;\n-import com.google.common.base.Function;\n-import com.google.common.collect.Iterables;\n-import com.google.common.collect.Sets;\n import org.apache.lucene.util.LuceneTestCase.Slow;\n import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;\n import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;\n@@ -30,7 +27,6 @@ import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n import org.elasticsearch.action.admin.indices.delete.DeleteIndexResponse;\n import org.elasticsearch.cluster.ClusterState;\n import org.elasticsearch.cluster.routing.RoutingNode;\n-import org.elasticsearch.cluster.routing.RoutingNodes;\n import org.elasticsearch.common.Priority;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.discovery.Discovery;\n@@ -40,6 +36,7 @@ import org.elasticsearch.test.ElasticsearchIntegrationTest.Scope;\n import org.elasticsearch.test.TestCluster;\n import org.junit.Test;\n \n+import java.util.Map;\n import java.util.Set;\n \n import static org.elasticsearch.client.Requests.*;\n@@ -92,7 +89,7 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n         assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.YELLOW));\n \n         ClusterState clusterState = client().admin().cluster().prepareState().get().getState();\n-        RoutingNode routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);\n+        RoutingNode routingNodeEntry1 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node1);\n         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED), equalTo(11));\n \n         logger.info(\"Starting server2\");\n@@ -124,11 +121,11 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n \n \n         clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node1, node2);\n-        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node1, node2);\n+        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node1);\n         assertThat(routingNodeEntry1.numberOfShardsWithState(RELOCATING), equalTo(0));\n         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED), equalTo(11));\n-        RoutingNode routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);\n+        RoutingNode routingNodeEntry2 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node2);\n         assertThat(routingNodeEntry2.numberOfShardsWithState(INITIALIZING), equalTo(0));\n         assertThat(routingNodeEntry2.numberOfShardsWithState(STARTED), equalTo(11));\n \n@@ -160,11 +157,11 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n \n \n         clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node1, node2, node3);\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node1, node2, node3);\n \n-        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);\n-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);\n-        RoutingNode routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);\n+        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node1);\n+        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node2);\n+        RoutingNode routingNodeEntry3 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node3);\n \n         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED) + routingNodeEntry2.numberOfShardsWithState(STARTED) + routingNodeEntry3.numberOfShardsWithState(STARTED), equalTo(22));\n \n@@ -200,9 +197,9 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n         assertThat(clusterHealth.getActivePrimaryShards(), equalTo(11));\n \n         clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node3, node2);\n-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);\n-        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node3, node2);\n+        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node2);\n+        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node3);\n \n         assertThat(routingNodeEntry2.numberOfShardsWithState(STARTED) + routingNodeEntry3.numberOfShardsWithState(STARTED), equalTo(22));\n \n@@ -219,12 +216,12 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n         assertThat(deleteIndexResponse.isAcknowledged(), equalTo(true));\n \n         clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node3, node2);\n-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);\n-        assertThat(routingNodeEntry2.isEmpty(), equalTo(true));\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node3, node2);\n+        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node2);\n+        assertThat(routingNodeEntry2.shards().isEmpty(), equalTo(true));\n \n-        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);\n-        assertThat(routingNodeEntry3.isEmpty(), equalTo(true));\n+        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node3);\n+        assertThat(routingNodeEntry3.shards().isEmpty(), equalTo(true));\n     }\n \n     private String getLocalNodeId(String name) {\n@@ -265,8 +262,8 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n         assertThat(clusterHealth.getActivePrimaryShards(), equalTo(11));\n \n         ClusterState clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node1);\n-        RoutingNode routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node1);\n+        RoutingNode routingNodeEntry1 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node1);\n         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED), equalTo(11));\n \n         // start another server\n@@ -295,11 +292,11 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n \n \n         clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node1, node2);\n-        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node1, node2);\n+        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node1);\n         assertThat(routingNodeEntry1.numberOfShardsWithState(RELOCATING), equalTo(0));\n         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED), anyOf(equalTo(6), equalTo(5)));\n-        RoutingNode routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);\n+        RoutingNode routingNodeEntry2 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node2);\n         assertThat(routingNodeEntry2.numberOfShardsWithState(INITIALIZING), equalTo(0));\n         assertThat(routingNodeEntry2.numberOfShardsWithState(STARTED), anyOf(equalTo(5), equalTo(6)));\n \n@@ -328,10 +325,10 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n \n \n         clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node1, node2, node3);\n-        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);\n-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);\n-        RoutingNode routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node1, node2, node3);\n+        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node1);\n+        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node2);\n+        RoutingNode routingNodeEntry3 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node3);\n \n         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED) + routingNodeEntry2.numberOfShardsWithState(STARTED) + routingNodeEntry3.numberOfShardsWithState(STARTED), equalTo(11));\n \n@@ -369,10 +366,10 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n         assertThat(clusterHealth.getActivePrimaryShards(), equalTo(11));\n \n         clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node3, node2);\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node3, node2);\n \n-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);\n-        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);\n+        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node2);\n+        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node3);\n \n         assertThat(routingNodeEntry2.numberOfShardsWithState(STARTED) + routingNodeEntry3.numberOfShardsWithState(STARTED), equalTo(11));\n \n@@ -390,22 +387,17 @@ public class IndexLifecycleActionTests extends ElasticsearchIntegrationTest {\n         assertThat(deleteIndexResponse.isAcknowledged(), equalTo(true));\n \n         clusterState = client().admin().cluster().prepareState().get().getState();\n-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node3, node2);\n+        assertNodesPresent(clusterState.readOnlyRoutingNodes().nodesToShards(), node3, node2);\n \n-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);\n-        assertThat(routingNodeEntry2.isEmpty(), equalTo(true));\n+        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node2);\n+        assertThat(routingNodeEntry2.shards().isEmpty(), equalTo(true));\n \n-        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);\n-        assertThat(routingNodeEntry3.isEmpty(), equalTo(true));\n+        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().nodesToShards().get(node3);\n+        assertThat(routingNodeEntry3.shards().isEmpty(), equalTo(true));\n     }\n \n-    private void assertNodesPresent(RoutingNodes routingNodes, String...nodes) {\n-        final Set<String> keySet = Sets.newHashSet(Iterables.transform(routingNodes, new Function<RoutingNode, String>() {\n-            @Override\n-            public String apply(RoutingNode input) {\n-                return input.nodeId();\n-            }\n-        }));\n+    private void assertNodesPresent(Map<String, ?> nodesToShards, String...nodes) {\n+        Set<String> keySet = nodesToShards.keySet();\n         assertThat(keySet, containsInAnyOrder(nodes));\n     }\n }\ndiff --git a/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerTests.java b/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerTests.java\ndeleted file mode 100644\nindex 9db1d0b367b..00000000000\n--- a/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerTests.java\n+++ /dev/null\n@@ -1,127 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.indices;\n-\n-import com.google.common.collect.Maps;\n-import org.elasticsearch.common.Nullable;\n-import org.elasticsearch.index.shard.IndexShardState;\n-import org.elasticsearch.index.shard.ShardId;\n-import org.elasticsearch.index.shard.service.IndexShard;\n-import org.elasticsearch.test.ElasticsearchIntegrationTest;\n-import org.junit.Test;\n-\n-import java.util.List;\n-import java.util.concurrent.ConcurrentMap;\n-import java.util.concurrent.CopyOnWriteArrayList;\n-\n-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;\n-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;\n-import static org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION;\n-import static org.elasticsearch.common.settings.ImmutableSettings.builder;\n-import static org.elasticsearch.index.shard.IndexShardState.*;\n-import static org.elasticsearch.test.ElasticsearchIntegrationTest.ClusterScope;\n-import static org.elasticsearch.test.ElasticsearchIntegrationTest.Scope;\n-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n-import static org.hamcrest.CoreMatchers.equalTo;\n-import static org.hamcrest.CoreMatchers.notNullValue;\n-\n-@ClusterScope(scope = Scope.TEST, numNodes = 0)\n-public class IndicesLifecycleListenerTests extends ElasticsearchIntegrationTest {\n-\n-    @Test\n-    public void testIndexStateShardChanged() throws Throwable {\n-\n-        //start with a single node\n-        String node1 = cluster().startNode();\n-        IndexShardStateChangeListener stateChangeListenerNode1 = new IndexShardStateChangeListener();\n-        //add a listener that keeps track of the shard state changes\n-        cluster().getInstance(IndicesLifecycle.class, node1).addListener(stateChangeListenerNode1);\n-\n-        //create an index\n-        assertAcked(client().admin().indices().prepareCreate(\"test\")\n-                .setSettings(SETTING_NUMBER_OF_SHARDS, 6, SETTING_NUMBER_OF_REPLICAS, 0));\n-        ensureGreen();\n-\n-        //new shards got started\n-        assertShardStatesMatch(stateChangeListenerNode1, 6, CREATED, RECOVERING, POST_RECOVERY, STARTED);\n-\n-\n-        //add a node: 3 out of the 6 shards will be relocated to it\n-        //disable allocation before starting a new node, as we need to register the listener first\n-        assertAcked(client().admin().cluster().prepareUpdateSettings()\n-                .setPersistentSettings(builder().put(CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)));\n-        String node2 = cluster().startNode();\n-        IndexShardStateChangeListener stateChangeListenerNode2 = new IndexShardStateChangeListener();\n-        //add a listener that keeps track of the shard state changes\n-        cluster().getInstance(IndicesLifecycle.class, node2).addListener(stateChangeListenerNode2);\n-        //re-enable allocation\n-        assertAcked(client().admin().cluster().prepareUpdateSettings()\n-                .setPersistentSettings(builder().put(CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, false)));\n-        ensureGreen();\n-\n-        //the 3 relocated shards get closed on the first node\n-        assertShardStatesMatch(stateChangeListenerNode1, 3, CLOSED);\n-        //the 3 relocated shards get created on the second node\n-        assertShardStatesMatch(stateChangeListenerNode2, 3, CREATED, RECOVERING, POST_RECOVERY, STARTED);\n-\n-\n-        //increase replicas from 0 to 1\n-        assertAcked(client().admin().indices().prepareUpdateSettings(\"test\").setSettings(builder().put(SETTING_NUMBER_OF_REPLICAS, 1)));\n-        ensureGreen();\n-\n-        //3 replicas are allocated to the first node\n-        assertShardStatesMatch(stateChangeListenerNode1, 3, CREATED, RECOVERING, POST_RECOVERY, STARTED);\n-\n-        //3 replicas are allocated to the second node\n-        assertShardStatesMatch(stateChangeListenerNode2, 3, CREATED, RECOVERING, POST_RECOVERY, STARTED);\n-\n-\n-        //close the index\n-        assertAcked(client().admin().indices().prepareClose(\"test\"));\n-\n-        assertShardStatesMatch(stateChangeListenerNode1, 6, CLOSED);\n-        assertShardStatesMatch(stateChangeListenerNode2, 6, CLOSED);\n-    }\n-\n-    private static void assertShardStatesMatch(IndexShardStateChangeListener stateChangeListener, int numShards, IndexShardState... shardStates) {\n-        assertThat(stateChangeListener.shardStates.size(), equalTo(numShards));\n-        for (List<IndexShardState> indexShardStates : stateChangeListener.shardStates.values()) {\n-            assertThat(indexShardStates, notNullValue());\n-            assertThat(indexShardStates.size(), equalTo(shardStates.length));\n-            for (int i = 0; i < shardStates.length; i++) {\n-                assertThat(indexShardStates.get(i), equalTo(shardStates[i]));\n-            }\n-        }\n-        stateChangeListener.shardStates.clear();\n-    }\n-\n-    private static class IndexShardStateChangeListener extends IndicesLifecycle.Listener {\n-        //we keep track of all the states (ordered) a shard goes through\n-        final ConcurrentMap<ShardId, List<IndexShardState>> shardStates = Maps.newConcurrentMap();\n-\n-        @Override\n-        public void indexShardStateChanged(IndexShard indexShard, @Nullable IndexShardState previousState, IndexShardState newState, @Nullable String reason) {\n-            List<IndexShardState> shardStates = this.shardStates.putIfAbsent(indexShard.shardId(),\n-                    new CopyOnWriteArrayList<IndexShardState>(new IndexShardState[]{newState}));\n-            if (shardStates != null) {\n-                shardStates.add(newState);\n-            }\n-        }\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/indices/cache/CacheTests.java b/src/test/java/org/elasticsearch/indices/cache/CacheTests.java\nindex 47b81364968..40f909a8094 100644\n--- a/src/test/java/org/elasticsearch/indices/cache/CacheTests.java\n+++ b/src/test/java/org/elasticsearch/indices/cache/CacheTests.java\n@@ -141,11 +141,11 @@ public class CacheTests extends ElasticsearchIntegrationTest {\n \n         // sort to load it to field data and filter to load filter cache\n         client().prepareSearch()\n-                .setPostFilter(FilterBuilders.termFilter(\"field\", \"value1\"))\n+                .setFilter(FilterBuilders.termFilter(\"field\", \"value1\"))\n                 .addSort(\"field\", SortOrder.ASC)\n                 .execute().actionGet();\n         client().prepareSearch()\n-                .setPostFilter(FilterBuilders.termFilter(\"field\", \"value2\"))\n+                .setFilter(FilterBuilders.termFilter(\"field\", \"value2\"))\n                 .addSort(\"field\", SortOrder.ASC)\n                 .execute().actionGet();\n \ndiff --git a/src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java b/src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java\nindex de451120370..606d78d1666 100644\n--- a/src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java\n+++ b/src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java\n@@ -38,7 +38,7 @@ import static org.hamcrest.Matchers.is;\n /**\n  *\n  */\n-@ClusterScope(scope=Scope.SUITE, numNodes=1)\n+@ClusterScope(scope= Scope.SUITE, numNodes=1)\n public class IndexTemplateFileLoadingTests extends ElasticsearchIntegrationTest {\n \n     @Rule\n@@ -57,8 +57,7 @@ public class IndexTemplateFileLoadingTests extends ElasticsearchIntegrationTest\n             templatesDir.mkdir();\n \n             File dst = new File(templatesDir, \"template.json\");\n-            // random template, one uses the 'setting.index.number_of_shards', the other 'settings.number_of_shards'\n-            String template = Streams.copyToStringFromClasspath(\"/org/elasticsearch/indices/template/template\" + randomInt(1) + \".json\");\n+            String template = Streams.copyToStringFromClasspath(\"/org/elasticsearch/indices/template/template.json\");\n             Files.write(template, dst, Charsets.UTF_8);\n         } catch (Exception e) {\n             throw new RuntimeException(e);\ndiff --git a/src/test/java/org/elasticsearch/indices/template/template0.json b/src/test/java/org/elasticsearch/indices/template/template.json\nsimilarity index 100%\nrename from src/test/java/org/elasticsearch/indices/template/template0.json\nrename to src/test/java/org/elasticsearch/indices/template/template.json\ndiff --git a/src/test/java/org/elasticsearch/indices/template/template1.json b/src/test/java/org/elasticsearch/indices/template/template1.json\ndeleted file mode 100644\nindex f91866865e7..00000000000\n--- a/src/test/java/org/elasticsearch/indices/template/template1.json\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-{\n-    \"template\" : \"foo*\",\n-    \"settings\" : {\n-        \"number_of_shards\": 10,\n-        \"number_of_replicas\": 0\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/indices/template/template2.json b/src/test/java/org/elasticsearch/indices/template/template2.json\ndeleted file mode 100644\nindex c48169f15a5..00000000000\n--- a/src/test/java/org/elasticsearch/indices/template/template2.json\n+++ /dev/null\n@@ -1,9 +0,0 @@\n-{\n-    \"template\" : \"foo*\",\n-    \"settings\" : {\n-        \"index\" : {\n-            \"number_of_shards\": 10,\n-            \"number_of_replicas\": 0\n-        }\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/percolator/PercolatorTests.java b/src/test/java/org/elasticsearch/percolator/PercolatorTests.java\nindex 584fc0fa58e..dfbc66d1cb7 100644\n--- a/src/test/java/org/elasticsearch/percolator/PercolatorTests.java\n+++ b/src/test/java/org/elasticsearch/percolator/PercolatorTests.java\n@@ -1305,26 +1305,15 @@ public class PercolatorTests extends ElasticsearchIntegrationTest {\n         ensureGreen();\n \n         if (randomBoolean()) {\n-            // FVH HL\n             client.admin().indices().preparePutMapping(\"test\").setType(\"type\")\n                     .setSource(\n                             jsonBuilder().startObject().startObject(\"type\")\n                                     .startObject(\"properties\")\n-                                    .startObject(\"field1\").field(\"type\", \"string\").field(\"store\", randomBoolean())\n-                                        .field(\"term_vector\", \"with_positions_offsets\").endObject()\n+                                    .startObject(\"field1\").field(\"type\", \"string\").field(\"term_vector\", \"with_positions_offsets\").endObject()\n                                     .endObject()\n                                     .endObject().endObject()\n-                    ).get();\n-        } else if (randomBoolean()) {\n-            // plain hl with stored fields\n-            client.admin().indices().preparePutMapping(\"test\").setType(\"type\")\n-                    .setSource(\n-                            jsonBuilder().startObject().startObject(\"type\")\n-                                    .startObject(\"properties\")\n-                                    .startObject(\"field1\").field(\"type\", \"string\").field(\"store\", true).endObject()\n-                                    .endObject()\n-                                    .endObject().endObject()\n-                    ).get();\n+                    )\n+                    .execute().actionGet();\n         } else if (randomBoolean()) {\n             // positions hl\n             client.admin().indices().preparePutMapping(\"test\").setType(\"type\")\n@@ -1336,7 +1325,8 @@ public class PercolatorTests extends ElasticsearchIntegrationTest {\n                                     .endObject()\n                                     .endObject()\n                                     .endObject().endObject()\n-                    ).get();\n+                    )\n+                    .execute().actionGet();\n         }\n \n         logger.info(\"--> register a queries\");\ndiff --git a/src/test/java/org/elasticsearch/search/aggregations/RandomTests.java b/src/test/java/org/elasticsearch/search/aggregations/RandomTests.java\nindex 606c365f26b..8381f670b34 100644\n--- a/src/test/java/org/elasticsearch/search/aggregations/RandomTests.java\n+++ b/src/test/java/org/elasticsearch/search/aggregations/RandomTests.java\n@@ -28,12 +28,11 @@ import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.xcontent.XContentBuilder;\n import org.elasticsearch.index.query.FilterBuilders;\n import org.elasticsearch.index.query.RangeFilterBuilder;\n-import org.elasticsearch.search.aggregations.bucket.filter.Filter;\n import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;\n import org.elasticsearch.search.aggregations.bucket.range.Range;\n import org.elasticsearch.search.aggregations.bucket.range.RangeBuilder;\n import org.elasticsearch.search.aggregations.bucket.terms.Terms;\n-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;\n+import org.elasticsearch.search.aggregations.bucket.filter.Filter;\n import org.elasticsearch.test.ElasticsearchIntegrationTest;\n \n import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n@@ -195,29 +194,23 @@ public class RandomTests extends ElasticsearchIntegrationTest {\n         SearchResponse resp = client().prepareSearch(\"idx\")\n                 .addAggregation(terms(\"long\").field(\"long_values\").size(maxNumTerms).subAggregation(min(\"min\").field(\"num\")))\n                 .addAggregation(terms(\"double\").field(\"double_values\").size(maxNumTerms).subAggregation(max(\"max\").field(\"num\")))\n-                .addAggregation(terms(\"string_map\").field(\"string_values\").executionHint(TermsAggregatorFactory.EXECUTION_HINT_VALUE_MAP).size(maxNumTerms).subAggregation(stats(\"stats\").field(\"num\")))\n-                .addAggregation(terms(\"string_ordinals\").field(\"string_values\").executionHint(TermsAggregatorFactory.EXECUTION_HINT_VALUE_ORDINALS).size(maxNumTerms).subAggregation(extendedStats(\"stats\").field(\"num\"))).execute().actionGet();\n+                .addAggregation(terms(\"string\").field(\"string_values\").size(maxNumTerms).subAggregation(stats(\"stats\").field(\"num\"))).execute().actionGet();\n         assertEquals(0, resp.getFailedShards());\n \n         final Terms longTerms = resp.getAggregations().get(\"long\");\n         final Terms doubleTerms = resp.getAggregations().get(\"double\");\n-        final Terms stringMapTerms = resp.getAggregations().get(\"string_map\");\n-        final Terms stringOrdinalsTerms = resp.getAggregations().get(\"string_ordinals\");\n+        final Terms stringTerms = resp.getAggregations().get(\"string\");\n \n         assertEquals(valuesSet.size(), longTerms.buckets().size());\n         assertEquals(valuesSet.size(), doubleTerms.buckets().size());\n-        assertEquals(valuesSet.size(), stringMapTerms.buckets().size());\n-        assertEquals(valuesSet.size(), stringOrdinalsTerms.buckets().size());\n+        assertEquals(valuesSet.size(), stringTerms.buckets().size());\n         for (Terms.Bucket bucket : longTerms.buckets()) {\n             final Terms.Bucket doubleBucket = doubleTerms.getByTerm(Double.toString(Long.parseLong(bucket.getKey().string())));\n-            final Terms.Bucket stringMapBucket = stringMapTerms.getByTerm(bucket.getKey().string());\n-            final Terms.Bucket stringOrdinalsBucket = stringOrdinalsTerms.getByTerm(bucket.getKey().string());\n+            final Terms.Bucket stringBucket = stringTerms.getByTerm(bucket.getKey().string());\n             assertNotNull(doubleBucket);\n-            assertNotNull(stringMapBucket);\n-            assertNotNull(stringOrdinalsBucket);\n+            assertNotNull(stringBucket);\n             assertEquals(bucket.getDocCount(), doubleBucket.getDocCount());\n-            assertEquals(bucket.getDocCount(), stringMapBucket.getDocCount());\n-            assertEquals(bucket.getDocCount(), stringOrdinalsBucket.getDocCount());\n+            assertEquals(bucket.getDocCount(), stringBucket.getDocCount());\n         }\n     }\n \ndiff --git a/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java b/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java\nindex 1b86b1a052a..bb5121a0754 100644\n--- a/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java\n+++ b/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java\n@@ -24,10 +24,8 @@ import org.elasticsearch.action.index.IndexRequestBuilder;\n import org.elasticsearch.action.search.SearchResponse;\n import org.elasticsearch.common.settings.ImmutableSettings;\n import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.search.aggregations.bucket.filter.Filter;\n import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;\n import org.elasticsearch.search.aggregations.bucket.terms.Terms;\n-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;\n import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCount;\n import org.elasticsearch.test.ElasticsearchIntegrationTest;\n import org.hamcrest.Matchers;\n@@ -35,12 +33,10 @@ import org.junit.Before;\n import org.junit.Test;\n \n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.List;\n import java.util.regex.Pattern;\n \n import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n-import static org.elasticsearch.index.query.FilterBuilders.termFilter;\n import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;\n import static org.elasticsearch.search.aggregations.AggregationBuilders.*;\n import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;\n@@ -62,10 +58,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n                 .build();\n     }\n \n-    private String randomExecutionHint() {\n-        return randomFrom(Arrays.asList(null, TermsAggregatorFactory.EXECUTION_HINT_VALUE_MAP, TermsAggregatorFactory.EXECUTION_HINT_VALUE_ORDINALS));\n-    }\n-\n     @Before\n     public void init() throws Exception {\n         createIndex(\"idx\");\n@@ -96,7 +88,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void singleValueField() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\"))\n                 .execute().actionGet();\n \n@@ -263,7 +254,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void singleValueField_WithMaxSize() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"high_card_type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\")\n                         .size(20)\n                         .order(Terms.Order.TERM_ASC)) // we need to sort by terms cause we're checking the first 20 values\n@@ -288,7 +278,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void singleValueField_OrderedByTermAsc() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\")\n                         .order(Terms.Order.TERM_ASC))\n                 .execute().actionGet();\n@@ -313,7 +302,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void singleValueField_OrderedByTermDesc() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\")\n                         .order(Terms.Order.TERM_DESC))\n                 .execute().actionGet();\n@@ -338,7 +326,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void singleValuedField_WithSubAggregation() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\")\n                         .subAggregation(count(\"count\").field(\"values\")))\n                 .execute().actionGet();\n@@ -365,7 +352,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void singleValuedField_WithSubAggregation_Inherited() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\")\n                         .subAggregation(count(\"count\")))\n                 .execute().actionGet();\n@@ -392,7 +378,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void singleValuedField_WithValueScript() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\")\n                         .script(\"'foo_' + _value\"))\n                 .execute().actionGet();\n@@ -416,7 +401,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void multiValuedField_WithValueScript_NotUnique() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"values\")\n                         .script(\"_value.substring(0,3)\"))\n                 .execute().actionGet();\n@@ -438,7 +422,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void multiValuedField() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"values\"))\n                 .execute().actionGet();\n \n@@ -465,7 +448,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void multiValuedField_WithValueScript() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"values\")\n                         .script(\"'foo_' + _value\"))\n                 .execute().actionGet();\n@@ -511,7 +493,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void multiValuedField_WithValueScript_WithInheritedSubAggregator() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"values\")\n                         .script(\"'foo_' + _value\")\n                         .subAggregation(count(\"count\")))\n@@ -546,7 +527,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void script_SingleValue() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .script(\"doc['value'].value\"))\n                 .execute().actionGet();\n \n@@ -569,7 +549,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void script_SingleValue_ExplicitSingleValue() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .script(\"doc['value'].value\"))\n                 .execute().actionGet();\n \n@@ -592,7 +571,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void script_SingleValue_WithSubAggregator_Inherited() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .script(\"doc['value'].value\")\n                         .subAggregation(count(\"count\")))\n                 .execute().actionGet();\n@@ -619,7 +597,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void script_MultiValued() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .script(\"doc['values'].values\"))\n                 .execute().actionGet();\n \n@@ -646,7 +623,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void script_MultiValued_WithAggregatorInherited() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .script(\"doc['values'].values\")\n                         .subAggregation(count(\"count\")))\n                 .execute().actionGet();\n@@ -680,7 +656,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void unmapped() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx_unmapped\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\"))\n                 .execute().actionGet();\n \n@@ -696,7 +671,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n     public void partiallyUnmapped() throws Exception {\n         SearchResponse response = client().prepareSearch(\"idx\", \"idx_unmapped\").setTypes(\"type\")\n                 .addAggregation(terms(\"terms\")\n-                        .executionHint(randomExecutionHint())\n                         .field(\"value\"))\n                 .execute().actionGet();\n \n@@ -715,30 +689,6 @@ public class StringTermsTests extends ElasticsearchIntegrationTest {\n         }\n     }\n \n-    @Test\n-    public void stringTermsNestedIntoPerBucketAggregator() throws Exception {\n-        // no execution hint so that the logic that decides whether or not to use ordinals is executed\n-        SearchResponse response = client().prepareSearch(\"idx\").setTypes(\"type\")\n-                .addAggregation(filter(\"filter\").filter(termFilter(\"values\", \"val3\")).subAggregation(terms(\"terms\").field(\"values\")))\n-                .execute().actionGet();\n-\n-        assertThat(response.getFailedShards(), equalTo(0));\n-\n-        Filter filter = response.getAggregations().get(\"filter\");\n-\n-        Terms terms = filter.getAggregations().get(\"terms\");\n-        assertThat(terms, notNullValue());\n-        assertThat(terms.getName(), equalTo(\"terms\"));\n-        assertThat(terms.buckets().size(), equalTo(3));\n-\n-        for (int i = 2; i <= 4; i++) {\n-            Terms.Bucket bucket = terms.getByTerm(\"val\" + i);\n-            assertThat(bucket, notNullValue());\n-            assertThat(bucket.getKey().string(), equalTo(\"val\" + i));\n-            assertThat(bucket.getDocCount(), equalTo(i == 3 ? 2L : 1L));\n-        }\n-    }\n-\n     @Test\n     public void emptyAggregation() throws Exception {\n         prepareCreate(\"empty_bucket_idx\").addMapping(\"type\", \"value\", \"type=integer\").execute().actionGet();\ndiff --git a/src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java b/src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java\nindex 384d278861f..781826b7a42 100644\n--- a/src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java\n+++ b/src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java\n@@ -671,7 +671,7 @@ public class SimpleFacetsTests extends ElasticsearchIntegrationTest {\n \n             searchResponse = client().prepareSearch()\n                     .setQuery(matchAllQuery())\n-                    .setPostFilter(termFilter(\"tag\", \"blue\"))\n+                    .setFilter(termFilter(\"tag\", \"blue\"))\n                     .addFacet(termsFacet(\"facet1\").field(\"tag\").size(10))\n                     .execute().actionGet();\n \ndiff --git a/src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java b/src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java\nindex 59928bddf33..a0a6b9fd547 100644\n--- a/src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java\n+++ b/src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java\n@@ -38,7 +38,6 @@ import org.elasticsearch.test.ElasticsearchIntegrationTest;\n import org.junit.Test;\n \n import java.io.IOException;\n-import java.util.Arrays;\n \n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n@@ -55,8 +54,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n     @Test\n     public void simpleDistanceTests() throws Exception {\n         String mapping = XContentFactory.jsonBuilder().startObject().startObject(\"type1\")\n-                .startObject(\"properties\").startObject(\"location\").field(\"type\", \"geo_point\").field(\"lat_lon\", true)\n-                .startObject(\"fielddata\").field(\"format\", randomFrom(Arrays.asList(\"array\", \"compressed\"))).endObject().endObject().endObject()\n+                .startObject(\"properties\").startObject(\"location\").field(\"type\", \"geo_point\").field(\"lat_lon\", true).endObject().endObject()\n                 .endObject().endObject().string();\n         client().admin().indices().prepareCreate(\"test\").addMapping(\"type1\", mapping).execute().actionGet();\n         client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().execute().actionGet();\n@@ -207,8 +205,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n     @Test\n     public void testDistanceSortingMVFields() throws Exception {\n         String mapping = XContentFactory.jsonBuilder().startObject().startObject(\"type1\")\n-                .startObject(\"properties\").startObject(\"locations\").field(\"type\", \"geo_point\").field(\"lat_lon\", true)\n-                .startObject(\"fielddata\").field(\"format\", randomFrom(Arrays.asList(\"array\", \"compressed\"))).endObject().endObject().endObject()\n+                .startObject(\"properties\").startObject(\"locations\").field(\"type\", \"geo_point\").field(\"lat_lon\", true).endObject().endObject()\n                 .endObject().endObject().string();\n \n         client().admin().indices().prepareCreate(\"test\")\n@@ -262,7 +259,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n \n         assertHitCount(searchResponse, 4);\n         assertOrderedSearchHits(searchResponse, \"1\", \"2\", \"3\", \"4\");\n-        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), equalTo(0d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(0.4621d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(1.055d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(2.029d, 0.01d));\n@@ -274,7 +271,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n \n         assertHitCount(searchResponse, 4);\n         assertOrderedSearchHits(searchResponse, \"1\", \"3\", \"2\", \"4\");\n-        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), equalTo(0d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(1.258d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(5.286d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(8.572d, 0.01d));\n@@ -289,7 +286,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n         assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(8.572d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(5.286d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(1.258d, 0.01d));\n-        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), equalTo(0d));\n \n         // Order: Desc, Mode: min\n         searchResponse = client().prepareSearch(\"test\").setQuery(matchAllQuery())\n@@ -301,7 +298,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n         assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(2.029d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(1.055d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(0.4621d, 0.01d));\n-        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), equalTo(0d));\n \n         searchResponse = client().prepareSearch(\"test\").setQuery(matchAllQuery())\n                 .addSort(SortBuilders.geoDistanceSort(\"locations\").point(40.7143528, -74.0059731).sortMode(\"avg\").order(SortOrder.ASC))\n@@ -309,7 +306,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n \n         assertHitCount(searchResponse, 4);\n         assertOrderedSearchHits(searchResponse, \"1\", \"3\", \"2\", \"4\");\n-        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), equalTo(0d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(1.157d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(2.874d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(5.301d, 0.01d));\n@@ -323,7 +320,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n         assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(5.301d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(2.874d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(1.157d, 0.01d));\n-        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), equalTo(0d));\n \n         try {\n             client().prepareSearch(\"test\").setQuery(matchAllQuery())\n@@ -339,8 +336,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n     // Regression bug: https://github.com/elasticsearch/elasticsearch/issues/2851\n     public void testDistanceSortingWithMissingGeoPoint() throws Exception {\n         String mapping = XContentFactory.jsonBuilder().startObject().startObject(\"type1\")\n-                .startObject(\"properties\").startObject(\"locations\").field(\"type\", \"geo_point\").field(\"lat_lon\", true)\n-                .startObject(\"fielddata\").field(\"format\", randomFrom(Arrays.asList(\"array\", \"compressed\"))).endObject().endObject().endObject()\n+                .startObject(\"properties\").startObject(\"locations\").field(\"type\", \"geo_point\").field(\"lat_lon\", true).endObject().endObject()\n                 .endObject().endObject().string();\n \n         client().admin().indices().prepareCreate(\"test\")\n@@ -409,27 +405,27 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n \n         SearchResponse searchResponse1 = client().prepareSearch().addField(\"_source\").addScriptField(\"distance\", \"doc['location'].arcDistance(\" + target_lat + \",\" + target_long + \")\").execute().actionGet();\n         Double resultDistance1 = searchResponse1.getHits().getHits()[0].getFields().get(\"distance\").getValue();\n-        assertThat(resultDistance1, closeTo(GeoDistance.ARC.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.MILES), 0.0001d));\n+        assertThat(resultDistance1, equalTo(GeoDistance.ARC.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.MILES)));\n \n         SearchResponse searchResponse2 = client().prepareSearch().addField(\"_source\").addScriptField(\"distance\", \"doc['location'].distance(\" + target_lat + \",\" + target_long + \")\").execute().actionGet();\n         Double resultDistance2 = searchResponse2.getHits().getHits()[0].getFields().get(\"distance\").getValue();\n-        assertThat(resultDistance2, closeTo(GeoDistance.PLANE.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.MILES), 0.0001d));\n+        assertThat(resultDistance2, equalTo(GeoDistance.PLANE.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.MILES)));\n \n         SearchResponse searchResponse3 = client().prepareSearch().addField(\"_source\").addScriptField(\"distance\", \"doc['location'].arcDistanceInKm(\" + target_lat + \",\" + target_long + \")\").execute().actionGet();\n         Double resultArcDistance3 = searchResponse3.getHits().getHits()[0].getFields().get(\"distance\").getValue();\n-        assertThat(resultArcDistance3, closeTo(GeoDistance.ARC.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.KILOMETERS), 0.0001d));\n+        assertThat(resultArcDistance3, equalTo(GeoDistance.ARC.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.KILOMETERS)));\n \n         SearchResponse searchResponse4 = client().prepareSearch().addField(\"_source\").addScriptField(\"distance\", \"doc['location'].distanceInKm(\" + target_lat + \",\" + target_long + \")\").execute().actionGet();\n         Double resultDistance4 = searchResponse4.getHits().getHits()[0].getFields().get(\"distance\").getValue();\n-        assertThat(resultDistance4, closeTo(GeoDistance.PLANE.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.KILOMETERS), 0.0001d));\n+        assertThat(resultDistance4, equalTo(GeoDistance.PLANE.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.KILOMETERS)));\n \n         SearchResponse searchResponse5 = client().prepareSearch().addField(\"_source\").addScriptField(\"distance\", \"doc['location'].arcDistanceInKm(\" + (target_lat) + \",\" + (target_long + 360) + \")\").execute().actionGet();\n         Double resultArcDistance5 = searchResponse5.getHits().getHits()[0].getFields().get(\"distance\").getValue();\n-        assertThat(resultArcDistance5, closeTo(GeoDistance.ARC.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.KILOMETERS), 0.0001d));\n+        assertThat(resultArcDistance5, equalTo(GeoDistance.ARC.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.KILOMETERS)));\n \n         SearchResponse searchResponse6 = client().prepareSearch().addField(\"_source\").addScriptField(\"distance\", \"doc['location'].arcDistanceInKm(\" + (target_lat + 360) + \",\" + (target_long) + \")\").execute().actionGet();\n         Double resultArcDistance6 = searchResponse6.getHits().getHits()[0].getFields().get(\"distance\").getValue();\n-        assertThat(resultArcDistance6, closeTo(GeoDistance.ARC.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.KILOMETERS), 0.0001d));\n+        assertThat(resultArcDistance6, equalTo(GeoDistance.ARC.calculate(source_lat, source_long, target_lat, target_long, DistanceUnit.KILOMETERS)));\n     }\n \n     @Test\n@@ -441,8 +437,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n                     .field(\"type\", \"nested\")\n                     .startObject(\"properties\")\n                         .startObject(\"name\").field(\"type\", \"string\").endObject()\n-                        .startObject(\"location\").field(\"type\", \"geo_point\").field(\"lat_lon\", true)\n-                        .startObject(\"fielddata\").field(\"format\", randomFrom(Arrays.asList(\"array\", \"compressed\"))).endObject().endObject()\n+                        .startObject(\"location\").field(\"type\", \"geo_point\").field(\"lat_lon\", true).endObject()\n                     .endObject()\n                 .endObject()\n                 .endObject()\n@@ -509,7 +504,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n \n         assertHitCount(searchResponse, 4);\n         assertOrderedSearchHits(searchResponse, \"1\", \"2\", \"3\", \"4\");\n-        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), equalTo(0d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(0.4621d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(1.055d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(2.029d, 0.01d));\n@@ -521,7 +516,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n \n         assertHitCount(searchResponse, 4);\n         assertOrderedSearchHits(searchResponse, \"1\", \"3\", \"2\", \"4\");\n-        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), equalTo(0d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(1.258d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(5.286d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(8.572d, 0.01d));\n@@ -536,7 +531,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n         assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(8.572d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(5.286d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(1.258d, 0.01d));\n-        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), equalTo(0d));\n \n         // Order: Desc, Mode: min\n         searchResponse = client().prepareSearch(\"companies\").setQuery(matchAllQuery())\n@@ -548,7 +543,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n         assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(2.029d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(1.055d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(0.4621d, 0.01d));\n-        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), equalTo(0d));\n \n         searchResponse = client().prepareSearch(\"companies\").setQuery(matchAllQuery())\n                 .addSort(SortBuilders.geoDistanceSort(\"branches.location\").point(40.7143528, -74.0059731).sortMode(\"avg\").order(SortOrder.ASC))\n@@ -556,7 +551,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n \n         assertHitCount(searchResponse, 4);\n         assertOrderedSearchHits(searchResponse, \"1\", \"3\", \"2\", \"4\");\n-        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), equalTo(0d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(1.157d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(2.874d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(5.301d, 0.01d));\n@@ -573,7 +568,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n         assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), closeTo(5.301d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(2.874d, 0.01d));\n         assertThat(((Number) searchResponse.getHits().getAt(2).sortValues()[0]).doubleValue(), closeTo(1.157d, 0.01d));\n-        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), closeTo(0d, 0.01d));\n+        assertThat(((Number) searchResponse.getHits().getAt(3).sortValues()[0]).doubleValue(), equalTo(0d));\n \n         searchResponse = client().prepareSearch(\"companies\").setQuery(matchAllQuery())\n                 .addSort(\n@@ -616,9 +611,6 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n                                 .field(\"geohash\", true)\n                                 .field(\"geohash_precision\", 24)\n                                 .field(\"lat_lon\", true)\n-                                .startObject(\"fielddata\")\n-                                    .field(\"format\", randomFrom(Arrays.asList(\"array\", \"compressed\")))\n-                                .endObject()\n                             .endObject()\n                         .endObject()\n                     .endObject()\n@@ -638,7 +630,7 @@ public class GeoDistanceTests extends ElasticsearchIntegrationTest {\n \n         SearchResponse result = client().prepareSearch(\"locations\")\n                 .setQuery(QueryBuilders.matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoDistanceFilter(\"pin\")\n+                .setFilter(FilterBuilders.geoDistanceFilter(\"pin\")\n                         .geoDistance(GeoDistance.ARC)\n                         .lat(lat).lon(lon)\n                         .distance(\"1m\"))\ndiff --git a/src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java b/src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java\nindex 8a04ff920c4..5279deae836 100644\n--- a/src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java\n+++ b/src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java\n@@ -255,7 +255,7 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n         // Point in polygon\n         SearchResponse result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(3, 3)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(3, 3)))\n                 .execute().actionGet();\n         assertHitCount(result, 1);\n         assertFirstHit(result, hasId(\"1\"));\n@@ -263,7 +263,7 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n         // Point in polygon hole\n         result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(4.5, 4.5)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(4.5, 4.5)))\n                 .execute().actionGet();\n         assertHitCount(result, 0);\n \n@@ -274,7 +274,7 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n         // Point on polygon border\n         result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(10.0, 5.0)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(10.0, 5.0)))\n                 .execute().actionGet();\n         assertHitCount(result, 1);\n         assertFirstHit(result, hasId(\"1\"));\n@@ -282,7 +282,7 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n         // Point on hole border\n         result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(5.0, 2.0)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(5.0, 2.0)))\n                 .execute().actionGet();\n         assertHitCount(result, 1);\n         assertFirstHit(result, hasId(\"1\"));\n@@ -291,14 +291,14 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n             // Point not in polygon\n             result = client().prepareSearch()\n                     .setQuery(matchAllQuery())\n-                    .setPostFilter(FilterBuilders.geoDisjointFilter(\"area\", ShapeBuilder.newPoint(3, 3)))\n+                    .setFilter(FilterBuilders.geoDisjointFilter(\"area\", ShapeBuilder.newPoint(3, 3)))\n                     .execute().actionGet();\n             assertHitCount(result, 0);\n \n             // Point in polygon hole\n             result = client().prepareSearch()\n                     .setQuery(matchAllQuery())\n-                    .setPostFilter(FilterBuilders.geoDisjointFilter(\"area\", ShapeBuilder.newPoint(4.5, 4.5)))\n+                    .setFilter(FilterBuilders.geoDisjointFilter(\"area\", ShapeBuilder.newPoint(4.5, 4.5)))\n                     .execute().actionGet();\n             assertHitCount(result, 1);\n             assertFirstHit(result, hasId(\"1\"));\n@@ -319,7 +319,7 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n         // re-check point on polygon hole\n         result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(4.5, 4.5)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(4.5, 4.5)))\n                 .execute().actionGet();\n         assertHitCount(result, 1);\n         assertFirstHit(result, hasId(\"2\"));\n@@ -339,7 +339,7 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n \n             result = client().prepareSearch()\n                     .setQuery(matchAllQuery())\n-                    .setPostFilter(FilterBuilders.geoWithinFilter(\"area\", builder))\n+                    .setFilter(FilterBuilders.geoWithinFilter(\"area\", builder))\n                     .execute().actionGet();\n             assertHitCount(result, 2);\n         }\n@@ -365,25 +365,25 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n \n         result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(174, -4)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(174, -4)))\n                 .execute().actionGet();\n         assertHitCount(result, 1);\n \n         result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(-174, -4)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(-174, -4)))\n                 .execute().actionGet();\n         assertHitCount(result, 1);\n \n         result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(180, -4)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(180, -4)))\n                 .execute().actionGet();\n         assertHitCount(result, 0);\n \n         result = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(180, -6)))\n+                .setFilter(FilterBuilders.geoIntersectionFilter(\"area\", ShapeBuilder.newPoint(180, -6)))\n                 .execute().actionGet();\n         assertHitCount(result, 1);\n     }\n@@ -423,7 +423,7 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n         String key = \"DE\";\n \n         SearchResponse searchResponse = client().prepareSearch()\n-                .setQuery(matchQuery(\"_id\", key))\n+                .setQuery(fieldQuery(\"_id\", key))\n                 .execute().actionGet();\n \n         assertHitCount(searchResponse, 1);\n@@ -497,15 +497,15 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n         client().admin().indices().prepareRefresh(\"locations\").execute().actionGet();\n \n         // Result of this geohash search should contain the geohash only\n-        SearchResponse results1 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(\"{\\\"geohash_cell\\\": {\\\"pin\\\": \\\"\" + geohash + \"\\\", \\\"neighbors\\\": false}}\").execute().actionGet();\n+        SearchResponse results1 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setFilter(\"{\\\"geohash_cell\\\": {\\\"pin\\\": \\\"\" + geohash + \"\\\", \\\"neighbors\\\": false}}\").execute().actionGet();\n         assertHitCount(results1, 1);\n \n         // test the same, just with the builder\n-        results1 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(geoHashCellFilter(\"pin\", geohash, false)).execute().actionGet();\n+        results1 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setFilter(geoHashCellFilter(\"pin\", geohash, false)).execute().actionGet();\n         assertHitCount(results1, 1);\n \n         // Result of the parent query should contain the parent it self, its neighbors, the child and all its neighbors\n-        SearchResponse results2 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(\"{\\\"geohash_cell\\\": {\\\"pin\\\": \\\"\" + geohash.substring(0, geohash.length() - 1) + \"\\\", \\\"neighbors\\\": true}}\").execute().actionGet();\n+        SearchResponse results2 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setFilter(\"{\\\"geohash_cell\\\": {\\\"pin\\\": \\\"\" + geohash.substring(0, geohash.length() - 1) + \"\\\", \\\"neighbors\\\": true}}\").execute().actionGet();\n         assertHitCount(results2, 2 + neighbors.size() + parentNeighbors.size());\n \n         // Testing point formats and precision\n@@ -514,17 +514,17 @@ public class GeoFilterTests extends ElasticsearchIntegrationTest {\n \n         logger.info(\"Testing lat/lon format\");\n         String pointTest1 = \"{\\\"geohash_cell\\\": {\\\"pin\\\": {\\\"lat\\\": \" + point.lat() + \",\\\"lon\\\": \" + point.lon() + \"},\\\"precision\\\": \" + precision + \",\\\"neighbors\\\": true}}\";\n-        SearchResponse results3 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest1).execute().actionGet();\n+        SearchResponse results3 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setFilter(pointTest1).execute().actionGet();\n         assertHitCount(results3, neighbors.size() + 1);\n \n         logger.info(\"Testing String format\");\n         String pointTest2 = \"{\\\"geohash_cell\\\": {\\\"pin\\\": \\\"\" + point.lat() + \",\" + point.lon() + \"\\\",\\\"precision\\\": \" + precision + \",\\\"neighbors\\\": true}}\";\n-        SearchResponse results4 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest2).execute().actionGet();\n+        SearchResponse results4 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setFilter(pointTest2).execute().actionGet();\n         assertHitCount(results4, neighbors.size() + 1);\n \n         logger.info(\"Testing Array format\");\n         String pointTest3 = \"{\\\"geohash_cell\\\": {\\\"pin\\\": [\" + point.lon() + \",\" + point.lat() + \"],\\\"precision\\\": \" + precision + \",\\\"neighbors\\\": true}}\";\n-        SearchResponse results5 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest3).execute().actionGet();\n+        SearchResponse results5 = client().prepareSearch(\"locations\").setQuery(QueryBuilders.matchAllQuery()).setFilter(pointTest3).execute().actionGet();\n         assertHitCount(results5, neighbors.size() + 1);\n     }\n \ndiff --git a/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java b/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java\nindex 716bac1cfae..08f78b32696 100644\n--- a/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java\n+++ b/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java\n@@ -251,7 +251,7 @@ public class GeoShapeIntegrationTests extends ElasticsearchIntegrationTest {\n                 + \"\\\"shape_field_name\\\": \\\"location2\\\"\"\n                 + \"}}}}\";\n \n-        SearchResponse result = client().prepareSearch(\"test\").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(filter).execute().actionGet();\n+        SearchResponse result = client().prepareSearch(\"test\").setQuery(QueryBuilders.matchAllQuery()).setFilter(filter).execute().actionGet();\n         assertHitCount(result, 1);\n     }\n \ndiff --git a/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java b/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java\nindex 32bd2a213d5..ae5dc9f6a0f 100644\n--- a/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java\n+++ b/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java\n@@ -318,7 +318,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         indexRandom(true, indexRequestBuilders);\n \n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"bug\"))\n+                .setQuery(fieldQuery(\"title\", \"bug\"))\n                 .addHighlightedField(\"title\", -1, 0)\n                 .get();\n \n@@ -327,7 +327,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         }\n \n         search = client().prepareSearch()\n-                .setQuery(matchQuery(\"attachments.body\", \"attachment\"))\n+                .setQuery(fieldQuery(\"attachments.body\", \"attachment\"))\n                 .addHighlightedField(\"attachments.body\", -1, 0)\n                 .get();\n \n@@ -358,7 +358,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         indexRandom(true, indexRequestBuilders);\n \n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"bug\"))\n+                .setQuery(fieldQuery(\"title\", \"bug\"))\n                 .addHighlightedField(\"title\", -1, 0)\n                 .get();\n \n@@ -367,7 +367,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         }\n \n         search = client().prepareSearch()\n-                .setQuery(matchQuery(\"attachments.body\", \"attachment\"))\n+                .setQuery(fieldQuery(\"attachments.body\", \"attachment\"))\n                 .addHighlightedField(\"attachments.body\", -1, 2)\n                 .execute().get();\n \n@@ -443,7 +443,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n                 .setSource(\"titleTV\", new String[]{\"some text to highlight\", \"highlight other text\"}));\n \n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"bug\"))\n+                .setQuery(fieldQuery(\"title\", \"bug\"))\n                 .addHighlightedField(\"title\", -1, 2)\n                 .addHighlightedField(\"titleTV\", -1, 2)\n                 .get();\n@@ -454,7 +454,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         assertHighlight(search, 0, \"titleTV\", 1, 2,  equalTo(\"The <em>bug</em> is bugging us\"));\n \n         search = client().prepareSearch()\n-                .setQuery(matchQuery(\"titleTV\", \"highlight\"))\n+                .setQuery(fieldQuery(\"titleTV\", \"highlight\"))\n                 .addHighlightedField(\"titleTV\", -1, 2)\n                 .get();\n \n@@ -506,54 +506,6 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         assertHighlight(searchResponse, 0, \"field2\", 0, 1, equalTo(\"this is another <xxx>test</xxx>\"));\n     }\n \n-    @Test\n-    public void testPlainHighlighterForceSource() throws Exception {\n-        prepareCreate(\"test\")\n-                .addMapping(\"type1\", \"field1\", \"type=string,store=yes,term_vector=with_positions_offsets,index_options=offsets\")\n-                .get();\n-        ensureGreen();\n-\n-        client().prepareIndex(\"test\", \"type1\")\n-                .setSource(\"field1\", \"The quick brown fox jumps over the lazy dog\").get();\n-        refresh();\n-\n-        SearchResponse searchResponse = client().prepareSearch(\"test\")\n-                .setQuery(termQuery(\"field1\", \"quick\"))\n-                .addHighlightedField(new Field(\"field1\").preTags(\"<xxx>\").postTags(\"</xxx>\").highlighterType(\"fvh\").forceSource(true))\n-                .get();\n-        assertHighlight(searchResponse, 0, \"field1\", 0, 1, equalTo(\"The <xxx>quick</xxx> brown fox jumps over the lazy dog\"));\n-\n-        searchResponse = client().prepareSearch(\"test\")\n-                .setQuery(termQuery(\"field1\", \"quick\"))\n-                .addHighlightedField(new Field(\"field1\").preTags(\"<xxx>\").postTags(\"</xxx>\").highlighterType(\"plain\").forceSource(true))\n-                .get();\n-        assertHighlight(searchResponse, 0, \"field1\", 0, 1, equalTo(\"The <xxx>quick</xxx> brown fox jumps over the lazy dog\"));\n-\n-        searchResponse = client().prepareSearch(\"test\")\n-                .setQuery(termQuery(\"field1\", \"quick\"))\n-                .addHighlightedField(new Field(\"field1\").preTags(\"<xxx>\").postTags(\"</xxx>\").highlighterType(\"postings\").forceSource(true))\n-                .get();\n-        assertHighlight(searchResponse, 0, \"field1\", 0, 1, equalTo(\"The <xxx>quick</xxx> brown fox jumps over the lazy dog\"));\n-\n-        searchResponse = client().prepareSearch(\"test\")\n-                .setQuery(termQuery(\"field1\", \"quick\"))\n-                .addHighlightedField(new Field(\"field1\").preTags(\"<xxx>\").postTags(\"</xxx>\").highlighterType(\"fvh\").forceSource(false))\n-                .get();\n-        assertHighlight(searchResponse, 0, \"field1\", 0, 1, equalTo(\"The <xxx>quick</xxx> brown fox jumps over the lazy dog\"));\n-\n-        searchResponse = client().prepareSearch(\"test\")\n-                .setQuery(termQuery(\"field1\", \"quick\"))\n-                .addHighlightedField(new Field(\"field1\").preTags(\"<xxx>\").postTags(\"</xxx>\").highlighterType(\"plain\").forceSource(false))\n-                .get();\n-        assertHighlight(searchResponse, 0, \"field1\", 0, 1, equalTo(\"The <xxx>quick</xxx> brown fox jumps over the lazy dog\"));\n-\n-        searchResponse = client().prepareSearch(\"test\")\n-                .setQuery(termQuery(\"field1\", \"quick\"))\n-                .addHighlightedField(new Field(\"field1\").preTags(\"<xxx>\").postTags(\"</xxx>\").highlighterType(\"postings\").forceSource(false))\n-                .get();\n-        assertHighlight(searchResponse, 0, \"field1\", 0, 1, equalTo(\"The <xxx>quick</xxx> brown fox jumps over the lazy dog\"));\n-    }\n-\n     @Test\n     public void testPlainHighlighter() throws Exception {\n         createIndex(\"test\");\n@@ -909,7 +861,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         indexRandom(true, indexRequestBuilders);\n \n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"bug\"))\n+                .setQuery(fieldQuery(\"title\", \"bug\"))\n                 .addHighlightedField(\"title\", -1, 0)\n                 .get();\n \n@@ -932,7 +884,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         indexRandom(true, indexRequestBuilders);\n \n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"bug\"))\n+                .setQuery(fieldQuery(\"title\", \"bug\"))\n                 .addHighlightedField(\"title\", 30, 1, 10)\n                 .get();\n \n@@ -956,7 +908,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         indexRandom(true, indexRequestBuilders);\n \n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"test\"))\n+                .setQuery(fieldQuery(\"title\", \"test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title\", 50, 1, 10)\n                 .get();\n@@ -980,7 +932,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         indexRandom(true, indexRequestBuilders);\n \n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"test\"))\n+                .setQuery(fieldQuery(\"title\", \"test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title\", 30, 1, 10)\n                 .get();\n@@ -1004,7 +956,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         refresh();\n         // simple search on body with standard analyzer with a simple field query\n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"this is a test\"))\n+                .setQuery(fieldQuery(\"title\", \"this is a test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title\", 50, 1)\n                 .get();\n@@ -1013,7 +965,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n \n         // search on title.key and highlight on title\n         search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title.key\", \"this is a test\"))\n+                .setQuery(fieldQuery(\"title.key\", \"this is a test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title.key\", 50, 1)\n                 .get();\n@@ -1037,7 +989,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n \n         // simple search on body with standard analyzer with a simple field query\n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"this is a test\"))\n+                .setQuery(fieldQuery(\"title\", \"this is a test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title\", 50, 1)\n                 .get();\n@@ -1046,7 +998,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n \n         // search on title.key and highlight on title.key\n         search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title.key\", \"this is a test\"))\n+                .setQuery(fieldQuery(\"title.key\", \"this is a test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title.key\", 50, 1)\n                 .get();\n@@ -1070,7 +1022,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n \n         // simple search on body with standard analyzer with a simple field query\n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"this is a test\"))\n+                .setQuery(fieldQuery(\"title\", \"this is a test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title\", 50, 1)\n                 .get();\n@@ -1079,7 +1031,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n \n         // search on title.key and highlight on title\n         search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title.key\", \"this is a test\"))\n+                .setQuery(fieldQuery(\"title.key\", \"this is a test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title.key\", 50, 1)\n                 .get();\n@@ -1102,7 +1054,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n \n         // simple search on body with standard analyzer with a simple field query\n         SearchResponse search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title\", \"this is a test\"))\n+                .setQuery(fieldQuery(\"title\", \"this is a test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title\", 50, 1)\n                 .get();\n@@ -1111,7 +1063,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n \n         // search on title.key and highlight on title.key\n         search = client().prepareSearch()\n-                .setQuery(matchQuery(\"title.key\", \"this is a test\"))\n+                .setQuery(fieldQuery(\"title.key\", \"this is a test\"))\n                 .setHighlighterEncoder(\"html\")\n                 .addHighlightedField(\"title.key\", 50, 1)\n                 .get();\n@@ -1337,7 +1289,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n                 .setSource(\"field4\", \"a quick fast blue car\").get();\n         refresh();\n \n-        source = searchSource().postFilter(FilterBuilders.typeFilter(\"type2\")).query(matchPhrasePrefixQuery(\"field3\", \"fast bro\")).from(0).size(60).explain(true)\n+        source = searchSource().filter(FilterBuilders.typeFilter(\"type2\")).query(matchPhrasePrefixQuery(\"field3\", \"fast bro\")).from(0).size(60).explain(true)\n                 .highlight(highlight().field(\"field3\").order(\"score\").preTags(\"<x>\").postTags(\"</x>\"));\n \n         searchResponse = client().search(searchRequest(\"test\").source(source).searchType(QUERY_THEN_FETCH)).actionGet();\n@@ -1345,7 +1297,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         assertHighlight(searchResponse, 0, \"field3\", 0, 1, equalTo(\"The <x>quick</x> <x>brown</x> fox jumps over the lazy dog\"));\n \n         logger.info(\"--> highlighting and searching on field4\");\n-        source = searchSource().postFilter(FilterBuilders.typeFilter(\"type2\")).query(matchPhrasePrefixQuery(\"field4\", \"the fast bro\")).from(0).size(60).explain(true)\n+        source = searchSource().filter(FilterBuilders.typeFilter(\"type2\")).query(matchPhrasePrefixQuery(\"field4\", \"the fast bro\")).from(0).size(60).explain(true)\n                 .highlight(highlight().field(\"field4\").order(\"score\").preTags(\"<x>\").postTags(\"</x>\"));\n         searchResponse = client().search(searchRequest(\"test\").source(source).searchType(QUERY_THEN_FETCH)).actionGet();\n \n@@ -1353,7 +1305,7 @@ public class HighlighterSearchTests extends ElasticsearchIntegrationTest {\n         assertHighlight(searchResponse, 1, \"field4\", 0, 1, equalTo(\"<x>The quick brown</x> fox jumps over the lazy dog\"));\n \n         logger.info(\"--> highlighting and searching on field4\");\n-        source = searchSource().postFilter(FilterBuilders.typeFilter(\"type2\")).query(matchPhrasePrefixQuery(\"field4\", \"a fast quick blue ca\")).from(0).size(60).explain(true)\n+        source = searchSource().filter(FilterBuilders.typeFilter(\"type2\")).query(matchPhrasePrefixQuery(\"field4\", \"a fast quick blue ca\")).from(0).size(60).explain(true)\n                 .highlight(highlight().field(\"field4\").order(\"score\").preTags(\"<x>\").postTags(\"</x>\"));\n         searchResponse = client().search(searchRequest(\"test\").source(source).searchType(QUERY_THEN_FETCH)).actionGet();\n \ndiff --git a/src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java b/src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java\nindex 9c52caf30df..1767a949f2e 100644\n--- a/src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java\n+++ b/src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java\n@@ -88,7 +88,7 @@ public class MatchedQueriesTests extends ElasticsearchIntegrationTest {\n \n         SearchResponse searchResponse = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(orFilter(\n+                .setFilter(orFilter(\n                         termFilter(\"name\", \"test\").filterName(\"name\"),\n                         termFilter(\"title\", \"title1\").filterName(\"title\"))).get();\n         assertHitCount(searchResponse, 3l);\n@@ -107,7 +107,7 @@ public class MatchedQueriesTests extends ElasticsearchIntegrationTest {\n \n         searchResponse = client().prepareSearch()\n                 .setQuery(matchAllQuery())\n-                .setPostFilter(queryFilter(boolQuery()\n+                .setFilter(queryFilter(boolQuery()\n                         .should(termQuery(\"name\", \"test\").queryName(\"name\"))\n                         .should(termQuery(\"title\", \"title1\").queryName(\"title\")))).get();\n \n@@ -138,7 +138,7 @@ public class MatchedQueriesTests extends ElasticsearchIntegrationTest {\n \n         SearchResponse searchResponse = client().prepareSearch()\n                 .setQuery(filteredQuery(matchAllQuery(), termsFilter(\"title\", \"title1\", \"title2\", \"title3\").filterName(\"title\")))\n-                        .setPostFilter(termFilter(\"name\", \"test\").filterName(\"name\")).get();\n+                        .setFilter(termFilter(\"name\", \"test\").filterName(\"name\")).get();\n         assertHitCount(searchResponse, 3l);\n         for (SearchHit hit : searchResponse.getHits()) {\n             if (hit.id().equals(\"1\") || hit.id().equals(\"2\") || hit.id().equals(\"3\")) {\n@@ -152,7 +152,7 @@ public class MatchedQueriesTests extends ElasticsearchIntegrationTest {\n \n         searchResponse = client().prepareSearch()\n                 .setQuery(termsQuery(\"title\", \"title1\", \"title2\", \"title3\").queryName(\"title\"))\n-                .setPostFilter(queryFilter(matchQuery(\"name\", \"test\").queryName(\"name\"))).get();\n+                .setFilter(queryFilter(matchQuery(\"name\", \"test\").queryName(\"name\"))).get();\n         assertHitCount(searchResponse, 3l);\n         for (SearchHit hit : searchResponse.getHits()) {\n             if (hit.id().equals(\"1\") || hit.id().equals(\"2\") || hit.id().equals(\"3\")) {\ndiff --git a/src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java b/src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java\nindex 5bc242e3ef1..c972565ea70 100644\n--- a/src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java\n+++ b/src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java\n@@ -71,7 +71,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         assertHitCount(\n                 client().prepareSearch()\n                         .setQuery(matchAllQuery())\n-                        .setPostFilter(\n+                        .setFilter(\n                                 andFilter(\n                                         queryFilter(matchAllQuery()),\n                                         notFilter(andFilter(queryFilter(termQuery(\"field1\", \"value1\")),\n@@ -87,7 +87,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n                                                 queryFilter(termQuery(\"field1\", \"value2\")))))).get(),\n                 3l);\n         assertHitCount(\n-                client().prepareSearch().setQuery(matchAllQuery()).setPostFilter(notFilter(termFilter(\"field1\", \"value3\"))).get(),\n+                client().prepareSearch().setQuery(matchAllQuery()).setFilter(notFilter(termFilter(\"field1\", \"value3\"))).get(), \n                 2l);\n     }\n \n@@ -551,7 +551,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         assertHitCount(client().prepareSearch().setQuery(bool).get(), 1l);\n \n         WrapperFilterBuilder wrapperFilter = new WrapperFilterBuilder(\"{ \\\"term\\\" : { \\\"field1\\\" : \\\"value1_1\\\" } }\");\n-        assertHitCount(client().prepareSearch().setPostFilter(wrapperFilter).get(), 1l);\n+        assertHitCount(client().prepareSearch().setFilter(wrapperFilter).get(), 1l);\n     }\n \n     @Test\n@@ -1026,7 +1026,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         client().prepareIndex(\"test\", \"type2\", \"2\").setSource(\"field1\", \"value2\").get();\n         refresh();\n \n-        SearchResponse searchResponse = client().prepareSearch().setQuery(matchAllQuery()).setPostFilter(idsFilter(\"type1\").ids(\"1\")).get();\n+        SearchResponse searchResponse = client().prepareSearch().setQuery(matchAllQuery()).setFilter(idsFilter(\"type1\").ids(\"1\")).get();\n         assertHitCount(searchResponse, 1l);\n         assertThat(searchResponse.getHits().hits().length, equalTo(1));\n \n@@ -1034,11 +1034,11 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         assertHitCount(searchResponse, 2l);\n         assertThat(searchResponse.getHits().hits().length, equalTo(2));\n \n-        searchResponse = client().prepareSearch().setQuery(matchAllQuery()).setPostFilter(idsFilter().ids(\"1\")).get();\n+        searchResponse = client().prepareSearch().setQuery(matchAllQuery()).setFilter(idsFilter().ids(\"1\")).get();\n         assertHitCount(searchResponse, 1l);\n         assertThat(searchResponse.getHits().hits().length, equalTo(1));\n \n-        searchResponse = client().prepareSearch().setQuery(matchAllQuery()).setPostFilter(idsFilter().ids(\"1\", \"2\")).get();\n+        searchResponse = client().prepareSearch().setQuery(matchAllQuery()).setFilter(idsFilter().ids(\"1\", \"2\")).get();\n         assertHitCount(searchResponse, 2l);\n         assertThat(searchResponse.getHits().hits().length, equalTo(2));\n \n@@ -1220,7 +1220,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         client().prepareIndex(\"test\", \"type1\", \"4\").setSource(\"field1\", \"test2\", \"num_long\", 4).get();\n         refresh();\n \n-        SearchResponse searchResponse = client().prepareSearch(\"test\").setPostFilter(\n+        SearchResponse searchResponse = client().prepareSearch(\"test\").setFilter(\n                 boolFilter()\n                         .should(rangeFilter(\"num_long\", 1, 2))\n                         .should(rangeFilter(\"num_long\", 3, 4))\n@@ -1228,7 +1228,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         assertHitCount(searchResponse, 4l);\n \n         // This made 2826 fail! (only with bit based filters)\n-        searchResponse = client().prepareSearch(\"test\").setPostFilter(\n+        searchResponse = client().prepareSearch(\"test\").setFilter(\n                 boolFilter()\n                         .should(rangeFilter(\"num_long\", 1, 2))\n                         .should(rangeFilter(\"num_long\", 3, 4))\n@@ -1236,7 +1236,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         assertHitCount(searchResponse, 4l);\n \n         // This made #2979 fail!\n-        searchResponse = client().prepareSearch(\"test\").setPostFilter(\n+        searchResponse = client().prepareSearch(\"test\").setFilter(\n                 boolFilter()\n                         .must(termFilter(\"field1\", \"test1\"))\n                         .should(rangeFilter(\"num_long\", 1, 2))\n@@ -1248,7 +1248,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n     @Test\n     public void testEmptyTopLevelFilter() {\n         client().prepareIndex(\"test\", \"type\", \"1\").setSource(\"field\", \"value\").setRefresh(true).get();\n-        SearchResponse searchResponse = client().prepareSearch().setPostFilter(\"{}\").get();\n+        SearchResponse searchResponse = client().prepareSearch().setFilter(\"{}\").get();\n         assertHitCount(searchResponse, 1l);\n     }\n \n@@ -1597,25 +1597,25 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         refresh();\n \n         SearchResponse searchResponse = client().prepareSearch(\"index1\", \"index2\", \"index3\")\n-                .setPostFilter(indicesFilter(termFilter(\"text\", \"value1\"), \"index1\")\n+                .setFilter(indicesFilter(termFilter(\"text\", \"value1\"), \"index1\")\n                         .noMatchFilter(termFilter(\"text\", \"value2\"))).get();\n         assertHitCount(searchResponse, 2l);\n         assertSearchHits(searchResponse, \"1\", \"2\");\n \n         //default no match filter is \"all\"\n         searchResponse = client().prepareSearch(\"index1\", \"index2\", \"index3\")\n-                .setPostFilter(indicesFilter(termFilter(\"text\", \"value1\"), \"index1\")).get();\n+                .setFilter(indicesFilter(termFilter(\"text\", \"value1\"), \"index1\")).get();\n         assertHitCount(searchResponse, 3l);\n         assertSearchHits(searchResponse, \"1\", \"2\", \"3\");\n \n         searchResponse = client().prepareSearch(\"index1\", \"index2\", \"index3\")\n-                .setPostFilter(indicesFilter(termFilter(\"text\", \"value1\"), \"index1\")\n+                .setFilter(indicesFilter(termFilter(\"text\", \"value1\"), \"index1\")\n                         .noMatchFilter(\"all\")).get();\n         assertHitCount(searchResponse, 3l);\n         assertSearchHits(searchResponse, \"1\", \"2\", \"3\");\n \n         searchResponse = client().prepareSearch(\"index1\", \"index2\", \"index3\")\n-                .setPostFilter(indicesFilter(termFilter(\"text\", \"value1\"), \"index1\")\n+                .setFilter(indicesFilter(termFilter(\"text\", \"value1\"), \"index1\")\n                         .noMatchFilter(\"none\")).get();\n         assertHitCount(searchResponse, 1l);\n         assertFirstHit(searchResponse, hasId(\"1\"));\n@@ -1670,7 +1670,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         //has_child fails if executed on \"simple\" index\n         try {\n             client().prepareSearch(\"simple\")\n-                    .setPostFilter(hasChildFilter(\"child\", termFilter(\"text\", \"value1\"))).get();\n+                    .setFilter(hasChildFilter(\"child\", termFilter(\"text\", \"value1\"))).get();\n             fail(\"Should have failed as has_child query can only be executed against parent-child types\");\n         } catch (SearchPhaseExecutionException e) {\n             assertThat(e.shardFailures().length, greaterThan(0));\n@@ -1680,7 +1680,7 @@ public class SimpleQueryTests extends ElasticsearchIntegrationTest {\n         }\n \n         SearchResponse searchResponse = client().prepareSearch(\"related\", \"simple\")\n-                .setPostFilter(indicesFilter(hasChildFilter(\"child\", termFilter(\"text\", \"value2\")), \"related\")\n+                .setFilter(indicesFilter(hasChildFilter(\"child\", termFilter(\"text\", \"value2\")), \"related\")\n                         .noMatchFilter(termFilter(\"text\", \"value1\"))).get();\n         assertHitCount(searchResponse, 2l);\n         assertSearchHits(searchResponse, \"1\", \"2\");\ndiff --git a/src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java b/src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java\nindex 9cf563b3983..918b0ee1c2a 100644\n--- a/src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java\n+++ b/src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java\n@@ -169,7 +169,7 @@ public class SimpleSortTests extends ElasticsearchIntegrationTest {\n         if (!sparseBytes.isEmpty()) {\n             int size = between(1, sparseBytes.size());\n             SearchResponse searchResponse = client().prepareSearch().setQuery(matchAllQuery())\n-                    .setPostFilter(FilterBuilders.existsFilter(\"sparse_bytes\")).setSize(size).addSort(\"sparse_bytes\", SortOrder.ASC).execute()\n+                    .setFilter(FilterBuilders.existsFilter(\"sparse_bytes\")).setSize(size).addSort(\"sparse_bytes\", SortOrder.ASC).execute()\n                     .actionGet();\n             assertNoFailures(searchResponse);\n             assertThat(searchResponse.getHits().getTotalHits(), equalTo((long) sparseBytes.size()));\ndiff --git a/src/test/java/org/elasticsearch/search/suggest/SuggestSearchTests.java b/src/test/java/org/elasticsearch/search/suggest/SuggestSearchTests.java\nindex afbae2fd436..fff1eeb8c11 100644\n--- a/src/test/java/org/elasticsearch/search/suggest/SuggestSearchTests.java\n+++ b/src/test/java/org/elasticsearch/search/suggest/SuggestSearchTests.java\n@@ -23,9 +23,11 @@ import com.google.common.base.Charsets;\n import com.google.common.io.Resources;\n import org.apache.lucene.util.LuceneTestCase.Slow;\n import org.elasticsearch.ElasticSearchException;\n-import org.elasticsearch.ElasticSearchIllegalStateException;\n import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;\n-import org.elasticsearch.action.search.*;\n+import org.elasticsearch.action.search.SearchPhaseExecutionException;\n+import org.elasticsearch.action.search.SearchRequestBuilder;\n+import org.elasticsearch.action.search.SearchResponse;\n+import org.elasticsearch.action.search.SearchType;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.common.xcontent.XContentBuilder;\n import org.elasticsearch.common.xcontent.XContentFactory;\n@@ -49,7 +51,8 @@ import static org.elasticsearch.search.suggest.SuggestBuilder.phraseSuggestion;\n import static org.elasticsearch.search.suggest.SuggestBuilder.termSuggestion;\n import static org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.candidateGenerator;\n import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;\n-import static org.hamcrest.Matchers.*;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.nullValue;\n \n /**\n  * Integration tests for term and phrase suggestions.  Many of these tests many requests that vary only slightly from one another.  Where\n@@ -57,103 +60,6 @@ import static org.hamcrest.Matchers.*;\n  * request, modify again, request again, etc.  This makes it very obvious what changes between requests.\n  */\n public class SuggestSearchTests extends ElasticsearchIntegrationTest {\n-\n-\n-    @Test // see #3196\n-    public void testSuggestAcrossMultipleIndices() throws IOException {\n-        prepareCreate(\"test\").setSettings(\n-                SETTING_NUMBER_OF_SHARDS, between(1, 5),\n-                SETTING_NUMBER_OF_REPLICAS, between(0, 1)).get();\n-        ensureGreen();\n-\n-        index(\"test\", \"type1\", \"1\", \"text\", \"abcd\");\n-        index(\"test\", \"type1\", \"2\", \"text\", \"aacd\");\n-        index(\"test\", \"type1\", \"3\", \"text\", \"abbd\");\n-        index(\"test\", \"type1\", \"4\", \"text\", \"abcc\");\n-        refresh();\n-\n-        TermSuggestionBuilder termSuggest = termSuggestion(\"test\")\n-                .suggestMode(\"always\") // Always, otherwise the results can vary between requests.\n-                .text(\"abcd\")\n-                .field(\"text\");\n-        logger.info(\"--> run suggestions with one index\");\n-        searchSuggest(client(), termSuggest);\n-        prepareCreate(\"test_1\").setSettings(\n-                SETTING_NUMBER_OF_SHARDS, between(1, 5),\n-                SETTING_NUMBER_OF_REPLICAS, between(0, 1)).get();\n-        ensureGreen();\n-\n-        index(\"test_1\", \"type1\", \"1\", \"text\", \"ab cd\");\n-        index(\"test_1\", \"type1\", \"2\", \"text\", \"aa cd\");\n-        index(\"test_1\", \"type1\", \"3\", \"text\", \"ab bd\");\n-        index(\"test_1\", \"type1\", \"4\", \"text\", \"ab cc\");\n-        refresh();\n-        termSuggest = termSuggestion(\"test\")\n-                .suggestMode(\"always\") // Always, otherwise the results can vary between requests.\n-                .text(\"ab cd\")\n-                .minWordLength(1)\n-                .field(\"text\");\n-        logger.info(\"--> run suggestions with two indices\");\n-        searchSuggest(client(), termSuggest);\n-\n-\n-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject(\"type1\")\n-                .startObject(\"properties\")\n-                .startObject(\"text\").field(\"type\", \"string\").field(\"analyzer\", \"keyword\").endObject()\n-                .endObject()\n-                .endObject().endObject();\n-        assertAcked(prepareCreate(\"test_2\").setSettings(settingsBuilder()\n-                .put(SETTING_NUMBER_OF_SHARDS, between(1, 5))\n-                .put(SETTING_NUMBER_OF_REPLICAS, between(0, 1))\n-                ).addMapping(\"type1\", mapping));\n-        ensureGreen();\n-\n-        index(\"test_2\", \"type1\", \"1\", \"text\", \"ab cd\");\n-        index(\"test_2\", \"type1\", \"2\", \"text\", \"aa cd\");\n-        index(\"test_2\", \"type1\", \"3\", \"text\", \"ab bd\");\n-        index(\"test_2\", \"type1\", \"4\", \"text\", \"ab cc\");\n-        index(\"test_2\", \"type1\", \"1\", \"text\", \"abcd\");\n-        index(\"test_2\", \"type1\", \"2\", \"text\", \"aacd\");\n-        index(\"test_2\", \"type1\", \"3\", \"text\", \"abbd\");\n-        index(\"test_2\", \"type1\", \"4\", \"text\", \"abcc\");\n-        refresh();\n-\n-        termSuggest = termSuggestion(\"test\")\n-                .suggestMode(\"always\") // Always, otherwise the results can vary between requests.\n-                .text(\"ab cd\")\n-                .minWordLength(1)\n-                .field(\"text\");\n-        logger.info(\"--> run suggestions with three indices\");\n-        try {\n-            searchSuggest(client(), termSuggest);\n-            fail(\" can not suggest across multiple indices with different analysis chains\");\n-        } catch (ReduceSearchPhaseException ex) {\n-            assertThat(ex.getCause(), instanceOf(ElasticSearchIllegalStateException.class));\n-            assertThat(ex.getCause().getMessage(), endsWith(\"Suggest entries have different sizes actual [1] expected [2]\"));\n-        } catch (ElasticSearchIllegalStateException ex) {\n-            assertThat(ex.getMessage(), endsWith(\"Suggest entries have different sizes actual [1] expected [2]\"));\n-        }\n-\n-\n-        termSuggest = termSuggestion(\"test\")\n-                .suggestMode(\"always\") // Always, otherwise the results can vary between requests.\n-                .text(\"ABCD\")\n-                .minWordLength(1)\n-                .field(\"text\");\n-        logger.info(\"--> run suggestions with four indices\");\n-        try {\n-            searchSuggest(client(), termSuggest);\n-            fail(\" can not suggest across multiple indices with different analysis chains\");\n-        } catch (ReduceSearchPhaseException ex) {\n-            assertThat(ex.getCause(), instanceOf(ElasticSearchIllegalStateException.class));\n-            assertThat(ex.getCause().getMessage(), endsWith(\"Suggest entries have different text actual [ABCD] expected [abcd]\"));\n-        } catch (ElasticSearchIllegalStateException ex) {\n-            assertThat(ex.getMessage(), endsWith(\"Suggest entries have different text actual [ABCD] expected [abcd]\"));\n-        }\n-\n-\n-    }\n-\n     @Test // see #3037\n     public void testSuggestModes() throws IOException {\n         CreateIndexRequestBuilder builder = prepareCreate(\"test\").setSettings(settingsBuilder()\ndiff --git a/src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java b/src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java\nindex 9d36a3b60a3..5c68553b285 100644\n--- a/src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java\n+++ b/src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java\n@@ -69,16 +69,16 @@ public class RefreshStressTest1 {\n //      Thread.sleep(100);\n \n             System.out.println(\"searching \" + loop);\n-            SearchResponse result = client.prepareSearch(indexName).setPostFilter(FilterBuilders.termFilter(\"name\", name)).execute().actionGet();\n+            SearchResponse result = client.prepareSearch(indexName).setFilter(FilterBuilders.termFilter(\"name\", name)).execute().actionGet();\n             if (result.getHits().hits().length != 1) {\n                 for (int i = 1; i <= 100; i++) {\n                     System.out.println(\"retry \" + loop + \", \" + i + \", previous total hits: \" + result.getHits().getTotalHits());\n                     client.admin().indices().prepareRefresh(indexName).execute().actionGet();\n                     Thread.sleep(100);\n-                    result = client.prepareSearch(indexName).setPostFilter(FilterBuilders.termFilter(\"name\", name)).execute().actionGet();\n+                    result = client.prepareSearch(indexName).setFilter(FilterBuilders.termFilter(\"name\", name)).execute().actionGet();\n                     if (result.getHits().hits().length == 1) {\n                         client.admin().indices().prepareRefresh(indexName).execute().actionGet();\n-                        result = client.prepareSearch(indexName).setPostFilter(FilterBuilders.termFilter(\"name\", name)).execute().actionGet();\n+                        result = client.prepareSearch(indexName).setFilter(FilterBuilders.termFilter(\"name\", name)).execute().actionGet();\n                         throw new RuntimeException(\"Record found after \" + (i * 100) + \" ms, second go: \" + result.getHits().hits().length);\n                     } else if (i == 100) {\n                         if (client.prepareGet(indexName, typeName, id).execute().actionGet().isExists())\ndiff --git a/src/test/java/org/elasticsearch/test/ElasticsearchAllocationTestCase.java b/src/test/java/org/elasticsearch/test/ElasticsearchAllocationTestCase.java\ndeleted file mode 100644\nindex 57c0f56770d..00000000000\n--- a/src/test/java/org/elasticsearch/test/ElasticsearchAllocationTestCase.java\n+++ /dev/null\n@@ -1,84 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.elasticsearch.test;\n-\n-import com.google.common.collect.ImmutableSet;\n-import org.elasticsearch.cluster.ClusterInfoService;\n-import org.elasticsearch.cluster.routing.allocation.AllocationService;\n-import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;\n-import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n-import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;\n-import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecidersModule;\n-import org.elasticsearch.common.settings.ImmutableSettings;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.node.settings.NodeSettingsService;\n-\n-import java.lang.reflect.Constructor;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Random;\n-\n-import static org.hamcrest.CoreMatchers.equalTo;\n-import static org.hamcrest.CoreMatchers.is;\n-\n-/**\n- */\n-public class ElasticsearchAllocationTestCase extends ElasticsearchTestCase {\n-\n-    public static AllocationService createAllocationService() {\n-        return createAllocationService(ImmutableSettings.Builder.EMPTY_SETTINGS);\n-    }\n-\n-    public static AllocationService createAllocationService(Settings settings) {\n-        return createAllocationService(settings, getRandom())    ;\n-    }\n-\n-    public static AllocationService createAllocationService(Settings settings, Random random) {\n-        return new AllocationService(settings,\n-                randomAllocationDeciders(settings, new NodeSettingsService(ImmutableSettings.Builder.EMPTY_SETTINGS), random),\n-                new ShardsAllocators(settings), ClusterInfoService.EMPTY);\n-    }\n-\n-    public static AllocationDeciders randomAllocationDeciders(Settings settings, NodeSettingsService nodeSettingsService, Random random) {\n-        final ImmutableSet<Class<? extends AllocationDecider>> defaultAllocationDeciders = AllocationDecidersModule.DEFAULT_ALLOCATION_DECIDERS;\n-        final List<AllocationDecider> list = new ArrayList<AllocationDecider>();\n-        for (Class<? extends AllocationDecider> deciderClass : defaultAllocationDeciders) {\n-            try {\n-                try {\n-                    Constructor<? extends AllocationDecider> constructor = deciderClass.getConstructor(Settings.class, NodeSettingsService.class);\n-                    list.add(constructor.newInstance(settings, nodeSettingsService));\n-                } catch (NoSuchMethodException e) {\n-                    Constructor<? extends AllocationDecider> constructor = null;\n-                    constructor = deciderClass.getConstructor(Settings.class);\n-                    list.add(constructor.newInstance(settings));\n-                }\n-            } catch (Exception ex) {\n-                throw new RuntimeException(ex);\n-            }\n-        }\n-        assertThat(list.size(), equalTo(defaultAllocationDeciders.size()));\n-        for (AllocationDecider d : list) {\n-            assertThat(defaultAllocationDeciders.contains(d.getClass()), is(true));\n-        }\n-        Collections.shuffle(list, random);\n-        return new AllocationDeciders(settings, list.toArray(new AllocationDecider[0]));\n-\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java b/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java\nindex 7e573404a07..40ea83f459a 100644\n--- a/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java\n+++ b/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java\n@@ -18,6 +18,7 @@\n  */\n package org.elasticsearch.test;\n \n+import com.carrotsearch.randomizedtesting.SeedUtils;\n import com.google.common.base.Joiner;\n import org.apache.lucene.util.AbstractRandomizedTest;\n import org.elasticsearch.ExceptionsHelper;\n@@ -67,8 +68,6 @@ import java.util.concurrent.CopyOnWriteArrayList;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ExecutionException;\n \n-import static org.elasticsearch.test.TestCluster.SHARED_CLUSTER_SEED;\n-import static org.elasticsearch.test.TestCluster.clusterName;\n import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;\n import static org.hamcrest.Matchers.emptyIterable;\n@@ -124,7 +123,7 @@ import static org.hamcrest.Matchers.equalTo;\n  *     This class supports the following system properties (passed with -Dkey=value to the application)\n  *   <ul>\n  *   <li>-D{@value #TESTS_CLIENT_RATIO} - a double value in the interval [0..1] which defines the ration between node and transport clients used</li>\n- *   <li>-D{@value TestCluster#TESTS_CLUSTER_SEED} - a random seed used to initialize the clusters random context.\n+ *   <li>-D{@value #TESTS_CLUSTER_SEED} - a random seed used to initialize the clusters random context.\n  *   <li>-D{@value #INDEX_SEED_SETTING} - a random seed used to initialize the index random context.\n  *   </ul>\n  * </p>\n@@ -133,13 +132,24 @@ import static org.hamcrest.Matchers.equalTo;\n @AbstractRandomizedTest.IntegrationTests\n public abstract class ElasticsearchIntegrationTest extends ElasticsearchTestCase {\n \n-    private static final TestCluster GLOBAL_CLUSTER = new TestCluster(SHARED_CLUSTER_SEED, clusterName(\"shared\", ElasticsearchTestCase.CHILD_VM_ID, SHARED_CLUSTER_SEED));\n+\n+    /**\n+     * The random seed for the shared  test cluster used in the current JVM.\n+     */\n+    public static final long SHARED_CLUSTER_SEED = clusterSeed();\n+\n+    private static final TestCluster GLOBAL_CLUSTER = new TestCluster(SHARED_CLUSTER_SEED, TestCluster.clusterName(\"shared\", ElasticsearchTestCase.CHILD_VM_ID, SHARED_CLUSTER_SEED));\n \n     /**\n      * Key used to set the transport client ratio via the commandline -D{@value #TESTS_CLIENT_RATIO}\n      */\n     public static final String TESTS_CLIENT_RATIO = \"tests.client.ratio\";\n \n+    /**\n+     * Key used to set the shared cluster random seed via the commandline -D{@value #TESTS_CLUSTER_SEED}\n+     */\n+    public static final String TESTS_CLUSTER_SEED = \"tests.cluster_seed\";\n+\n     /**\n      * Key used to retrieve the index random seed from the index settings on a running node.\n      * The value of this seed can be used to initialize a random context for a specific index.\n@@ -820,7 +830,7 @@ public abstract class ElasticsearchIntegrationTest extends ElasticsearchTestCase\n             };\n         }\n \n-        return new TestCluster(currentClusterSeed, numNodes, clusterName(scope.name(), ElasticsearchTestCase.CHILD_VM_ID, currentClusterSeed), nodeSettingsSource);\n+        return new TestCluster(currentClusterSeed, numNodes, TestCluster.clusterName(scope.name(), ElasticsearchTestCase.CHILD_VM_ID, currentClusterSeed), nodeSettingsSource);\n     }\n \n     /**\n@@ -849,6 +859,14 @@ public abstract class ElasticsearchIntegrationTest extends ElasticsearchTestCase\n         double transportClientRatio() default -1;\n     }\n     \n+    private static long clusterSeed() {\n+        String property = System.getProperty(TESTS_CLUSTER_SEED);\n+        if (property == null || property.isEmpty()) {\n+            return System.nanoTime();\n+        }\n+        return SeedUtils.parseSeed(property);\n+    }\n+\n     /**\n      *  Returns the client ratio configured via\n      */\ndiff --git a/src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java b/src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java\nindex 1d0df441ad9..16448a3d417 100644\n--- a/src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java\n+++ b/src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java\n@@ -18,11 +18,8 @@\n  */\n package org.elasticsearch.test;\n \n-import com.carrotsearch.randomizedtesting.annotations.Listeners;\n-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;\n-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope;\n+import com.carrotsearch.randomizedtesting.annotations.*;\n import com.carrotsearch.randomizedtesting.annotations.ThreadLeakScope.Scope;\n-import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;\n import com.google.common.base.Predicate;\n import com.google.common.collect.ImmutableList;\n import org.apache.lucene.store.MockDirectoryWrapper;\n@@ -34,8 +31,8 @@ import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n import org.elasticsearch.common.util.concurrent.EsAbortPolicy;\n import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;\n-import org.elasticsearch.test.engine.MockRobinEngine;\n import org.elasticsearch.test.junit.listeners.LoggingListener;\n+import org.elasticsearch.test.engine.MockRobinEngine;\n import org.elasticsearch.test.store.MockDirectoryHelper;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n@@ -55,20 +52,20 @@ import java.util.concurrent.TimeUnit;\n  */\n @ThreadLeakFilters(defaultFilters = true, filters = {ElasticsearchThreadFilter.class})\n @ThreadLeakScope(Scope.NONE)\n-@TimeoutSuite(millis = 20 * TimeUnits.MINUTE) // timeout the suite after 20min and fail the test.\n+@TimeoutSuite(millis = TimeUnits.HOUR) // timeout the suite after 1h and fail the test.\n @Listeners(LoggingListener.class)\n public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n \n     private static Thread.UncaughtExceptionHandler defaultHandler;\n-\n+    \n     protected final ESLogger logger = Loggers.getLogger(getClass());\n \n     public static final String CHILD_VM_ID = System.getProperty(\"junit4.childvm.id\", \"\" + System.currentTimeMillis());\n-\n+    \n     public static boolean awaitBusy(Predicate<?> breakPredicate) throws InterruptedException {\n         return awaitBusy(breakPredicate, 10, TimeUnit.SECONDS);\n     }\n-\n+    \n     public static boolean awaitBusy(Predicate<?> breakPredicate, long maxWaitTime, TimeUnit unit) throws InterruptedException {\n         long maxTimeInMillis = TimeUnit.MILLISECONDS.convert(maxWaitTime, unit);\n         long iterations = Math.max(Math.round(Math.log10(maxTimeInMillis) / Math.log10(2)), 1);\n@@ -86,9 +83,9 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n         Thread.sleep(Math.max(timeInMillis, 0));\n         return breakPredicate.apply(null);\n     }\n-\n-    private static final String[] numericTypes = new String[]{\"byte\", \"short\", \"integer\", \"long\"};\n-\n+    \n+    private static final String[] numericTypes = new String[] {\"byte\", \"short\", \"integer\", \"long\"};\n+    \n     public static String randomNumericType(Random random) {\n         return numericTypes[random.nextInt(numericTypes.length)];\n     }\n@@ -104,7 +101,7 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n         URI uri = URI.create(getClass().getResource(relativePath).toString());\n         return new File(uri);\n     }\n-\n+    \n     public static void ensureAllFilesClosed() throws IOException {\n         try {\n             for (MockDirectoryHelper.ElasticsearchMockDirectoryWrapper w : MockDirectoryHelper.wrappers) {\n@@ -116,7 +113,7 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n             forceClearMockWrappers();\n         }\n     }\n-\n+    \n     public static void ensureAllSearchersClosed() {\n         /* in some cases we finish a test faster than the freeContext calls make it to the\n          * shards. Let's wait for some time if there are still searchers. If the are really \n@@ -147,11 +144,11 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n             MockRobinEngine.INFLIGHT_ENGINE_SEARCHERS.clear();\n         }\n     }\n-\n+    \n     public static void forceClearMockWrappers() {\n         MockDirectoryHelper.wrappers.clear();\n     }\n-\n+    \n     public static boolean hasUnclosedWrapper() {\n         for (MockDirectoryWrapper w : MockDirectoryHelper.wrappers) {\n             if (w.isOpen()) {\n@@ -169,7 +166,7 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n                 ensureAllFilesClosed();\n             }\n         });\n-\n+        \n         closeAfterSuite(new Closeable() {\n             @Override\n             public void close() throws IOException {\n@@ -182,15 +179,15 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n \n     @AfterClass\n     public static void resetUncaughtExceptionHandler() {\n-        Thread.setDefaultUncaughtExceptionHandler(defaultHandler);\n+       Thread.setDefaultUncaughtExceptionHandler(defaultHandler);\n     }\n \n     public static boolean maybeDocValues() {\n         return LuceneTestCase.defaultCodecSupportsSortedSet() && randomBoolean();\n     }\n-\n+    \n     private static final List<Version> SORTED_VERSIONS;\n-\n+    \n     static {\n         Field[] declaredFields = Version.class.getDeclaredFields();\n         Set<Integer> ids = new HashSet<Integer>();\n@@ -216,17 +213,17 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n         }\n         SORTED_VERSIONS = version.build();\n     }\n-\n+    \n     public static Version getPreviousVersion() {\n         Version version = SORTED_VERSIONS.get(1);\n         assert version.before(Version.CURRENT);\n         return version;\n     }\n-\n+    \n     public static Version randomVersion() {\n         return randomVersion(getRandom());\n     }\n-\n+    \n     public static Version randomVersion(Random random) {\n         return SORTED_VERSIONS.get(random.nextInt(SORTED_VERSIONS.size()));\n     }\n@@ -249,7 +246,7 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n                 }\n             } else if (e instanceof OutOfMemoryError) {\n                 if (e.getMessage().contains(\"unable to create new native thread\")) {\n-                    printStackDump(logger);\n+                   printStackDump(logger);\n                 }\n             }\n             parent.uncaughtException(t, e);\n@@ -266,11 +263,11 @@ public abstract class ElasticsearchTestCase extends AbstractRandomizedTest {\n     /**\n      * Dump threads and their current stack trace.\n      */\n-    private static String formatThreadStacks(Map<Thread, StackTraceElement[]> threads) {\n+    private static String formatThreadStacks(Map<Thread,StackTraceElement[]> threads) {\n         StringBuilder message = new StringBuilder();\n         int cnt = 1;\n         final Formatter f = new Formatter(message, Locale.ENGLISH);\n-        for (Map.Entry<Thread, StackTraceElement[]> e : threads.entrySet()) {\n+        for (Map.Entry<Thread,StackTraceElement[]> e : threads.entrySet()) {\n             if (e.getKey().isAlive())\n                 f.format(Locale.ENGLISH, \"\\n  %2d) %s\", cnt++, threadName(e.getKey())).flush();\n             if (e.getValue().length == 0) {\ndiff --git a/src/test/java/org/elasticsearch/test/TestCluster.java b/src/test/java/org/elasticsearch/test/TestCluster.java\nindex 3fbefd816d5..101a18fbd49 100644\n--- a/src/test/java/org/elasticsearch/test/TestCluster.java\n+++ b/src/test/java/org/elasticsearch/test/TestCluster.java\n@@ -33,7 +33,6 @@ import org.elasticsearch.cluster.ClusterState;\n import org.elasticsearch.cluster.node.DiscoveryNode;\n import org.elasticsearch.cluster.node.DiscoveryNodes;\n import org.elasticsearch.cluster.routing.ShardRouting;\n-import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.io.FileSystemUtils;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n@@ -49,7 +48,6 @@ import org.elasticsearch.node.internal.InternalNode;\n import org.elasticsearch.test.engine.MockEngineModule;\n import org.elasticsearch.test.store.MockFSIndexStoreModule;\n import org.elasticsearch.test.transport.AssertingLocalTransportModule;\n-import org.elasticsearch.transport.Transport;\n import org.elasticsearch.transport.TransportModule;\n import org.elasticsearch.transport.TransportService;\n import org.junit.Assert;\n@@ -79,24 +77,6 @@ public final class TestCluster implements Iterable<Client> {\n \n     private final ESLogger logger = Loggers.getLogger(getClass());\n \n-    /**\n-     * The random seed for the shared  test cluster used in the current JVM.\n-     */\n-    public static final long SHARED_CLUSTER_SEED = clusterSeed();\n-\n-    /**\n-     * Key used to set the shared cluster random seed via the commandline -D{@value #TESTS_CLUSTER_SEED}\n-     */\n-    public static final String TESTS_CLUSTER_SEED = \"tests.cluster_seed\";\n-\n-    private static long clusterSeed() {\n-        String property = System.getProperty(TESTS_CLUSTER_SEED);\n-        if (!Strings.hasLength(property)) {\n-            return System.nanoTime();\n-        }\n-        return SeedUtils.parseSeed(property);\n-    }\n-\n     /* sorted map to make traverse order reproducible */\n     private final TreeMap<String, NodeAndClient> nodes = newTreeMap();\n \n@@ -128,10 +108,6 @@ public final class TestCluster implements Iterable<Client> {\n         this(clusterSeed, -1, clusterName, NodeSettingsSource.EMPTY);\n     }\n \n-    public TestCluster(long clusterSeed, int numNodes, String clusterName) {\n-        this(clusterSeed, numNodes, clusterName, NodeSettingsSource.EMPTY);\n-    }\n-\n     TestCluster(long clusterSeed, int numNodes, String clusterName, NodeSettingsSource nodeSettingsSource) {\n         this.clusterName = clusterName;\n         Random random = new Random(clusterSeed);\n@@ -161,8 +137,6 @@ public final class TestCluster implements Iterable<Client> {\n         .put(\"gateway.type\", \"none\");\n         if (isLocalTransportConfigured()) {\n             builder.put(TransportModule.TRANSPORT_TYPE_KEY, AssertingLocalTransportModule.class.getName());\n-        } else {\n-            builder.put(Transport.TransportSettings.TRANSPORT_TCP_COMPRESS, random.nextInt(10) == 0);\n         }\n         this.defaultSettings = builder.build();\n         this.nodeSettingsSource = nodeSettingsSource;\n@@ -187,7 +161,7 @@ public final class TestCluster implements Iterable<Client> {\n         return builder.build();\n     }\n \n-    public static String clusterName(String prefix, String childVMId, long clusterSeed) {\n+    static String clusterName(String prefix, String childVMId, long clusterSeed) {\n         StringBuilder builder = new StringBuilder(prefix);\n         builder.append('-').append(NetworkUtils.getLocalAddress().getHostName());\n         builder.append(\"-CHILD_VM=[\").append(childVMId).append(']');\n@@ -402,7 +376,7 @@ public final class TestCluster implements Iterable<Client> {\n         return null;\n     }\n \n-    public void close() {\n+    void close() {\n         ensureOpen();\n         if (this.open.compareAndSet(true, false)) {\n             IOUtils.closeWhileHandlingException(nodes.values());\n@@ -577,7 +551,7 @@ public final class TestCluster implements Iterable<Client> {\n     /**\n      * This method should be executed before each test to reset the cluster to it's initial state.\n      */\n-    public synchronized void beforeTest(Random random, double transportClientRatio) {\n+    synchronized void beforeTest(Random random, double transportClientRatio) {\n         reset(random, true, transportClientRatio);\n     }\n \n@@ -642,7 +616,7 @@ public final class TestCluster implements Iterable<Client> {\n     /**\n      * This method should be executed during tearDown\n      */\n-    public synchronized void afterTest() {\n+    synchronized void afterTest() {\n         wipeDataDirectories();\n         resetClients(); /* reset all clients - each test gets its own client based on the Random instance created above. */\n \ndiff --git a/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java b/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java\nindex 56f2c231fc2..9b90589fe17 100644\n--- a/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java\n+++ b/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java\n@@ -4,7 +4,6 @@ import com.carrotsearch.randomizedtesting.RandomizedContext;\n import com.carrotsearch.randomizedtesting.ReproduceErrorMessageBuilder;\n import com.carrotsearch.randomizedtesting.SeedUtils;\n import com.carrotsearch.randomizedtesting.TraceFormatting;\n-import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.logging.ESLogger;\n import org.elasticsearch.common.logging.Loggers;\n import org.elasticsearch.test.ElasticsearchIntegrationTest;\n@@ -14,9 +13,9 @@ import org.junit.runner.Description;\n import org.junit.runner.notification.Failure;\n import org.junit.runner.notification.RunListener;\n \n-import static com.carrotsearch.randomizedtesting.SysGlobals.SYSPROP_ITERATIONS;\n-import static org.elasticsearch.test.TestCluster.SHARED_CLUSTER_SEED;\n-import static org.elasticsearch.test.TestCluster.TESTS_CLUSTER_SEED;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Set;\n \n /**\n  * A {@link RunListener} that emits to {@link System#err} a string with command\n@@ -47,44 +46,26 @@ public class ReproduceInfoPrinter extends RunListener {\n         final StringBuilder b = new StringBuilder();\n         b.append(\"FAILURE  : \").append(d.getDisplayName()).append(\"\\n\");\n         b.append(\"REPRODUCE WITH  : mvn test\");\n-        ReproduceErrorMessageBuilder builder = reproduceErrorMessageBuilder(b).appendAllOpts(failure.getDescription());\n-\n-        if (mustAppendClusterSeed(failure)) {\n-            appendClusterSeed(builder);\n+        ReproduceErrorMessageBuilder builder = new MavenMessageBuilder(b).appendAllOpts(failure.getDescription());\n+        if (ElasticsearchIntegrationTest.class.isAssignableFrom(failure.getDescription().getTestClass())) {\n+            builder.appendOpt(\"tests.cluster_seed\", SeedUtils.formatSeed(ElasticsearchIntegrationTest.SHARED_CLUSTER_SEED));\n         }\n \n         b.append(\"\\n\");\n         b.append(\"Throwable:\\n\");\n         if (failure.getException() != null) {\n-            traces().formatThrowable(b, failure.getException());\n+            TraceFormatting traces = new TraceFormatting();\n+            try {\n+                traces = RandomizedContext.current().getRunner().getTraceFormatting();\n+            } catch (IllegalStateException e) {\n+                // Ignore if no context.\n+            }\n+            traces.formatThrowable(b, failure.getException());\n         }\n-\n         logger.error(b.toString());\n     }\n \n-    protected boolean mustAppendClusterSeed(Failure failure) {\n-        return ElasticsearchIntegrationTest.class.isAssignableFrom(failure.getDescription().getTestClass());\n-    }\n-\n-    protected void appendClusterSeed(ReproduceErrorMessageBuilder builder) {\n-        builder.appendOpt(TESTS_CLUSTER_SEED, SeedUtils.formatSeed(SHARED_CLUSTER_SEED));\n-    }\n-\n-    protected ReproduceErrorMessageBuilder reproduceErrorMessageBuilder(StringBuilder b) {\n-        return new MavenMessageBuilder(b);\n-    }\n-\n-    protected TraceFormatting traces() {\n-        TraceFormatting traces = new TraceFormatting();\n-        try {\n-            traces = RandomizedContext.current().getRunner().getTraceFormatting();\n-        } catch (IllegalStateException e) {\n-            // Ignore if no context.\n-        }\n-        return traces;\n-    }\n-\n-    protected static class MavenMessageBuilder extends ReproduceErrorMessageBuilder {\n+    private static class MavenMessageBuilder extends ReproduceErrorMessageBuilder {\n \n         public MavenMessageBuilder(StringBuilder b) {\n             super(b);\n@@ -99,32 +80,26 @@ public class ReproduceInfoPrinter extends RunListener {\n         /**\n          * Append a single VM option.\n          */\n-        @Override\n         public ReproduceErrorMessageBuilder appendOpt(String sysPropName, String value) {\n-            if (sysPropName.equals(SYSPROP_ITERATIONS())) { // we don't want the iters to be in there!\n+            if (sysPropName.equals(\"tests.iters\")) { // we don't want the iters to be in there!\n                 return this;\n             }\n-            if (Strings.hasLength(value)) {\n+            if (value != null && !value.isEmpty()) {\n                 return super.appendOpt(sysPropName, value);\n             } \n             return this;\n         }\n \n         public ReproduceErrorMessageBuilder appendESProperties() {\n-            appendProperties(\"es.logger.level\", \"es.node.mode\", \"es.node.local\");\n-\n-            if (System.getProperty(\"tests.jvm.argline\") != null && !System.getProperty(\"tests.jvm.argline\").isEmpty()) {\n-                appendOpt(\"tests.jvm.argline\", \"\\\"\" + System.getProperty(\"tests.jvm.argline\") + \"\\\"\");\n-            }\n-            return this;\n-        }\n-\n-        protected ReproduceErrorMessageBuilder appendProperties(String... properties) {\n-            for (String sysPropName : properties) {\n-                if (Strings.hasLength(System.getProperty(sysPropName))) {\n+            for (String sysPropName : Arrays.asList(\n+                    \"es.logger.level\", \"es.node.mode\", \"es.node.local\")) {\n+                if (System.getProperty(sysPropName) != null && !System.getProperty(sysPropName).isEmpty()) {\n                     appendOpt(sysPropName, System.getProperty(sysPropName));\n                 }\n             }\n+            if (System.getProperty(\"tests.jvm.argline\") != null && !System.getProperty(\"tests.jvm.argline\").isEmpty()) {\n+                appendOpt(\"tests.jvm.argline\", \"\\\"\" + System.getProperty(\"tests.jvm.argline\") + \"\\\"\");\n+            }\n             return this;\n         }\n \ndiff --git a/src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java b/src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java\ndeleted file mode 100644\nindex 08860210ca3..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java\n+++ /dev/null\n@@ -1,37 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest;\n-\n-import org.elasticsearch.test.rest.junit.RestTestSuiteRunner;\n-import org.junit.runner.RunWith;\n-\n-import static org.apache.lucene.util.LuceneTestCase.Slow;\n-\n-/**\n- * Runs the clients test suite against an elasticsearch node, which can be an external node or an automatically created cluster.\n- * Communicates with elasticsearch exclusively via REST layer.\n- *\n- * @see RestTestSuiteRunner for extensive documentation and all the supported options\n- */\n-@Slow\n-@RunWith(RestTestSuiteRunner.class)\n-public class ElasticsearchRestTests {\n-\n-\n-}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/test/rest/RestTestExecutionContext.java b/src/test/java/org/elasticsearch/test/rest/RestTestExecutionContext.java\ndeleted file mode 100644\nindex aad2b73fc68..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/RestTestExecutionContext.java\n+++ /dev/null\n@@ -1,159 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest;\n-\n-import com.google.common.collect.Maps;\n-import org.elasticsearch.common.Strings;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.rest.client.RestClient;\n-import org.elasticsearch.test.rest.client.RestException;\n-import org.elasticsearch.test.rest.client.RestResponse;\n-import org.elasticsearch.test.rest.spec.RestSpec;\n-\n-import java.io.Closeable;\n-import java.io.IOException;\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-/**\n- * Execution context passed across the REST tests.\n- * Holds the REST client used to communicate with elasticsearch.\n- * Caches the last obtained test response and allows to stash part of it within variables\n- * that can be used as input values in following requests.\n- */\n-public class RestTestExecutionContext implements Closeable {\n-\n-    private static final ESLogger logger = Loggers.getLogger(RestTestExecutionContext.class);\n-\n-    private final RestClient restClient;\n-\n-    private final String esVersion;\n-\n-    private final Map<String, Object> stash = Maps.newHashMap();\n-\n-    private RestResponse response;\n-\n-    public RestTestExecutionContext(String host, int port, RestSpec restSpec) throws RestException, IOException {\n-\n-        this.restClient = new RestClient(host, port, restSpec);\n-        this.esVersion = restClient.getEsVersion();\n-    }\n-\n-    /**\n-     * Calls an elasticsearch api with the parameters and request body provided as arguments.\n-     * Saves the obtained response in the execution context.\n-     * @throws RestException if the returned status code is non ok\n-     */\n-    public RestResponse callApi(String apiName, Map<String, String> params, String body) throws IOException, RestException  {\n-        //makes a copy of the parameters before modifying them for this specific request\n-        HashMap<String, String> requestParams = Maps.newHashMap(params);\n-        for (Map.Entry<String, String> entry : requestParams.entrySet()) {\n-            if (isStashed(entry.getValue())) {\n-                entry.setValue(unstash(entry.getValue()).toString());\n-            }\n-        }\n-        try {\n-            return response = callApiInternal(apiName, requestParams, body);\n-        } catch(RestException e) {\n-            response = e.restResponse();\n-            throw e;\n-        }\n-    }\n-\n-    /**\n-     * Calls an elasticsearch api internally without saving the obtained response in the context.\n-     * Useful for internal calls (e.g. delete index during teardown)\n-     * @throws RestException if the returned status code is non ok\n-     */\n-    public RestResponse callApiInternal(String apiName, String... params) throws IOException, RestException {\n-        return restClient.callApi(apiName, params);\n-    }\n-\n-    private RestResponse callApiInternal(String apiName, Map<String, String> params, String body) throws IOException, RestException  {\n-        return restClient.callApi(apiName, params, body);\n-    }\n-\n-    /**\n-     * Extracts a specific value from the last saved response\n-     */\n-    public Object response(String path) throws IOException {\n-        return response.evaluate(path);\n-    }\n-\n-    /**\n-     * Clears the last obtained response and the stashed fields\n-     */\n-    public void clear() {\n-        logger.debug(\"resetting response and stash\");\n-        response = null;\n-        stash.clear();\n-    }\n-\n-    /**\n-     * Tells whether a particular value needs to be looked up in the stash\n-     * The stash contains fields eventually extracted from previous responses that can be reused\n-     * as arguments for following requests (e.g. scroll_id)\n-     */\n-    public boolean isStashed(Object key) {\n-        if (key == null) {\n-            return false;\n-        }\n-        String stashKey = key.toString();\n-        return Strings.hasLength(stashKey) && stashKey.startsWith(\"$\");\n-    }\n-\n-    /**\n-     * Extracts a value from the current stash\n-     * The stash contains fields eventually extracted from previous responses that can be reused\n-     * as arguments for following requests (e.g. scroll_id)\n-     */\n-    public Object unstash(String value) {\n-        Object stashedValue = stash.get(value.substring(1));\n-        if (stashedValue == null) {\n-            throw new IllegalArgumentException(\"stashed value not found for key [\" + value + \"]\");\n-        }\n-        return stashedValue;\n-    }\n-\n-    /**\n-     * Allows to saved a specific field in the stash as key-value pair\n-     */\n-    public void stash(String key, Object value) {\n-        logger.debug(\"stashing [{}]=[{}]\", key, value);\n-        Object old = stash.put(key, value);\n-        if (old != null && old != value) {\n-            logger.trace(\"replaced stashed value [{}] with same key [{}]\", old, key);\n-        }\n-    }\n-\n-    /**\n-     * Returns the current es version as a string\n-     */\n-    public String esVersion() {\n-        return esVersion;\n-    }\n-\n-    /**\n-     * Closes the execution context and releases the underlying resources\n-     */\n-    public void close() {\n-        this.restClient.close();\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/client/RestClient.java b/src/test/java/org/elasticsearch/test/rest/client/RestClient.java\ndeleted file mode 100644\nindex a8ef9ecb752..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/client/RestClient.java\n+++ /dev/null\n@@ -1,206 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.client;\n-\n-import com.carrotsearch.randomizedtesting.RandomizedTest;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import org.apache.http.impl.client.CloseableHttpClient;\n-import org.apache.http.impl.client.HttpClients;\n-import org.elasticsearch.common.Strings;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.rest.client.http.HttpRequestBuilder;\n-import org.elasticsearch.test.rest.client.http.HttpResponse;\n-import org.elasticsearch.test.rest.spec.RestApi;\n-import org.elasticsearch.test.rest.spec.RestSpec;\n-\n-import java.io.Closeable;\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.Map;\n-\n-/**\n- * REST client used to test the elasticsearch REST layer\n- * Holds the {@link RestSpec} used to translate api calls into REST calls\n- */\n-public class RestClient implements Closeable {\n-\n-    private static final ESLogger logger = Loggers.getLogger(RestClient.class);\n-\n-    private final RestSpec restSpec;\n-    private final CloseableHttpClient httpClient;\n-\n-    private final String host;\n-    private final int port;\n-\n-    private final String esVersion;\n-\n-    public RestClient(String host, int port, RestSpec restSpec) throws IOException, RestException {\n-        this.restSpec = restSpec;\n-        this.httpClient = createHttpClient();\n-        this.host = host;\n-        this.port = port;\n-        this.esVersion = readVersion();\n-        logger.info(\"REST client initialized [{}:{}], elasticsearch version: [{}]\", host, port, esVersion);\n-    }\n-\n-    private String readVersion() throws IOException, RestException {\n-        //we make a manual call here without using callApi method, mainly because we are initializing\n-        //and the randomized context doesn't exist for the current thread (would be used to choose the method otherwise)\n-        RestApi restApi = restApi(\"info\");\n-        assert restApi.getPaths().size() == 1;\n-        assert restApi.getMethods().size() == 1;\n-        RestResponse restResponse = new RestResponse(httpRequestBuilder()\n-                .path(restApi.getPaths().get(0))\n-                .method(restApi.getMethods().get(0)).execute());\n-        checkStatusCode(restResponse);\n-        Object version = restResponse.evaluate(\"version.number\");\n-        if (version == null) {\n-            throw new RuntimeException(\"elasticsearch version not found in the response\");\n-        }\n-        return version.toString();\n-    }\n-\n-    public String getEsVersion() {\n-        return esVersion;\n-    }\n-\n-    /**\n-     * Calls an api with the provided parameters\n-     * @throws RestException if the obtained status code is non ok, unless the specific error code needs to be ignored\n-     * according to the ignore parameter received as input (which won't get sent to elasticsearch)\n-     */\n-    public RestResponse callApi(String apiName, String... params) throws IOException, RestException {\n-        if (params.length % 2 != 0) {\n-            throw new IllegalArgumentException(\"The number of params passed must be even but was [\" + params.length + \"]\");\n-        }\n-\n-        Map<String, String> paramsMap = Maps.newHashMap();\n-        for (int i = 0; i < params.length; i++) {\n-            paramsMap.put(params[i++], params[i]);\n-        }\n-\n-        return callApi(apiName, paramsMap, null);\n-    }\n-\n-    /**\n-     * Calls an api with the provided parameters and body\n-     * @throws RestException if the obtained status code is non ok, unless the specific error code needs to be ignored\n-     * according to the ignore parameter received as input (which won't get sent to elasticsearch)\n-     */\n-    public RestResponse callApi(String apiName, Map<String, String> params, String body) throws IOException, RestException {\n-\n-        List<Integer> ignores = Lists.newArrayList();\n-        Map<String, String> requestParams = null;\n-        if (params != null) {\n-            //makes a copy of the parameters before modifying them for this specific request\n-            requestParams = Maps.newHashMap(params);\n-            //ignore is a special parameter supported by the clients, shouldn't be sent to es\n-            String ignoreString = requestParams.remove(\"ignore\");\n-            if (Strings.hasLength(ignoreString)) {\n-                try {\n-                    ignores.add(Integer.valueOf(ignoreString));\n-                } catch(NumberFormatException e) {\n-                    throw new IllegalArgumentException(\"ignore value should be a number, found [\" + ignoreString + \"] instead\");\n-                }\n-            }\n-        }\n-\n-        HttpRequestBuilder httpRequestBuilder = callApiBuilder(apiName, requestParams, body);\n-        logger.debug(\"calling api [{}]\", apiName);\n-        HttpResponse httpResponse = httpRequestBuilder.execute();\n-\n-        //http HEAD doesn't support response body\n-        // For the few api (exists class of api) that use it we need to accept 404 too\n-        if (!httpResponse.supportsBody()) {\n-            ignores.add(404);\n-        }\n-\n-        RestResponse restResponse = new RestResponse(httpResponse);\n-        checkStatusCode(restResponse, ignores);\n-        return restResponse;\n-    }\n-\n-    private void checkStatusCode(RestResponse restResponse, List<Integer> ignores) throws RestException {\n-        //ignore is a catch within the client, to prevent the client from throwing error if it gets non ok codes back\n-        if (ignores.contains(restResponse.getStatusCode())) {\n-            if (logger.isDebugEnabled()) {\n-                logger.debug(\"ignored non ok status codes {} as requested\", ignores);\n-            }\n-            return;\n-        }\n-        checkStatusCode(restResponse);\n-    }\n-\n-    private void checkStatusCode(RestResponse restResponse) throws RestException {\n-        if (restResponse.isError()) {\n-            throw new RestException(\"non ok status code [\" + restResponse.getStatusCode() + \"] returned\", restResponse);\n-        }\n-    }\n-\n-    private HttpRequestBuilder callApiBuilder(String apiName, Map<String, String> params, String body) {\n-        RestApi restApi = restApi(apiName);\n-\n-        HttpRequestBuilder httpRequestBuilder = httpRequestBuilder().body(body);\n-\n-        //divide params between ones that go within query string and ones that go within path\n-        Map<String, String> pathParts = Maps.newHashMap();\n-        if (params != null) {\n-            for (Map.Entry<String, String> entry : params.entrySet()) {\n-                if (restApi.getPathParts().contains(entry.getKey())) {\n-                    pathParts.put(entry.getKey(), entry.getValue());\n-                } else {\n-                    httpRequestBuilder.addParam(entry.getKey(), entry.getValue());\n-                }\n-            }\n-        }\n-\n-        //the http method is randomized (out of the available ones with the chosen api)\n-        return httpRequestBuilder.method(RandomizedTest.randomFrom(restApi.getSupportedMethods(pathParts.keySet())))\n-                .path(restApi.getFinalPath(pathParts));\n-    }\n-\n-    private RestApi restApi(String apiName) {\n-        RestApi restApi = restSpec.getApi(apiName);\n-        if (restApi == null) {\n-            throw new IllegalArgumentException(\"rest api [\" + apiName + \"] doesn't exist in the rest spec\");\n-        }\n-        return restApi;\n-    }\n-\n-    protected HttpRequestBuilder httpRequestBuilder() {\n-        return new HttpRequestBuilder(httpClient).host(host).port(port);\n-    }\n-\n-    protected CloseableHttpClient createHttpClient() {\n-        return HttpClients.createDefault();\n-    }\n-\n-    /**\n-     * Closes the REST client and the underlying http client\n-     */\n-    public void close() {\n-        try {\n-            httpClient.close();\n-        } catch(IOException e) {\n-            logger.error(e.getMessage(), e);\n-        }\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/client/RestException.java b/src/test/java/org/elasticsearch/test/rest/client/RestException.java\ndeleted file mode 100644\nindex 5c03f4a2a21..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/client/RestException.java\n+++ /dev/null\n@@ -1,41 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.client;\n-\n-/**\n- * Thrown when a status code that holds an error is received (unless needs to be ignored)\n- * Holds the original {@link RestResponse}\n- */\n-public class RestException extends Exception {\n-\n-    private final RestResponse restResponse;\n-\n-    public RestException(String message, RestResponse restResponse) {\n-        super(message);\n-        this.restResponse = restResponse;\n-    }\n-\n-    public RestResponse restResponse() {\n-        return restResponse;\n-    }\n-\n-    public int statusCode() {\n-        return restResponse.getStatusCode();\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/client/RestResponse.java b/src/test/java/org/elasticsearch/test/rest/client/RestResponse.java\ndeleted file mode 100644\nindex 5e77044f181..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/client/RestResponse.java\n+++ /dev/null\n@@ -1,88 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.client;\n-\n-import org.elasticsearch.test.rest.client.http.HttpResponse;\n-import org.elasticsearch.test.rest.json.JsonPath;\n-\n-import java.io.IOException;\n-\n-/**\n- * Response obtained from a REST call\n- * Supports parsing the response body as json when needed and returning specific values extracted from it\n- */\n-public class RestResponse {\n-\n-    private final HttpResponse response;\n-    private JsonPath parsedResponse;\n-\n-    RestResponse(HttpResponse response) {\n-        this.response = response;\n-    }\n-\n-    public int getStatusCode() {\n-        return response.getStatusCode();\n-    }\n-\n-    public String getReasonPhrase() {\n-        return response.getReasonPhrase();\n-    }\n-\n-    public String getBody() {\n-        return response.getBody();\n-    }\n-\n-    public boolean isError() {\n-        return response.isError();\n-    }\n-\n-    /**\n-     * Parses the response body as json and extracts a specific value from it (identified by the provided path)\n-     */\n-    public Object evaluate(String path) throws IOException {\n-\n-        if (response == null) {\n-            return null;\n-        }\n-\n-        JsonPath jsonPath = parsedResponse();\n-\n-        if (jsonPath == null) {\n-            //special case: api that don't support body (e.g. exists) return true if 200, false if 404, even if no body\n-            //is_true: '' means the response had no body but the client returned true (caused by 200)\n-            //is_false: '' means the response had no body but the client returned false (caused by 404)\n-            if (\"\".equals(path) && !response.supportsBody()) {\n-                return !response.isError();\n-            }\n-            return null;\n-        }\n-\n-        return jsonPath.evaluate(path);\n-    }\n-\n-    private JsonPath parsedResponse() throws IOException {\n-        if (parsedResponse != null) {\n-            return parsedResponse;\n-        }\n-        if (response == null || !response.hasBody()) {\n-            return null;\n-        }\n-        return parsedResponse = new JsonPath(response.getBody());\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/client/http/HttpDeleteWithEntity.java b/src/test/java/org/elasticsearch/test/rest/client/http/HttpDeleteWithEntity.java\ndeleted file mode 100644\nindex e3ec79c3ebd..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/client/http/HttpDeleteWithEntity.java\n+++ /dev/null\n@@ -1,40 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.client.http;\n-\n-import org.apache.http.client.methods.HttpEntityEnclosingRequestBase;\n-\n-import java.net.URI;\n-\n-/**\n- * Allows to send DELETE requests providing a body (not supported out of the box)\n- */\n-public class HttpDeleteWithEntity extends HttpEntityEnclosingRequestBase {\n-\n-    public final static String METHOD_NAME = \"DELETE\";\n-\n-    public HttpDeleteWithEntity(final URI uri) {\n-        setURI(uri);\n-    }\n-\n-    @Override\n-    public String getMethod() {\n-        return METHOD_NAME;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/client/http/HttpGetWithEntity.java b/src/test/java/org/elasticsearch/test/rest/client/http/HttpGetWithEntity.java\ndeleted file mode 100644\nindex bf4dbfd0ef2..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/client/http/HttpGetWithEntity.java\n+++ /dev/null\n@@ -1,40 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.client.http;\n-\n-import org.apache.http.client.methods.HttpEntityEnclosingRequestBase;\n-\n-import java.net.URI;\n-\n-/**\n- * Allows to send GET requests providing a body (not supported out of the box)\n- */\n-public class HttpGetWithEntity extends HttpEntityEnclosingRequestBase {\n-\n-    public final static String METHOD_NAME = \"GET\";\n-\n-    public HttpGetWithEntity(final URI uri) {\n-        setURI(uri);\n-    }\n-\n-    @Override\n-    public String getMethod() {\n-        return METHOD_NAME;\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/test/rest/client/http/HttpRequestBuilder.java b/src/test/java/org/elasticsearch/test/rest/client/http/HttpRequestBuilder.java\ndeleted file mode 100644\nindex 5c0b84da721..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/client/http/HttpRequestBuilder.java\n+++ /dev/null\n@@ -1,186 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.client.http;\n-\n-import com.google.common.base.Joiner;\n-import com.google.common.collect.Maps;\n-import org.apache.http.client.methods.*;\n-import org.apache.http.entity.StringEntity;\n-import org.apache.http.impl.client.CloseableHttpClient;\n-import org.apache.lucene.util.IOUtils;\n-import org.elasticsearch.common.Strings;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-\n-import java.io.IOException;\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.nio.charset.Charset;\n-import java.util.Map;\n-\n-/**\n- * Executable builder for an http request\n- * Holds an {@link org.apache.http.client.HttpClient} that is used to send the built http request\n- */\n-public class HttpRequestBuilder {\n-\n-    private static final ESLogger logger = Loggers.getLogger(HttpRequestBuilder.class);\n-\n-    static final Charset DEFAULT_CHARSET = Charset.forName(\"utf-8\");\n-\n-    private final CloseableHttpClient httpClient;\n-\n-    private String host;\n-\n-    private int port;\n-\n-    private String path = \"\";\n-\n-    private final Map<String, String> params = Maps.newHashMap();\n-\n-    private String method = HttpGetWithEntity.METHOD_NAME;\n-\n-    private String body;\n-\n-    public HttpRequestBuilder(CloseableHttpClient httpClient) {\n-        this.httpClient = httpClient;\n-    }\n-\n-    public HttpRequestBuilder host(String host) {\n-        this.host = host;\n-        return this;\n-    }\n-\n-    public HttpRequestBuilder port(int port) {\n-        this.port = port;\n-        return this;\n-    }\n-\n-    public HttpRequestBuilder path(String path) {\n-        this.path = path;\n-        return this;\n-    }\n-\n-    public HttpRequestBuilder addParam(String name, String value) {\n-        this.params.put(name, value);\n-        return this;\n-    }\n-\n-    public HttpRequestBuilder method(String method) {\n-        this.method = method;\n-        return this;\n-    }\n-\n-    public HttpRequestBuilder body(String body) {\n-        if (Strings.hasLength(body)) {\n-            this.body = body;\n-        }\n-        return this;\n-    }\n-\n-    public HttpResponse execute() throws IOException {\n-        CloseableHttpResponse closeableHttpResponse = null;\n-        try {\n-            HttpUriRequest httpUriRequest = buildRequest();\n-            if (logger.isTraceEnabled()) {\n-                StringBuilder stringBuilder = new StringBuilder(httpUriRequest.getMethod()).append(\" \").append(httpUriRequest.getURI());\n-                if (Strings.hasLength(body)) {\n-                    stringBuilder.append(\"\\n\").append(body);\n-                }\n-                logger.trace(\"sending request \\n{}\", stringBuilder.toString());\n-            }\n-            closeableHttpResponse = httpClient.execute(httpUriRequest);\n-            HttpResponse httpResponse = new HttpResponse(httpUriRequest, closeableHttpResponse);\n-            logger.trace(\"got response \\n{}\\n{}\", closeableHttpResponse, httpResponse.hasBody() ? httpResponse.getBody() : \"\");\n-            return httpResponse;\n-        } finally {\n-            try {\n-                IOUtils.close(closeableHttpResponse);\n-            } catch (IOException e) {\n-                logger.error(\"error closing http response\", e);\n-            }\n-        }\n-    }\n-\n-    private HttpUriRequest buildRequest() {\n-\n-        if (HttpGetWithEntity.METHOD_NAME.equalsIgnoreCase(method)) {\n-            return addOptionalBody(new HttpGetWithEntity(buildUri()));\n-        }\n-\n-        if (HttpHead.METHOD_NAME.equalsIgnoreCase(method)) {\n-            checkBodyNotSupported();\n-            return new HttpHead(buildUri());\n-        }\n-\n-        if (HttpDeleteWithEntity.METHOD_NAME.equalsIgnoreCase(method)) {\n-            return addOptionalBody(new HttpDeleteWithEntity(buildUri()));\n-        }\n-\n-        if (HttpPut.METHOD_NAME.equalsIgnoreCase(method)) {\n-            return addOptionalBody(new HttpPut(buildUri()));\n-        }\n-\n-        if (HttpPost.METHOD_NAME.equalsIgnoreCase(method)) {\n-            return addOptionalBody(new HttpPost(buildUri()));\n-        }\n-\n-        throw new UnsupportedOperationException(\"method [\" + method + \"] not supported\");\n-    }\n-\n-    private URI buildUri() {\n-        String query;\n-        if (params.size() == 0) {\n-            query = null;\n-        } else {\n-            query = Joiner.on('&').withKeyValueSeparator(\"=\").join(params);\n-        }\n-        try {\n-            return new URI(\"http\", null, host, port, path, query, null);\n-        } catch (URISyntaxException e) {\n-            throw new IllegalArgumentException(e);\n-        }\n-    }\n-\n-    private HttpEntityEnclosingRequestBase addOptionalBody(HttpEntityEnclosingRequestBase requestBase) {\n-        if (Strings.hasText(body)) {\n-            requestBase.setEntity(new StringEntity(body, DEFAULT_CHARSET));\n-        }\n-        return requestBase;\n-    }\n-\n-    private void checkBodyNotSupported() {\n-        if (Strings.hasText(body)) {\n-            throw new IllegalArgumentException(\"request body not supported with head request\");\n-        }\n-    }\n-\n-    @Override\n-    public String toString() {\n-        StringBuilder stringBuilder = new StringBuilder(method).append(\" '\")\n-                .append(host).append(\":\").append(port).append(path).append(\"'\");\n-        if (!params.isEmpty()) {\n-            stringBuilder.append(\", params=\").append(params);\n-        }\n-        if (Strings.hasLength(body)) {\n-            stringBuilder.append(\", body=\\n\").append(body);\n-        }\n-        return stringBuilder.toString();\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/client/http/HttpResponse.java b/src/test/java/org/elasticsearch/test/rest/client/http/HttpResponse.java\ndeleted file mode 100644\nindex 538aeeb3bc8..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/client/http/HttpResponse.java\n+++ /dev/null\n@@ -1,97 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.client.http;\n-\n-import org.apache.http.client.methods.CloseableHttpResponse;\n-import org.apache.http.client.methods.HttpHead;\n-import org.apache.http.client.methods.HttpUriRequest;\n-import org.apache.http.util.EntityUtils;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-\n-import java.io.IOException;\n-\n-/**\n- * Response obtained from an http request\n- * Always consumes the whole response body loading it entirely into a string\n- */\n-public class HttpResponse {\n-\n-    private static final ESLogger logger = Loggers.getLogger(HttpResponse.class);\n-\n-    private final HttpUriRequest httpRequest;\n-    private final int statusCode;\n-    private final String reasonPhrase;\n-    private final String body;\n-\n-    HttpResponse(HttpUriRequest httpRequest, CloseableHttpResponse httpResponse) {\n-        this.httpRequest = httpRequest;\n-        this.statusCode = httpResponse.getStatusLine().getStatusCode();\n-        this.reasonPhrase = httpResponse.getStatusLine().getReasonPhrase();\n-        if (httpResponse.getEntity() != null) {\n-            try {\n-                this.body = EntityUtils.toString(httpResponse.getEntity(), HttpRequestBuilder.DEFAULT_CHARSET);\n-            } catch (IOException e) {\n-                EntityUtils.consumeQuietly(httpResponse.getEntity());\n-                throw new RuntimeException(e);\n-            } finally {\n-                try {\n-                    httpResponse.close();\n-                } catch (IOException e) {\n-                    logger.error(e.getMessage(), e);\n-                }\n-            }\n-        } else {\n-            this.body = null;\n-        }\n-    }\n-\n-    public boolean isError() {\n-        return statusCode >= 400;\n-    }\n-\n-    public int getStatusCode() {\n-        return statusCode;\n-    }\n-\n-    public String getReasonPhrase() {\n-        return reasonPhrase;\n-    }\n-\n-    public String getBody() {\n-        return body;\n-    }\n-\n-    public boolean hasBody() {\n-        return body != null;\n-    }\n-\n-    public boolean supportsBody() {\n-        return !HttpHead.METHOD_NAME.equals(httpRequest.getMethod());\n-    }\n-\n-    @Override\n-    public String toString() {\n-        StringBuilder stringBuilder = new StringBuilder(statusCode).append(\" \").append(reasonPhrase);\n-        if (hasBody()) {\n-            stringBuilder.append(\"\\n\").append(body);\n-        }\n-        return stringBuilder.toString();\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/json/JsonPath.java b/src/test/java/org/elasticsearch/test/rest/json/JsonPath.java\ndeleted file mode 100644\nindex 1d5e86007d6..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/json/JsonPath.java\n+++ /dev/null\n@@ -1,111 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.json;\n-\n-import com.google.common.collect.Lists;\n-import org.elasticsearch.common.xcontent.json.JsonXContent;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.Map;\n-\n-/**\n- * Holds a json object and allows to extract specific values from it\n- */\n-public class JsonPath {\n-\n-    final String json;\n-    final Map<String, Object> jsonMap;\n-\n-    public JsonPath(String json) throws IOException {\n-        this.json = json;\n-        this.jsonMap = convertToMap(json);\n-    }\n-\n-    private static Map<String, Object> convertToMap(String json) throws IOException {\n-        return JsonXContent.jsonXContent.createParser(json).mapOrderedAndClose();\n-    }\n-\n-    /**\n-     * Returns the object corresponding to the provided path if present, null otherwise\n-     */\n-    public Object evaluate(String path) {\n-        String[] parts = parsePath(path);\n-        Object object = jsonMap;\n-        for (String part : parts) {\n-            object = evaluate(part, object);\n-            if (object == null) {\n-                return null;\n-            }\n-        }\n-        return object;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private Object evaluate(String key, Object object) {\n-        if (object instanceof Map) {\n-            return ((Map<String, Object>) object).get(key);\n-        }\n-        if (object instanceof List) {\n-            List<Object> list = (List<Object>) object;\n-            try {\n-                return list.get(Integer.valueOf(key));\n-            } catch (NumberFormatException e) {\n-                throw new IllegalArgumentException(\"element was a list, but [\" + key + \"] was not numeric\", e);\n-            } catch (IndexOutOfBoundsException e) {\n-                throw new IllegalArgumentException(\"element was a list with \" + list.size() + \" elements, but [\" + key + \"] was out of bounds\", e);\n-            }\n-        }\n-\n-        throw new IllegalArgumentException(\"no object found for [\" + key + \"] within object of class [\" + object.getClass() + \"]\");\n-    }\n-\n-    private String[] parsePath(String path) {\n-        List<String> list = Lists.newArrayList();\n-        StringBuilder current = new StringBuilder();\n-        boolean escape = false;\n-        for (int i = 0; i < path.length(); i++) {\n-            char c = path.charAt(i);\n-            if (c == '\\\\') {\n-                escape = true;\n-                continue;\n-            }\n-\n-            if (c == '.') {\n-                if (escape) {\n-                    escape = false;\n-                } else {\n-                    if (current.length() > 0) {\n-                        list.add(current.toString());\n-                        current.setLength(0);\n-                    }\n-                    continue;\n-                }\n-            }\n-\n-            current.append(c);\n-        }\n-\n-        if (current.length() > 0) {\n-            list.add(current.toString());\n-        }\n-\n-        return list.toArray(new String[list.size()]);\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/junit/DescriptionHelper.java b/src/test/java/org/elasticsearch/test/rest/junit/DescriptionHelper.java\ndeleted file mode 100644\nindex 0a2179e5ff4..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/junit/DescriptionHelper.java\n+++ /dev/null\n@@ -1,79 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.junit;\n-\n-import com.google.common.base.Joiner;\n-import org.elasticsearch.test.rest.section.RestTestSuite;\n-import org.elasticsearch.test.rest.section.TestSection;\n-import org.junit.runner.Description;\n-\n-import java.util.Map;\n-\n-/**\n- * Helper that knows how to assign proper junit {@link Description}s to each of the node in the tests tree\n- */\n-public final class DescriptionHelper {\n-\n-    private DescriptionHelper() {\n-\n-    }\n-\n-    /*\n-    The following generated ids need to be unique throughout a tests run.\n-    Ids are also shown by IDEs (with junit 4.11 unique ids can be different from what gets shown, not yet in 4.10).\n-    Some tricks are applied to control what gets shown in IDEs in order to keep the ids unique and nice to see at the same time.\n-     */\n-\n-    static Description createRootDescription(String name) {\n-        return Description.createSuiteDescription(name);\n-    }\n-\n-    static Description createApiDescription(String api) {\n-        return Description.createSuiteDescription(api);\n-    }\n-\n-    static Description createTestSuiteDescription(RestTestSuite restTestSuite) {\n-        //e.g. \"indices_open (10_basic)\", which leads to 10_basic being returned by Description#getDisplayName\n-        String name = restTestSuite.getApi() + \" (\" + restTestSuite.getName() + \")\";\n-        return Description.createSuiteDescription(name);\n-    }\n-\n-    static Description createTestSectionWithRepetitionsDescription(RestTestSuite restTestSuite, TestSection testSection) {\n-        //e.g. \"indices_open/10_basic (Basic test for index open/close)\", which leads to\n-        //\"Basic test for index open/close\" being returned by Description#getDisplayName\n-        String name = restTestSuite.getDescription() + \" (\" + testSection.getName() + \")\";\n-        return Description.createSuiteDescription(name);\n-    }\n-\n-    static Description createTestSectionIterationDescription(RestTestSuite restTestSuite, TestSection testSection, Map<String, Object> args) {\n-        //e.g. \"Basic test for index open/close {#0} (indices_open/10_basic)\" some IDEs might strip out the part between parentheses\n-        String name = testSection.getName() + formatMethodArgs(args) + \" (\"  + restTestSuite.getDescription() + \")\";\n-        return Description.createSuiteDescription(name);\n-    }\n-\n-    private static String formatMethodArgs(Map<String, Object> args) {\n-        if (args == null || args.isEmpty()) return \"\";\n-\n-        StringBuilder b = new StringBuilder(\" {\");\n-        Joiner.on(\" \").withKeyValueSeparator(\"\").appendTo(b, args);\n-        b.append(\"}\");\n-\n-        return b.toString();\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/junit/RestReproduceInfoPrinter.java b/src/test/java/org/elasticsearch/test/rest/junit/RestReproduceInfoPrinter.java\ndeleted file mode 100644\nindex 3f8f8d7d4b9..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/junit/RestReproduceInfoPrinter.java\n+++ /dev/null\n@@ -1,112 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.junit;\n-\n-import com.carrotsearch.randomizedtesting.ReproduceErrorMessageBuilder;\n-import com.carrotsearch.randomizedtesting.StandaloneRandomizedContext;\n-import com.carrotsearch.randomizedtesting.TraceFormatting;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.junit.listeners.ReproduceInfoPrinter;\n-import org.elasticsearch.test.rest.ElasticsearchRestTests;\n-import org.junit.runner.Description;\n-import org.junit.runner.notification.Failure;\n-\n-import java.util.Arrays;\n-\n-import static com.carrotsearch.randomizedtesting.SysGlobals.SYSPROP_RANDOM_SEED;\n-import static com.carrotsearch.randomizedtesting.SysGlobals.SYSPROP_TESTCLASS;\n-import static org.elasticsearch.test.rest.junit.RestTestSuiteRunner.*;\n-\n-/**\n- * A {@link org.junit.runner.notification.RunListener} that emits to {@link System#err} a string with command\n- * line parameters allowing quick REST test re-run under MVN command line.\n- */\n-class RestReproduceInfoPrinter extends ReproduceInfoPrinter {\n-\n-    protected static final ESLogger logger = Loggers.getLogger(RestReproduceInfoPrinter.class);\n-\n-    @Override\n-    protected boolean mustAppendClusterSeed(Failure failure) {\n-        return isTestCluster();\n-    }\n-\n-    private static boolean isTestCluster() {\n-        return runMode() == RunMode.TEST_CLUSTER;\n-    }\n-\n-    @Override\n-    protected TraceFormatting traces() {\n-        return new TraceFormatting(\n-                Arrays.asList(\n-                    \"org.junit.\",\n-                    \"junit.framework.\",\n-                    \"sun.\",\n-                    \"java.lang.reflect.\",\n-                    \"com.carrotsearch.randomizedtesting.\",\n-                    \"org.elasticsearch.test.rest.junit.\"\n-                ));\n-    }\n-\n-    @Override\n-    protected ReproduceErrorMessageBuilder reproduceErrorMessageBuilder(StringBuilder b) {\n-        return new MavenMessageBuilder(b);\n-    }\n-\n-    private static class MavenMessageBuilder extends ReproduceInfoPrinter.MavenMessageBuilder {\n-\n-        public MavenMessageBuilder(StringBuilder b) {\n-            super(b);\n-        }\n-\n-        @Override\n-        public ReproduceErrorMessageBuilder appendAllOpts(Description description) {\n-\n-            try {\n-                appendOpt(SYSPROP_RANDOM_SEED(), StandaloneRandomizedContext.getSeedAsString());\n-            } catch (IllegalStateException e) {\n-                logger.warn(\"No context available when dumping reproduce options?\");\n-            }\n-\n-            //we know that ElasticsearchRestTests is the only one that runs with RestTestSuiteRunner\n-            appendOpt(SYSPROP_TESTCLASS(), ElasticsearchRestTests.class.getName());\n-\n-            if (description.getClassName() != null) {\n-                appendOpt(REST_TESTS_SUITE, description.getClassName());\n-            }\n-\n-            appendRunnerProperties();\n-            appendEnvironmentSettings();\n-\n-            appendProperties(\"es.logger.level\");\n-\n-            if (isTestCluster()) {\n-                appendProperties(\"es.node.mode\", \"es.node.local\");\n-            }\n-\n-            appendRestTestsProperties();\n-\n-            return this;\n-        }\n-\n-        public ReproduceErrorMessageBuilder appendRestTestsProperties() {\n-            return appendProperties(REST_TESTS_MODE, REST_TESTS_SPEC);\n-        }\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/junit/RestTestCandidate.java b/src/test/java/org/elasticsearch/test/rest/junit/RestTestCandidate.java\ndeleted file mode 100644\nindex aaf9d10a765..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/junit/RestTestCandidate.java\n+++ /dev/null\n@@ -1,83 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.junit;\n-\n-import org.elasticsearch.test.rest.section.RestTestSuite;\n-import org.elasticsearch.test.rest.section.SetupSection;\n-import org.elasticsearch.test.rest.section.TestSection;\n-import org.junit.runner.Description;\n-\n-/**\n- * Wraps {@link org.elasticsearch.test.rest.section.TestSection}s ready to be run,\n- * properly enriched with the needed execution information.\n- * The tests tree structure gets flattened to the leaves (test sections)\n- */\n-public class RestTestCandidate {\n-\n-    private final RestTestSuite restTestSuite;\n-    private final Description suiteDescription;\n-    private final TestSection testSection;\n-    private final Description testDescription;\n-    private final long seed;\n-\n-    static RestTestCandidate empty(RestTestSuite restTestSuite, Description suiteDescription) {\n-        return new RestTestCandidate(restTestSuite, suiteDescription, null, null, -1);\n-    }\n-\n-    RestTestCandidate(RestTestSuite restTestSuite, Description suiteDescription,\n-                      TestSection testSection, Description testDescription, long seed) {\n-        this.restTestSuite = restTestSuite;\n-        this.suiteDescription = suiteDescription;\n-        this.testSection = testSection;\n-        this.testDescription = testDescription;\n-        this.seed = seed;\n-    }\n-\n-    public String getApi() {\n-        return restTestSuite.getApi();\n-    }\n-\n-    public String getName() {\n-        return restTestSuite.getName();\n-    }\n-\n-    public String getSuiteDescription() {\n-        return restTestSuite.getDescription();\n-    }\n-\n-    public Description describeSuite() {\n-        return suiteDescription;\n-    }\n-\n-    public Description describeTest() {\n-        return testDescription;\n-    }\n-\n-    public SetupSection getSetupSection() {\n-        return restTestSuite.getSetupSection();\n-    }\n-\n-    public TestSection getTestSection() {\n-        return testSection;\n-    }\n-\n-    public long getSeed() {\n-        return seed;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/junit/RestTestSuiteRunner.java b/src/test/java/org/elasticsearch/test/rest/junit/RestTestSuiteRunner.java\ndeleted file mode 100644\nindex 453f77dfbab..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/junit/RestTestSuiteRunner.java\n+++ /dev/null\n@@ -1,546 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.junit;\n-\n-import com.carrotsearch.hppc.hash.MurmurHash3;\n-import com.carrotsearch.randomizedtesting.RandomizedTest;\n-import com.carrotsearch.randomizedtesting.Randomness;\n-import com.carrotsearch.randomizedtesting.SeedUtils;\n-import com.google.common.collect.Lists;\n-import org.elasticsearch.common.Strings;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.common.transport.InetSocketTransportAddress;\n-import org.elasticsearch.http.HttpServerTransport;\n-import org.elasticsearch.test.ElasticsearchTestCase;\n-import org.elasticsearch.test.TestCluster;\n-import org.elasticsearch.test.rest.RestTestExecutionContext;\n-import org.elasticsearch.test.rest.client.RestException;\n-import org.elasticsearch.test.rest.client.RestResponse;\n-import org.elasticsearch.test.rest.parser.RestTestParseException;\n-import org.elasticsearch.test.rest.parser.RestTestSuiteParser;\n-import org.elasticsearch.test.rest.section.DoSection;\n-import org.elasticsearch.test.rest.section.ExecutableSection;\n-import org.elasticsearch.test.rest.section.RestTestSuite;\n-import org.elasticsearch.test.rest.section.TestSection;\n-import org.elasticsearch.test.rest.spec.RestSpec;\n-import org.elasticsearch.test.rest.support.FileUtils;\n-import org.junit.runner.Description;\n-import org.junit.runner.notification.Failure;\n-import org.junit.runner.notification.RunNotifier;\n-import org.junit.runners.ParentRunner;\n-import org.junit.runners.model.InitializationError;\n-import org.junit.runners.model.Statement;\n-\n-import java.io.File;\n-import java.io.IOException;\n-import java.util.*;\n-import java.util.concurrent.atomic.AtomicInteger;\n-\n-import static com.carrotsearch.randomizedtesting.SeedUtils.parseSeedChain;\n-import static com.carrotsearch.randomizedtesting.StandaloneRandomizedContext.*;\n-import static com.carrotsearch.randomizedtesting.SysGlobals.*;\n-import static org.elasticsearch.test.TestCluster.SHARED_CLUSTER_SEED;\n-import static org.elasticsearch.test.TestCluster.clusterName;\n-import static org.elasticsearch.test.rest.junit.DescriptionHelper.*;\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.instanceOf;\n-import static org.junit.Assert.assertThat;\n-\n-/**\n- * JUnit runner for elasticsearch REST tests\n- *\n- * Supports the following options provided as system properties:\n- * - tests.rest[true|false|host:port]: determines whether the REST tests need to be run and if so\n- *                                     whether to rely on an external cluster (providing host and port) or fire a test cluster (default)\n- * - tests.rest.suite: comma separated paths of the test suites to be run (by default loaded from /rest-spec/test)\n- *                     it is possible to run only a subset of the tests providing a directory or a single yaml file\n- *                     (the default /rest-spec/test prefix is optional when files are loaded from classpath)\n- * - tests.rest.spec: REST spec path (default /rest-spec/api)\n- * - tests.iters: runs multiple iterations\n- * - tests.seed: seed to base the random behaviours on\n- * - tests.appendseed[true|false]: enables adding the seed to each test section's description (default false)\n- * - tests.cluster_seed: seed used to create the test cluster (if enabled)\n- *\n- */\n-public class RestTestSuiteRunner extends ParentRunner<RestTestCandidate> {\n-\n-    private static final ESLogger logger = Loggers.getLogger(RestTestSuiteRunner.class);\n-\n-    public static final String REST_TESTS_MODE = \"tests.rest\";\n-    public static final String REST_TESTS_SUITE = \"tests.rest.suite\";\n-    public static final String REST_TESTS_SPEC = \"tests.rest.spec\";\n-\n-    private static final String DEFAULT_TESTS_PATH = \"/rest-spec/test\";\n-    private static final String DEFAULT_SPEC_PATH = \"/rest-spec/api\";\n-    private static final int DEFAULT_ITERATIONS = 1;\n-\n-    private static final String PATHS_SEPARATOR = \",\";\n-\n-    private final RestTestExecutionContext restTestExecutionContext;\n-    private final List<RestTestCandidate> restTestCandidates;\n-    private final Description rootDescription;\n-\n-    private final RunMode runMode;\n-\n-    private final TestCluster testCluster;\n-\n-    private static final AtomicInteger sequencer = new AtomicInteger();\n-\n-    /** The runner's seed (master). */\n-    private final Randomness runnerRandomness;\n-\n-    /**\n-     * If {@link com.carrotsearch.randomizedtesting.SysGlobals#SYSPROP_RANDOM_SEED} property is used with two arguments\n-     * (master:test_section) then this field contains test section level override.\n-     */\n-    private final Randomness testSectionRandomnessOverride;\n-\n-    enum RunMode {\n-        NO, TEST_CLUSTER, EXTERNAL_CLUSTER\n-    }\n-\n-    static RunMode runMode() {\n-        String mode = System.getProperty(REST_TESTS_MODE);\n-        if (!Strings.hasLength(mode)) {\n-            //default true: we run the tests starting our own test cluster\n-            mode = Boolean.TRUE.toString();\n-        }\n-\n-        if (Boolean.FALSE.toString().equalsIgnoreCase(mode)) {\n-            return RunMode.NO;\n-        }\n-        if (Boolean.TRUE.toString().equalsIgnoreCase(mode)) {\n-            return RunMode.TEST_CLUSTER;\n-        }\n-        return RunMode.EXTERNAL_CLUSTER;\n-    }\n-\n-    public RestTestSuiteRunner(Class<?> testClass) throws InitializationError {\n-        super(testClass);\n-\n-        this.runMode = runMode();\n-\n-        if (runMode == RunMode.NO) {\n-            //the tests won't be run. the run method will be called anyway but we'll just mark the whole suite as ignored\n-            //no need to go ahead and parse the test suites then\n-            this.runnerRandomness = null;\n-            this.testSectionRandomnessOverride = null;\n-            this.restTestExecutionContext = null;\n-            this.restTestCandidates = null;\n-            this.rootDescription = createRootDescription(getRootSuiteTitle());\n-            this.rootDescription.addChild(createApiDescription(\"empty suite\"));\n-            this.testCluster = null;\n-            return;\n-        }\n-\n-        //the REST test suite is supposed to be run only once per jvm against either an external es node or a self started one\n-        if (sequencer.getAndIncrement() > 0) {\n-            throw new InitializationError(\"only one instance of RestTestSuiteRunner can be created per jvm\");\n-        }\n-\n-        //either read the seed from system properties (first one in the chain) or generate a new one\n-        final String globalSeed = System.getProperty(SYSPROP_RANDOM_SEED());\n-        final long initialSeed;\n-        Randomness randomnessOverride = null;\n-        if (Strings.hasLength(globalSeed)) {\n-            final long[] seedChain = parseSeedChain(globalSeed);\n-            if (seedChain.length == 0 || seedChain.length > 2) {\n-                throw new IllegalArgumentException(\"Invalid system property \"\n-                        + SYSPROP_RANDOM_SEED() + \" specification: \" + globalSeed);\n-            }\n-            if (seedChain.length > 1) {\n-                //read the test section level seed if present\n-                randomnessOverride = new Randomness(seedChain[1]);\n-            }\n-            initialSeed = seedChain[0];\n-        } else {\n-            initialSeed = MurmurHash3.hash(System.nanoTime());\n-        }\n-        this.runnerRandomness = new Randomness(initialSeed);\n-        this.testSectionRandomnessOverride = randomnessOverride;\n-        logger.info(\"Master seed: {}\", SeedUtils.formatSeed(initialSeed));\n-\n-        String host;\n-        int port;\n-        if (runMode == RunMode.TEST_CLUSTER) {\n-            this.testCluster = new TestCluster(SHARED_CLUSTER_SEED, 1, clusterName(\"REST-tests\", ElasticsearchTestCase.CHILD_VM_ID, SHARED_CLUSTER_SEED));\n-            this.testCluster.beforeTest(runnerRandomness.getRandom(), 0.0f);\n-            HttpServerTransport httpServerTransport = testCluster.getInstance(HttpServerTransport.class);\n-            InetSocketTransportAddress inetSocketTransportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().publishAddress();\n-            host = inetSocketTransportAddress.address().getHostName();\n-            port = inetSocketTransportAddress.address().getPort();\n-        } else {\n-            this.testCluster = null;\n-            String testsMode = System.getProperty(REST_TESTS_MODE);\n-            String[] split = testsMode.split(\":\");\n-            if (split.length < 2) {\n-                throw new InitializationError(\"address [\" + testsMode + \"] not valid\");\n-            }\n-            host = split[0];\n-            try {\n-                port = Integer.valueOf(split[1]);\n-            } catch(NumberFormatException e) {\n-                throw new InitializationError(\"port is not valid, expected number but was [\" + split[1] + \"]\");\n-            }\n-        }\n-\n-        try {\n-            String[] specPaths = resolvePathsProperty(REST_TESTS_SPEC, DEFAULT_SPEC_PATH);\n-            RestSpec restSpec = RestSpec.parseFrom(DEFAULT_SPEC_PATH, specPaths);\n-\n-            this.restTestExecutionContext = new RestTestExecutionContext(host, port, restSpec);\n-            this.rootDescription = createRootDescription(getRootSuiteTitle());\n-            this.restTestCandidates = collectTestCandidates(rootDescription);\n-        } catch (Throwable e) {\n-            stopTestCluster();\n-            throw new InitializationError(e);\n-        }\n-    }\n-\n-    /**\n-     * Parse the test suites and creates the test candidates to be run, together with their junit descriptions.\n-     * The descriptions will be part of a tree containing api/yaml file/test section/eventual multiple iterations.\n-     * The test candidates will be instead flattened out to the leaves level (iterations), the part that needs to be run.\n-     */\n-    protected List<RestTestCandidate> collectTestCandidates(Description rootDescription)\n-            throws RestTestParseException, IOException {\n-\n-        String[] paths = resolvePathsProperty(REST_TESTS_SUITE, DEFAULT_TESTS_PATH);\n-        Map<String, Set<File>> yamlSuites = FileUtils.findYamlSuites(DEFAULT_TESTS_PATH, paths);\n-\n-        int iterations = determineTestSectionIterationCount();\n-        boolean appendSeedParameter = RandomizedTest.systemPropertyAsBoolean(SYSPROP_APPEND_SEED(), false);\n-\n-        //we iterate over the files and we shuffle them (grouped by api, and by yaml file)\n-        //meanwhile we create the junit descriptions and test candidates (one per iteration)\n-\n-        //yaml suites are grouped by directory (effectively by api)\n-        List<String> apis = Lists.newArrayList(yamlSuites.keySet());\n-        Collections.shuffle(apis, runnerRandomness.getRandom());\n-\n-        final boolean fixedSeed = testSectionRandomnessOverride != null;\n-        final boolean hasRepetitions = iterations > 1;\n-\n-        List<RestTestCandidate> testCandidates = Lists.newArrayList();\n-        RestTestSuiteParser restTestSuiteParser = new RestTestSuiteParser();\n-        for (String api : apis) {\n-\n-            Description apiDescription = createApiDescription(api);\n-            rootDescription.addChild(apiDescription);\n-\n-            List<File> yamlFiles = Lists.newArrayList(yamlSuites.get(api));\n-            Collections.shuffle(yamlFiles, runnerRandomness.getRandom());\n-\n-            for (File yamlFile : yamlFiles) {\n-                RestTestSuite restTestSuite = restTestSuiteParser.parse(restTestExecutionContext.esVersion(), api, yamlFile);\n-                Description testSuiteDescription = createTestSuiteDescription(restTestSuite);\n-                apiDescription.addChild(testSuiteDescription);\n-\n-                if (restTestSuite.getTestSections().size() == 0) {\n-                    assert restTestSuite.getSetupSection().getSkipSection().skipVersion(restTestExecutionContext.esVersion());\n-                    testCandidates.add(RestTestCandidate.empty(restTestSuite, testSuiteDescription));\n-                    continue;\n-                }\n-\n-                Collections.shuffle(restTestSuite.getTestSections(), runnerRandomness.getRandom());\n-\n-                for (TestSection testSection : restTestSuite.getTestSections()) {\n-\n-                    //no need to generate seed if we are going to skip the test section\n-                    if (testSection.getSkipSection().skipVersion(restTestExecutionContext.esVersion())) {\n-                        Description testSectionDescription = createTestSectionIterationDescription(restTestSuite, testSection, null);\n-                        testSuiteDescription.addChild(testSectionDescription);\n-                        testCandidates.add(new RestTestCandidate(restTestSuite, testSuiteDescription, testSection, testSectionDescription, -1));\n-                        continue;\n-                    }\n-\n-                    Description parentDescription;\n-                    if (hasRepetitions) {\n-                        //additional level to group multiple iterations under the same test section's node\n-                        parentDescription = createTestSectionWithRepetitionsDescription(restTestSuite, testSection);\n-                        testSuiteDescription.addChild(parentDescription);\n-                    } else {\n-                        parentDescription = testSuiteDescription;\n-                    }\n-\n-                    final long testSectionSeed = determineTestSectionSeed(restTestSuite.getDescription() + \"/\" + testSection.getName());\n-                    for (int i = 0; i < iterations; i++) {\n-                        //test section name argument needs to be unique here\n-                        long thisSeed = (fixedSeed ? testSectionSeed : testSectionSeed ^ MurmurHash3.hash((long) i));\n-\n-                        final LinkedHashMap<String, Object> args = new LinkedHashMap<String, Object>();\n-                        if (hasRepetitions) {\n-                            args.put(\"#\", i);\n-                        }\n-                        if (hasRepetitions || appendSeedParameter) {\n-                            args.put(\"seed=\", SeedUtils.formatSeedChain(runnerRandomness, new Randomness(thisSeed)));\n-                        }\n-\n-                        Description testSectionDescription = createTestSectionIterationDescription(restTestSuite, testSection, args);\n-                        parentDescription.addChild(testSectionDescription);\n-                        testCandidates.add(new RestTestCandidate(restTestSuite, testSuiteDescription, testSection, testSectionDescription, thisSeed));\n-                    }\n-                }\n-            }\n-        }\n-\n-        return testCandidates;\n-    }\n-\n-    protected String getRootSuiteTitle() {\n-        if (runMode == RunMode.NO) {\n-            return \"elasticsearch REST Tests - not run\";\n-        }\n-        if (runMode == RunMode.TEST_CLUSTER) {\n-            return String.format(Locale.ROOT, \"elasticsearch REST Tests - test cluster %s\", SeedUtils.formatSeed(SHARED_CLUSTER_SEED));\n-        }\n-        if (runMode == RunMode.EXTERNAL_CLUSTER) {\n-            return String.format(Locale.ROOT, \"elasticsearch REST Tests - external cluster %s\", System.getProperty(REST_TESTS_MODE));\n-        }\n-        throw new UnsupportedOperationException(\"runMode [\" + runMode + \"] not supported\");\n-    }\n-\n-    private int determineTestSectionIterationCount() {\n-        int iterations = RandomizedTest.systemPropertyAsInt(SYSPROP_ITERATIONS(), DEFAULT_ITERATIONS);\n-        if (iterations < 1) {\n-            throw new IllegalArgumentException(\"System property \" + SYSPROP_ITERATIONS() + \" must be >= 1 but was [\" + iterations + \"]\");\n-        }\n-        return iterations;\n-    }\n-\n-    protected static String[] resolvePathsProperty(String propertyName, String defaultValue) {\n-        String property = System.getProperty(propertyName);\n-        if (!Strings.hasLength(property)) {\n-            return new String[]{defaultValue};\n-        } else {\n-            return property.split(PATHS_SEPARATOR);\n-        }\n-    }\n-\n-    /**\n-     * Determine a given test section's initial random seed\n-     */\n-    private long determineTestSectionSeed(String testSectionName) {\n-        if (testSectionRandomnessOverride != null) {\n-            return getSeed(testSectionRandomnessOverride);\n-        }\n-\n-        // We assign each test section a different starting hash based on the global seed\n-        // and a hash of their name (so that the order of sections does not matter, only their names)\n-        return getSeed(runnerRandomness) ^ MurmurHash3.hash((long) testSectionName.hashCode());\n-    }\n-\n-    @Override\n-    protected List<RestTestCandidate> getChildren() {\n-        return restTestCandidates;\n-    }\n-\n-    @Override\n-    public Description getDescription() {\n-        return rootDescription;\n-    }\n-\n-    @Override\n-    protected Description describeChild(RestTestCandidate child) {\n-        return child.describeTest();\n-    }\n-\n-    @Override\n-    protected Statement classBlock(RunNotifier notifier) {\n-        //we remove support for @BeforeClass & @AfterClass and JUnit Rules (as we don't call super)\n-        Statement statement = childrenInvoker(notifier);\n-        statement = withExecutionContextClose(statement);\n-        if (testCluster != null) {\n-            return withTestClusterClose(statement);\n-        }\n-        return statement;\n-    }\n-\n-    protected Statement withExecutionContextClose(Statement statement) {\n-        return new RunAfter(statement, new Statement() {\n-            @Override\n-            public void evaluate() throws Throwable {\n-                restTestExecutionContext.close();\n-            }\n-        });\n-    }\n-\n-    protected Statement withTestClusterClose(Statement statement) {\n-        return new RunAfter(statement, new Statement() {\n-            @Override\n-            public void evaluate() throws Throwable {\n-                stopTestCluster();\n-            }\n-        });\n-    }\n-\n-    @Override\n-    public void run(final RunNotifier notifier) {\n-\n-        if (runMode == RunMode.NO) {\n-            notifier.fireTestIgnored(rootDescription.getChildren().get(0));\n-            return;\n-        }\n-\n-        notifier.addListener(new RestReproduceInfoPrinter());\n-\n-        //the test suite gets run on a separate thread as the randomized context is per thread\n-        //once the randomized context is disposed it's not possible to create it again on the same thread\n-        final Thread thread = new Thread() {\n-            @Override\n-            public void run() {\n-                try {\n-                    createRandomizedContext(getTestClass().getJavaClass(), runnerRandomness);\n-                    RestTestSuiteRunner.super.run(notifier);\n-                } finally {\n-                    disposeRandomizedContext();\n-                }\n-            }\n-        };\n-\n-        thread.start();\n-        try {\n-            thread.join();\n-        } catch (InterruptedException e) {\n-            notifier.fireTestFailure(new Failure(getDescription(),\n-                    new RuntimeException(\"Interrupted while waiting for the suite runner? Weird.\", e)));\n-        }\n-    }\n-\n-    @Override\n-    protected void runChild(RestTestCandidate testCandidate, RunNotifier notifier) {\n-\n-        //if the while suite needs to be skipped, no test sections were loaded, only an empty one that we need to mark as ignored\n-        if (testCandidate.getSetupSection().getSkipSection().skipVersion(restTestExecutionContext.esVersion())) {\n-            logger.info(\"skipped test suite [{}]\\nreason: {}\\nskip versions: {} (current version: {})\",\n-                    testCandidate.getSuiteDescription(), testCandidate.getSetupSection().getSkipSection().getReason(),\n-                    testCandidate.getSetupSection().getSkipSection().getVersion(), restTestExecutionContext.esVersion());\n-\n-            notifier.fireTestIgnored(testCandidate.describeSuite());\n-            return;\n-        }\n-\n-        //from now on no more empty test candidates are expected\n-        assert testCandidate.getTestSection() != null;\n-\n-        if (testCandidate.getTestSection().getSkipSection().skipVersion(restTestExecutionContext.esVersion())) {\n-            logger.info(\"skipped test [{}/{}]\\nreason: {}\\nskip versions: {} (current version: {})\",\n-                    testCandidate.getSuiteDescription(), testCandidate.getTestSection().getName(),\n-                    testCandidate.getTestSection().getSkipSection().getReason(),\n-                    testCandidate.getTestSection().getSkipSection().getVersion(), restTestExecutionContext.esVersion());\n-\n-            notifier.fireTestIgnored(testCandidate.describeTest());\n-            return;\n-        }\n-\n-        runLeaf(methodBlock(testCandidate), testCandidate.describeTest(), notifier);\n-    }\n-\n-    protected Statement methodBlock(final RestTestCandidate testCandidate) {\n-        return new Statement() {\n-            @Override\n-            public void evaluate() throws Throwable {\n-                final String testThreadName = \"TEST-\" + testCandidate.getSuiteDescription() +\n-                        \".\" + testCandidate.getTestSection().getName() + \"-seed#\" + SeedUtils.formatSeedChain(runnerRandomness);\n-                // This has a side effect of setting up a nested context for the test thread.\n-                final String restoreName = Thread.currentThread().getName();\n-                try {\n-                    Thread.currentThread().setName(testThreadName);\n-                    pushRandomness(new Randomness(testCandidate.getSeed()));\n-                    runTestSection(testCandidate);\n-                } finally {\n-                    Thread.currentThread().setName(restoreName);\n-                    popAndDestroy();\n-                }\n-            }\n-        };\n-    }\n-\n-    protected void runTestSection(RestTestCandidate testCandidate)\n-            throws IOException, RestException {\n-\n-        //let's check that there is something to run, otherwise there might be a problem with the test section\n-        if (testCandidate.getTestSection().getExecutableSections().size() == 0) {\n-            throw new IllegalArgumentException(\"No executable sections loaded for [\"\n-                    + testCandidate.getSuiteDescription() + \"/\" + testCandidate.getTestSection().getName() + \"]\");\n-        }\n-\n-        logger.info(\"cleaning up before test [{}: {}]\", testCandidate.getSuiteDescription(), testCandidate.getTestSection().getName());\n-        tearDown();\n-\n-        logger.info(\"start test [{}: {}]\", testCandidate.getSuiteDescription(), testCandidate.getTestSection().getName());\n-\n-        if (!testCandidate.getSetupSection().isEmpty()) {\n-            logger.info(\"start setup test [{}: {}]\", testCandidate.getSuiteDescription(), testCandidate.getTestSection().getName());\n-            for (DoSection doSection : testCandidate.getSetupSection().getDoSections()) {\n-                doSection.execute(restTestExecutionContext);\n-            }\n-            logger.info(\"end setup test [{}: {}]\", testCandidate.getSuiteDescription(), testCandidate.getTestSection().getName());\n-        }\n-\n-        restTestExecutionContext.clear();\n-\n-        for (ExecutableSection executableSection : testCandidate.getTestSection().getExecutableSections()) {\n-            executableSection.execute(restTestExecutionContext);\n-        }\n-\n-        logger.info(\"end test [{}: {}]\", testCandidate.getSuiteDescription(), testCandidate.getTestSection().getName());\n-\n-        logger.info(\"cleaning up after test [{}: {}]\", testCandidate.getSuiteDescription(), testCandidate.getTestSection().getName());\n-        tearDown();\n-    }\n-\n-    private void tearDown() throws IOException, RestException {\n-        wipeIndices();\n-        wipeTemplates();\n-        restTestExecutionContext.clear();\n-    }\n-\n-    private void wipeIndices() throws IOException, RestException {\n-        logger.debug(\"deleting all indices\");\n-        RestResponse restResponse = restTestExecutionContext.callApiInternal(\"indices.delete\", \"index\", \"_all\");\n-        assertThat(restResponse.getStatusCode(), equalTo(200));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    public void wipeTemplates() throws IOException, RestException {\n-        logger.debug(\"deleting all templates\");\n-        //delete templates by wildcard was only added in 0.90.6\n-        //httpResponse = restTestExecutionContext.callApi(\"indices.delete_template\", \"name\", \"*\");\n-        RestResponse restResponse = restTestExecutionContext.callApiInternal(\"cluster.state\", \"filter_nodes\", \"true\",\n-                \"filter_routing_table\", \"true\", \"filter_blocks\", \"true\");\n-        assertThat(restResponse.getStatusCode(), equalTo(200));\n-        Object object = restResponse.evaluate(\"metadata.templates\");\n-        assertThat(object, instanceOf(Map.class));\n-        Set<String> templates = ((Map<String, Object>) object).keySet();\n-        for (String template : templates) {\n-            restResponse = restTestExecutionContext.callApiInternal(\"indices.delete_template\", \"name\", template);\n-            assertThat(restResponse.getStatusCode(), equalTo(200));\n-        }\n-    }\n-\n-    private void stopTestCluster() {\n-        if (runMode == RunMode.TEST_CLUSTER) {\n-            assert testCluster != null;\n-            testCluster.afterTest();\n-            testCluster.close();\n-        }\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/junit/RunAfter.java b/src/test/java/org/elasticsearch/test/rest/junit/RunAfter.java\ndeleted file mode 100644\nindex abd1b2b98c7..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/junit/RunAfter.java\n+++ /dev/null\n@@ -1,56 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.junit;\n-\n-import com.google.common.collect.Lists;\n-import org.junit.runners.model.MultipleFailureException;\n-import org.junit.runners.model.Statement;\n-\n-import java.util.List;\n-\n-/**\n- * {@link Statement} that allows to run a specific statement after another one\n- */\n-public class RunAfter extends Statement {\n-\n-    private final Statement next;\n-    private final Statement after;\n-\n-    public RunAfter(Statement next, Statement after) {\n-        this.next = next;\n-        this.after = after;\n-    }\n-\n-    @Override\n-    public void evaluate() throws Throwable {\n-        List<Throwable> errors = Lists.newArrayList();\n-        try {\n-            next.evaluate();\n-        } catch (Throwable e) {\n-            errors.add(e);\n-        } finally {\n-            try {\n-                after.evaluate();\n-            } catch (Throwable e) {\n-                errors.add(e);\n-            }\n-        }\n-        MultipleFailureException.assertEmpty(errors);\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/DoSectionParser.java b/src/test/java/org/elasticsearch/test/rest/parser/DoSectionParser.java\ndeleted file mode 100644\nindex cdbfe0fe9f2..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/DoSectionParser.java\n+++ /dev/null\n@@ -1,86 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.xcontent.XContentBuilder;\n-import org.elasticsearch.common.xcontent.XContentFactory;\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.test.rest.section.ApiCallSection;\n-import org.elasticsearch.test.rest.section.DoSection;\n-\n-import java.io.IOException;\n-import java.util.Map;\n-\n-/**\n- * Parser for do sections\n- */\n-public class DoSectionParser implements RestTestFragmentParser<DoSection> {\n-\n-    @Override\n-    public DoSection parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-\n-        XContentParser parser = parseContext.parser();\n-\n-        String currentFieldName = null;\n-        XContentParser.Token token;\n-\n-        DoSection doSection = new DoSection();\n-\n-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n-            if (token == XContentParser.Token.FIELD_NAME) {\n-                currentFieldName = parser.currentName();\n-            } else if (token.isValue()) {\n-                if (\"catch\".equals(currentFieldName)) {\n-                    doSection.setCatch(parser.text());\n-                }\n-            } else if (token == XContentParser.Token.START_OBJECT) {\n-                if (currentFieldName != null) {\n-                    ApiCallSection apiCallSection = new ApiCallSection(currentFieldName);\n-                    String paramName = null;\n-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n-                        if (token == XContentParser.Token.FIELD_NAME) {\n-                            paramName = parser.currentName();\n-                        } else if (token.isValue()) {\n-                            if (\"body\".equals(paramName)) {\n-                                apiCallSection.addBody(parser.text());\n-                            } else {\n-                                apiCallSection.addParam(paramName, parser.text());\n-                            }\n-                        } else if (token == XContentParser.Token.START_OBJECT) {\n-                            if (\"body\".equals(paramName)) {\n-                                Map<String,Object> map = parser.mapOrdered();\n-                                XContentBuilder contentBuilder = XContentFactory.jsonBuilder().map(map);\n-                                apiCallSection.addBody(contentBuilder.string());\n-                            }\n-                        }\n-                    }\n-                    doSection.setApiCallSection(apiCallSection);\n-                }\n-            }\n-        }\n-\n-        parser.nextToken();\n-\n-        if (doSection.getApiCallSection() == null) {\n-            throw new RestTestParseException(\"client call section is mandatory within a do section\");\n-        }\n-\n-        return doSection;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/GreaterThanParser.java b/src/test/java/org/elasticsearch/test/rest/parser/GreaterThanParser.java\ndeleted file mode 100644\nindex 3ca5214346f..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/GreaterThanParser.java\n+++ /dev/null\n@@ -1,39 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.collect.Tuple;\n-import org.elasticsearch.test.rest.section.GreaterThanAssertion;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for gt assert sections\n- */\n-public class GreaterThanParser implements RestTestFragmentParser<GreaterThanAssertion> {\n-\n-    @Override\n-    public GreaterThanAssertion parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-        Tuple<String,Object> stringObjectTuple = parseContext.parseTuple();\n-        if (! (stringObjectTuple.v2() instanceof Comparable) ) {\n-            throw new RestTestParseException(\"gt section can only be used with objects that support natural ordering, found \" + stringObjectTuple.v2().getClass().getSimpleName());\n-        }\n-        return new GreaterThanAssertion(stringObjectTuple.v1(), stringObjectTuple.v2());\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/IsFalseParser.java b/src/test/java/org/elasticsearch/test/rest/parser/IsFalseParser.java\ndeleted file mode 100644\nindex 2c113a874a4..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/IsFalseParser.java\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.test.rest.section.IsFalseAssertion;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for is_false assert sections\n- */\n-public class IsFalseParser implements RestTestFragmentParser<IsFalseAssertion> {\n-\n-    @Override\n-    public IsFalseAssertion parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-        return new IsFalseAssertion(parseContext.parseField());\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/IsTrueParser.java b/src/test/java/org/elasticsearch/test/rest/parser/IsTrueParser.java\ndeleted file mode 100644\nindex 37e5cb2cfeb..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/IsTrueParser.java\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.test.rest.section.IsTrueAssertion;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for is_true assert sections\n- */\n-public class IsTrueParser implements RestTestFragmentParser<IsTrueAssertion> {\n-\n-    @Override\n-    public IsTrueAssertion parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-        return new IsTrueAssertion(parseContext.parseField());\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/LengthParser.java b/src/test/java/org/elasticsearch/test/rest/parser/LengthParser.java\ndeleted file mode 100644\nindex 63ce031a47e..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/LengthParser.java\n+++ /dev/null\n@@ -1,48 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.collect.Tuple;\n-import org.elasticsearch.test.rest.section.LengthAssertion;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for length assert sections\n- */\n-public class LengthParser implements RestTestFragmentParser<LengthAssertion> {\n-\n-    @Override\n-    public LengthAssertion parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-        Tuple<String,Object> stringObjectTuple = parseContext.parseTuple();\n-        assert stringObjectTuple.v2() != null;\n-        int value;\n-        if (stringObjectTuple.v2() instanceof  Number) {\n-            value = ((Number) stringObjectTuple.v2()).intValue();\n-        } else {\n-            try {\n-                value = Integer.valueOf(stringObjectTuple.v2().toString());\n-            } catch(NumberFormatException e) {\n-                throw new RestTestParseException(\"length is not a valid number\", e);\n-            }\n-\n-        }\n-        return new LengthAssertion(stringObjectTuple.v1(), value);\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/LessThanParser.java b/src/test/java/org/elasticsearch/test/rest/parser/LessThanParser.java\ndeleted file mode 100644\nindex 245989e3599..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/LessThanParser.java\n+++ /dev/null\n@@ -1,39 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.collect.Tuple;\n-import org.elasticsearch.test.rest.section.LessThanAssertion;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for lt assert sections\n- */\n-public class LessThanParser implements RestTestFragmentParser<LessThanAssertion> {\n-\n-    @Override\n-    public LessThanAssertion parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-        Tuple<String,Object> stringObjectTuple = parseContext.parseTuple();\n-        if (! (stringObjectTuple.v2() instanceof Comparable) ) {\n-            throw new RestTestParseException(\"lt section can only be used with objects that support natural ordering, found \" + stringObjectTuple.v2().getClass().getSimpleName());\n-        }\n-        return new LessThanAssertion(stringObjectTuple.v1(), stringObjectTuple.v2());\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/MatchParser.java b/src/test/java/org/elasticsearch/test/rest/parser/MatchParser.java\ndeleted file mode 100644\nindex 4c349890917..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/MatchParser.java\n+++ /dev/null\n@@ -1,36 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.collect.Tuple;\n-import org.elasticsearch.test.rest.section.MatchAssertion;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for match assert sections\n- */\n-public class MatchParser implements RestTestFragmentParser<MatchAssertion> {\n-\n-    @Override\n-    public MatchAssertion parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-        Tuple<String,Object> stringObjectTuple = parseContext.parseTuple();\n-        return new MatchAssertion(stringObjectTuple.v1(), stringObjectTuple.v2());\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/RestTestFragmentParser.java b/src/test/java/org/elasticsearch/test/rest/parser/RestTestFragmentParser.java\ndeleted file mode 100644\nindex 0528b8e9fdf..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/RestTestFragmentParser.java\n+++ /dev/null\n@@ -1,33 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import java.io.IOException;\n-\n-/**\n- * Base parser for a REST test suite fragment\n- * @param <T> the test fragment's type that gets parsed and returned\n- */\n-public interface RestTestFragmentParser<T> {\n-\n-    /**\n-     * Parses a test fragment given the current {@link RestTestSuiteParseContext}\n-     */\n-    T parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException;\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/RestTestParseException.java b/src/test/java/org/elasticsearch/test/rest/parser/RestTestParseException.java\ndeleted file mode 100644\nindex 4ad0b76840f..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/RestTestParseException.java\n+++ /dev/null\n@@ -1,33 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-/**\n- * Exception thrown whenever there is a problem parsing any of the REST test suite fragment\n- */\n-public class RestTestParseException extends Exception {\n-\n-    RestTestParseException(String message) {\n-        super(message);\n-    }\n-\n-    RestTestParseException(String message, Throwable cause) {\n-        super(message, cause);\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/RestTestSectionParser.java b/src/test/java/org/elasticsearch/test/rest/parser/RestTestSectionParser.java\ndeleted file mode 100644\nindex 6689ac6cefd..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/RestTestSectionParser.java\n+++ /dev/null\n@@ -1,66 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.test.rest.section.TestSection;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for a complete test section\n- *\n- * Depending on the elasticsearch version the tests are going to run against, test sections might need to get skipped\n- * In that case the relevant test sections parsing is entirely skipped\n- */\n-public class RestTestSectionParser implements RestTestFragmentParser<TestSection> {\n-\n-    @Override\n-    public TestSection parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-        XContentParser parser = parseContext.parser();\n-        parseContext.advanceToFieldName();\n-        TestSection testSection = new TestSection(parser.currentName());\n-        parser.nextToken();\n-        testSection.setSkipSection(parseContext.parseSkipSection());\n-\n-        boolean skip = testSection.getSkipSection().skipVersion(parseContext.getCurrentVersion());\n-\n-        while ( parser.currentToken() != XContentParser.Token.END_ARRAY) {\n-            if (skip) {\n-                //if there was a skip section, there was a setup section as well, which means that we are sure\n-                // the current token is at the beginning of a new object\n-                assert parser.currentToken() == XContentParser.Token.START_OBJECT;\n-                //we need to be at the beginning of an object to be able to skip children\n-                parser.skipChildren();\n-                //after skipChildren we are at the end of the skipped object, need to move on\n-                parser.nextToken();\n-            } else {\n-                parseContext.advanceToFieldName();\n-                testSection.addExecutableSection(parseContext.parseExecutableSection());\n-            }\n-        }\n-\n-        parser.nextToken();\n-        assert parser.currentToken() == XContentParser.Token.END_OBJECT;\n-        parser.nextToken();\n-\n-        return testSection;\n-    }\n-\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParseContext.java b/src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParseContext.java\ndeleted file mode 100644\nindex 1b4f868e77e..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParseContext.java\n+++ /dev/null\n@@ -1,165 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import com.google.common.collect.Maps;\n-import org.elasticsearch.common.collect.Tuple;\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.test.rest.section.*;\n-\n-import java.io.IOException;\n-import java.util.Map;\n-\n-/**\n- * Context shared across the whole tests parse phase.\n- * Provides shared parse methods and holds information needed to parse the test sections (e.g. es version)\n- */\n-public class RestTestSuiteParseContext {\n-\n-    private static final SetupSectionParser SETUP_SECTION_PARSER = new SetupSectionParser();\n-    private static final RestTestSectionParser TEST_SECTION_PARSER = new RestTestSectionParser();\n-    private static final SkipSectionParser SKIP_SECTION_PARSER = new SkipSectionParser();\n-    private static final DoSectionParser DO_SECTION_PARSER = new DoSectionParser();\n-    private static final Map<String, RestTestFragmentParser<? extends ExecutableSection>> EXECUTABLE_SECTIONS_PARSERS = Maps.newHashMap();\n-    static {\n-        EXECUTABLE_SECTIONS_PARSERS.put(\"do\", DO_SECTION_PARSER);\n-        EXECUTABLE_SECTIONS_PARSERS.put(\"set\", new SetSectionParser());\n-        EXECUTABLE_SECTIONS_PARSERS.put(\"match\", new MatchParser());\n-        EXECUTABLE_SECTIONS_PARSERS.put(\"is_true\", new IsTrueParser());\n-        EXECUTABLE_SECTIONS_PARSERS.put(\"is_false\", new IsFalseParser());\n-        EXECUTABLE_SECTIONS_PARSERS.put(\"gt\", new GreaterThanParser());\n-        EXECUTABLE_SECTIONS_PARSERS.put(\"lt\", new LessThanParser());\n-        EXECUTABLE_SECTIONS_PARSERS.put(\"length\", new LengthParser());\n-    }\n-\n-    private final String api;\n-    private final String suiteName;\n-    private final XContentParser parser;\n-    private final String currentVersion;\n-\n-    public RestTestSuiteParseContext(String api, String suiteName, XContentParser parser, String currentVersion) {\n-        this.api = api;\n-        this.suiteName = suiteName;\n-        this.parser = parser;\n-        this.currentVersion = currentVersion;\n-    }\n-\n-    public String getApi() {\n-        return api;\n-    }\n-\n-    public String getSuiteName() {\n-        return suiteName;\n-    }\n-\n-    public XContentParser parser() {\n-        return parser;\n-    }\n-\n-    public String getCurrentVersion() {\n-        return currentVersion;\n-    }\n-\n-    public SetupSection parseSetupSection() throws IOException, RestTestParseException {\n-\n-        advanceToFieldName();\n-\n-        if (\"setup\".equals(parser.currentName())) {\n-            parser.nextToken();\n-            SetupSection setupSection = SETUP_SECTION_PARSER.parse(this);\n-            parser.nextToken();\n-            return setupSection;\n-        }\n-\n-        return SetupSection.EMPTY;\n-    }\n-\n-    public TestSection parseTestSection() throws IOException, RestTestParseException {\n-        return TEST_SECTION_PARSER.parse(this);\n-    }\n-\n-    public SkipSection parseSkipSection() throws IOException, RestTestParseException {\n-\n-        advanceToFieldName();\n-\n-        if (\"skip\".equals(parser.currentName())) {\n-            SkipSection skipSection = SKIP_SECTION_PARSER.parse(this);\n-            parser.nextToken();\n-            return skipSection;\n-        }\n-\n-        return SkipSection.EMPTY;\n-    }\n-\n-    public ExecutableSection parseExecutableSection() throws IOException, RestTestParseException {\n-        advanceToFieldName();\n-        String section = parser.currentName();\n-        RestTestFragmentParser<? extends ExecutableSection> execSectionParser = EXECUTABLE_SECTIONS_PARSERS.get(section);\n-        if (execSectionParser == null) {\n-            throw new RestTestParseException(\"no parser found for executable section [\" + section + \"]\");\n-        }\n-        ExecutableSection executableSection = execSectionParser.parse(this);\n-        parser.nextToken();\n-        return executableSection;\n-    }\n-\n-    public DoSection parseDoSection() throws IOException, RestTestParseException {\n-        return DO_SECTION_PARSER.parse(this);\n-    }\n-\n-    public void advanceToFieldName() throws IOException, RestTestParseException {\n-        XContentParser.Token token = parser.currentToken();\n-        //we are in the beginning, haven't called nextToken yet\n-        if (token == null) {\n-            token = parser.nextToken();\n-        }\n-        if (token == XContentParser.Token.START_ARRAY) {\n-            token = parser.nextToken();\n-        }\n-        if (token == XContentParser.Token.START_OBJECT) {\n-            token = parser.nextToken();\n-        }\n-        if (token != XContentParser.Token.FIELD_NAME) {\n-            throw new RestTestParseException(\"malformed test section: field suiteName expected but found \" + token);\n-        }\n-    }\n-\n-    public String parseField() throws IOException, RestTestParseException {\n-        parser.nextToken();\n-        assert parser.currentToken().isValue();\n-        String field = parser.text();\n-        parser.nextToken();\n-        return field;\n-    }\n-\n-    public Tuple<String, Object> parseTuple() throws IOException, RestTestParseException {\n-        parser.nextToken();\n-        advanceToFieldName();\n-        Map<String,Object> map = parser.map();\n-        assert parser.currentToken() == XContentParser.Token.END_OBJECT;\n-        parser.nextToken();\n-\n-        if (map.size() != 1) {\n-            throw new RestTestParseException(\"expected key value pair but found \" + map.size() + \" \");\n-        }\n-\n-        Map.Entry<String, Object> entry = map.entrySet().iterator().next();\n-        return Tuple.tuple(entry.getKey(), entry.getValue());\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParser.java b/src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParser.java\ndeleted file mode 100644\nindex 3d0203424be..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParser.java\n+++ /dev/null\n@@ -1,96 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.common.xcontent.yaml.YamlXContent;\n-import org.elasticsearch.test.rest.section.RestTestSuite;\n-\n-import java.io.File;\n-import java.io.FileInputStream;\n-import java.io.IOException;\n-\n-/**\n- * Parser for a complete test suite (yaml file)\n- *\n- * Depending on the elasticsearch version the tests are going to run against, a whole test suite might need to get skipped\n- * In that case the relevant test sections parsing is entirely skipped\n- */\n-public class RestTestSuiteParser implements RestTestFragmentParser<RestTestSuite> {\n-\n-    public RestTestSuite parse(String currentVersion, String api, File file) throws IOException, RestTestParseException {\n-\n-        if (!file.isFile()) {\n-            throw new IllegalArgumentException(file.getAbsolutePath() + \" is not a file\");\n-        }\n-\n-        XContentParser parser = YamlXContent.yamlXContent.createParser(new FileInputStream(file));\n-        try {\n-            String filename = file.getName();\n-            //remove the file extension\n-            int i = filename.lastIndexOf('.');\n-            if (i > 0) {\n-                filename = filename.substring(0, i);\n-            }\n-            RestTestSuiteParseContext testParseContext = new RestTestSuiteParseContext(api, filename, parser, currentVersion);\n-            return parse(testParseContext);\n-        } finally {\n-            parser.close();\n-        }\n-    }\n-\n-    @Override\n-    public RestTestSuite parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-        XContentParser parser = parseContext.parser();\n-\n-        parser.nextToken();\n-        assert parser.currentToken() == XContentParser.Token.START_OBJECT;\n-\n-        RestTestSuite restTestSuite = new RestTestSuite(parseContext.getApi(), parseContext.getSuiteName());\n-\n-        restTestSuite.setSetupSection(parseContext.parseSetupSection());\n-\n-        boolean skip = restTestSuite.getSetupSection().getSkipSection().skipVersion(parseContext.getCurrentVersion());\n-\n-        while(true) {\n-            //the \"---\" section separator is not understood by the yaml parser. null is returned, same as when the parser is closed\n-            //we need to somehow distinguish between a null in the middle of a test (\"---\")\n-            // and a null at the end of the file (at least two consecutive null tokens)\n-            if(parser.currentToken() == null) {\n-                if (parser.nextToken() == null) {\n-                    break;\n-                }\n-            }\n-\n-            if (skip) {\n-                //if there was a skip section, there was a setup section as well, which means that we are sure\n-                // the current token is at the beginning of a new object\n-                assert parser.currentToken() == XContentParser.Token.START_OBJECT;\n-                //we need to be at the beginning of an object to be able to skip children\n-                parser.skipChildren();\n-                //after skipChildren we are at the end of the skipped object, need to move on\n-                parser.nextToken();\n-            } else {\n-                restTestSuite.addTestSection(parseContext.parseTestSection());\n-            }\n-        }\n-\n-        return restTestSuite;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/SetSectionParser.java b/src/test/java/org/elasticsearch/test/rest/parser/SetSectionParser.java\ndeleted file mode 100644\nindex 1caa96d9972..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/SetSectionParser.java\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.test.rest.section.SetSection;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for set sections\n- */\n-public class SetSectionParser implements RestTestFragmentParser<SetSection> {\n-\n-    @Override\n-    public SetSection parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-\n-        XContentParser parser = parseContext.parser();\n-\n-        String currentFieldName = null;\n-        XContentParser.Token token;\n-\n-        SetSection setSection = new SetSection();\n-\n-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n-            if (token == XContentParser.Token.FIELD_NAME) {\n-                currentFieldName = parser.currentName();\n-            } else if (token.isValue()) {\n-                setSection.addSet(currentFieldName, parser.text());\n-            }\n-        }\n-\n-        parser.nextToken();\n-\n-        if (setSection.getStash().isEmpty()) {\n-            throw new RestTestParseException(\"set section must set at least a value\");\n-        }\n-\n-        return setSection;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/SetupSectionParser.java b/src/test/java/org/elasticsearch/test/rest/parser/SetupSectionParser.java\ndeleted file mode 100644\nindex 8e0f2293060..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/SetupSectionParser.java\n+++ /dev/null\n@@ -1,66 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.test.rest.section.SetupSection;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for setup sections\n- */\n-public class SetupSectionParser implements RestTestFragmentParser<SetupSection> {\n-\n-    @Override\n-    public SetupSection parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-\n-        XContentParser parser = parseContext.parser();\n-\n-        SetupSection setupSection = new SetupSection();\n-        setupSection.setSkipSection(parseContext.parseSkipSection());\n-\n-        boolean skip = setupSection.getSkipSection().skipVersion(parseContext.getCurrentVersion());\n-\n-        while (parser.currentToken() != XContentParser.Token.END_ARRAY) {\n-            if (skip) {\n-                //if there was a skip section, there was a setup section as well, which means that we are sure\n-                // the current token is at the beginning of a new object\n-                assert parser.currentToken() == XContentParser.Token.START_OBJECT;\n-                //we need to be at the beginning of an object to be able to skip children\n-                parser.skipChildren();\n-                //after skipChildren we are at the end of the skipped object, need to move on\n-                parser.nextToken();\n-            } else {\n-                parseContext.advanceToFieldName();\n-                if (!\"do\".equals(parser.currentName())) {\n-                    throw new RestTestParseException(\"section [\" + parser.currentName() + \"] not supported within setup section\");\n-                }\n-\n-                parser.nextToken();\n-                setupSection.addDoSection(parseContext.parseDoSection());\n-                parser.nextToken();\n-            }\n-        }\n-\n-        parser.nextToken();\n-\n-        return setupSection;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/parser/SkipSectionParser.java b/src/test/java/org/elasticsearch/test/rest/parser/SkipSectionParser.java\ndeleted file mode 100644\nindex 7c8d58389e4..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/parser/SkipSectionParser.java\n+++ /dev/null\n@@ -1,68 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.parser;\n-\n-import org.elasticsearch.common.Strings;\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.test.rest.section.SkipSection;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for skip sections\n- */\n-public class SkipSectionParser implements RestTestFragmentParser<SkipSection> {\n-\n-    @Override\n-    public SkipSection parse(RestTestSuiteParseContext parseContext) throws IOException, RestTestParseException {\n-\n-        XContentParser parser = parseContext.parser();\n-\n-        String currentFieldName = null;\n-        XContentParser.Token token;\n-        String version = null;\n-        String reason = null;\n-\n-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n-            if (token == XContentParser.Token.FIELD_NAME) {\n-                currentFieldName = parser.currentName();\n-            } else if (token.isValue()) {\n-                if (\"version\".equals(currentFieldName)) {\n-                    version = parser.text();\n-                } else if (\"reason\".equals(currentFieldName)) {\n-                    reason = parser.text();\n-                } else {\n-                    throw new RestTestParseException(\"field \" + currentFieldName + \" not supported within skip section\");\n-                }\n-            }\n-        }\n-\n-        parser.nextToken();\n-\n-        if (!Strings.hasLength(version)) {\n-            throw new RestTestParseException(\"version is mandatory within skip section\");\n-        }\n-        if (!Strings.hasLength(reason)) {\n-            throw new RestTestParseException(\"reason is mandatory within skip section\");\n-        }\n-\n-        return new SkipSection(version, reason);\n-\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java b/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java\ndeleted file mode 100644\nindex 159d750eea4..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java\n+++ /dev/null\n@@ -1,89 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import com.google.common.base.Joiner;\n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.ImmutableMap;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-\n-import java.util.List;\n-import java.util.Map;\n-\n-/**\n- * Represents a test fragment that contains the information needed to call an api\n- */\n-public class ApiCallSection {\n-\n-    private final String api;\n-    private final Map<String, String> params = Maps.newHashMap();\n-    private final List<String> bodies = Lists.newArrayList();\n-\n-    private static final String EMPTY_BODY = \"\";\n-\n-    public ApiCallSection(String api) {\n-        this.api = api;\n-    }\n-\n-    public String getApi() {\n-        return api;\n-    }\n-\n-    public Map<String, String> getParams() {\n-        //make sure we never modify the parameters once returned\n-        return ImmutableMap.copyOf(params);\n-    }\n-\n-    public void addParam(String key, String value) {\n-        String existingValue = params.get(key);\n-        if (existingValue != null) {\n-            value = Joiner.on(\",\").join(existingValue, value);\n-        }\n-        this.params.put(key, value);\n-    }\n-\n-    public List<String> getBodiesAsList() {\n-        return ImmutableList.copyOf(bodies);\n-    }\n-\n-    public String getBody() {\n-        if (bodies.size() == 0) {\n-            return EMPTY_BODY;\n-        }\n-\n-        if (bodies.size() == 1) {\n-            return bodies.get(0);\n-        }\n-\n-        StringBuilder bodyBuilder = new StringBuilder();\n-        for (String body : bodies) {\n-            bodyBuilder.append(body).append(\"\\n\");\n-        }\n-        return bodyBuilder.toString();\n-    }\n-\n-    public void addBody(String body) {\n-        this.bodies.add(body);\n-    }\n-\n-    public boolean hasBody() {\n-        return bodies.size() > 0;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/Assertion.java b/src/test/java/org/elasticsearch/test/rest/section/Assertion.java\ndeleted file mode 100644\nindex d719623011a..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/Assertion.java\n+++ /dev/null\n@@ -1,66 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.test.rest.RestTestExecutionContext;\n-\n-import java.io.IOException;\n-\n-/**\n- * Base class for executable sections that hold assertions\n- */\n-public abstract class Assertion implements ExecutableSection {\n-\n-    private final String field;\n-    private final Object expectedValue;\n-\n-    protected Assertion(String field, Object expectedValue) {\n-        this.field = field;\n-        this.expectedValue = expectedValue;\n-    }\n-\n-    public final String getField() {\n-        return field;\n-    }\n-\n-    public final Object getExpectedValue() {\n-        return expectedValue;\n-    }\n-\n-    protected final Object resolveExpectedValue(RestTestExecutionContext executionContext) {\n-        if (executionContext.isStashed(expectedValue)) {\n-            return executionContext.unstash(expectedValue.toString());\n-        }\n-        return expectedValue;\n-    }\n-\n-    protected final Object getActualValue(RestTestExecutionContext executionContext) throws IOException {\n-        return executionContext.response(field);\n-    }\n-\n-    @Override\n-    public final void execute(RestTestExecutionContext executionContext) throws IOException {\n-        doAssert(getActualValue(executionContext), resolveExpectedValue(executionContext));\n-    }\n-\n-    /**\n-     * Executes the assertion comparing the actual value (parsed from the response) with the expected one\n-     */\n-    protected abstract void doAssert(Object actualValue, Object expectedValue);\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/DoSection.java b/src/test/java/org/elasticsearch/test/rest/section/DoSection.java\ndeleted file mode 100644\nindex c528a0b8bb2..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/DoSection.java\n+++ /dev/null\n@@ -1,116 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.common.Strings;\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-import org.elasticsearch.test.rest.RestTestExecutionContext;\n-import org.elasticsearch.test.rest.client.RestException;\n-import org.elasticsearch.test.rest.client.RestResponse;\n-\n-import java.io.IOException;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n-\n-import static org.hamcrest.Matchers.*;\n-import static org.junit.Assert.assertThat;\n-import static org.junit.Assert.fail;\n-\n-/**\n- * Represents a do section:\n- *\n- *   - do:\n- *      catch:      missing\n- *      update:\n- *          index:  test_1\n- *          type:   test\n- *          id:     1\n- *          body:   { doc: { foo: bar } }\n- *\n- */\n-public class DoSection implements ExecutableSection {\n-\n-    private static final ESLogger logger = Loggers.getLogger(DoSection.class);\n-\n-    private String catchParam;\n-    private ApiCallSection apiCallSection;\n-\n-    public String getCatch() {\n-        return catchParam;\n-    }\n-\n-    public void setCatch(String catchParam) {\n-        this.catchParam = catchParam;\n-    }\n-\n-    public ApiCallSection getApiCallSection() {\n-        return apiCallSection;\n-    }\n-\n-    public void setApiCallSection(ApiCallSection apiCallSection) {\n-        this.apiCallSection = apiCallSection;\n-    }\n-\n-    @Override\n-    public void execute(RestTestExecutionContext executionContext) throws IOException {\n-\n-        try {\n-            executionContext.callApi(apiCallSection.getApi(), apiCallSection.getParams(), apiCallSection.getBody());\n-\n-        } catch(RestException e) {\n-            if (!Strings.hasLength(catchParam)) {\n-                fail(formatStatusCodeMessage(e.restResponse(), \"2xx\"));\n-            }\n-\n-            if (\"param\".equals(catchParam)) {\n-                //client should throw validation error before sending request\n-                //lets just return without doing anything as we don't have any client to test here\n-                logger.info(\"found [catch: param], no request sent\");\n-            } else if (\"missing\".equals(catchParam)) {\n-                assertThat(formatStatusCodeMessage(e.restResponse(), \"404\"), e.statusCode(), equalTo(404));\n-            } else if (\"conflict\".equals(catchParam)) {\n-                assertThat(formatStatusCodeMessage(e.restResponse(), \"409\"), e.statusCode(), equalTo(409));\n-            }  else if (\"forbidden\".equals(catchParam)) {\n-                assertThat(formatStatusCodeMessage(e.restResponse(), \"403\"), e.statusCode(), equalTo(403));\n-            } else if (\"request\".equals(catchParam)) {\n-                //generic error response from ES\n-                assertThat(formatStatusCodeMessage(e.restResponse(), \"4xx|5xx\"), e.statusCode(), greaterThanOrEqualTo(400));\n-            } else if (catchParam.startsWith(\"/\") && catchParam.endsWith(\"/\")) {\n-                //the text of the error message matches regular expression\n-                assertThat(formatStatusCodeMessage(e.restResponse(), \"4xx|5xx\"), e.statusCode(), greaterThanOrEqualTo(400));\n-                Object error = executionContext.response(\"error\");\n-                assertThat(\"error was expected in the response\", error, notNullValue());\n-                //remove delimiters from regex\n-                String regex = catchParam.substring(1, catchParam.length() - 1);\n-                String errorMessage = error.toString();\n-                Matcher matcher = Pattern.compile(regex).matcher(errorMessage);\n-                assertThat(\"error message [\" + errorMessage + \"] was expected to match [\" + catchParam + \"] but didn't\",\n-                        matcher.find(), equalTo(true));\n-            } else {\n-                throw new UnsupportedOperationException(\"catch value [\" + catchParam + \"] not supported\");\n-            }\n-        }\n-    }\n-\n-    private String formatStatusCodeMessage(RestResponse restResponse, String expected) {\n-        return \"expected [\" + expected + \"] status code but api [\" + apiCallSection.getApi() + \"] returned [\"\n-                + restResponse.getStatusCode() + \" \" + restResponse.getReasonPhrase() + \"] [\" + restResponse.getBody() + \"]\";\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/ExecutableSection.java b/src/test/java/org/elasticsearch/test/rest/section/ExecutableSection.java\ndeleted file mode 100644\nindex 5044bcdba6b..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/ExecutableSection.java\n+++ /dev/null\n@@ -1,34 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.test.rest.RestTestExecutionContext;\n-\n-import java.io.IOException;\n-\n-/**\n- * Represents a test fragment that can be executed (e.g. api call, assertion)\n- */\n-public interface ExecutableSection {\n-\n-    /**\n-     * Executes the section passing in the execution context\n-     */\n-    void execute(RestTestExecutionContext executionContext) throws IOException;\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java b/src/test/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java\ndeleted file mode 100644\nindex 95efa901307..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-\n-import static org.hamcrest.Matchers.greaterThan;\n-import static org.hamcrest.Matchers.instanceOf;\n-import static org.junit.Assert.assertThat;\n-\n-/**\n- * Represents a gt assert section:\n- *\n- *  - gt:    { fields._ttl: 0}\n- *\n- */\n-public class GreaterThanAssertion extends Assertion {\n-\n-    private static final ESLogger logger = Loggers.getLogger(GreaterThanAssertion.class);\n-\n-    public GreaterThanAssertion(String field, Object expectedValue) {\n-        super(field, expectedValue);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected void doAssert(Object actualValue, Object expectedValue) {\n-        logger.trace(\"assert that [{}] is greater than [{}]\", actualValue, expectedValue);\n-        assertThat(actualValue, instanceOf(Comparable.class));\n-        assertThat(expectedValue, instanceOf(Comparable.class));\n-        assertThat(errorMessage(), (Comparable)actualValue, greaterThan((Comparable) expectedValue));\n-    }\n-\n-    private String errorMessage() {\n-        return \"field [\" + getField() + \"] is not greater than [\" + getExpectedValue() + \"]\";\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/IsFalseAssertion.java b/src/test/java/org/elasticsearch/test/rest/section/IsFalseAssertion.java\ndeleted file mode 100644\nindex d563d38e424..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/IsFalseAssertion.java\n+++ /dev/null\n@@ -1,61 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-\n-import static org.hamcrest.Matchers.*;\n-import static org.junit.Assert.assertThat;\n-\n-/**\n- * Represents an is_false assert section:\n- *\n- *   - is_false:  get.fields.bar\n- *\n- */\n-public class IsFalseAssertion extends Assertion {\n-\n-    private static final ESLogger logger = Loggers.getLogger(IsFalseAssertion.class);\n-\n-    public IsFalseAssertion(String field) {\n-        super(field, false);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected void doAssert(Object actualValue, Object expectedValue) {\n-        logger.trace(\"assert that [{}] doesn't have a true value\", actualValue);\n-\n-        if (actualValue == null) {\n-            return;\n-        }\n-\n-        String actualString = actualValue.toString();\n-        assertThat(errorMessage(), actualString, anyOf(\n-                equalTo(\"\"),\n-                equalToIgnoringCase(Boolean.FALSE.toString()),\n-                equalTo(\"0\")\n-        ));\n-    }\n-\n-    private String errorMessage() {\n-        return \"field [\" + getField() + \"] has a true value but it shouldn't\";\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/IsTrueAssertion.java b/src/test/java/org/elasticsearch/test/rest/section/IsTrueAssertion.java\ndeleted file mode 100644\nindex 176b5ef98ff..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/IsTrueAssertion.java\n+++ /dev/null\n@@ -1,55 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-\n-import static org.hamcrest.Matchers.*;\n-import static org.junit.Assert.assertThat;\n-\n-/**\n- * Represents an is_true assert section:\n- *\n- *   - is_true:  get.fields.bar\n- *\n- */\n-public class IsTrueAssertion extends Assertion {\n-\n-    private static final ESLogger logger = Loggers.getLogger(IsTrueAssertion.class);\n-\n-    public IsTrueAssertion(String field) {\n-        super(field, true);\n-    }\n-\n-    @Override\n-    protected void doAssert(Object actualValue, Object expectedValue) {\n-        logger.trace(\"assert that [{}] has a true value\", actualValue);\n-        String errorMessage = errorMessage();\n-        assertThat(errorMessage, actualValue, notNullValue());\n-        String actualString = actualValue.toString();\n-        assertThat(errorMessage, actualString, not(equalTo(\"\")));\n-        assertThat(errorMessage, actualString, not(equalToIgnoringCase(Boolean.FALSE.toString())));\n-        assertThat(errorMessage, actualString, not(equalTo(\"0\")));\n-    }\n-\n-    private String errorMessage() {\n-        return \"field [\" + getField() + \"] doesn't have a true value\";\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/LengthAssertion.java b/src/test/java/org/elasticsearch/test/rest/section/LengthAssertion.java\ndeleted file mode 100644\nindex 8070634325d..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/LengthAssertion.java\n+++ /dev/null\n@@ -1,64 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-\n-import java.util.List;\n-import java.util.Map;\n-\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.instanceOf;\n-import static org.junit.Assert.assertThat;\n-\n-/**\n- * Represents a length assert section:\n- *\n- *   - length:   { hits.hits: 1  }\n- *\n- */\n-public class LengthAssertion extends Assertion {\n-\n-    private static final ESLogger logger = Loggers.getLogger(LengthAssertion.class);\n-\n-    public LengthAssertion(String field, Object expectedValue) {\n-        super(field, expectedValue);\n-    }\n-\n-    @Override\n-    protected void doAssert(Object actualValue, Object expectedValue) {\n-        logger.trace(\"assert that [{}] has length [{}]\", actualValue, expectedValue);\n-        assertThat(expectedValue, instanceOf(Number.class));\n-        int length = ((Number) expectedValue).intValue();\n-        if (actualValue instanceof String) {\n-            assertThat(errorMessage(), ((String) actualValue).length(), equalTo(length));\n-        } else if (actualValue instanceof List) {\n-            assertThat(errorMessage(), ((List) actualValue).size(), equalTo(length));\n-        } else if (actualValue instanceof Map) {\n-            assertThat(errorMessage(), ((Map) actualValue).keySet().size(), equalTo(length));\n-        } else {\n-            throw new UnsupportedOperationException(\"value is of unsupported type [\" + actualValue.getClass().getSimpleName() + \"]\");\n-        }\n-    }\n-\n-    private String errorMessage() {\n-        return \"field [\" + getField() + \"] doesn't have length [\" + getExpectedValue() + \"]\";\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/LessThanAssertion.java b/src/test/java/org/elasticsearch/test/rest/section/LessThanAssertion.java\ndeleted file mode 100644\nindex b5e60903120..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/LessThanAssertion.java\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-\n-import static org.hamcrest.Matchers.instanceOf;\n-import static org.hamcrest.Matchers.lessThan;\n-import static org.junit.Assert.assertThat;\n-\n-/**\n- * Represents a lt assert section:\n- *\n- *  - lt:    { fields._ttl: 20000}\n- *\n- */\n-public class LessThanAssertion extends Assertion {\n-\n-    private static final ESLogger logger = Loggers.getLogger(LessThanAssertion.class);\n-\n-    public LessThanAssertion(String field, Object expectedValue) {\n-        super(field, expectedValue);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected void doAssert(Object actualValue, Object expectedValue) {\n-        logger.trace(\"assert that [{}] is less than [{}]\", actualValue, expectedValue);\n-        assertThat(actualValue, instanceOf(Comparable.class));\n-        assertThat(expectedValue, instanceOf(Comparable.class));\n-        assertThat(errorMessage(), (Comparable)actualValue, lessThan((Comparable)expectedValue));\n-    }\n-\n-    private String errorMessage() {\n-        return \"field [\" + getField() + \"] is not less than [\" + getExpectedValue() + \"]\";\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/MatchAssertion.java b/src/test/java/org/elasticsearch/test/rest/section/MatchAssertion.java\ndeleted file mode 100644\nindex c4823466f3d..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/MatchAssertion.java\n+++ /dev/null\n@@ -1,59 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.common.logging.ESLogger;\n-import org.elasticsearch.common.logging.Loggers;\n-\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.notNullValue;\n-import static org.junit.Assert.assertThat;\n-\n-/**\n- * Represents a match assert section:\n- *\n- *   - match:   { get.fields._routing: \"5\" }\n- *\n- */\n-public class MatchAssertion extends Assertion {\n-\n-    private static final ESLogger logger = Loggers.getLogger(MatchAssertion.class);\n-\n-    public MatchAssertion(String field, Object expectedValue) {\n-        super(field, expectedValue);\n-    }\n-\n-    @Override\n-    protected void doAssert(Object actualValue, Object expectedValue) {\n-        assertThat(errorMessage(), actualValue, notNullValue());\n-        logger.trace(\"assert that [{}] matches [{}]\", actualValue, expectedValue);\n-        if (!actualValue.getClass().equals(expectedValue.getClass())) {\n-            if (actualValue instanceof Number && expectedValue instanceof Number) {\n-                //Double 1.0 is equals to Integer 1\n-                assertThat(errorMessage(), ((Number) actualValue).doubleValue(), equalTo(((Number) expectedValue).doubleValue()));\n-            }\n-        } else {\n-            assertThat(errorMessage(), actualValue, equalTo(expectedValue));\n-        }\n-    }\n-\n-    private String errorMessage() {\n-        return \"field [\" + getField() + \"] doesn't match the expected value\";\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/RestTestSuite.java b/src/test/java/org/elasticsearch/test/rest/section/RestTestSuite.java\ndeleted file mode 100644\nindex ea19cb73895..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/RestTestSuite.java\n+++ /dev/null\n@@ -1,78 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import com.google.common.collect.Lists;\n-\n-import java.io.File;\n-import java.util.List;\n-\n-/**\n- * Holds a REST test suite loaded from a specific yaml file.\n- * Supports a setup section and multiple test sections.\n- */\n-public class RestTestSuite {\n-\n-    private final String api;\n-    private final String name;\n-\n-    private SetupSection setupSection;\n-\n-    private List<TestSection> testSections = Lists.newArrayList();\n-\n-    public RestTestSuite(String api, String name) {\n-        this.api = replaceDot(api);\n-        this.name = replaceDot(name);\n-    }\n-\n-    public String getApi() {\n-        return api;\n-    }\n-\n-    public String getName() {\n-        return name;\n-    }\n-\n-    //describes the rest test suite (e.g. index/10_with_id)\n-    //useful also to reproduce failures (RestReproduceInfoPrinter)\n-    public String getDescription() {\n-        return api + File.separator + name;\n-    }\n-\n-    private static String replaceDot(String value) {\n-        // '.' is used as separator internally and not expected to be within suite or test names, better replace it\n-        return value.replace('.', '_');\n-    }\n-\n-    public SetupSection getSetupSection() {\n-        return setupSection;\n-    }\n-\n-    public void setSetupSection(SetupSection setupSection) {\n-        this.setupSection = setupSection;\n-    }\n-\n-    public void addTestSection(TestSection testSection) {\n-        this.testSections.add(testSection);\n-    }\n-\n-    public List<TestSection> getTestSections() {\n-        return testSections;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/SetSection.java b/src/test/java/org/elasticsearch/test/rest/section/SetSection.java\ndeleted file mode 100644\nindex 08db06006d7..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/SetSection.java\n+++ /dev/null\n@@ -1,52 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import com.google.common.collect.Maps;\n-import org.elasticsearch.test.rest.RestTestExecutionContext;\n-\n-import java.io.IOException;\n-import java.util.Map;\n-\n-/**\n- * Represents a set section:\n- *\n- *   - set: {_scroll_id: scroll_id}\n- *\n- */\n-public class SetSection implements ExecutableSection {\n-\n-    private Map<String, String> stash = Maps.newHashMap();\n-\n-    public void addSet(String responseField, String stashedField) {\n-        stash.put(responseField, stashedField);\n-    }\n-\n-    public Map<String, String> getStash() {\n-        return stash;\n-    }\n-\n-    @Override\n-    public void execute(RestTestExecutionContext executionContext) throws IOException {\n-        for (Map.Entry<String, String> entry : stash.entrySet()) {\n-            Object actualValue = executionContext.response(entry.getKey());\n-            executionContext.stash(entry.getValue(), actualValue);\n-        }\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/SetupSection.java b/src/test/java/org/elasticsearch/test/rest/section/SetupSection.java\ndeleted file mode 100644\nindex a8edfcc5169..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/SetupSection.java\n+++ /dev/null\n@@ -1,60 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import com.google.common.collect.Lists;\n-\n-import java.util.List;\n-\n-/**\n- * Represents a setup section. Holds a skip section and multiple do sections.\n- */\n-public class SetupSection {\n-\n-    public static final SetupSection EMPTY;\n-\n-    static {\n-        EMPTY = new SetupSection();\n-        EMPTY.setSkipSection(SkipSection.EMPTY);\n-    }\n-\n-    private SkipSection skipSection;\n-\n-    private List<DoSection> doSections = Lists.newArrayList();\n-\n-    public SkipSection getSkipSection() {\n-        return skipSection;\n-    }\n-\n-    public void setSkipSection(SkipSection skipSection) {\n-        this.skipSection = skipSection;\n-    }\n-\n-    public List<DoSection> getDoSections() {\n-        return doSections;\n-    }\n-\n-    public void addDoSection(DoSection doSection) {\n-        this.doSections.add(doSection);\n-    }\n-\n-    public boolean isEmpty() {\n-        return EMPTY.equals(this);\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/SkipSection.java b/src/test/java/org/elasticsearch/test/rest/section/SkipSection.java\ndeleted file mode 100644\nindex bfd713f84ce..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/SkipSection.java\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import org.elasticsearch.test.rest.support.VersionUtils;\n-\n-/**\n- * Represents a skip section that tells whether a specific test section or suite needs to be skipped\n- * based on the elasticsearch version the tests are running against.\n- */\n-public class SkipSection {\n-\n-    public static final SkipSection EMPTY = new SkipSection(\"\", \"\");\n-\n-    private final String version;\n-    private final String reason;\n-\n-    public SkipSection(String version, String reason) {\n-        this.version = version;\n-        this.reason = reason;\n-    }\n-\n-    public String getVersion() {\n-        return version;\n-    }\n-\n-    public String getReason() {\n-        return reason;\n-    }\n-\n-    public boolean skipVersion(String currentVersion) {\n-        if (isEmpty()) {\n-            return false;\n-        }\n-        return VersionUtils.skipCurrentVersion(version, currentVersion);\n-    }\n-\n-    public boolean isEmpty() {\n-        return EMPTY.equals(this);\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/section/TestSection.java b/src/test/java/org/elasticsearch/test/rest/section/TestSection.java\ndeleted file mode 100644\nindex 0b32ba57050..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/section/TestSection.java\n+++ /dev/null\n@@ -1,57 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.section;\n-\n-import com.google.common.collect.Lists;\n-\n-import java.util.List;\n-\n-/**\n- * Represents a test section, which is composed of a skip section and multiple executable sections.\n- */\n-public class TestSection {\n-    private final String name;\n-    private SkipSection skipSection;\n-    private final List<ExecutableSection> executableSections;\n-\n-    public TestSection(String name) {\n-        this.name = name;\n-        this.executableSections = Lists.newArrayList();\n-    }\n-\n-    public String getName() {\n-        return name;\n-    }\n-\n-    public SkipSection getSkipSection() {\n-        return skipSection;\n-    }\n-\n-    public void setSkipSection(SkipSection skipSection) {\n-        this.skipSection = skipSection;\n-    }\n-\n-    public List<ExecutableSection> getExecutableSections() {\n-        return executableSections;\n-    }\n-\n-    public void addExecutableSection(ExecutableSection executableSection) {\n-        this.executableSections.add(executableSection);\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/spec/RestApi.java b/src/test/java/org/elasticsearch/test/rest/spec/RestApi.java\ndeleted file mode 100644\nindex 478484bf628..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/spec/RestApi.java\n+++ /dev/null\n@@ -1,214 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.spec;\n-\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import org.apache.http.client.methods.HttpPost;\n-import org.apache.http.client.methods.HttpPut;\n-import org.apache.lucene.util.PriorityQueue;\n-\n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n-\n-/**\n- * Represents an elasticsearch REST endpoint (api)\n- */\n-public class RestApi {\n-\n-    private static final String ALL = \"_all\";\n-\n-    private final String name;\n-    private List<String> methods = Lists.newArrayList();\n-    private List<String> paths = Lists.newArrayList();\n-    private List<String> pathParts = Lists.newArrayList();\n-\n-    RestApi(String name) {\n-        this.name = name;\n-    }\n-\n-    RestApi(RestApi restApi, String name, String... methods) {\n-        this.name = name;\n-        this.methods = Arrays.asList(methods);\n-        paths.addAll(restApi.getPaths());\n-        pathParts.addAll(restApi.getPathParts());\n-    }\n-\n-    RestApi(RestApi restApi, List<String> paths) {\n-        this.name = restApi.getName();\n-        this.methods = restApi.getMethods();\n-        this.paths.addAll(paths);\n-        pathParts.addAll(restApi.getPathParts());\n-    }\n-\n-    public String getName() {\n-        return name;\n-    }\n-\n-    public List<String> getMethods() {\n-        return methods;\n-    }\n-\n-    /**\n-     * Returns the supported http methods given the rest parameters provided\n-     */\n-    public List<String> getSupportedMethods(Set<String> restParams) {\n-        //we try to avoid hardcoded mappings but the index api is the exception\n-        if (\"index\".equals(name) || \"create\".equals(name)) {\n-            List<String> indexMethods = Lists.newArrayList();\n-            for (String method : methods) {\n-                if (restParams.contains(\"id\")) {\n-                    //PUT when the id is provided\n-                    if (HttpPut.METHOD_NAME.equals(method)) {\n-                        indexMethods.add(method);\n-                    }\n-                } else {\n-                    //POST without id\n-                    if (HttpPost.METHOD_NAME.equals(method)) {\n-                        indexMethods.add(method);\n-                    }\n-                }\n-            }\n-            return indexMethods;\n-        }\n-\n-        return methods;\n-    }\n-\n-    void addMethod(String method) {\n-        this.methods.add(method);\n-    }\n-\n-    public List<String> getPaths() {\n-        return paths;\n-    }\n-\n-    void addPath(String path) {\n-        this.paths.add(path);\n-    }\n-\n-    public List<String> getPathParts() {\n-        return pathParts;\n-    }\n-\n-    void addPathPart(String pathPart) {\n-        this.pathParts.add(pathPart);\n-    }\n-\n-    /**\n-     * Finds the best matching rest path given the current parameters and replaces\n-     * placeholders with their corresponding values received as arguments\n-     */\n-    public String getFinalPath(Map<String, String> pathParams) {\n-        RestPath matchingRestPath = findMatchingRestPath(pathParams.keySet());\n-        String path = matchingRestPath.path;\n-        for (Map.Entry<String, String> paramEntry : matchingRestPath.params.entrySet()) {\n-            //replace path placeholders with actual values\n-            String value = pathParams.get(paramEntry.getValue());\n-            if (value == null) {\n-                //there might be additional placeholder to replace, not available as input params\n-                //it can only be {index} or {type} to be replaced with _all\n-                if (paramEntry.getValue().equals(\"index\") || paramEntry.getValue().equals(\"type\")) {\n-                    value = ALL;\n-                } else {\n-                    throw new IllegalArgumentException(\"path [\" + path + \"] contains placeholders that weren't replaced with proper values\");\n-                }\n-            }\n-            path = path.replace(paramEntry.getKey(), value);\n-        }\n-        return path;\n-    }\n-\n-    /**\n-     * Finds the best matching rest path out of the available ones with the current api (based on REST spec).\n-     *\n-     * The best path is the one that has exactly the same number of placeholders to replace\n-     * (e.g. /{index}/{type}/{id} when the params are exactly index, type and id).\n-     * Otherwise there might be additional placeholders, thus we use the path with the least additional placeholders.\n-     * (e.g. get with only index and id as parameters, the closest (and only) path contains {type} too, which becomes _all)\n-     */\n-    private RestPath findMatchingRestPath(Set<String> restParams) {\n-\n-        RestPath[] restPaths = buildRestPaths();\n-\n-        //We need to find the path that has exactly the placeholders corresponding to our params\n-        //If there's no exact match we fallback to the closest one (with as less additional placeholders as possible)\n-        //The fallback is needed for:\n-        //1) get, get_source and exists with only index and id => /{index}/_all/{id} (\n-        //2) search with only type => /_all/{type/_search\n-        PriorityQueue<RestPath> restPathQueue = new PriorityQueue<RestPath>(1) {\n-            @Override\n-            protected boolean lessThan(RestPath a, RestPath b) {\n-                return a.params.size() >= b.params.size();\n-            }\n-        };\n-        for (RestPath restPath : restPaths) {\n-            if (restPath.params.values().containsAll(restParams)) {\n-                restPathQueue.insertWithOverflow(restPath);\n-            }\n-        }\n-\n-        if (restPathQueue.size() > 0) {\n-            return restPathQueue.top();\n-        }\n-\n-        throw new IllegalArgumentException(\"unable to find best path for api [\" + name + \"] and params \" + restParams);\n-    }\n-\n-    private RestPath[] buildRestPaths() {\n-        RestPath[] restPaths = new RestPath[paths.size()];\n-        for (int i = 0; i < restPaths.length; i++) {\n-            restPaths[i] = new RestPath(paths.get(i));\n-        }\n-        return restPaths;\n-    }\n-\n-    private static class RestPath {\n-        private static final Pattern PLACEHOLDERS_PATTERN = Pattern.compile(\"(\\\\{(.*?)})\");\n-\n-        final String path;\n-        //contains param to replace (e.g. {index}) and param key to use for lookup in the current values map (e.g. index)\n-        final Map<String, String> params;\n-\n-        RestPath(String path) {\n-            this.path = path;\n-            this.params = extractParams(path);\n-        }\n-\n-        private static Map<String,String> extractParams(String input) {\n-            Map<String, String> params = Maps.newHashMap();\n-            Matcher matcher = PLACEHOLDERS_PATTERN.matcher(input);\n-            while (matcher.find()) {\n-                //key is e.g. {index}\n-                String key = input.substring(matcher.start(), matcher.end());\n-                if (matcher.groupCount() != 2) {\n-                    throw new IllegalArgumentException(\"no lookup key found for param [\" + key + \"]\");\n-                }\n-                //to be replaced with current value found with key e.g. index\n-                String value = matcher.group(2);\n-                params.put(key, value);\n-            }\n-            return params;\n-        }\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/spec/RestApiParser.java b/src/test/java/org/elasticsearch/test/rest/spec/RestApiParser.java\ndeleted file mode 100644\nindex 114615d7664..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/spec/RestApiParser.java\n+++ /dev/null\n@@ -1,103 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.spec;\n-\n-import org.elasticsearch.common.xcontent.XContentParser;\n-\n-import java.io.IOException;\n-\n-/**\n- * Parser for a REST api spec (single json file)\n- */\n-public class RestApiParser {\n-\n-    public RestApi parse(XContentParser parser) throws IOException {\n-\n-        try {\n-            while ( parser.nextToken() != XContentParser.Token.FIELD_NAME ) {\n-                //move to first field name\n-            }\n-\n-            RestApi restApi = new RestApi(parser.currentName());\n-\n-            int level = -1;\n-            while (parser.nextToken() != XContentParser.Token.END_OBJECT || level >= 0) {\n-\n-                if (parser.currentToken() == XContentParser.Token.FIELD_NAME) {\n-                    if (\"methods\".equals(parser.currentName())) {\n-                        parser.nextToken();\n-                        while (parser.nextToken() == XContentParser.Token.VALUE_STRING) {\n-                            restApi.addMethod(parser.text());\n-                        }\n-                    }\n-\n-                    if (\"url\".equals(parser.currentName())) {\n-                        String currentFieldName = \"url\";\n-                        int innerLevel = -1;\n-                        while(parser.nextToken() != XContentParser.Token.END_OBJECT || innerLevel >= 0) {\n-                            if (parser.currentToken() == XContentParser.Token.FIELD_NAME) {\n-                                currentFieldName = parser.currentName();\n-                            }\n-\n-                            if (parser.currentToken() == XContentParser.Token.START_ARRAY && \"paths\".equals(currentFieldName)) {\n-                                while (parser.nextToken() == XContentParser.Token.VALUE_STRING) {\n-                                    restApi.addPath(parser.text());\n-                                }\n-                            }\n-\n-                            if (parser.currentToken() == XContentParser.Token.START_OBJECT && \"parts\".equals(currentFieldName)) {\n-                                while (parser.nextToken() == XContentParser.Token.FIELD_NAME) {\n-                                    restApi.addPathPart(parser.currentName());\n-                                    parser.nextToken();\n-                                    assert parser.currentToken() == XContentParser.Token.START_OBJECT;\n-                                    parser.skipChildren();\n-                                }\n-                            }\n-\n-                            if (parser.currentToken() == XContentParser.Token.START_OBJECT) {\n-                                innerLevel++;\n-                            }\n-                            if (parser.currentToken() == XContentParser.Token.END_OBJECT) {\n-                                innerLevel--;\n-                            }\n-                        }\n-                    }\n-                }\n-\n-                if (parser.currentToken() == XContentParser.Token.START_OBJECT) {\n-                    level++;\n-                }\n-                if (parser.currentToken() == XContentParser.Token.END_OBJECT) {\n-                    level--;\n-                }\n-\n-            }\n-\n-            parser.nextToken();\n-            assert parser.currentToken() == XContentParser.Token.END_OBJECT;\n-            parser.nextToken();\n-\n-            return restApi;\n-\n-        } finally {\n-            parser.close();\n-        }\n-    }\n-\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/spec/RestSpec.java b/src/test/java/org/elasticsearch/test/rest/spec/RestSpec.java\ndeleted file mode 100644\nindex f697b13e0d1..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/spec/RestSpec.java\n+++ /dev/null\n@@ -1,81 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.spec;\n-\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.common.xcontent.json.JsonXContent;\n-import org.elasticsearch.test.rest.support.FileUtils;\n-\n-import java.io.File;\n-import java.io.FileInputStream;\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.Map;\n-\n-/**\n- * Holds the elasticsearch REST spec\n- */\n-public class RestSpec {\n-    Map<String, RestApi> restApiMap = Maps.newHashMap();\n-\n-    private RestSpec() {\n-    }\n-\n-    void addApi(RestApi restApi) {\n-        if (\"info\".equals(restApi.getName())) {\n-            //info and ping should really be two different api in the rest spec\n-            //info (GET|HEAD /) needs to be manually split into 1) info: GET /  2) ping: HEAD /\n-            restApiMap.put(\"info\", new RestApi(restApi, \"info\", \"GET\"));\n-            restApiMap.put(\"ping\", new RestApi(restApi, \"ping\", \"HEAD\"));\n-        } else if (\"get\".equals(restApi.getName())) {\n-            //get_source endpoint shouldn't be present in the rest spec for the get api\n-            //as get_source is already a separate api\n-            List<String> paths = Lists.newArrayList();\n-            for (String path : restApi.getPaths()) {\n-                if (!path.endsWith(\"/_source\")) {\n-                    paths.add(path);\n-                }\n-            }\n-            restApiMap.put(restApi.getName(), new RestApi(restApi, paths));\n-        } else {\n-            restApiMap.put(restApi.getName(), restApi);\n-        }\n-    }\n-\n-    public RestApi getApi(String api) {\n-        return restApiMap.get(api);\n-    }\n-\n-    /**\n-     * Parses the complete set of REST spec available under the provided directories\n-     */\n-    public static RestSpec parseFrom(String optionalPathPrefix, String... paths) throws IOException {\n-        RestSpec restSpec = new RestSpec();\n-        for (String path : paths) {\n-            for (File jsonFile : FileUtils.findJsonSpec(optionalPathPrefix, path)) {\n-                XContentParser parser = JsonXContent.jsonXContent.createParser(new FileInputStream(jsonFile));\n-                RestApi restApi = new RestApiParser().parse(parser);\n-                restSpec.addApi(restApi);\n-            }\n-        }\n-        return restSpec;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/support/FileUtils.java b/src/test/java/org/elasticsearch/test/rest/support/FileUtils.java\ndeleted file mode 100644\nindex 3b93f4affca..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/support/FileUtils.java\n+++ /dev/null\n@@ -1,144 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.support;\n-\n-import com.google.common.collect.Maps;\n-import com.google.common.collect.Sets;\n-import org.elasticsearch.common.Strings;\n-\n-import java.io.File;\n-import java.io.FileFilter;\n-import java.io.FileNotFoundException;\n-import java.net.URL;\n-import java.util.Map;\n-import java.util.Set;\n-\n-public final class FileUtils {\n-\n-    private static final String YAML_SUFFIX = \".yaml\";\n-    private static final String JSON_SUFFIX = \".json\";\n-\n-    private FileUtils() {\n-\n-    }\n-\n-    /**\n-     * Returns the json files found within the directory provided as argument.\n-     * Files are looked up in the classpath first, then outside of it if not found.\n-     */\n-    public static Set<File> findJsonSpec(String optionalPathPrefix, String path) throws FileNotFoundException {\n-        File dir = resolveFile(optionalPathPrefix, path, null);\n-\n-        if (!dir.isDirectory()) {\n-            throw new FileNotFoundException(\"file [\" + path + \"] is not a directory\");\n-        }\n-\n-        File[] jsonFiles = dir.listFiles(new FileFilter() {\n-            @Override\n-            public boolean accept(File pathname) {\n-                return pathname.getName().endsWith(JSON_SUFFIX);\n-            }\n-        });\n-\n-        if (jsonFiles == null || jsonFiles.length == 0) {\n-            throw new FileNotFoundException(\"no json files found within [\" + path + \"]\");\n-        }\n-\n-        return Sets.newHashSet(jsonFiles);\n-    }\n-\n-    /**\n-     * Returns the yaml files found within the paths provided.\n-     * Each input path can either be a single file (the .yaml suffix is optional) or a directory.\n-     * Each path is looked up in the classpath first, then outside of it if not found yet.\n-     */\n-    public static Map<String, Set<File>> findYamlSuites(final String optionalPathPrefix, final String... paths) throws FileNotFoundException {\n-        Map<String, Set<File>> yamlSuites = Maps.newHashMap();\n-        for (String path : paths) {\n-            collectFiles(resolveFile(optionalPathPrefix, path, YAML_SUFFIX), YAML_SUFFIX, yamlSuites);\n-        }\n-        return yamlSuites;\n-    }\n-\n-    private static File resolveFile(String optionalPathPrefix, String path, String optionalFileSuffix) throws FileNotFoundException {\n-        //try within classpath with and without file suffix (as it could be a single test suite)\n-        URL resource = findResource(path, optionalFileSuffix);\n-        if (resource == null) {\n-            //try within classpath with optional prefix: /rest-spec/test (or /rest-test/api) is optional\n-            String newPath = optionalPathPrefix + File.separator + path;\n-            resource = findResource(newPath, optionalFileSuffix);\n-            if (resource == null) {\n-                //if it wasn't on classpath we look outside ouf the classpath\n-                File file = findFile(path, optionalFileSuffix);\n-                if (!file.exists()) {\n-                    throw new FileNotFoundException(\"file [\" + path + \"] doesn't exist\");\n-                }\n-                return file;\n-            }\n-        }\n-        return new File(resource.getFile());\n-    }\n-\n-    private static URL findResource(String path, String optionalFileSuffix) {\n-        URL resource = FileUtils.class.getResource(path);\n-        if (resource == null) {\n-            //if not found we append the file suffix to the path (as it is optional)\n-            if (Strings.hasLength(optionalFileSuffix) && !path.endsWith(optionalFileSuffix)) {\n-                resource = FileUtils.class.getResource(path + optionalFileSuffix);\n-            }\n-        }\n-        return resource;\n-    }\n-\n-    private static File findFile(String path, String optionalFileSuffix) {\n-        File file = new File(path);\n-        if (!file.exists()) {\n-            file = new File(path + optionalFileSuffix);\n-        }\n-        return file;\n-    }\n-\n-    private static void collectFiles(final File file, final String fileSuffix, final Map<String, Set<File>> files) {\n-        if (file.isFile()) {\n-            // '.' is uses as separator internally and not expected to be within suite or test names, better replace it\n-            String groupName = file.getParentFile().getName().replace('.', '_');\n-            Set<File> filesSet = files.get(groupName);\n-            if (filesSet == null) {\n-                filesSet = Sets.newHashSet();\n-                files.put(groupName, filesSet);\n-            }\n-            filesSet.add(file);\n-        } else if (file.isDirectory()) {\n-            walkDir(file, fileSuffix, files);\n-        }\n-    }\n-\n-    private static void walkDir(final File dir, final String fileSuffix, final Map<String, Set<File>> files) {\n-        File[] children = dir.listFiles(new FileFilter() {\n-            @Override\n-            public boolean accept(File pathname) {\n-                return pathname.isDirectory() || pathname.getName().endsWith(fileSuffix);\n-            }\n-        });\n-\n-        for (File file : children) {\n-            collectFiles(file, fileSuffix, files);\n-        }\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/support/VersionUtils.java b/src/test/java/org/elasticsearch/test/rest/support/VersionUtils.java\ndeleted file mode 100644\nindex f3290f8ab75..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/support/VersionUtils.java\n+++ /dev/null\n@@ -1,87 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.support;\n-\n-public final class VersionUtils {\n-\n-    private VersionUtils() {\n-\n-    }\n-\n-    /**\n-     * Parses an elasticsearch version string into an int array with an element per part\n-     * e.g. 0.90.7 => [0,90,7]\n-     */\n-    public static int[] parseVersionNumber(String version) {\n-        String[] split = version.split(\"\\\\.\");\n-        //we only take the first 3 parts if there are more, but less is ok too (e.g. 999)\n-        int length = Math.min(3, split.length);\n-        int[] versionNumber = new int[length];\n-        for (int i = 0; i < length; i++) {\n-            try {\n-                versionNumber[i] = Integer.valueOf(split[i]);\n-            } catch (NumberFormatException e) {\n-                throw new IllegalArgumentException(\"version is not a number\", e);\n-            }\n-\n-        }\n-        return versionNumber;\n-    }\n-\n-    /**\n-     * Compares the skip version read from a test fragment with the elasticsearch version\n-     * the tests are running against and determines whether the test fragment needs to be skipped\n-     */\n-    public static boolean skipCurrentVersion(String skipVersion, String currentVersion) {\n-        int[] currentVersionNumber = parseVersionNumber(currentVersion);\n-\n-        String[] skipVersions = skipVersion.split(\"-\");\n-        if (skipVersions.length > 2) {\n-            throw new IllegalArgumentException(\"too many skip versions found\");\n-        }\n-\n-        String skipVersionLowerBound = skipVersions[0].trim();\n-        String skipVersionUpperBound = skipVersions[1].trim();\n-\n-        int[] skipVersionLowerBoundNumber = parseVersionNumber(skipVersionLowerBound);\n-        int[] skipVersionUpperBoundNumber = parseVersionNumber(skipVersionUpperBound);\n-\n-        int length = Math.min(skipVersionLowerBoundNumber.length, currentVersionNumber.length);\n-        for (int i = 0; i < length; i++) {\n-            if (currentVersionNumber[i] < skipVersionLowerBoundNumber[i]) {\n-                return false;\n-            }\n-            if (currentVersionNumber[i] > skipVersionLowerBoundNumber[i]) {\n-                break;\n-            }\n-        }\n-\n-        length = Math.min(skipVersionUpperBoundNumber.length, currentVersionNumber.length);\n-        for (int i = 0; i < length; i++) {\n-            if (currentVersionNumber[i] > skipVersionUpperBoundNumber[i]) {\n-                return false;\n-            }\n-            if (currentVersionNumber[i] < skipVersionUpperBoundNumber[i]) {\n-                break;\n-            }\n-        }\n-\n-        return true;\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/AbstractParserTests.java b/src/test/java/org/elasticsearch/test/rest/test/AbstractParserTests.java\ndeleted file mode 100644\nindex 78e1fb9a982..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/AbstractParserTests.java\n+++ /dev/null\n@@ -1,41 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.test.ElasticsearchTestCase;\n-import org.junit.After;\n-import org.junit.Ignore;\n-\n-import static org.hamcrest.Matchers.nullValue;\n-\n-@Ignore\n-public class AbstractParserTests extends ElasticsearchTestCase {\n-\n-    protected XContentParser parser;\n-\n-    @After\n-    public void tearDown() throws Exception {\n-        super.tearDown();\n-        //this is the way to make sure that we consumed the whole yaml\n-        assertThat(parser.currentToken(), nullValue());\n-        parser.close();\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/AssertionParsersTests.java b/src/test/java/org/elasticsearch/test/rest/test/AssertionParsersTests.java\ndeleted file mode 100644\nindex df9d9ef15f4..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/AssertionParsersTests.java\n+++ /dev/null\n@@ -1,174 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.yaml.YamlXContent;\n-import org.elasticsearch.test.rest.parser.*;\n-import org.elasticsearch.test.rest.section.*;\n-import org.junit.Test;\n-\n-import java.util.List;\n-import java.util.Map;\n-\n-import static org.hamcrest.Matchers.*;\n-\n-public class AssertionParsersTests extends AbstractParserTests {\n-\n-    @Test\n-    public void testParseIsTrue() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"get.fields._timestamp\"\n-        );\n-\n-        IsTrueParser isTrueParser = new IsTrueParser();\n-        IsTrueAssertion trueAssertion = isTrueParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(trueAssertion, notNullValue());\n-        assertThat(trueAssertion.getField(), equalTo(\"get.fields._timestamp\"));\n-    }\n-\n-    @Test\n-    public void testParseIsFalse() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"docs.1._source\"\n-        );\n-\n-        IsFalseParser isFalseParser = new IsFalseParser();\n-        IsFalseAssertion falseAssertion = isFalseParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(falseAssertion, notNullValue());\n-        assertThat(falseAssertion.getField(), equalTo(\"docs.1._source\"));\n-    }\n-\n-    @Test\n-    public void testParseGreaterThan() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{ field: 3}\"\n-        );\n-\n-        GreaterThanParser greaterThanParser = new GreaterThanParser();\n-        GreaterThanAssertion greaterThanAssertion = greaterThanParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        assertThat(greaterThanAssertion, notNullValue());\n-        assertThat(greaterThanAssertion.getField(), equalTo(\"field\"));\n-        assertThat(greaterThanAssertion.getExpectedValue(), instanceOf(Integer.class));\n-        assertThat((Integer) greaterThanAssertion.getExpectedValue(), equalTo(3));\n-    }\n-\n-    @Test\n-    public void testParseLessThan() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{ field: 3}\"\n-        );\n-\n-        LessThanParser lessThanParser = new LessThanParser();\n-        LessThanAssertion lessThanAssertion = lessThanParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        assertThat(lessThanAssertion, notNullValue());\n-        assertThat(lessThanAssertion.getField(), equalTo(\"field\"));\n-        assertThat(lessThanAssertion.getExpectedValue(), instanceOf(Integer.class));\n-        assertThat((Integer) lessThanAssertion.getExpectedValue(), equalTo(3));\n-    }\n-\n-    @Test\n-    public void testParseLength() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{ _id: 22}\"\n-        );\n-\n-        LengthParser lengthParser = new LengthParser();\n-        LengthAssertion lengthAssertion = lengthParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        assertThat(lengthAssertion, notNullValue());\n-        assertThat(lengthAssertion.getField(), equalTo(\"_id\"));\n-        assertThat(lengthAssertion.getExpectedValue(), instanceOf(Integer.class));\n-        assertThat((Integer) lengthAssertion.getExpectedValue(), equalTo(22));\n-    }\n-\n-    @Test\n-    @SuppressWarnings(\"unchecked\")\n-    public void testParseMatchSimpleIntegerValue() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{ field: 10 }\"\n-        );\n-\n-        MatchParser matchParser = new MatchParser();\n-        MatchAssertion matchAssertion = matchParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(matchAssertion, notNullValue());\n-        assertThat(matchAssertion.getField(), equalTo(\"field\"));\n-        assertThat(matchAssertion.getExpectedValue(), instanceOf(Integer.class));\n-        assertThat((Integer) matchAssertion.getExpectedValue(), equalTo(10));\n-    }\n-\n-    @Test\n-    @SuppressWarnings(\"unchecked\")\n-    public void testParseMatchSimpleStringValue() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{ foo: bar }\"\n-        );\n-\n-        MatchParser matchParser = new MatchParser();\n-        MatchAssertion matchAssertion = matchParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(matchAssertion, notNullValue());\n-        assertThat(matchAssertion.getField(), equalTo(\"foo\"));\n-        assertThat(matchAssertion.getExpectedValue(), instanceOf(String.class));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"bar\"));\n-    }\n-\n-    @Test\n-    @SuppressWarnings(\"unchecked\")\n-    public void testParseMatchArray() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{'matches': ['test_percolator_1', 'test_percolator_2']}\"\n-        );\n-\n-        MatchParser matchParser = new MatchParser();\n-        MatchAssertion matchAssertion = matchParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(matchAssertion, notNullValue());\n-        assertThat(matchAssertion.getField(), equalTo(\"matches\"));\n-        assertThat(matchAssertion.getExpectedValue(), instanceOf(List.class));\n-        List strings = (List) matchAssertion.getExpectedValue();\n-        assertThat(strings.size(), equalTo(2));\n-        assertThat(strings.get(0).toString(), equalTo(\"test_percolator_1\"));\n-        assertThat(strings.get(1).toString(), equalTo(\"test_percolator_2\"));\n-    }\n-\n-    @Test\n-    @SuppressWarnings(\"unchecked\")\n-    public void testParseMatchSourceValues() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{ _source: { responses.0.hits.total: 3, foo: bar  }}\"\n-        );\n-\n-        MatchParser matchParser = new MatchParser();\n-        MatchAssertion matchAssertion = matchParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(matchAssertion, notNullValue());\n-        assertThat(matchAssertion.getField(), equalTo(\"_source\"));\n-        assertThat(matchAssertion.getExpectedValue(), instanceOf(Map.class));\n-        Map<String, Object> expectedValue = (Map<String, Object>) matchAssertion.getExpectedValue();\n-        assertThat(expectedValue.size(), equalTo(2));\n-        Object o = expectedValue.get(\"responses.0.hits.total\");\n-        assertThat(o, instanceOf(Integer.class));\n-        assertThat((Integer)o, equalTo(3));\n-        o = expectedValue.get(\"foo\");\n-        assertThat(o, instanceOf(String.class));\n-        assertThat(o.toString(), equalTo(\"bar\"));\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/DoSectionParserTests.java b/src/test/java/org/elasticsearch/test/rest/test/DoSectionParserTests.java\ndeleted file mode 100644\nindex 7b6958bccb2..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/DoSectionParserTests.java\n+++ /dev/null\n@@ -1,420 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.json.JsonXContent;\n-import org.elasticsearch.common.xcontent.yaml.YamlXContent;\n-import org.elasticsearch.test.rest.parser.DoSectionParser;\n-import org.elasticsearch.test.rest.parser.RestTestParseException;\n-import org.elasticsearch.test.rest.parser.RestTestSuiteParseContext;\n-import org.elasticsearch.test.rest.section.ApiCallSection;\n-import org.elasticsearch.test.rest.section.DoSection;\n-import org.hamcrest.MatcherAssert;\n-import org.junit.Test;\n-\n-import java.io.IOException;\n-import java.util.Map;\n-\n-import static org.hamcrest.CoreMatchers.equalTo;\n-import static org.hamcrest.Matchers.notNullValue;\n-import static org.hamcrest.Matchers.nullValue;\n-\n-public class DoSectionParserTests extends AbstractParserTests {\n-\n-    @Test\n-    public void testParseDoSectionNoBody() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"get:\\n\" +\n-                \"    index:    test_index\\n\" +\n-                \"    type:    test_type\\n\" +\n-                \"    id:        1\"\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"get\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(3));\n-        assertThat(apiCallSection.getParams().get(\"index\"), equalTo(\"test_index\"));\n-        assertThat(apiCallSection.getParams().get(\"type\"), equalTo(\"test_type\"));\n-        assertThat(apiCallSection.getParams().get(\"id\"), equalTo(\"1\"));\n-        assertThat(apiCallSection.hasBody(), equalTo(false));\n-    }\n-\n-    @Test\n-    public void testParseDoSectionNoParamsNoBody() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"cluster.node_info: {}\"\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"cluster.node_info\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(0));\n-        assertThat(apiCallSection.hasBody(), equalTo(false));\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithJsonBody() throws Exception {\n-        String body = \"{ \\\"include\\\": { \\\"field1\\\": \\\"v1\\\", \\\"field2\\\": \\\"v2\\\" }, \\\"count\\\": 1 }\";\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"index:\\n\" +\n-                \"    index:  test_1\\n\" +\n-                \"    type:   test\\n\" +\n-                \"    id:     1\\n\" +\n-                \"    body:   \" + body\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"index\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(3));\n-        assertThat(apiCallSection.getParams().get(\"index\"), equalTo(\"test_1\"));\n-        assertThat(apiCallSection.getParams().get(\"type\"), equalTo(\"test\"));\n-        assertThat(apiCallSection.getParams().get(\"id\"), equalTo(\"1\"));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-\n-        assertJsonEquals(apiCallSection.getBodiesAsList().get(0), body);\n-        assertJsonEquals(apiCallSection.getBody(), body);\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithJsonMultipleBodiesAsLongString() throws Exception {\n-        String bodies[] = new String[]{\n-                \"{ \\\"index\\\": { \\\"_index\\\":\\\"test_index\\\", \\\"_type\\\":\\\"test_type\\\", \\\"_id\\\":\\\"test_id\\\" } }\\n\",\n-                \"{ \\\"f1\\\":\\\"v1\\\", \\\"f2\\\":42 }\\n\",\n-                \"{ \\\"index\\\": { \\\"_index\\\":\\\"test_index2\\\", \\\"_type\\\":\\\"test_type2\\\", \\\"_id\\\":\\\"test_id2\\\" } }\\n\",\n-                \"{ \\\"f1\\\":\\\"v2\\\", \\\"f2\\\":47 }\\n\"\n-        };\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"bulk:\\n\" +\n-                \"    refresh: true\\n\" +\n-                \"    body: |\\n\" +\n-                \"        \" + bodies[0] +\n-                \"        \" + bodies[1] +\n-                \"        \" + bodies[2] +\n-                \"        \" + bodies[3]\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"bulk\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(1));\n-        assertThat(apiCallSection.getParams().get(\"refresh\"), equalTo(\"true\"));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-        assertThat(apiCallSection.getBodiesAsList().size(), equalTo(1));\n-        StringBuilder bodyBuilder = new StringBuilder();\n-        for (String body : bodies) {\n-            bodyBuilder.append(body);\n-        }\n-        assertThat(apiCallSection.getBody(), equalTo(bodyBuilder.toString()));\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithJsonMultipleBodiesRepeatedProperty() throws Exception {\n-        String[] bodies = new String[] {\n-                \"{ \\\"index\\\": { \\\"_index\\\":\\\"test_index\\\", \\\"_type\\\":\\\"test_type\\\", \\\"_id\\\":\\\"test_id\\\" } }\",\n-                \"{ \\\"f1\\\":\\\"v1\\\", \\\"f2\\\":42 }\",\n-        };\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"bulk:\\n\" +\n-                \"    refresh: true\\n\" +\n-                \"    body: \\n\" +\n-                \"        \" + bodies[0] + \"\\n\" +\n-                \"    body: \\n\" +\n-                \"        \" + bodies[1]\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"bulk\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(1));\n-        assertThat(apiCallSection.getParams().get(\"refresh\"), equalTo(\"true\"));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-        assertThat(apiCallSection.getBodiesAsList().size(), equalTo(bodies.length));\n-        for (int i = 0; i < bodies.length; i++) {\n-            assertJsonEquals(apiCallSection.getBodiesAsList().get(i), bodies[i]);\n-        }\n-\n-        String[] returnedBodies = apiCallSection.getBody().split(\"\\n\");\n-        assertThat(returnedBodies.length, equalTo(bodies.length));\n-        for (int i = 0; i < bodies.length; i++) {\n-            assertJsonEquals(returnedBodies[i], bodies[i]);\n-        }\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithYamlBody() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"search:\\n\" +\n-                \"    body:\\n\" +\n-                \"        _source: [ include.field1, include.field2 ]\\n\" +\n-                \"        query: { match_all: {} }\"\n-        );\n-        String body = \"{ \\\"_source\\\": [ \\\"include.field1\\\", \\\"include.field2\\\" ], \\\"query\\\": { \\\"match_all\\\": {} }}\";\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"search\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(0));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-        assertThat(apiCallSection.getBodiesAsList().size(), equalTo(1));\n-        assertJsonEquals(apiCallSection.getBodiesAsList().get(0), body);\n-        assertJsonEquals(apiCallSection.getBody(), body);\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithYamlMultipleBodies() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"bulk:\\n\" +\n-                \"    refresh: true\\n\" +\n-                \"    body:\\n\" +\n-                \"        - index:\\n\" +\n-                \"            _index: test_index\\n\" +\n-                \"            _type:  test_type\\n\" +\n-                \"            _id:    test_id\\n\" +\n-                \"        - f1: v1\\n\" +\n-                \"          f2: 42\\n\" +\n-                \"        - index:\\n\" +\n-                \"            _index: test_index2\\n\" +\n-                \"            _type:  test_type2\\n\" +\n-                \"            _id:    test_id2\\n\" +\n-                \"        - f1: v2\\n\" +\n-                \"          f2: 47\"\n-        );\n-        String[] bodies = new String[4];\n-        bodies[0] = \"{\\\"index\\\": {\\\"_index\\\": \\\"test_index\\\", \\\"_type\\\":  \\\"test_type\\\", \\\"_id\\\": \\\"test_id\\\"}}\";\n-        bodies[1] = \"{ \\\"f1\\\":\\\"v1\\\", \\\"f2\\\": 42 }\";\n-        bodies[2] = \"{\\\"index\\\": {\\\"_index\\\": \\\"test_index2\\\", \\\"_type\\\":  \\\"test_type2\\\", \\\"_id\\\": \\\"test_id2\\\"}}\";\n-        bodies[3] = \"{ \\\"f1\\\":\\\"v2\\\", \\\"f2\\\": 47 }\";\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"bulk\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(1));\n-        assertThat(apiCallSection.getParams().get(\"refresh\"), equalTo(\"true\"));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-        assertThat(apiCallSection.getBodiesAsList().size(), equalTo(bodies.length));\n-\n-        for (int i = 0; i < bodies.length; i++) {\n-            assertJsonEquals(apiCallSection.getBodiesAsList().get(i), bodies[i]);\n-        }\n-\n-        String[] returnedBodies = apiCallSection.getBody().split(\"\\n\");\n-        assertThat(returnedBodies.length, equalTo(bodies.length));\n-        for (int i = 0; i < bodies.length; i++) {\n-            assertJsonEquals(returnedBodies[i], bodies[i]);\n-        }\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithYamlMultipleBodiesRepeatedProperty() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"bulk:\\n\" +\n-                \"    refresh: true\\n\" +\n-                \"    body:\\n\" +\n-                \"        index:\\n\" +\n-                \"            _index: test_index\\n\" +\n-                \"            _type:  test_type\\n\" +\n-                \"            _id:    test_id\\n\" +\n-                \"    body:\\n\" +\n-                \"        f1: v1\\n\" +\n-                \"        f2: 42\\n\"\n-        );\n-        String[] bodies = new String[2];\n-        bodies[0] = \"{\\\"index\\\": {\\\"_index\\\": \\\"test_index\\\", \\\"_type\\\":  \\\"test_type\\\", \\\"_id\\\": \\\"test_id\\\"}}\";\n-        bodies[1] = \"{ \\\"f1\\\":\\\"v1\\\", \\\"f2\\\": 42 }\";\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"bulk\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(1));\n-        assertThat(apiCallSection.getParams().get(\"refresh\"), equalTo(\"true\"));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-        assertThat(apiCallSection.getBodiesAsList().size(), equalTo(bodies.length));\n-\n-        for (int i = 0; i < bodies.length; i++) {\n-            assertJsonEquals(apiCallSection.getBodiesAsList().get(i), bodies[i]);\n-        }\n-\n-        String[] returnedBodies = apiCallSection.getBody().split(\"\\n\");\n-        assertThat(returnedBodies.length, equalTo(bodies.length));\n-        for (int i = 0; i < bodies.length; i++) {\n-            assertJsonEquals(returnedBodies[i], bodies[i]);\n-        }\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithYamlBodyMultiGet() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"mget:\\n\" +\n-                \"    body:\\n\" +\n-                \"        docs:\\n\" +\n-                \"            - { _index: test_2, _type: test, _id: 1}\\n\" +\n-                \"            - { _index: test_1, _type: none, _id: 1}\"\n-        );\n-        String body = \"{ \\\"docs\\\": [ \" +\n-                \"{\\\"_index\\\": \\\"test_2\\\", \\\"_type\\\":\\\"test\\\", \\\"_id\\\":1}, \" +\n-                \"{\\\"_index\\\": \\\"test_1\\\", \\\"_type\\\":\\\"none\\\", \\\"_id\\\":1} \" +\n-                \"]}\";\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"mget\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(0));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-        assertThat(apiCallSection.getBodiesAsList().size(), equalTo(1));\n-        assertJsonEquals(apiCallSection.getBodiesAsList().get(0), body);\n-        assertJsonEquals(apiCallSection.getBody(), body);\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithBodyStringified() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"index:\\n\" +\n-                \"    index:  test_1\\n\" +\n-                \"    type:   test\\n\" +\n-                \"    id:     1\\n\" +\n-                \"    body:   \\\"{ _source: true, query: { match_all: {} } }\\\"\"\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection, notNullValue());\n-        assertThat(apiCallSection.getApi(), equalTo(\"index\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(3));\n-        assertThat(apiCallSection.getParams().get(\"index\"), equalTo(\"test_1\"));\n-        assertThat(apiCallSection.getParams().get(\"type\"), equalTo(\"test\"));\n-        assertThat(apiCallSection.getParams().get(\"id\"), equalTo(\"1\"));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-        assertThat(apiCallSection.getBodiesAsList().size(), equalTo(1));\n-        //stringified body is taken as is\n-        assertThat(apiCallSection.getBodiesAsList().get(0), equalTo(\"{ _source: true, query: { match_all: {} } }\"));\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithBodiesStringifiedAndNot() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"index:\\n\" +\n-                \"    body:\\n\" +\n-                \"        - \\\"{ _source: true, query: { match_all: {} } }\\\"\\n\" +\n-                \"        - { size: 100, query: { match_all: {} } }\"\n-        );\n-\n-        String body = \"{ \\\"size\\\": 100, \\\"query\\\": { \\\"match_all\\\": {} } }\";\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-        ApiCallSection apiCallSection = doSection.getApiCallSection();\n-\n-        assertThat(apiCallSection.getApi(), equalTo(\"index\"));\n-        assertThat(apiCallSection.getParams().size(), equalTo(0));\n-        assertThat(apiCallSection.hasBody(), equalTo(true));\n-        assertThat(apiCallSection.getBodiesAsList().size(), equalTo(2));\n-        //stringified body is taken as is\n-        assertThat(apiCallSection.getBodiesAsList().get(0), equalTo(\"{ _source: true, query: { match_all: {} } }\"));\n-        assertJsonEquals(apiCallSection.getBodiesAsList().get(1), body);\n-    }\n-\n-    @Test\n-    public void testParseDoSectionWithCatch() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"catch: missing\\n\" +\n-                \"indices.get_warmer:\\n\" +\n-                \"    index: test_index\\n\" +\n-                \"    name: test_warmer\"\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(doSection.getCatch(), equalTo(\"missing\"));\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"indices.get_warmer\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(2));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));\n-    }\n-\n-    @Test (expected = RestTestParseException.class)\n-    public void testParseDoSectionWithoutClientCallSection() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"catch: missing\\n\"\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-    }\n-\n-    @Test\n-    public void testParseDoSectionMultivaluedField() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"indices.get_field_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"        type: test_type\\n\" +\n-                        \"        field: [ text , text1 ]\"\n-        );\n-\n-        DoSectionParser doSectionParser = new DoSectionParser();\n-        DoSection doSection = doSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"indices.get_field_mapping\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().getParams().get(\"index\"), equalTo(\"test_index\"));\n-        assertThat(doSection.getApiCallSection().getParams().get(\"type\"), equalTo(\"test_type\"));\n-        assertThat(doSection.getApiCallSection().getParams().get(\"field\"), equalTo(\"text,text1\"));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));\n-        assertThat(doSection.getApiCallSection().getBodiesAsList().size(), equalTo(0));\n-    }\n-\n-    private static void assertJsonEquals(String actual, String expected) throws IOException {\n-        Map<String,Object> actualMap = JsonXContent.jsonXContent.createParser(actual).mapOrderedAndClose();\n-        Map<String,Object> expectedMap = JsonXContent.jsonXContent.createParser(expected).mapOrderedAndClose();\n-        MatcherAssert.assertThat(actualMap, equalTo(expectedMap));\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java b/src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java\ndeleted file mode 100644\nindex 1fdbbb84ba9..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java\n+++ /dev/null\n@@ -1,116 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.test.ElasticsearchTestCase;\n-import org.elasticsearch.test.rest.support.FileUtils;\n-import org.junit.Test;\n-\n-import java.io.File;\n-import java.util.Map;\n-import java.util.Set;\n-\n-import static org.hamcrest.CoreMatchers.equalTo;\n-import static org.hamcrest.CoreMatchers.notNullValue;\n-import static org.hamcrest.Matchers.greaterThan;\n-\n-public class FileUtilsTests extends ElasticsearchTestCase {\n-\n-    @Test\n-    public void testLoadSingleYamlSuite() throws Exception {\n-        Map<String,Set<File>> yamlSuites = FileUtils.findYamlSuites(\"/rest-spec/test\", \"/rest-spec/test/get/10_basic\");\n-        assertSingleFile(yamlSuites, \"get\", \"10_basic.yaml\");\n-\n-        //the path prefix is optional\n-        yamlSuites = FileUtils.findYamlSuites(\"/rest-spec/test\", \"get/10_basic.yaml\");\n-        assertSingleFile(yamlSuites, \"get\", \"10_basic.yaml\");\n-\n-        //extension .yaml is optional\n-        yamlSuites = FileUtils.findYamlSuites(\"/rest-spec/test\", \"get/10_basic\");\n-        assertSingleFile(yamlSuites, \"get\", \"10_basic.yaml\");\n-    }\n-\n-    @Test\n-    public void testLoadMultipleYamlSuites() throws Exception {\n-        //single directory\n-        Map<String,Set<File>> yamlSuites = FileUtils.findYamlSuites(\"/rest-spec/test\", \"get\");\n-        assertThat(yamlSuites, notNullValue());\n-        assertThat(yamlSuites.size(), equalTo(1));\n-        assertThat(yamlSuites.containsKey(\"get\"), equalTo(true));\n-        assertThat(yamlSuites.get(\"get\").size(), greaterThan(1));\n-\n-        //multiple directories\n-        yamlSuites = FileUtils.findYamlSuites(\"/rest-spec/test\", \"get\", \"index\");\n-        assertThat(yamlSuites, notNullValue());\n-        assertThat(yamlSuites.size(), equalTo(2));\n-        assertThat(yamlSuites.containsKey(\"get\"), equalTo(true));\n-        assertThat(yamlSuites.get(\"get\").size(), greaterThan(1));\n-        assertThat(yamlSuites.containsKey(\"index\"), equalTo(true));\n-        assertThat(yamlSuites.get(\"index\").size(), greaterThan(1));\n-\n-        //multiple paths, which can be both directories or yaml test suites (with optional file extension)\n-        yamlSuites = FileUtils.findYamlSuites(\"/rest-spec/test\", \"get/10_basic\", \"index\");\n-        assertThat(yamlSuites, notNullValue());\n-        assertThat(yamlSuites.size(), equalTo(2));\n-        assertThat(yamlSuites.containsKey(\"get\"), equalTo(true));\n-        assertThat(yamlSuites.get(\"get\").size(), equalTo(1));\n-        assertSingleFile(yamlSuites.get(\"get\"), \"get\", \"10_basic.yaml\");\n-        assertThat(yamlSuites.containsKey(\"index\"), equalTo(true));\n-        assertThat(yamlSuites.get(\"index\").size(), greaterThan(1));\n-\n-        //files can be loaded from classpath and from file system too\n-        File dir = newTempDir();\n-        File file = new File(dir, \"test_loading.yaml\");\n-        assertThat(file.createNewFile(), equalTo(true));\n-\n-        //load from directory outside of the classpath\n-        yamlSuites = FileUtils.findYamlSuites(\"/rest-spec/test\", \"get/10_basic\", dir.getAbsolutePath());\n-        assertThat(yamlSuites, notNullValue());\n-        assertThat(yamlSuites.size(), equalTo(2));\n-        assertThat(yamlSuites.containsKey(\"get\"), equalTo(true));\n-        assertThat(yamlSuites.get(\"get\").size(), equalTo(1));\n-        assertSingleFile(yamlSuites.get(\"get\"), \"get\", \"10_basic.yaml\");\n-        assertThat(yamlSuites.containsKey(dir.getName()), equalTo(true));\n-        assertSingleFile(yamlSuites.get(dir.getName()), dir.getName(), file.getName());\n-\n-        //load from external file (optional extension)\n-        yamlSuites = FileUtils.findYamlSuites(\"/rest-spec/test\", \"get/10_basic\", dir.getAbsolutePath() + File.separator + \"test_loading\");\n-        assertThat(yamlSuites, notNullValue());\n-        assertThat(yamlSuites.size(), equalTo(2));\n-        assertThat(yamlSuites.containsKey(\"get\"), equalTo(true));\n-        assertThat(yamlSuites.get(\"get\").size(), equalTo(1));\n-        assertSingleFile(yamlSuites.get(\"get\"), \"get\", \"10_basic.yaml\");\n-        assertThat(yamlSuites.containsKey(dir.getName()), equalTo(true));\n-        assertSingleFile(yamlSuites.get(dir.getName()), dir.getName(), file.getName());\n-    }\n-\n-    private static void assertSingleFile(Map<String, Set<File>> yamlSuites, String dirName, String fileName) {\n-        assertThat(yamlSuites, notNullValue());\n-        assertThat(yamlSuites.size(), equalTo(1));\n-        assertThat(yamlSuites.containsKey(dirName), equalTo(true));\n-        assertSingleFile(yamlSuites.get(dirName), dirName, fileName);\n-    }\n-\n-    private static void assertSingleFile(Set<File> files, String dirName, String fileName) {\n-        assertThat(files.size(), equalTo(1));\n-        File file = files.iterator().next();\n-        assertThat(file.getName(), equalTo(fileName));\n-        assertThat(file.getParentFile().getName(), equalTo(dirName));\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/JsonPathTests.java b/src/test/java/org/elasticsearch/test/rest/test/JsonPathTests.java\ndeleted file mode 100644\nindex 3382ca5430a..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/JsonPathTests.java\n+++ /dev/null\n@@ -1,150 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.test.ElasticsearchTestCase;\n-import org.elasticsearch.test.rest.json.JsonPath;\n-import org.junit.Test;\n-\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-\n-import static org.hamcrest.Matchers.*;\n-\n-public class JsonPathTests extends ElasticsearchTestCase {\n-\n-    @Test\n-    public void testEvaluateObjectPathEscape() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"field2.field3\\\" : \\\"value2\\\" } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.field2\\\\.field3\");\n-        assertThat(object, instanceOf(String.class));\n-        assertThat((String)object, equalTo(\"value2\"));\n-    }\n-\n-    @Test\n-    public void testEvaluateObjectPathWithDoubleDot() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"field2\\\" : \\\"value2\\\" } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1..field2\");\n-        assertThat(object, instanceOf(String.class));\n-        assertThat((String)object, equalTo(\"value2\"));\n-    }\n-\n-    @Test\n-    public void testEvaluateObjectPathEndsWithDot() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"field2\\\" : \\\"value2\\\" } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.field2.\");\n-        assertThat(object, instanceOf(String.class));\n-        assertThat((String)object, equalTo(\"value2\"));\n-    }\n-\n-    @Test\n-    public void testEvaluateString() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"field2\\\" : \\\"value2\\\" } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.field2\");\n-        assertThat(object, instanceOf(String.class));\n-        assertThat((String)object, equalTo(\"value2\"));\n-    }\n-\n-    @Test\n-    public void testEvaluateInteger() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"field2\\\" : 333 } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.field2\");\n-        assertThat(object, instanceOf(Integer.class));\n-        assertThat((Integer)object, equalTo(333));\n-    }\n-\n-    @Test\n-    public void testEvaluateDouble() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"field2\\\" : 3.55 } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.field2\");\n-        assertThat(object, instanceOf(Double.class));\n-        assertThat((Double)object, equalTo(3.55));\n-    }\n-\n-    @Test\n-    public void testEvaluateArray() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"array1\\\" : [ \\\"value1\\\", \\\"value2\\\" ] } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.array1\");\n-        assertThat(object, instanceOf(List.class));\n-        List list = (List) object;\n-        assertThat(list.size(), equalTo(2));\n-        assertThat(list.get(0), instanceOf(String.class));\n-        assertThat((String)list.get(0), equalTo(\"value1\"));\n-        assertThat(list.get(1), instanceOf(String.class));\n-        assertThat((String)list.get(1), equalTo(\"value2\"));\n-    }\n-\n-    @Test\n-    public void testEvaluateArrayElement() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"array1\\\" : [ \\\"value1\\\", \\\"value2\\\" ] } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.array1.1\");\n-        assertThat(object, instanceOf(String.class));\n-        assertThat((String)object, equalTo(\"value2\"));\n-    }\n-\n-    @Test\n-    public void testEvaluateArrayElementObject() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"array1\\\" : [ {\\\"element\\\": \\\"value1\\\"}, {\\\"element\\\":\\\"value2\\\"} ] } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.array1.1.element\");\n-        assertThat(object, instanceOf(String.class));\n-        assertThat((String)object, equalTo(\"value2\"));\n-    }\n-\n-    @Test\n-    public void testEvaluateArrayElementObjectWrongPath() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"array1\\\" : [ {\\\"element\\\": \\\"value1\\\"}, {\\\"element\\\":\\\"value2\\\"} ] } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"field1.array2.1.element\");\n-        assertThat(object, nullValue());\n-    }\n-\n-    @Test\n-    @SuppressWarnings(\"unchecked\")\n-    public void testEvaluateObjectKeys() throws Exception {\n-        String json = \"{ \\\"metadata\\\": { \\\"templates\\\" : {\\\"template_1\\\": { \\\"field\\\" : \\\"value\\\"}, \\\"template_2\\\": { \\\"field\\\" : \\\"value\\\"} } } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"metadata.templates\");\n-        assertThat(object, instanceOf(Map.class));\n-        Map<String, Object> map = (Map<String, Object>)object;\n-        assertThat(map.size(), equalTo(2));\n-        Set<String> strings = map.keySet();\n-        assertThat(strings, contains(\"template_1\", \"template_2\"));\n-    }\n-\n-    @Test\n-    @SuppressWarnings(\"unchecked\")\n-    public void testEvaluateEmptyPath() throws Exception {\n-        String json = \"{ \\\"field1\\\": { \\\"array1\\\" : [ {\\\"element\\\": \\\"value1\\\"}, {\\\"element\\\":\\\"value2\\\"} ] } }\";\n-        JsonPath jsonPath = new JsonPath(json);\n-        Object object = jsonPath.evaluate(\"\");\n-        assertThat(object, notNullValue());\n-        assertThat(object, instanceOf(Map.class));\n-        assertThat(((Map<String, Object>)object).containsKey(\"field1\"), equalTo(true));\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/RestApiParserTests.java b/src/test/java/org/elasticsearch/test/rest/test/RestApiParserTests.java\ndeleted file mode 100644\nindex 718598c7c8e..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/RestApiParserTests.java\n+++ /dev/null\n@@ -1,172 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.json.JsonXContent;\n-import org.elasticsearch.test.rest.spec.RestApi;\n-import org.elasticsearch.test.rest.spec.RestApiParser;\n-import org.junit.Test;\n-\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.notNullValue;\n-\n-public class RestApiParserTests extends AbstractParserTests {\n-\n-    @Test\n-    public void testParseRestSpecIndexApi() throws Exception {\n-        parser = JsonXContent.jsonXContent.createParser(REST_SPEC_INDEX_API);\n-        RestApi restApi = new RestApiParser().parse(parser);\n-\n-        assertThat(restApi, notNullValue());\n-        assertThat(restApi.getName(), equalTo(\"index\"));\n-        assertThat(restApi.getMethods().size(), equalTo(2));\n-        assertThat(restApi.getMethods().get(0), equalTo(\"POST\"));\n-        assertThat(restApi.getMethods().get(1), equalTo(\"PUT\"));\n-        assertThat(restApi.getPaths().size(), equalTo(2));\n-        assertThat(restApi.getPaths().get(0), equalTo(\"/{index}/{type}\"));\n-        assertThat(restApi.getPaths().get(1), equalTo(\"/{index}/{type}/{id}\"));\n-        assertThat(restApi.getPathParts().size(), equalTo(3));\n-        assertThat(restApi.getPathParts().get(0), equalTo(\"id\"));\n-        assertThat(restApi.getPathParts().get(1), equalTo(\"index\"));\n-        assertThat(restApi.getPathParts().get(2), equalTo(\"type\"));\n-    }\n-\n-    @Test\n-    public void testParseRestSpecGetTemplateApi() throws Exception {\n-        parser = JsonXContent.jsonXContent.createParser(REST_SPEC_GET_TEMPLATE_API);\n-        RestApi restApi = new RestApiParser().parse(parser);\n-        assertThat(restApi, notNullValue());\n-        assertThat(restApi.getName(), equalTo(\"indices.get_template\"));\n-        assertThat(restApi.getMethods().size(), equalTo(1));\n-        assertThat(restApi.getMethods().get(0), equalTo(\"GET\"));\n-        assertThat(restApi.getPaths().size(), equalTo(2));\n-        assertThat(restApi.getPaths().get(0), equalTo(\"/_template\"));\n-        assertThat(restApi.getPaths().get(1), equalTo(\"/_template/{name}\"));\n-        assertThat(restApi.getPathParts().size(), equalTo(1));\n-        assertThat(restApi.getPathParts().get(0), equalTo(\"name\"));\n-    }\n-\n-    private static final String REST_SPEC_GET_TEMPLATE_API = \"{\\n\" +\n-            \"  \\\"indices.get_template\\\": {\\n\" +\n-            \"    \\\"documentation\\\": \\\"http://www.elasticsearch.org/guide/reference/api/admin-indices-templates/\\\",\\n\" +\n-            \"    \\\"methods\\\": [\\\"GET\\\"],\\n\" +\n-            \"    \\\"url\\\": {\\n\" +\n-            \"      \\\"path\\\": \\\"/_template/{name}\\\",\\n\" +\n-            \"      \\\"paths\\\": [\\\"/_template\\\", \\\"/_template/{name}\\\"],\\n\" +\n-            \"      \\\"parts\\\": {\\n\" +\n-            \"        \\\"name\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"string\\\",\\n\" +\n-            \"          \\\"required\\\" : false,\\n\" +\n-            \"          \\\"description\\\" : \\\"The name of the template\\\"\\n\" +\n-            \"        }\\n\" +\n-            \"      },\\n\" +\n-            \"      \\\"params\\\": {\\n\" +\n-            \"      }\\n\" +\n-            \"    },\\n\" +\n-            \"    \\\"body\\\": null\\n\" +\n-            \"  }\\n\" +\n-            \"}\";\n-\n-    private static final String REST_SPEC_INDEX_API = \"{\\n\" +\n-            \"  \\\"index\\\": {\\n\" +\n-            \"    \\\"documentation\\\": \\\"http://elasticsearch.org/guide/reference/api/index_/\\\",\\n\" +\n-            \"    \\\"methods\\\": [\\\"POST\\\", \\\"PUT\\\"],\\n\" +\n-            \"    \\\"url\\\": {\\n\" +\n-            \"      \\\"path\\\": \\\"/{index}/{type}\\\",\\n\" +\n-            \"      \\\"paths\\\": [\\\"/{index}/{type}\\\", \\\"/{index}/{type}/{id}\\\"],\\n\" +\n-            \"      \\\"parts\\\": {\\n\" +\n-            \"        \\\"id\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"string\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Document ID\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"index\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"string\\\",\\n\" +\n-            \"          \\\"required\\\" : true,\\n\" +\n-            \"          \\\"description\\\" : \\\"The name of the index\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"type\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"string\\\",\\n\" +\n-            \"          \\\"required\\\" : true,\\n\" +\n-            \"          \\\"description\\\" : \\\"The type of the document\\\"\\n\" +\n-            \"        }\\n\" +\n-            \"      }   ,\\n\" +\n-            \"      \\\"params\\\": {\\n\" +\n-            \"        \\\"consistency\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"enum\\\",\\n\" +\n-            \"          \\\"options\\\" : [\\\"one\\\", \\\"quorum\\\", \\\"all\\\"],\\n\" +\n-            \"          \\\"description\\\" : \\\"Explicit write consistency setting for the operation\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"op_type\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"enum\\\",\\n\" +\n-            \"          \\\"options\\\" : [\\\"index\\\", \\\"create\\\"],\\n\" +\n-            \"          \\\"default\\\" : \\\"index\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Explicit operation type\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"parent\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"string\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"ID of the parent document\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"percolate\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"string\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Percolator queries to execute while indexing the document\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"refresh\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"boolean\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Refresh the index after performing the operation\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"replication\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"enum\\\",\\n\" +\n-            \"          \\\"options\\\" : [\\\"sync\\\",\\\"async\\\"],\\n\" +\n-            \"          \\\"default\\\" : \\\"sync\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Specific replication type\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"routing\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"string\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Specific routing value\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"timeout\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"time\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Explicit operation timeout\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"timestamp\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"time\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Explicit timestamp for the document\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"ttl\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"duration\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Expiration time for the document\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"version\\\" : {\\n\" +\n-            \"          \\\"type\\\" : \\\"number\\\",\\n\" +\n-            \"          \\\"description\\\" : \\\"Explicit version number for concurrency control\\\"\\n\" +\n-            \"        },\\n\" +\n-            \"        \\\"version_type\\\": {\\n\" +\n-            \"          \\\"type\\\" : \\\"enum\\\",\\n\" +\n-            \"          \\\"options\\\" : [\\\"internal\\\",\\\"external\\\"],\\n\" +\n-            \"          \\\"description\\\" : \\\"Specific version type\\\"\\n\" +\n-            \"        }\\n\" +\n-            \"      }\\n\" +\n-            \"    },\\n\" +\n-            \"    \\\"body\\\": {\\n\" +\n-            \"      \\\"description\\\" : \\\"The document\\\",\\n\" +\n-            \"      \\\"required\\\" : true\\n\" +\n-            \"    }\\n\" +\n-            \"  }\\n\" +\n-            \"}\\n\";\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/RestTestParserTests.java b/src/test/java/org/elasticsearch/test/rest/test/RestTestParserTests.java\ndeleted file mode 100644\nindex 04e1fe30fbd..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/RestTestParserTests.java\n+++ /dev/null\n@@ -1,501 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.XContentParser;\n-import org.elasticsearch.common.xcontent.yaml.YamlXContent;\n-import org.elasticsearch.test.ElasticsearchTestCase;\n-import org.elasticsearch.test.rest.parser.RestTestSuiteParseContext;\n-import org.elasticsearch.test.rest.parser.RestTestSuiteParser;\n-import org.elasticsearch.test.rest.section.DoSection;\n-import org.elasticsearch.test.rest.section.IsTrueAssertion;\n-import org.elasticsearch.test.rest.section.MatchAssertion;\n-import org.elasticsearch.test.rest.section.RestTestSuite;\n-import org.junit.After;\n-import org.junit.Test;\n-\n-import java.util.Map;\n-\n-import static org.hamcrest.Matchers.*;\n-\n-public class RestTestParserTests extends ElasticsearchTestCase {\n-\n-    private XContentParser parser;\n-\n-    @After\n-    public void tearDown() throws Exception {\n-        super.tearDown();\n-        //makes sure that we consumed the whole stream, XContentParser doesn't expose isClosed method\n-        //next token can be null even in the middle of the document (e.g. with \"---\"), but not too many consecutive times\n-        assertThat(parser.currentToken(), nullValue());\n-        assertThat(parser.nextToken(), nullValue());\n-        assertThat(parser.nextToken(), nullValue());\n-        parser.close();\n-    }\n-\n-    @Test\n-    public void testParseTestSetupAndSections() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                        \"setup:\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"        indices.create:\\n\" +\n-                        \"          index: test_index\\n\" +\n-                        \"\\n\" +\n-                        \"---\\n\" +\n-                        \"\\\"Get index mapping\\\":\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      indices.get_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"\\n\" +\n-                        \"  - match: {test_index.test_type.properties.text.type:     string}\\n\" +\n-                        \"  - match: {test_index.test_type.properties.text.analyzer: whitespace}\\n\" +\n-                        \"\\n\" +\n-                        \"---\\n\" +\n-                        \"\\\"Get type mapping - pre 1.0\\\":\\n\" +\n-                        \"\\n\" +\n-                        \"  - skip:\\n\" +\n-                        \"      version:     \\\"0.90.9 - 999\\\"\\n\" +\n-                        \"      reason:      \\\"for newer versions the index name is always returned\\\"\\n\" +\n-                        \"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      indices.get_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"        type: test_type\\n\" +\n-                        \"\\n\" +\n-                        \"  - match: {test_type.properties.text.type:     string}\\n\" +\n-                        \"  - match: {test_type.properties.text.analyzer: whitespace}\\n\"\n-        );\n-\n-        RestTestSuiteParser testParser = new RestTestSuiteParser();\n-        RestTestSuite restTestSuite = testParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-\n-        assertThat(restTestSuite, notNullValue());\n-        assertThat(restTestSuite.getName(), equalTo(\"suite\"));\n-        assertThat(restTestSuite.getSetupSection(), notNullValue());\n-        assertThat(restTestSuite.getSetupSection().getSkipSection().isEmpty(), equalTo(true));\n-\n-        assertThat(restTestSuite.getSetupSection().getDoSections().size(), equalTo(1));\n-        assertThat(restTestSuite.getSetupSection().getDoSections().get(0).getApiCallSection().getApi(), equalTo(\"indices.create\"));\n-        assertThat(restTestSuite.getSetupSection().getDoSections().get(0).getApiCallSection().getParams().size(), equalTo(1));\n-        assertThat(restTestSuite.getSetupSection().getDoSections().get(0).getApiCallSection().getParams().get(\"index\"), equalTo(\"test_index\"));\n-\n-        assertThat(restTestSuite.getTestSections().size(), equalTo(2));\n-\n-        assertThat(restTestSuite.getTestSections().get(0).getName(), equalTo(\"Get index mapping\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getSkipSection().isEmpty(), equalTo(true));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().size(), equalTo(3));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(0), instanceOf(DoSection.class));\n-        DoSection doSection = (DoSection) restTestSuite.getTestSections().get(0).getExecutableSections().get(0);\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"indices.get_mapping\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(1));\n-        assertThat(doSection.getApiCallSection().getParams().get(\"index\"), equalTo(\"test_index\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(1), instanceOf(MatchAssertion.class));\n-        MatchAssertion matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(1);\n-        assertThat(matchAssertion.getField(), equalTo(\"test_index.test_type.properties.text.type\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"string\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(2), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(2);\n-        assertThat(matchAssertion.getField(), equalTo(\"test_index.test_type.properties.text.analyzer\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"whitespace\"));\n-\n-        assertThat(restTestSuite.getTestSections().get(1).getName(), equalTo(\"Get type mapping - pre 1.0\"));\n-        assertThat(restTestSuite.getTestSections().get(1).getSkipSection().isEmpty(), equalTo(false));\n-        assertThat(restTestSuite.getTestSections().get(1).getSkipSection().getReason(), equalTo(\"for newer versions the index name is always returned\"));\n-        assertThat(restTestSuite.getTestSections().get(1).getSkipSection().getVersion(), equalTo(\"0.90.9 - 999\"));\n-        assertThat(restTestSuite.getTestSections().get(1).getExecutableSections().size(), equalTo(3));\n-        assertThat(restTestSuite.getTestSections().get(1).getExecutableSections().get(0), instanceOf(DoSection.class));\n-        doSection = (DoSection) restTestSuite.getTestSections().get(1).getExecutableSections().get(0);\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"indices.get_mapping\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(2));\n-        assertThat(doSection.getApiCallSection().getParams().get(\"index\"), equalTo(\"test_index\"));\n-        assertThat(doSection.getApiCallSection().getParams().get(\"type\"), equalTo(\"test_type\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(1), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(1).getExecutableSections().get(1);\n-        assertThat(matchAssertion.getField(), equalTo(\"test_type.properties.text.type\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"string\"));\n-        assertThat(restTestSuite.getTestSections().get(1).getExecutableSections().get(2), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(1).getExecutableSections().get(2);\n-        assertThat(matchAssertion.getField(), equalTo(\"test_type.properties.text.analyzer\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"whitespace\"));\n-    }\n-\n-    @Test\n-    public void testParseTestSetupAndSectionsSkipLastSection() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"setup:\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"        indices.create:\\n\" +\n-                        \"          index: test_index\\n\" +\n-                        \"\\n\" +\n-                        \"---\\n\" +\n-                        \"\\\"Get index mapping\\\":\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      indices.get_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"\\n\" +\n-                        \"  - match: {test_index.test_type.properties.text.type:     string}\\n\" +\n-                        \"  - match: {test_index.test_type.properties.text.analyzer: whitespace}\\n\" +\n-                        \"\\n\" +\n-                        \"---\\n\" +\n-                        \"\\\"Get type mapping - pre 1.0\\\":\\n\" +\n-                        \"\\n\" +\n-                        \"  - skip:\\n\" +\n-                        \"      version:     \\\"0.90.9 - 999\\\"\\n\" +\n-                        \"      reason:      \\\"for newer versions the index name is always returned\\\"\\n\" +\n-                        \"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      indices.get_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"        type: test_type\\n\" +\n-                        \"\\n\" +\n-                        \"  - match: {test_type.properties.text.type:     string}\\n\" +\n-                        \"  - match: {test_type.properties.text.analyzer: whitespace}\\n\"\n-        );\n-\n-        RestTestSuiteParser testParser = new RestTestSuiteParser();\n-        RestTestSuite restTestSuite = testParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"1.0.0\"));\n-\n-        assertThat(restTestSuite, notNullValue());\n-        assertThat(restTestSuite.getName(), equalTo(\"suite\"));\n-        assertThat(restTestSuite.getSetupSection(), notNullValue());\n-        assertThat(restTestSuite.getSetupSection().getSkipSection().isEmpty(), equalTo(true));\n-\n-        assertThat(restTestSuite.getSetupSection().getDoSections().size(), equalTo(1));\n-        assertThat(restTestSuite.getSetupSection().getDoSections().get(0).getApiCallSection().getApi(), equalTo(\"indices.create\"));\n-        assertThat(restTestSuite.getSetupSection().getDoSections().get(0).getApiCallSection().getParams().size(), equalTo(1));\n-        assertThat(restTestSuite.getSetupSection().getDoSections().get(0).getApiCallSection().getParams().get(\"index\"), equalTo(\"test_index\"));\n-\n-        assertThat(restTestSuite.getTestSections().size(), equalTo(2));\n-\n-        assertThat(restTestSuite.getTestSections().get(0).getName(), equalTo(\"Get index mapping\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getSkipSection().isEmpty(), equalTo(true));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().size(), equalTo(3));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(0), instanceOf(DoSection.class));\n-        DoSection doSection = (DoSection) restTestSuite.getTestSections().get(0).getExecutableSections().get(0);\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"indices.get_mapping\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(1));\n-        assertThat(doSection.getApiCallSection().getParams().get(\"index\"), equalTo(\"test_index\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(1), instanceOf(MatchAssertion.class));\n-        MatchAssertion matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(1);\n-        assertThat(matchAssertion.getField(), equalTo(\"test_index.test_type.properties.text.type\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"string\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(2), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(2);\n-        assertThat(matchAssertion.getField(), equalTo(\"test_index.test_type.properties.text.analyzer\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"whitespace\"));\n-\n-        assertThat(restTestSuite.getTestSections().get(1).getName(), equalTo(\"Get type mapping - pre 1.0\"));\n-        assertThat(restTestSuite.getTestSections().get(1).getSkipSection().isEmpty(), equalTo(false));\n-        assertThat(restTestSuite.getTestSections().get(1).getSkipSection().getReason(), equalTo(\"for newer versions the index name is always returned\"));\n-        assertThat(restTestSuite.getTestSections().get(1).getSkipSection().getVersion(), equalTo(\"0.90.9 - 999\"));\n-        assertThat(restTestSuite.getTestSections().get(1).getExecutableSections().size(), equalTo(0));\n-    }\n-\n-    @Test\n-    public void testParseTestSetupAndSectionsSkipEntireFile() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"setup:\\n\" +\n-                        \"  - skip:\\n\" +\n-                        \"      version:     \\\"0.90.3 - 0.90.6\\\"\\n\" +\n-                        \"      reason:      \\\"test skip entire file\\\"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"        indices.create:\\n\" +\n-                        \"          index: test_index\\n\" +\n-                        \"\\n\" +\n-                        \"---\\n\" +\n-                        \"\\\"Get index mapping\\\":\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      indices.get_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"\\n\" +\n-                        \"  - match: {test_index.test_type.properties.text.type:     string}\\n\" +\n-                        \"  - match: {test_index.test_type.properties.text.analyzer: whitespace}\\n\" +\n-                        \"\\n\" +\n-                        \"---\\n\" +\n-                        \"\\\"Get type mapping - pre 1.0\\\":\\n\" +\n-                        \"\\n\" +\n-                        \"  - skip:\\n\" +\n-                        \"      version:     \\\"0.90.9 - 999\\\"\\n\" +\n-                        \"      reason:      \\\"for newer versions the index name is always returned\\\"\\n\" +\n-                        \"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      indices.get_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"        type: test_type\\n\" +\n-                        \"\\n\" +\n-                        \"  - match: {test_type.properties.text.type:     string}\\n\" +\n-                        \"  - match: {test_type.properties.text.analyzer: whitespace}\\n\"\n-        );\n-\n-        RestTestSuiteParser testParser = new RestTestSuiteParser();\n-        RestTestSuite restTestSuite = testParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-\n-        assertThat(restTestSuite, notNullValue());\n-        assertThat(restTestSuite.getName(), equalTo(\"suite\"));\n-        assertThat(restTestSuite.getSetupSection(), notNullValue());\n-\n-        assertThat(restTestSuite.getSetupSection().getSkipSection().isEmpty(), equalTo(false));\n-        assertThat(restTestSuite.getSetupSection().getSkipSection().getVersion(), equalTo(\"0.90.3 - 0.90.6\"));\n-        assertThat(restTestSuite.getSetupSection().getSkipSection().getReason(), equalTo(\"test skip entire file\"));\n-\n-        assertThat(restTestSuite.getSetupSection().getDoSections().size(), equalTo(0));\n-\n-        assertThat(restTestSuite.getTestSections().size(), equalTo(0));\n-    }\n-\n-    @Test\n-    public void testParseTestSetupAndSectionsSkipEntireFileNoDo() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"setup:\\n\" +\n-                        \"  - skip:\\n\" +\n-                        \"      version:     \\\"0.90.3 - 0.90.6\\\"\\n\" +\n-                        \"      reason:      \\\"test skip entire file\\\"\\n\" +\n-                        \"\\n\" +\n-                        \"---\\n\" +\n-                        \"\\\"Get index mapping\\\":\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      indices.get_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"\\n\" +\n-                        \"  - match: {test_index.test_type.properties.text.type:     string}\\n\" +\n-                        \"  - match: {test_index.test_type.properties.text.analyzer: whitespace}\\n\" +\n-                        \"\\n\" +\n-                        \"---\\n\" +\n-                        \"\\\"Get type mapping - pre 1.0\\\":\\n\" +\n-                        \"\\n\" +\n-                        \"  - skip:\\n\" +\n-                        \"      version:     \\\"0.90.9 - 999\\\"\\n\" +\n-                        \"      reason:      \\\"for newer versions the index name is always returned\\\"\\n\" +\n-                        \"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      indices.get_mapping:\\n\" +\n-                        \"        index: test_index\\n\" +\n-                        \"        type: test_type\\n\" +\n-                        \"\\n\" +\n-                        \"  - match: {test_type.properties.text.type:     string}\\n\" +\n-                        \"  - match: {test_type.properties.text.analyzer: whitespace}\\n\"\n-        );\n-\n-        RestTestSuiteParser testParser = new RestTestSuiteParser();\n-        RestTestSuite restTestSuite = testParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-\n-        assertThat(restTestSuite, notNullValue());\n-        assertThat(restTestSuite.getName(), equalTo(\"suite\"));\n-        assertThat(restTestSuite.getSetupSection(), notNullValue());\n-\n-        assertThat(restTestSuite.getSetupSection().getSkipSection().isEmpty(), equalTo(false));\n-        assertThat(restTestSuite.getSetupSection().getSkipSection().getVersion(), equalTo(\"0.90.3 - 0.90.6\"));\n-        assertThat(restTestSuite.getSetupSection().getSkipSection().getReason(), equalTo(\"test skip entire file\"));\n-\n-        assertThat(restTestSuite.getSetupSection().getDoSections().size(), equalTo(0));\n-\n-        assertThat(restTestSuite.getTestSections().size(), equalTo(0));\n-    }\n-\n-    @Test\n-    public void testParseTestSingleTestSection() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-        \"---\\n\" +\n-                \"\\\"Index with ID\\\":\\n\" +\n-                \"\\n\" +\n-                \" - do:\\n\" +\n-                \"      index:\\n\" +\n-                \"          index:  test-weird-index-\\n\" +\n-                \"          type:   weird.type\\n\" +\n-                \"          id:     1\\n\" +\n-                \"          body:   { foo: bar }\\n\" +\n-                \"\\n\" +\n-                \" - is_true:   ok\\n\" +\n-                \" - match:   { _index:   test-weird-index- }\\n\" +\n-                \" - match:   { _type:    weird.type }\\n\" +\n-                \" - match:   { _id:      \\\"1\\\"}\\n\" +\n-                \" - match:   { _version: 1}\\n\" +\n-                \"\\n\" +\n-                \" - do:\\n\" +\n-                \"      get:\\n\" +\n-                \"          index:  test-weird-index-\\n\" +\n-                \"          type:   weird.type\\n\" +\n-                \"          id:     1\\n\" +\n-                \"\\n\" +\n-                \" - match:   { _index:   test-weird-index- }\\n\" +\n-                \" - match:   { _type:    weird.type }\\n\" +\n-                \" - match:   { _id:      \\\"1\\\"}\\n\" +\n-                \" - match:   { _version: 1}\\n\" +\n-                \" - match:   { _source: { foo: bar }}\"\n-        );\n-\n-        RestTestSuiteParser testParser = new RestTestSuiteParser();\n-        RestTestSuite restTestSuite = testParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-\n-        assertThat(restTestSuite, notNullValue());\n-        assertThat(restTestSuite.getName(), equalTo(\"suite\"));\n-\n-        assertThat(restTestSuite.getSetupSection().isEmpty(), equalTo(true));\n-\n-        assertThat(restTestSuite.getTestSections().size(), equalTo(1));\n-\n-        assertThat(restTestSuite.getTestSections().get(0).getName(), equalTo(\"Index with ID\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getSkipSection().isEmpty(), equalTo(true));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().size(), equalTo(12));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(0), instanceOf(DoSection.class));\n-        DoSection doSection = (DoSection) restTestSuite.getTestSections().get(0).getExecutableSections().get(0);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"index\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(true));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(1), instanceOf(IsTrueAssertion.class));\n-        IsTrueAssertion trueAssertion = (IsTrueAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(1);\n-        assertThat(trueAssertion.getField(), equalTo(\"ok\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(2), instanceOf(MatchAssertion.class));\n-        MatchAssertion matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(2);\n-        assertThat(matchAssertion.getField(), equalTo(\"_index\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"test-weird-index-\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(3), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(3);\n-        assertThat(matchAssertion.getField(), equalTo(\"_type\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"weird.type\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(4), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(4);\n-        assertThat(matchAssertion.getField(), equalTo(\"_id\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"1\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(5), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(5);\n-        assertThat(matchAssertion.getField(), equalTo(\"_version\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"1\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(6), instanceOf(DoSection.class));\n-        doSection = (DoSection) restTestSuite.getTestSections().get(0).getExecutableSections().get(6);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"get\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(7), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(7);\n-        assertThat(matchAssertion.getField(), equalTo(\"_index\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"test-weird-index-\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(8), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(8);\n-        assertThat(matchAssertion.getField(), equalTo(\"_type\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"weird.type\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(9), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(9);\n-        assertThat(matchAssertion.getField(), equalTo(\"_id\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"1\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(10), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(10);\n-        assertThat(matchAssertion.getField(), equalTo(\"_version\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"1\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(11), instanceOf(MatchAssertion.class));\n-        matchAssertion = (MatchAssertion) restTestSuite.getTestSections().get(0).getExecutableSections().get(11);\n-        assertThat(matchAssertion.getField(), equalTo(\"_source\"));\n-        assertThat(matchAssertion.getExpectedValue(), instanceOf(Map.class));\n-        assertThat(((Map) matchAssertion.getExpectedValue()).get(\"foo\").toString(), equalTo(\"bar\"));\n-    }\n-\n-    @Test\n-    public void testParseTestMultipleTestSections() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-        \"---\\n\" +\n-                \"\\\"Missing document (partial doc)\\\":\\n\" +\n-                \"\\n\" +\n-                \"  - do:\\n\" +\n-                \"      catch:      missing\\n\" +\n-                \"      update:\\n\" +\n-                \"          index:  test_1\\n\" +\n-                \"          type:   test\\n\" +\n-                \"          id:     1\\n\" +\n-                \"          body:   { doc: { foo: bar } }\\n\" +\n-                \"\\n\" +\n-                \"  - do:\\n\" +\n-                \"      update:\\n\" +\n-                \"          index: test_1\\n\" +\n-                \"          type:  test\\n\" +\n-                \"          id:    1\\n\" +\n-                \"          body:  { doc: { foo: bar } }\\n\" +\n-                \"          ignore: 404\\n\" +\n-                \"\\n\" +\n-                \"---\\n\" +\n-                \"\\\"Missing document (script)\\\":\\n\" +\n-                \"\\n\" +\n-                \"\\n\" +\n-                \"  - do:\\n\" +\n-                \"      catch:      missing\\n\" +\n-                \"      update:\\n\" +\n-                \"          index:  test_1\\n\" +\n-                \"          type:   test\\n\" +\n-                \"          id:     1\\n\" +\n-                \"          body:\\n\" +\n-                \"            script: \\\"ctx._source.foo = bar\\\"\\n\" +\n-                \"            params: { bar: 'xxx' }\\n\" +\n-                \"\\n\" +\n-                \"  - do:\\n\" +\n-                \"      update:\\n\" +\n-                \"          index:  test_1\\n\" +\n-                \"          type:   test\\n\" +\n-                \"          id:     1\\n\" +\n-                \"          ignore: 404\\n\" +\n-                \"          body:\\n\" +\n-                \"            script:       \\\"ctx._source.foo = bar\\\"\\n\" +\n-                \"            params:       { bar: 'xxx' }\\n\"\n-        );\n-\n-        RestTestSuiteParser testParser = new RestTestSuiteParser();\n-        RestTestSuite restTestSuite = testParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-\n-        assertThat(restTestSuite, notNullValue());\n-        assertThat(restTestSuite.getName(), equalTo(\"suite\"));\n-\n-        assertThat(restTestSuite.getSetupSection().isEmpty(), equalTo(true));\n-\n-        assertThat(restTestSuite.getTestSections().size(), equalTo(2));\n-\n-        assertThat(restTestSuite.getTestSections().get(0).getName(), equalTo(\"Missing document (partial doc)\"));\n-        assertThat(restTestSuite.getTestSections().get(0).getSkipSection().isEmpty(), equalTo(true));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().size(), equalTo(2));\n-\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(0), instanceOf(DoSection.class));\n-        DoSection doSection = (DoSection) restTestSuite.getTestSections().get(0).getExecutableSections().get(0);\n-        assertThat(doSection.getCatch(), equalTo(\"missing\"));\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"update\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(true));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(1), instanceOf(DoSection.class));\n-        doSection = (DoSection) restTestSuite.getTestSections().get(0).getExecutableSections().get(1);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"update\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(4));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(true));\n-\n-        assertThat(restTestSuite.getTestSections().get(1).getName(), equalTo(\"Missing document (script)\"));\n-        assertThat(restTestSuite.getTestSections().get(1).getSkipSection().isEmpty(), equalTo(true));\n-        assertThat(restTestSuite.getTestSections().get(1).getExecutableSections().size(), equalTo(2));\n-        assertThat(restTestSuite.getTestSections().get(1).getExecutableSections().get(0), instanceOf(DoSection.class));\n-        assertThat(restTestSuite.getTestSections().get(1).getExecutableSections().get(1), instanceOf(DoSection.class));\n-        doSection = (DoSection) restTestSuite.getTestSections().get(1).getExecutableSections().get(0);\n-        assertThat(doSection.getCatch(), equalTo(\"missing\"));\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"update\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(true));\n-        assertThat(restTestSuite.getTestSections().get(0).getExecutableSections().get(1), instanceOf(DoSection.class));\n-        doSection = (DoSection) restTestSuite.getTestSections().get(1).getExecutableSections().get(1);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"update\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(4));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(true));\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/SetSectionParserTests.java b/src/test/java/org/elasticsearch/test/rest/test/SetSectionParserTests.java\ndeleted file mode 100644\nindex ee0a16290de..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/SetSectionParserTests.java\n+++ /dev/null\n@@ -1,77 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.yaml.YamlXContent;\n-import org.elasticsearch.test.rest.parser.RestTestParseException;\n-import org.elasticsearch.test.rest.parser.RestTestSuiteParseContext;\n-import org.elasticsearch.test.rest.parser.SetSectionParser;\n-import org.elasticsearch.test.rest.section.SetSection;\n-import org.junit.Test;\n-\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.notNullValue;\n-\n-public class SetSectionParserTests extends AbstractParserTests {\n-\n-    @Test\n-    public void testParseSetSectionSingleValue() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                        \"{ _id: id }\"\n-        );\n-\n-        SetSectionParser setSectionParser = new SetSectionParser();\n-\n-        SetSection setSection = setSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(setSection, notNullValue());\n-        assertThat(setSection.getStash(), notNullValue());\n-        assertThat(setSection.getStash().size(), equalTo(1));\n-        assertThat(setSection.getStash().get(\"_id\"), equalTo(\"id\"));\n-    }\n-\n-    @Test\n-    public void testParseSetSectionMultipleValues() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{ _id: id, _type: type, _index: index }\"\n-        );\n-\n-        SetSectionParser setSectionParser = new SetSectionParser();\n-\n-        SetSection setSection = setSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(setSection, notNullValue());\n-        assertThat(setSection.getStash(), notNullValue());\n-        assertThat(setSection.getStash().size(), equalTo(3));\n-        assertThat(setSection.getStash().get(\"_id\"), equalTo(\"id\"));\n-        assertThat(setSection.getStash().get(\"_type\"), equalTo(\"type\"));\n-        assertThat(setSection.getStash().get(\"_index\"), equalTo(\"index\"));\n-    }\n-\n-    @Test(expected = RestTestParseException.class)\n-    public void testParseSetSectionNoValues() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"{ }\"\n-        );\n-\n-        SetSectionParser setSectionParser = new SetSectionParser();\n-\n-        setSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/SetupSectionParserTests.java b/src/test/java/org/elasticsearch/test/rest/test/SetupSectionParserTests.java\ndeleted file mode 100644\nindex 9043e297634..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/SetupSectionParserTests.java\n+++ /dev/null\n@@ -1,125 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.yaml.YamlXContent;\n-import org.elasticsearch.test.rest.parser.RestTestSuiteParseContext;\n-import org.elasticsearch.test.rest.parser.SetupSectionParser;\n-import org.elasticsearch.test.rest.section.SetupSection;\n-import org.junit.Test;\n-\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.notNullValue;\n-\n-public class SetupSectionParserTests extends AbstractParserTests {\n-\n-    @Test\n-    public void testParseSetupSection() throws Exception {\n-\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"  - do:\\n\" +\n-                \"      index1:\\n\" +\n-                \"        index:  test_1\\n\" +\n-                \"        type:   test\\n\" +\n-                \"        id:     1\\n\" +\n-                \"        body:   { \\\"include\\\": { \\\"field1\\\": \\\"v1\\\", \\\"field2\\\": \\\"v2\\\" }, \\\"count\\\": 1 }\\n\" +\n-                \"  - do:\\n\" +\n-                \"      index2:\\n\" +\n-                \"        index:  test_1\\n\" +\n-                \"        type:   test\\n\" +\n-                \"        id:     2\\n\" +\n-                \"        body:   { \\\"include\\\": { \\\"field1\\\": \\\"v1\\\", \\\"field2\\\": \\\"v2\\\" }, \\\"count\\\": 1 }\\n\"\n-        );\n-\n-        SetupSectionParser setupSectionParser = new SetupSectionParser();\n-        SetupSection setupSection = setupSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(setupSection, notNullValue());\n-        assertThat(setupSection.getSkipSection().isEmpty(), equalTo(true));\n-        assertThat(setupSection.getDoSections().size(), equalTo(2));\n-        assertThat(setupSection.getDoSections().get(0).getApiCallSection().getApi(), equalTo(\"index1\"));\n-        assertThat(setupSection.getDoSections().get(1).getApiCallSection().getApi(), equalTo(\"index2\"));\n-    }\n-\n-    @Test\n-    public void testParseSetupAndSkipSectionSkip() throws Exception {\n-\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"  - skip:\\n\" +\n-                \"      version:  \\\"0.90.0 - 0.90.7\\\"\\n\" +\n-                \"      reason:   \\\"Update doesn't return metadata fields, waiting for #3259\\\"\\n\" +\n-                \"  - do:\\n\" +\n-                \"      index1:\\n\" +\n-                \"        index:  test_1\\n\" +\n-                \"        type:   test\\n\" +\n-                \"        id:     1\\n\" +\n-                \"        body:   { \\\"include\\\": { \\\"field1\\\": \\\"v1\\\", \\\"field2\\\": \\\"v2\\\" }, \\\"count\\\": 1 }\\n\" +\n-                \"  - do:\\n\" +\n-                \"      index2:\\n\" +\n-                \"        index:  test_1\\n\" +\n-                \"        type:   test\\n\" +\n-                \"        id:     2\\n\" +\n-                \"        body:   { \\\"include\\\": { \\\"field1\\\": \\\"v1\\\", \\\"field2\\\": \\\"v2\\\" }, \\\"count\\\": 1 }\\n\"\n-        );\n-\n-        SetupSectionParser setupSectionParser = new SetupSectionParser();\n-        SetupSection setupSection = setupSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-\n-        assertThat(setupSection, notNullValue());\n-        assertThat(setupSection.getSkipSection().isEmpty(), equalTo(false));\n-        assertThat(setupSection.getSkipSection(), notNullValue());\n-        assertThat(setupSection.getSkipSection().getVersion(), equalTo(\"0.90.0 - 0.90.7\"));\n-        assertThat(setupSection.getSkipSection().getReason(), equalTo(\"Update doesn't return metadata fields, waiting for #3259\"));\n-        assertThat(setupSection.getDoSections().size(), equalTo(0));\n-    }\n-\n-    @Test\n-    public void testParseSetupAndSkipSectionNoSkip() throws Exception {\n-\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"  - skip:\\n\" +\n-                        \"      version:  \\\"0.90.0 - 0.90.7\\\"\\n\" +\n-                        \"      reason:   \\\"Update doesn't return metadata fields, waiting for #3259\\\"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      index1:\\n\" +\n-                        \"        index:  test_1\\n\" +\n-                        \"        type:   test\\n\" +\n-                        \"        id:     1\\n\" +\n-                        \"        body:   { \\\"include\\\": { \\\"field1\\\": \\\"v1\\\", \\\"field2\\\": \\\"v2\\\" }, \\\"count\\\": 1 }\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      index2:\\n\" +\n-                        \"        index:  test_1\\n\" +\n-                        \"        type:   test\\n\" +\n-                        \"        id:     2\\n\" +\n-                        \"        body:   { \\\"include\\\": { \\\"field1\\\": \\\"v1\\\", \\\"field2\\\": \\\"v2\\\" }, \\\"count\\\": 1 }\\n\"\n-        );\n-\n-        SetupSectionParser setupSectionParser = new SetupSectionParser();\n-        SetupSection setupSection = setupSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.8\"));\n-\n-        assertThat(setupSection, notNullValue());\n-        assertThat(setupSection.getSkipSection().isEmpty(), equalTo(false));\n-        assertThat(setupSection.getSkipSection(), notNullValue());\n-        assertThat(setupSection.getSkipSection().getVersion(), equalTo(\"0.90.0 - 0.90.7\"));\n-        assertThat(setupSection.getSkipSection().getReason(), equalTo(\"Update doesn't return metadata fields, waiting for #3259\"));\n-        assertThat(setupSection.getDoSections().size(), equalTo(2));\n-        assertThat(setupSection.getDoSections().get(0).getApiCallSection().getApi(), equalTo(\"index1\"));\n-        assertThat(setupSection.getDoSections().get(1).getApiCallSection().getApi(), equalTo(\"index2\"));\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/SkipSectionParserTests.java b/src/test/java/org/elasticsearch/test/rest/test/SkipSectionParserTests.java\ndeleted file mode 100644\nindex 87f40a49ab7..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/SkipSectionParserTests.java\n+++ /dev/null\n@@ -1,68 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.yaml.YamlXContent;\n-import org.elasticsearch.test.rest.parser.RestTestParseException;\n-import org.elasticsearch.test.rest.parser.RestTestSuiteParseContext;\n-import org.elasticsearch.test.rest.parser.SkipSectionParser;\n-import org.elasticsearch.test.rest.section.SkipSection;\n-import org.junit.Test;\n-\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.notNullValue;\n-\n-public class SkipSectionParserTests extends AbstractParserTests {\n-\n-    @Test\n-    public void testParseSkipSection() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                        \"version:     \\\"0 - 0.90.2\\\"\\n\" +\n-                        \"reason:      Delete ignores the parent param\"\n-        );\n-\n-        SkipSectionParser skipSectionParser = new SkipSectionParser();\n-\n-        SkipSection skipSection = skipSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(skipSection, notNullValue());\n-        assertThat(skipSection.getVersion(), equalTo(\"0 - 0.90.2\"));\n-        assertThat(skipSection.getReason(), equalTo(\"Delete ignores the parent param\"));\n-    }\n-\n-    @Test(expected = RestTestParseException.class)\n-    public void testParseSkipSectionNoReason() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"version:     \\\"0 - 0.90.2\\\"\\n\"\n-        );\n-\n-        SkipSectionParser skipSectionParser = new SkipSectionParser();\n-        skipSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-    }\n-\n-    @Test(expected = RestTestParseException.class)\n-    public void testParseSkipSectionNoVersion() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"reason:      Delete ignores the parent param\\n\"\n-        );\n-\n-        SkipSectionParser skipSectionParser = new SkipSectionParser();\n-        skipSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-    }\n-}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/TestSectionParserTests.java b/src/test/java/org/elasticsearch/test/rest/test/TestSectionParserTests.java\ndeleted file mode 100644\nindex 43ca15807f1..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/TestSectionParserTests.java\n+++ /dev/null\n@@ -1,274 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.common.xcontent.yaml.YamlXContent;\n-import org.elasticsearch.test.rest.parser.RestTestSectionParser;\n-import org.elasticsearch.test.rest.parser.RestTestSuiteParseContext;\n-import org.elasticsearch.test.rest.section.*;\n-import org.junit.Test;\n-\n-import java.util.Map;\n-\n-import static org.hamcrest.Matchers.*;\n-\n-public class TestSectionParserTests extends AbstractParserTests {\n-\n-    @Test\n-    public void testParseTestSectionWithDoSection() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"\\\"First test section\\\": \\n\" +\n-                \" - do :\\n\" +\n-                \"     catch: missing\\n\" +\n-                \"     indices.get_warmer:\\n\" +\n-                \"         index: test_index\\n\" +\n-                \"         name: test_warmer\"\n-        );\n-\n-        RestTestSectionParser testSectionParser = new RestTestSectionParser();\n-        TestSection testSection = testSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(testSection, notNullValue());\n-        assertThat(testSection.getName(), equalTo(\"First test section\"));\n-        assertThat(testSection.getSkipSection(), equalTo(SkipSection.EMPTY));\n-        assertThat(testSection.getExecutableSections().size(), equalTo(1));\n-        DoSection doSection = (DoSection)testSection.getExecutableSections().get(0);\n-        assertThat(doSection.getCatch(), equalTo(\"missing\"));\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"indices.get_warmer\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(2));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));\n-    }\n-\n-    @Test\n-    public void testParseTestSectionWithDoSetAndSkipSectionsSkip() throws Exception {\n-        String yaml =\n-                \"\\\"First test section\\\": \\n\" +\n-                \"  - skip:\\n\" +\n-                \"      version:  \\\"0.90.0 - 0.90.7\\\"\\n\" +\n-                \"      reason:   \\\"Update doesn't return metadata fields, waiting for #3259\\\"\\n\" +\n-                \"  - do :\\n\" +\n-                \"      catch: missing\\n\" +\n-                \"      indices.get_warmer:\\n\" +\n-                \"          index: test_index\\n\" +\n-                \"          name: test_warmer\\n\" +\n-                \"  - set: {_scroll_id: scroll_id}\";\n-\n-\n-        RestTestSectionParser testSectionParser = new RestTestSectionParser();\n-        parser = YamlXContent.yamlXContent.createParser(yaml);\n-        TestSection testSection = testSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.7\"));\n-\n-        assertThat(testSection, notNullValue());\n-        assertThat(testSection.getName(), equalTo(\"First test section\"));\n-        assertThat(testSection.getSkipSection(), notNullValue());\n-        assertThat(testSection.getSkipSection().getVersion(), equalTo(\"0.90.0 - 0.90.7\"));\n-        assertThat(testSection.getSkipSection().getReason(), equalTo(\"Update doesn't return metadata fields, waiting for #3259\"));\n-        //skip parsing when needed\n-        assertThat(testSection.getExecutableSections().size(), equalTo(0));\n-    }\n-\n-    @Test\n-    public void testParseTestSectionWithDoSetAndSkipSectionsNoSkip() throws Exception {\n-        String yaml =\n-                \"\\\"First test section\\\": \\n\" +\n-                        \"  - skip:\\n\" +\n-                        \"      version:  \\\"0.90.0 - 0.90.7\\\"\\n\" +\n-                        \"      reason:   \\\"Update doesn't return metadata fields, waiting for #3259\\\"\\n\" +\n-                        \"  - do :\\n\" +\n-                        \"      catch: missing\\n\" +\n-                        \"      indices.get_warmer:\\n\" +\n-                        \"          index: test_index\\n\" +\n-                        \"          name: test_warmer\\n\" +\n-                        \"  - set: {_scroll_id: scroll_id}\";\n-\n-\n-        RestTestSectionParser testSectionParser = new RestTestSectionParser();\n-        parser = YamlXContent.yamlXContent.createParser(yaml);\n-        TestSection testSection = testSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.8\"));\n-\n-        assertThat(testSection, notNullValue());\n-        assertThat(testSection.getName(), equalTo(\"First test section\"));\n-        assertThat(testSection.getSkipSection(), notNullValue());\n-        assertThat(testSection.getSkipSection().getVersion(), equalTo(\"0.90.0 - 0.90.7\"));\n-        assertThat(testSection.getSkipSection().getReason(), equalTo(\"Update doesn't return metadata fields, waiting for #3259\"));\n-        assertThat(testSection.getExecutableSections().size(), equalTo(2));\n-        DoSection doSection = (DoSection)testSection.getExecutableSections().get(0);\n-        assertThat(doSection.getCatch(), equalTo(\"missing\"));\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"indices.get_warmer\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(2));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));\n-        SetSection setSection = (SetSection) testSection.getExecutableSections().get(1);\n-        assertThat(setSection.getStash().size(), equalTo(1));\n-        assertThat(setSection.getStash().get(\"_scroll_id\"), equalTo(\"scroll_id\"));\n-    }\n-\n-    @Test\n-    public void testParseTestSectionWithMultipleDoSections() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"\\\"Basic\\\":\\n\" +\n-                        \"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      index:\\n\" +\n-                        \"        index: test_1\\n\" +\n-                        \"        type:  test\\n\" +\n-                        \"        id:    \\n\" +\n-                        \"        body:  { \\\"foo\\\": \\\"Hello: \\\" }\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      get:\\n\" +\n-                        \"        index: test_1\\n\" +\n-                        \"        type:  test\\n\" +\n-                        \"        id:    \"\n-        );\n-\n-        RestTestSectionParser testSectionParser = new RestTestSectionParser();\n-        TestSection testSection = testSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-\n-        assertThat(testSection, notNullValue());\n-        assertThat(testSection.getName(), equalTo(\"Basic\"));\n-        assertThat(testSection.getSkipSection(), equalTo(SkipSection.EMPTY));\n-        assertThat(testSection.getExecutableSections().size(), equalTo(2));\n-        DoSection doSection = (DoSection)testSection.getExecutableSections().get(0);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"index\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(true));\n-        doSection = (DoSection)testSection.getExecutableSections().get(1);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"get\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));\n-    }\n-\n-    @Test\n-    public void testParseTestSectionWithDoSectionsAndAssertions() throws Exception {\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"\\\"Basic\\\":\\n\" +\n-                        \"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      index:\\n\" +\n-                        \"        index: test_1\\n\" +\n-                        \"        type:  test\\n\" +\n-                        \"        id:    \\n\" +\n-                        \"        body:  { \\\"foo\\\": \\\"Hello: \\\" }\\n\" +\n-                        \"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      get:\\n\" +\n-                        \"        index: test_1\\n\" +\n-                        \"        type:  test\\n\" +\n-                        \"        id:    \\n\" +\n-                        \"\\n\" +\n-                        \"  - match: { _index:   test_1 }\\n\" +\n-                        \"  - is_true: _source\\n\" +\n-                        \"  - match: { _source:  { foo: \\\"Hello: \\\" } }\\n\" +\n-                        \"\\n\" +\n-                        \"  - do:\\n\" +\n-                        \"      get:\\n\" +\n-                        \"        index: test_1\\n\" +\n-                        \"        id:    \\n\" +\n-                        \"\\n\" +\n-                        \"  - length: { _index:   6 }\\n\" +\n-                        \"  - is_false: whatever\\n\" +\n-                        \"  - gt: { size: 5      }\\n\" +\n-                        \"  - lt: { size: 10      }\"\n-        );\n-\n-        RestTestSectionParser testSectionParser = new RestTestSectionParser();\n-        TestSection testSection = testSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-\n-        assertThat(testSection, notNullValue());\n-        assertThat(testSection.getName(), equalTo(\"Basic\"));\n-        assertThat(testSection.getSkipSection(), equalTo(SkipSection.EMPTY));\n-        assertThat(testSection.getExecutableSections().size(), equalTo(10));\n-\n-        DoSection doSection = (DoSection)testSection.getExecutableSections().get(0);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"index\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(true));\n-\n-        doSection = (DoSection)testSection.getExecutableSections().get(1);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"get\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(3));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));\n-\n-        MatchAssertion matchAssertion = (MatchAssertion)testSection.getExecutableSections().get(2);\n-        assertThat(matchAssertion.getField(), equalTo(\"_index\"));\n-        assertThat(matchAssertion.getExpectedValue().toString(), equalTo(\"test_1\"));\n-\n-        IsTrueAssertion trueAssertion = (IsTrueAssertion)testSection.getExecutableSections().get(3);\n-        assertThat(trueAssertion.getField(), equalTo(\"_source\"));\n-\n-        matchAssertion = (MatchAssertion)testSection.getExecutableSections().get(4);\n-        assertThat(matchAssertion.getField(), equalTo(\"_source\"));\n-        assertThat(matchAssertion.getExpectedValue(), instanceOf(Map.class));\n-        Map map = (Map) matchAssertion.getExpectedValue();\n-        assertThat(map.size(), equalTo(1));\n-        assertThat(map.get(\"foo\").toString(), equalTo(\"Hello: \"));\n-\n-        doSection = (DoSection)testSection.getExecutableSections().get(5);\n-        assertThat(doSection.getCatch(), nullValue());\n-        assertThat(doSection.getApiCallSection(), notNullValue());\n-        assertThat(doSection.getApiCallSection().getApi(), equalTo(\"get\"));\n-        assertThat(doSection.getApiCallSection().getParams().size(), equalTo(2));\n-        assertThat(doSection.getApiCallSection().hasBody(), equalTo(false));\n-\n-        LengthAssertion lengthAssertion = (LengthAssertion) testSection.getExecutableSections().get(6);\n-        assertThat(lengthAssertion.getField(), equalTo(\"_index\"));\n-        assertThat(lengthAssertion.getExpectedValue(), instanceOf(Integer.class));\n-        assertThat((Integer) lengthAssertion.getExpectedValue(), equalTo(6));\n-\n-        IsFalseAssertion falseAssertion = (IsFalseAssertion)testSection.getExecutableSections().get(7);\n-        assertThat(falseAssertion.getField(), equalTo(\"whatever\"));\n-\n-        GreaterThanAssertion greaterThanAssertion = (GreaterThanAssertion) testSection.getExecutableSections().get(8);\n-        assertThat(greaterThanAssertion.getField(), equalTo(\"size\"));\n-        assertThat(greaterThanAssertion.getExpectedValue(), instanceOf(Integer.class));\n-        assertThat((Integer) greaterThanAssertion.getExpectedValue(), equalTo(5));\n-\n-        LessThanAssertion lessThanAssertion = (LessThanAssertion) testSection.getExecutableSections().get(9);\n-        assertThat(lessThanAssertion.getField(), equalTo(\"size\"));\n-        assertThat(lessThanAssertion.getExpectedValue(), instanceOf(Integer.class));\n-        assertThat((Integer) lessThanAssertion.getExpectedValue(), equalTo(10));\n-    }\n-\n-    @Test\n-    public void testSmallSection() throws Exception {\n-\n-        parser = YamlXContent.yamlXContent.createParser(\n-                \"\\\"node_info test\\\":\\n\" +\n-                \"  - do:\\n\" +\n-                \"      cluster.node_info: {}\\n\" +\n-                \"  \\n\" +\n-                \"  - is_true: ok\\n\" +\n-                \"  - is_true: nodes\\n\" +\n-                \"  - is_true: cluster_name\\n\");\n-        RestTestSectionParser testSectionParser = new RestTestSectionParser();\n-        TestSection testSection = testSectionParser.parse(new RestTestSuiteParseContext(\"api\", \"suite\", parser, \"0.90.5\"));\n-        assertThat(testSection, notNullValue());\n-        assertThat(testSection.getName(), equalTo(\"node_info test\"));\n-        assertThat(testSection.getExecutableSections().size(), equalTo(4));\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/rest/test/VersionUtilsTests.java b/src/test/java/org/elasticsearch/test/rest/test/VersionUtilsTests.java\ndeleted file mode 100644\nindex b05acb58d61..00000000000\n--- a/src/test/java/org/elasticsearch/test/rest/test/VersionUtilsTests.java\n+++ /dev/null\n@@ -1,120 +0,0 @@\n-/*\n- * Licensed to ElasticSearch and Shay Banon under one\n- * or more contributor license agreements. See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership. ElasticSearch licenses this\n- * file to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n- * License for the specific language governing permissions and limitations under\n- * the License.\n- */\n-\n-package org.elasticsearch.test.rest.test;\n-\n-import org.elasticsearch.test.ElasticsearchTestCase;\n-import org.junit.Test;\n-\n-import static org.elasticsearch.test.rest.support.VersionUtils.parseVersionNumber;\n-import static org.elasticsearch.test.rest.support.VersionUtils.skipCurrentVersion;\n-import static org.hamcrest.Matchers.*;\n-\n-public class VersionUtilsTests extends ElasticsearchTestCase {\n-\n-    @Test\n-    public void testParseVersionNumber() {\n-\n-        int[] versionNumber = parseVersionNumber(\"0.90.6\");\n-        assertThat(versionNumber.length, equalTo(3));\n-        assertThat(versionNumber[0], equalTo(0));\n-        assertThat(versionNumber[1], equalTo(90));\n-        assertThat(versionNumber[2], equalTo(6));\n-\n-        versionNumber = parseVersionNumber(\"0.90.999\");\n-        assertThat(versionNumber.length, equalTo(3));\n-        assertThat(versionNumber[0], equalTo(0));\n-        assertThat(versionNumber[1], equalTo(90));\n-        assertThat(versionNumber[2], equalTo(999));\n-\n-        versionNumber = parseVersionNumber(\"0.20.11\");\n-        assertThat(versionNumber.length, equalTo(3));\n-        assertThat(versionNumber[0], equalTo(0));\n-        assertThat(versionNumber[1], equalTo(20));\n-        assertThat(versionNumber[2], equalTo(11));\n-\n-        versionNumber = parseVersionNumber(\"1.0.0.Beta1\");\n-        assertThat(versionNumber.length, equalTo(3));\n-        assertThat(versionNumber[0], equalTo(1));\n-        assertThat(versionNumber[1], equalTo(0));\n-        assertThat(versionNumber[2], equalTo(0));\n-\n-        versionNumber = parseVersionNumber(\"1.0.0.RC1\");\n-        assertThat(versionNumber.length, equalTo(3));\n-        assertThat(versionNumber[0], equalTo(1));\n-        assertThat(versionNumber[1], equalTo(0));\n-        assertThat(versionNumber[2], equalTo(0));\n-\n-        versionNumber = parseVersionNumber(\"1.0.0\");\n-        assertThat(versionNumber.length, equalTo(3));\n-        assertThat(versionNumber[0], equalTo(1));\n-        assertThat(versionNumber[1], equalTo(0));\n-        assertThat(versionNumber[2], equalTo(0));\n-\n-        versionNumber = parseVersionNumber(\"1.0\");\n-        assertThat(versionNumber.length, equalTo(2));\n-        assertThat(versionNumber[0], equalTo(1));\n-        assertThat(versionNumber[1], equalTo(0));\n-\n-        versionNumber = parseVersionNumber(\"999\");\n-        assertThat(versionNumber.length, equalTo(1));\n-        assertThat(versionNumber[0], equalTo(999));\n-\n-        versionNumber = parseVersionNumber(\"0\");\n-        assertThat(versionNumber.length, equalTo(1));\n-        assertThat(versionNumber[0], equalTo(0));\n-\n-        try {\n-            parseVersionNumber(\"1.0.Beta1\");\n-            fail(\"parseVersionNumber should have thrown an error\");\n-        } catch(IllegalArgumentException e) {\n-            assertThat(e.getMessage(), containsString(\"version is not a number\"));\n-            assertThat(e.getCause(), instanceOf(NumberFormatException.class));\n-        }\n-    }\n-\n-    @Test\n-    public void testSkipCurrentVersion() {\n-        assertThat(skipCurrentVersion(\"0.90.2 - 0.90.6\", \"0.90.2\"), equalTo(true));\n-        assertThat(skipCurrentVersion(\"0.90.2 - 0.90.6\", \"0.90.3\"), equalTo(true));\n-        assertThat(skipCurrentVersion(\"0.90.2 - 0.90.6\", \"0.90.6\"), equalTo(true));\n-\n-        assertThat(skipCurrentVersion(\"0.90.2 - 0.90.6\", \"0.20.10\"), equalTo(false));\n-        assertThat(skipCurrentVersion(\"0.90.2 - 0.90.6\", \"0.90.1\"), equalTo(false));\n-        assertThat(skipCurrentVersion(\"0.90.2 - 0.90.6\", \"0.90.7\"), equalTo(false));\n-        assertThat(skipCurrentVersion(\"0.90.2 - 0.90.6\", \"1.0.0\"), equalTo(false));\n-\n-        assertThat(skipCurrentVersion(\" 0.90.2  -  0.90.999 \", \"0.90.15\"), equalTo(true));\n-        assertThat(skipCurrentVersion(\"0.90.2 - 0.90.999\", \"1.0.0\"), equalTo(false));\n-\n-        assertThat(skipCurrentVersion(\"0  -  999\", \"0.90.15\"), equalTo(true));\n-        assertThat(skipCurrentVersion(\"0  -  999\", \"0.20.1\"), equalTo(true));\n-        assertThat(skipCurrentVersion(\"0  -  999\", \"1.0.0\"), equalTo(true));\n-\n-        assertThat(skipCurrentVersion(\"0.90.9  -  999\", \"1.0.0\"), equalTo(true));\n-        assertThat(skipCurrentVersion(\"0.90.9  -  999\", \"0.90.8\"), equalTo(false));\n-\n-        try {\n-            assertThat(skipCurrentVersion(\"0.90.2 - 0.90.999 - 1.0.0\", \"1.0.0\"), equalTo(false));\n-            fail(\"skipCurrentVersion should have thrown an error\");\n-        } catch(IllegalArgumentException e) {\n-            assertThat(e.getMessage(), containsString(\"too many skip versions found\"));\n-        }\n-\n-    }\n-}\ndiff --git a/src/test/java/org/elasticsearch/test/store/MockDirectoryHelper.java b/src/test/java/org/elasticsearch/test/store/MockDirectoryHelper.java\nindex 0421de65cc4..b9eb7d4c296 100644\n--- a/src/test/java/org/elasticsearch/test/store/MockDirectoryHelper.java\n+++ b/src/test/java/org/elasticsearch/test/store/MockDirectoryHelper.java\n@@ -20,9 +20,7 @@\n package org.elasticsearch.test.store;\n \n import com.carrotsearch.randomizedtesting.SeedUtils;\n-import org.apache.lucene.store.Directory;\n-import org.apache.lucene.store.MMapDirectory;\n-import org.apache.lucene.store.MockDirectoryWrapper;\n+import org.apache.lucene.store.*;\n import org.apache.lucene.store.MockDirectoryWrapper.Throttling;\n import org.apache.lucene.util.Constants;\n import org.elasticsearch.cache.memory.ByteBufferCache;\n@@ -41,8 +39,10 @@ import org.elasticsearch.index.store.ram.RamDirectoryService;\n import org.elasticsearch.test.ElasticsearchIntegrationTest;\n \n import java.io.IOException;\n+import java.util.Map;\n import java.util.Random;\n import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n public class MockDirectoryHelper {\n     public static final String RANDOM_IO_EXCEPTION_RATE = \"index.store.mock.random.io_exception_rate\";\n@@ -76,7 +76,7 @@ public class MockDirectoryHelper {\n         noDeleteOpenFile = indexSettings.getAsBoolean(RANDOM_NO_DELETE_OPEN_FILE, random.nextBoolean()); // true is default in MDW\n         random.nextInt(shardId.getId() + 1); // some randomness per shard\n         throttle = Throttling.valueOf(indexSettings.get(RANDOM_THROTTLE, random.nextDouble() < 0.1 ? \"SOMETIMES\" : \"NEVER\"));\n-        checkIndexOnClose = indexSettings.getAsBoolean(CHECK_INDEX_ON_CLOSE, false);// we can't do this by default since it might close the index input that we still read from in a pending fetch phase.\n+        checkIndexOnClose = indexSettings.getAsBoolean(CHECK_INDEX_ON_CLOSE, random.nextDouble() < 0.1);\n         failOnClose = indexSettings.getAsBoolean(RANDOM_FAIL_ON_CLOSE, false);\n \n         if (logger.isDebugEnabled()) {\n@@ -162,5 +162,154 @@ public class MockDirectoryHelper {\n         public void closeWithRuntimeException() throws IOException {\n             super.close(); // force fail if open files etc. called in tear down of ElasticsearchIntegrationTest\n         }\n+\n+        @Override\n+        public synchronized IndexInput openInput(String name, IOContext context) throws IOException {\n+            return new CloseTrackingMockIndexInputWrapper(name, super.openInput(name, context), logger);\n+        }\n+\n+        @Override\n+        public IndexInputSlicer createSlicer(final String name, IOContext context) throws IOException {\n+            final IndexInputSlicer slicer = super.createSlicer(name, context);\n+            return new IndexInputSlicer() {\n+\n+                @Override\n+                public IndexInput openSlice(String sliceDescription, long offset, long length) throws IOException {\n+                    return new CloseTrackingMockIndexInputWrapper(name, slicer.openSlice(sliceDescription, offset, length), logger);\n+                }\n+\n+                @Override\n+                public IndexInput openFullSlice() throws IOException {\n+                    return new CloseTrackingMockIndexInputWrapper(name, slicer.openFullSlice(), logger);\n+                }\n+\n+                @Override\n+                public void close() throws IOException {\n+                    slicer.close();\n+                }\n+            };\n+        }\n+    }\n+\n+    private static class CloseTrackingMockIndexInputWrapper extends IndexInput {\n+        private final AtomicBoolean closed = new AtomicBoolean(false);\n+        private final String name;\n+        private IndexInput delegate;\n+        private final ESLogger logger;\n+        private volatile RuntimeException closingStack;\n+\n+        public CloseTrackingMockIndexInputWrapper(String name, IndexInput delegate, ESLogger logger) {\n+            super(name);\n+            this.delegate = delegate;\n+            this.logger = logger;\n+            this.name = name;\n+        }\n+\n+        @Override\n+        public void close() throws IOException {\n+            if (closed.compareAndSet(false, true)) {\n+                closingStack = new RuntimeException(\"IndexInput closed\");\n+                delegate.close();\n+            }\n+        }\n+\n+        @Override\n+        public IndexInput clone() {\n+            ensureOpen();\n+            return delegate.clone();\n+        }\n+\n+\n+        private void ensureOpen() {\n+            if (closed.get()) {\n+                logger.debug(\"Abusing IndexInput for: [\" + name + \"] - already closed\", closingStack);\n+                throw new RuntimeException(\"Abusing closed IndexInput!\");\n+            }\n+        }\n+\n+        @Override\n+        public long getFilePointer() {\n+            ensureOpen();\n+            return delegate.getFilePointer();\n+        }\n+\n+        @Override\n+        public void seek(long pos) throws IOException {\n+            ensureOpen();\n+            delegate.seek(pos);\n+        }\n+\n+        @Override\n+        public long length() {\n+            ensureOpen();\n+            return delegate.length();\n+        }\n+\n+        @Override\n+        public byte readByte() throws IOException {\n+            ensureOpen();\n+            return delegate.readByte();\n+        }\n+\n+        @Override\n+        public void readBytes(byte[] b, int offset, int len) throws IOException {\n+            ensureOpen();\n+            delegate.readBytes(b, offset, len);\n+        }\n+\n+        @Override\n+        public void readBytes(byte[] b, int offset, int len, boolean useBuffer)\n+                throws IOException {\n+            ensureOpen();\n+            delegate.readBytes(b, offset, len, useBuffer);\n+        }\n+\n+        @Override\n+        public short readShort() throws IOException {\n+            ensureOpen();\n+            return delegate.readShort();\n+        }\n+\n+        @Override\n+        public int readInt() throws IOException {\n+            ensureOpen();\n+            return delegate.readInt();\n+        }\n+\n+        @Override\n+        public long readLong() throws IOException {\n+            ensureOpen();\n+            return delegate.readLong();\n+        }\n+\n+        @Override\n+        public String readString() throws IOException {\n+            ensureOpen();\n+            return delegate.readString();\n+        }\n+\n+        @Override\n+        public Map<String,String> readStringStringMap() throws IOException {\n+            ensureOpen();\n+            return delegate.readStringStringMap();\n+        }\n+\n+        @Override\n+        public int readVInt() throws IOException {\n+            ensureOpen();\n+            return delegate.readVInt();\n+        }\n+\n+        @Override\n+        public long readVLong() throws IOException {\n+            ensureOpen();\n+            return delegate.readVLong();\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"CloseTrackingMockIndexInputWrapper(\" + delegate + \")\";\n+        }\n+\n     }\n }\ndiff --git a/src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java b/src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java\nindex 47a1db39d2c..649272c60a2 100644\n--- a/src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java\n+++ b/src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java\n@@ -174,7 +174,7 @@ public class SimpleValidateQueryTests extends ElasticsearchIntegrationTest {\n                 QueryBuilders.termQuery(\"foo\", \"1\"),\n                 FilterBuilders.hasChildFilter(\n                         \"child-type\",\n-                        QueryBuilders.matchQuery(\"foo\", \"1\")\n+                        QueryBuilders.fieldQuery(\"foo\", \"1\")\n                 )\n         ), equalTo(\"filtered(foo:1)->CustomQueryWrappingFilter(child_filter[child-type/type1](filtered(foo:1)->cache(_type:child-type)))\"));\n \ndiff --git a/src/test/resources/log4j.properties b/src/test/resources/log4j.properties\nindex 22f54ef68e5..526e22d4de0 100644\n--- a/src/test/resources/log4j.properties\n+++ b/src/test/resources/log4j.properties\n@@ -1,9 +1,6 @@\n es.logger.level=INFO\n log4j.rootLogger=${es.logger.level}, out\n \n-log4j.logger.org.apache.http=INFO, out\n-log4j.additivity.org.apache.http=false\n-\n log4j.appender.out=org.apache.log4j.ConsoleAppender\n log4j.appender.out.layout=org.apache.log4j.PatternLayout\n log4j.appender.out.layout.conversionPattern=[%d{ISO8601}][%-5p][%-25c] %m%n"
}