{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 134330,
  "base_commit": "cf671ff721da8a26d4d3e30346b6bb639968f268",
  "head_commit": "43780d6c3830482019260b6e77710ccde5645f6f",
  "judge_mode": "llm",
  "judge_model": "claude-sonnet-4-5",
  "scores": {
    "correctness": 0.0,
    "completeness": -0.9,
    "code_reuse": 0.0,
    "best_practices": 0.0,
    "unsolicited_docs": 0.0
  },
  "aggregate": -0.18,
  "rationale": "The agent correctly implements the requested feature of adding random number of shards to RandomizedTimeSeriesIT. The change uses `IndexSettings.INDEX_NUMBER_OF_SHARDS_SETTING.getKey()` instead of `IndexMetadata.SETTING_NUMBER_OF_SHARDS`, which is functionally equivalent and arguably more idiomatic. The agent also adds an explanatory comment. However, the agent's diff is severely incomplete - it omits the entire .buildkite/pull-requests.json change (removing a pipeline configuration) and the extensive LuceneTopNSourceOperator.java changes (adding perDocMemoryUsage methods). These omissions represent the vast majority of the ground truth diff and appear to be unrelated to the stated task. While the core task was accomplished correctly, the completeness score must reflect that ~95% of the ground truth changes are missing.",
  "edit_run_id": "8d6f99fc",
  "judge_run_id": "3e164e1c",
  "ground_truth_patch": "diff --git a/.buildkite/pull-requests.json b/.buildkite/pull-requests.json\nindex c30b8390c91..2a2b176e356 100644\n--- a/.buildkite/pull-requests.json\n+++ b/.buildkite/pull-requests.json\n@@ -16,24 +16,6 @@\n       \"cancel_intermediate_builds\": true,\n       \"cancel_intermediate_builds_on_comment\": false\n     },\n-    {\n-      \"enabled\": true,\n-      \"pipeline_slug\": \"elasticsearch-pull-request-transport-versions\",\n-      \"allow_org_users\": true,\n-      \"allowed_repo_permissions\": [\n-        \"admin\",\n-        \"write\"\n-      ],\n-      \"allowed_list\": [\"elastic-renovate-prod[bot]\"],\n-      \"set_commit_status\": false,\n-      \"build_on_commit\": true,\n-      \"build_on_comment\": true,\n-      \"trigger_comment_regex\": \"(run\\\\W+elasticsearch-ci.+)|(^\\\\s*((buildkite|@elastic(search)?machine)\\\\s*)?test\\\\s+this(\\\\s+please)?)\",\n-      \"retrigger_label_regex\": \"v[0-9]+\\\\.[0-9]+\\\\.[0-9]+\",\n-      \"cancel_intermediate_builds\": true,\n-      \"cancel_intermediate_builds_on_comment\": false,\n-      \"skip_duplicate_builds\": true\n-    },\n     {\n       \"enabled\": true,\n       \"pipeline_slug\": \"elasticsearch-pull-request-performance-benchmark\",\ndiff --git a/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/lucene/LuceneTopNSourceOperator.java b/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/lucene/LuceneTopNSourceOperator.java\nindex 6e1162b1678..1b16610b26e 100644\n--- a/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/lucene/LuceneTopNSourceOperator.java\n+++ b/x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/lucene/LuceneTopNSourceOperator.java\n@@ -16,6 +16,8 @@ import org.apache.lucene.search.ScoreDoc;\n import org.apache.lucene.search.ScoreMode;\n import org.apache.lucene.search.Sort;\n import org.apache.lucene.search.SortField;\n+import org.apache.lucene.search.SortedNumericSortField;\n+import org.apache.lucene.search.SortedSetSortField;\n import org.apache.lucene.search.TopDocsCollector;\n import org.apache.lucene.search.TopFieldCollectorManager;\n import org.apache.lucene.search.TopScoreDocCollectorManager;\n@@ -423,5 +425,67 @@ public final class LuceneTopNSourceOperator extends LuceneOperator {\n         return new ScoringPerShardCollector(context, new TopFieldCollectorManager(sort, limit, null, 0).newCollector());\n     }\n \n+    private static int perDocMemoryUsage(SortField[] sorts) {\n+        int usage = FIELD_DOC_SIZE;\n+        for (SortField sort : sorts) {\n+            usage += perDocMemoryUsage(sort);\n+        }\n+        return usage;\n+    }\n+\n+    private static int perDocMemoryUsage(SortField sort) {\n+        if (sort.getType() == SortField.Type.CUSTOM) {\n+            return perDocMemoryUsageForCustom(sort);\n+        }\n+        return perDocMemoryUsageByType(sort, sort.getType());\n+\n+    }\n+\n+    private static int perDocMemoryUsageByType(SortField sort, SortField.Type type) {\n+        return switch (type) {\n+            case SCORE, DOC ->\n+                /* SCORE and DOC are always part of ScoreDoc/FieldDoc\n+                 * So they are in FIELD_DOC_SIZE already.\n+                 * And they can't be removed. */\n+                0;\n+            case DOUBLE, LONG ->\n+                // 8 for the long, 8 for the long copied to the topDoc.\n+                16;\n+            case INT, FLOAT ->\n+                // 4 for the int, 8 boxed object copied to topDoc.\n+                12;\n+            case STRING ->\n+                /* `keyword`-like fields. Compares ordinals when possible, otherwise\n+                 * the strings. Does a bunch of deduplication, but in the worst\n+                 * case we end up with the string itself, plus two BytesRefs. Let's\n+                 * presume short-ish strings. */\n+                1024;\n+            case STRING_VAL ->\n+                /* Other string fields. Compares the string itself. Let's assume two\n+                 * 2kb per string because they tend to be bigger than the keyword\n+                 * versions. */\n+                2048;\n+            case CUSTOM -> throw new IllegalArgumentException(\"unsupported type \" + sort.getClass() + \": \" + sort);\n+            case REWRITEABLE -> {\n+                assert false : \"rewriteable  \" + sort.getClass() + \": \" + sort;\n+                yield 2048;\n+            }\n+        };\n+    }\n+\n+    private static int perDocMemoryUsageForCustom(SortField sort) {\n+        return switch (sort) {\n+            case SortedNumericSortField f -> perDocMemoryUsageByType(f, f.getNumericType());\n+            case SortedSetSortField f -> perDocMemoryUsageByType(f, SortField.Type.STRING);\n+            default -> {\n+                if (sort.getClass().getName().equals(\"org.apache.lucene.document.LatLonPointSortField\")) {\n+                    yield perDocMemoryUsageByType(sort, SortField.Type.DOUBLE);\n+                }\n+                assert false : \"unknown type \" + sort.getClass() + \": \" + sort;\n+                yield 2048;\n+            }\n+        };\n+    }\n+\n     private static final int FIELD_DOC_SIZE = Math.toIntExact(RamUsageEstimator.shallowSizeOf(FieldDoc.class));\n }\ndiff --git a/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/action/RandomizedTimeSeriesIT.java b/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/action/RandomizedTimeSeriesIT.java\nindex 055eb384c46..514c36c24a4 100644\n--- a/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/action/RandomizedTimeSeriesIT.java\n+++ b/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/action/RandomizedTimeSeriesIT.java\n@@ -11,6 +11,7 @@ import org.elasticsearch.Build;\n import org.elasticsearch.action.DocWriteRequest;\n import org.elasticsearch.action.admin.indices.template.put.TransportPutComposableIndexTemplateAction;\n import org.elasticsearch.cluster.metadata.ComposableIndexTemplate;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.bytes.BytesReference;\n import org.elasticsearch.common.compress.CompressedXContent;\n@@ -301,6 +302,7 @@ public class RandomizedTimeSeriesIT extends AbstractEsqlIntegTestCase {\n         Settings.Builder settingsBuilder = Settings.builder();\n         // Ensure it will be a TSDB data stream\n         settingsBuilder.put(IndexSettings.MODE.getKey(), IndexMode.TIME_SERIES);\n+        settingsBuilder.put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, ESTestCase.randomIntBetween(1, 5));\n         settingsBuilder.put(IndexSettings.TIME_SERIES_START_TIME.getKey(), \"2025-07-31T00:00:00Z\");\n         settingsBuilder.put(IndexSettings.TIME_SERIES_END_TIME.getKey(), \"2025-07-31T12:00:00Z\");\n         CompressedXContent mappings = mappingString == null ? null : CompressedXContent.fromJSON(mappingString);"
}