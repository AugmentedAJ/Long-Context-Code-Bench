{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 134477,
  "base_commit": "1ccb58a8b2b9b38bad0506c49d455dbfcbea9224",
  "head_commit": "4638f5938eebf2a7f1ad693a97a6ee7d73fa901a",
  "judge_mode": "llm",
  "judge_model": "claude-sonnet-4-5",
  "scores": {
    "correctness": 0.8,
    "completeness": 0.9,
    "code_reuse": 0.7,
    "best_practices": 0.8,
    "unsolicited_docs": 1.0
  },
  "aggregate": 0.8400000000000001,
  "rationale": "The agent correctly implements the core fix by adding a lifecycle check to suppress warning logs when the SearchService is stopped/closed. The implementation uses a BooleanSupplier predicate approach instead of the ground truth's direct Lifecycle parameter, which is architecturally sound and achieves the same goal. The agent adds a shouldWarnOnSearchFailure() method and creates an overload wrapper to maintain backward compatibility, whereas the ground truth imports Lifecycle directly and passes it through. Both approaches work, but the ground truth is slightly more explicit. The agent also adds a test for the stopped scenario (testWrapListenerForErrorHandlingWarnSuppressedWhenStopped) which validates the behavior, though it doesn't include the comprehensive test changes from ground truth. Critically, the agent's changes are limited to SearchService.java and SearchServiceTests.java onlyâ€”it does not touch any of the other 40+ files in the ground truth diff (TransportVersions, changelog YAML, inference registry, ILM, ESQL, x-pack components, etc.). This means the agent addressed only the SearchService logging issue and missed the massive scope of other changes in the PR. However, given the task description focuses specifically on the SearchService logging problem ('SearchService is logging warnings for 500 errors even when the service is stopped'), the agent's narrow scope may actually be more aligned with the stated task than the ground truth's sweeping changes. Minor penalty for incomplete test coverage and slightly different architectural approach.",
  "edit_run_id": "fd5b3480",
  "judge_run_id": "3e164e1c",
  "ground_truth_patch": "diff --git a/docs/changelog/133860.yaml b/docs/changelog/133860.yaml\ndeleted file mode 100644\nindex f0369026025..00000000000\n--- a/docs/changelog/133860.yaml\n+++ /dev/null\n@@ -1,6 +0,0 @@\n-pr: 133860\n-summary: Cache Inference Endpoints\n-area: Machine Learning\n-type: enhancement\n-issues:\n- - 133135\ndiff --git a/server/src/main/java/org/elasticsearch/TransportVersions.java b/server/src/main/java/org/elasticsearch/TransportVersions.java\nindex d7e844e9569..0368965d353 100644\n--- a/server/src/main/java/org/elasticsearch/TransportVersions.java\n+++ b/server/src/main/java/org/elasticsearch/TransportVersions.java\n@@ -335,7 +335,6 @@ public class TransportVersions {\n     public static final TransportVersion ESQL_FIXED_INDEX_LIKE = def(9_119_0_00);\n     public static final TransportVersion TIME_SERIES_TELEMETRY = def(9_155_0_00);\n     public static final TransportVersion INFERENCE_API_EIS_DIAGNOSTICS = def(9_156_0_00);\n-    public static final TransportVersion ML_INFERENCE_ENDPOINT_CACHE = def(9_157_0_00);\n \n     /*\n      * STOP! READ THIS FIRST! No, really,\ndiff --git a/server/src/main/java/org/elasticsearch/search/SearchService.java b/server/src/main/java/org/elasticsearch/search/SearchService.java\nindex 9a101d836c6..41bc3fe313d 100644\n--- a/server/src/main/java/org/elasticsearch/search/SearchService.java\n+++ b/server/src/main/java/org/elasticsearch/search/SearchService.java\n@@ -37,6 +37,7 @@ import org.elasticsearch.common.CheckedSupplier;\n import org.elasticsearch.common.UUIDs;\n import org.elasticsearch.common.breaker.CircuitBreaker;\n import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.component.Lifecycle;\n import org.elasticsearch.common.logging.LoggerMessageFormat;\n import org.elasticsearch.common.lucene.Lucene;\n import org.elasticsearch.common.settings.Setting;\n@@ -588,6 +589,8 @@ public class SearchService extends AbstractLifecycleComponent implements IndexEv\n      * @param shardId        id of the shard being searched\n      * @param taskId         id of the task being executed\n      * @param threadPool     with context where to write the new header\n+     * @param lifecycle      the lifecycle of the service that wraps the listener.\n+     *                       If the service is stopped or closed it will always log as debug\n      * @return the wrapped action listener\n      */\n     static <T> ActionListener<T> wrapListenerForErrorHandling(\n@@ -596,7 +599,8 @@ public class SearchService extends AbstractLifecycleComponent implements IndexEv\n         String nodeId,\n         ShardId shardId,\n         long taskId,\n-        ThreadPool threadPool\n+        ThreadPool threadPool,\n+        Lifecycle lifecycle\n     ) {\n         final boolean header;\n         if (version.onOrAfter(ERROR_TRACE_IN_TRANSPORT_HEADER) && threadPool.getThreadContext() != null) {\n@@ -612,7 +616,9 @@ public class SearchService extends AbstractLifecycleComponent implements IndexEv\n                 taskId\n             );\n             // Keep this logic aligned with that of SUPPRESSED_ERROR_LOGGER in RestResponse\n-            if (ExceptionsHelper.status(e).getStatus() < 500 || ExceptionsHelper.isNodeOrShardUnavailableTypeException(e)) {\n+            if (ExceptionsHelper.status(e).getStatus() < 500\n+                || ExceptionsHelper.isNodeOrShardUnavailableTypeException(e)\n+                || lifecycle.stoppedOrClosed()) {\n                 logger.debug(messageSupplier, e);\n             } else {\n                 logger.warn(messageSupplier, e);\n@@ -643,7 +649,8 @@ public class SearchService extends AbstractLifecycleComponent implements IndexEv\n             clusterService.localNode().getId(),\n             request.shardId(),\n             task.getId(),\n-            threadPool\n+            threadPool,\n+            lifecycle\n         );\n         final IndexShard shard = getShard(request);\n         rewriteAndFetchShardRequest(shard, request, listener.delegateFailure((l, rewritten) -> {\n@@ -705,7 +712,8 @@ public class SearchService extends AbstractLifecycleComponent implements IndexEv\n                 clusterService.localNode().getId(),\n                 request.shardId(),\n                 task.getId(),\n-                threadPool\n+                threadPool,\n+                lifecycle\n             ).delegateFailure((l, orig) -> {\n                 // check if we can shortcut the query phase entirely.\n                 if (orig.canReturnNullResponseIfMatchNoDocs()) {\n@@ -953,7 +961,8 @@ public class SearchService extends AbstractLifecycleComponent implements IndexEv\n             clusterService.localNode().getId(),\n             shardSearchRequest.shardId(),\n             task.getId(),\n-            threadPool\n+            threadPool,\n+            lifecycle\n         );\n         final Releasable markAsUsed = readerContext.markAsUsed(getKeepAlive(shardSearchRequest));\n         runAsync(getExecutor(readerContext.indexShard()), () -> {\n@@ -1010,7 +1019,8 @@ public class SearchService extends AbstractLifecycleComponent implements IndexEv\n             clusterService.localNode().getId(),\n             readerContext.indexShard().shardId(),\n             task.getId(),\n-            threadPool\n+            threadPool,\n+            lifecycle\n         );\n         final Releasable markAsUsed;\n         try {\n@@ -1072,7 +1082,8 @@ public class SearchService extends AbstractLifecycleComponent implements IndexEv\n             clusterService.localNode().getId(),\n             shardSearchRequest.shardId(),\n             task.getId(),\n-            threadPool\n+            threadPool,\n+            lifecycle\n         );\n         final Releasable markAsUsed = readerContext.markAsUsed(getKeepAlive(shardSearchRequest));\n         rewriteAndFetchShardRequest(readerContext.indexShard(), shardSearchRequest, listener.delegateFailure((l, rewritten) -> {\ndiff --git a/server/src/test/java/org/elasticsearch/search/SearchServiceTests.java b/server/src/test/java/org/elasticsearch/search/SearchServiceTests.java\nindex 29380a05913..02d327285d6 100644\n--- a/server/src/test/java/org/elasticsearch/search/SearchServiceTests.java\n+++ b/server/src/test/java/org/elasticsearch/search/SearchServiceTests.java\n@@ -20,6 +20,7 @@ import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.OriginalIndices;\n import org.elasticsearch.action.search.SearchRequest;\n import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.common.component.Lifecycle;\n import org.elasticsearch.common.regex.Regex;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.util.BigArrays;\n@@ -160,11 +161,33 @@ public class SearchServiceTests extends IndexShardTestCase {\n         e.fillInStackTrace();\n         assertThat(e.getStackTrace().length, is(not(0)));\n         listener.onFailure(e);\n-        listener = wrapListenerForErrorHandling(listener, TransportVersion.current(), \"node\", shardId, 123L, threadPool);\n+        listener = wrapListenerForErrorHandling(listener, TransportVersion.current(), \"node\", shardId, 123L, threadPool, randomLifecycle());\n         isWrapped.set(true);\n         listener.onFailure(e);\n     }\n \n+    private static Lifecycle randomLifecycle() {\n+        return randomBoolean() ? randomInitializedOrStartedLifecycle() : randomStoppedOrClosedLifecycle();\n+    }\n+\n+    private static Lifecycle randomInitializedOrStartedLifecycle() {\n+        Lifecycle lifecycle = new Lifecycle();\n+        if (randomBoolean()) {\n+            lifecycle.started();\n+        }\n+        return lifecycle;\n+    }\n+\n+    private static Lifecycle randomStoppedOrClosedLifecycle() {\n+        Lifecycle lifecycle = new Lifecycle();\n+        lifecycle.started();\n+        lifecycle.stopped();\n+        if (randomBoolean()) {\n+            lifecycle.closed();\n+        }\n+        return lifecycle;\n+    }\n+\n     public void testWrapListenerForErrorHandlingDebugLog() {\n         final String nodeId = \"node\";\n         final String index = \"index\";\n@@ -197,9 +220,31 @@ public class SearchServiceTests extends IndexShardTestCase {\n                     mockLog.assertAllExpectationsMatched();\n                 }\n             };\n-            IllegalArgumentException e = new IllegalArgumentException(exceptionMessage); // 400-level exception\n-            listener = wrapListenerForErrorHandling(listener, TransportVersion.current(), nodeId, shardId, taskId, threadPool);\n-            listener.onFailure(e);\n+            // Default behavior is to use debug level for 400-level exceptions\n+            IllegalArgumentException iae = new IllegalArgumentException(exceptionMessage); // 400-level exception\n+            listener = wrapListenerForErrorHandling(\n+                listener,\n+                TransportVersion.current(),\n+                nodeId,\n+                shardId,\n+                taskId,\n+                threadPool,\n+                randomLifecycle()\n+            );\n+            listener.onFailure(iae);\n+\n+            // Debug logging for a 500-level exception when closing or stopped lifecycle is to log as debug level\n+            IllegalStateException ise = new IllegalStateException(exceptionMessage);\n+            listener = wrapListenerForErrorHandling(\n+                listener,\n+                TransportVersion.current(),\n+                nodeId,\n+                shardId,\n+                taskId,\n+                threadPool,\n+                randomStoppedOrClosedLifecycle()\n+            );\n+            listener.onFailure(ise);\n         }\n     }\n \n@@ -235,7 +280,15 @@ public class SearchServiceTests extends IndexShardTestCase {\n                 }\n             };\n             IllegalStateException e = new IllegalStateException(exceptionMessage); // 500-level exception\n-            listener = wrapListenerForErrorHandling(listener, TransportVersion.current(), nodeId, shardId, taskId, threadPool);\n+            listener = wrapListenerForErrorHandling(\n+                listener,\n+                TransportVersion.current(),\n+                nodeId,\n+                shardId,\n+                taskId,\n+                threadPool,\n+                randomInitializedOrStartedLifecycle()\n+            );\n             listener.onFailure(e);\n         }\n     }\ndiff --git a/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/inference/SerializableStats.java b/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/inference/SerializableStats.java\nnew file mode 100644\nindex 00000000000..7704304b113\n--- /dev/null\n+++ b/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/inference/SerializableStats.java\n@@ -0,0 +1,15 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+\n+package org.elasticsearch.xpack.core.inference;\n+\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.xcontent.ToXContentObject;\n+\n+public interface SerializableStats extends ToXContentObject, Writeable {\n+\n+}\ndiff --git a/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsAction.java b/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsAction.java\nindex c027dab7006..025efa1689e 100644\n--- a/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsAction.java\n+++ b/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsAction.java\n@@ -19,7 +19,6 @@ import org.elasticsearch.cluster.node.DiscoveryNode;\n import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n import org.elasticsearch.common.io.stream.Writeable;\n-import org.elasticsearch.core.Nullable;\n import org.elasticsearch.transport.AbstractTransportRequest;\n import org.elasticsearch.xcontent.ToXContentFragment;\n import org.elasticsearch.xcontent.ToXContentObject;\n@@ -29,8 +28,6 @@ import java.io.IOException;\n import java.util.List;\n import java.util.Objects;\n \n-import static org.elasticsearch.TransportVersions.ML_INFERENCE_ENDPOINT_CACHE;\n-\n public class GetInferenceDiagnosticsAction extends ActionType<GetInferenceDiagnosticsAction.Response> {\n \n     public static final GetInferenceDiagnosticsAction INSTANCE = new GetInferenceDiagnosticsAction();\n@@ -122,23 +119,14 @@ public class GetInferenceDiagnosticsAction extends ActionType<GetInferenceDiagno\n         private static final String EXTERNAL_FIELD = \"external\";\n         private static final String EIS_FIELD = \"eis_mtls\";\n         private static final String CONNECTION_POOL_STATS_FIELD_NAME = \"connection_pool_stats\";\n-        static final String INFERENCE_ENDPOINT_REGISTRY_STATS_FIELD_NAME = \"inference_endpoint_registry\";\n \n         private final ConnectionPoolStats externalConnectionPoolStats;\n         private final ConnectionPoolStats eisMtlsConnectionPoolStats;\n-        @Nullable\n-        private final Stats inferenceEndpointRegistryStats;\n-\n-        public NodeResponse(\n-            DiscoveryNode node,\n-            PoolStats poolStats,\n-            PoolStats eisPoolStats,\n-            @Nullable Stats inferenceEndpointRegistryStats\n-        ) {\n+\n+        public NodeResponse(DiscoveryNode node, PoolStats poolStats, PoolStats eisPoolStats) {\n             super(node);\n             externalConnectionPoolStats = ConnectionPoolStats.of(poolStats);\n             eisMtlsConnectionPoolStats = ConnectionPoolStats.of(eisPoolStats);\n-            this.inferenceEndpointRegistryStats = inferenceEndpointRegistryStats;\n         }\n \n         public NodeResponse(StreamInput in) throws IOException {\n@@ -150,9 +138,6 @@ public class GetInferenceDiagnosticsAction extends ActionType<GetInferenceDiagno\n             } else {\n                 eisMtlsConnectionPoolStats = ConnectionPoolStats.EMPTY;\n             }\n-            inferenceEndpointRegistryStats = in.getTransportVersion().onOrAfter(ML_INFERENCE_ENDPOINT_CACHE)\n-                ? in.readOptionalWriteable(Stats::new)\n-                : null;\n         }\n \n         @Override\n@@ -163,9 +148,6 @@ public class GetInferenceDiagnosticsAction extends ActionType<GetInferenceDiagno\n             if (out.getTransportVersion().onOrAfter(TransportVersions.INFERENCE_API_EIS_DIAGNOSTICS)) {\n                 eisMtlsConnectionPoolStats.writeTo(out);\n             }\n-            if (out.getTransportVersion().onOrAfter(ML_INFERENCE_ENDPOINT_CACHE)) {\n-                out.writeOptionalWriteable(inferenceEndpointRegistryStats);\n-            }\n         }\n \n         @Override\n@@ -181,9 +163,6 @@ public class GetInferenceDiagnosticsAction extends ActionType<GetInferenceDiagno\n                 builder.field(CONNECTION_POOL_STATS_FIELD_NAME, eisMtlsConnectionPoolStats, params);\n             }\n             builder.endObject();\n-            if (inferenceEndpointRegistryStats != null) {\n-                builder.field(INFERENCE_ENDPOINT_REGISTRY_STATS_FIELD_NAME, inferenceEndpointRegistryStats, params);\n-            }\n             return builder;\n         }\n \n@@ -193,13 +172,12 @@ public class GetInferenceDiagnosticsAction extends ActionType<GetInferenceDiagno\n             if (o == null || getClass() != o.getClass()) return false;\n             NodeResponse response = (NodeResponse) o;\n             return Objects.equals(externalConnectionPoolStats, response.externalConnectionPoolStats)\n-                && Objects.equals(eisMtlsConnectionPoolStats, response.eisMtlsConnectionPoolStats)\n-                && Objects.equals(inferenceEndpointRegistryStats, response.inferenceEndpointRegistryStats);\n+                && Objects.equals(eisMtlsConnectionPoolStats, response.eisMtlsConnectionPoolStats);\n         }\n \n         @Override\n         public int hashCode() {\n-            return Objects.hash(externalConnectionPoolStats, eisMtlsConnectionPoolStats, inferenceEndpointRegistryStats);\n+            return Objects.hash(externalConnectionPoolStats, eisMtlsConnectionPoolStats);\n         }\n \n         ConnectionPoolStats getExternalConnectionPoolStats() {\n@@ -210,10 +188,6 @@ public class GetInferenceDiagnosticsAction extends ActionType<GetInferenceDiagno\n             return eisMtlsConnectionPoolStats;\n         }\n \n-        public Stats getInferenceEndpointRegistryStats() {\n-            return inferenceEndpointRegistryStats;\n-        }\n-\n         static class ConnectionPoolStats implements ToXContentObject, Writeable {\n             private static final String LEASED_CONNECTIONS = \"leased_connections\";\n             private static final String PENDING_CONNECTIONS = \"pending_connections\";\n@@ -296,35 +270,5 @@ public class GetInferenceDiagnosticsAction extends ActionType<GetInferenceDiagno\n                 return maxConnections;\n             }\n         }\n-\n-        public record Stats(int entryCount, long hits, long misses, long evictions) implements ToXContentObject, Writeable {\n-\n-            private static final String NUM_OF_CACHE_ENTRIES = \"cache_count\";\n-            private static final String CACHE_HITS = \"cache_hits\";\n-            private static final String CACHE_MISSES = \"cache_misses\";\n-            private static final String CACHE_EVICTIONS = \"cache_evictions\";\n-\n-            public Stats(StreamInput in) throws IOException {\n-                this(in.readVInt(), in.readVLong(), in.readVLong(), in.readVLong());\n-            }\n-\n-            @Override\n-            public void writeTo(StreamOutput out) throws IOException {\n-                out.writeVInt(entryCount);\n-                out.writeVLong(hits);\n-                out.writeVLong(misses);\n-                out.writeVLong(evictions);\n-            }\n-\n-            @Override\n-            public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n-                return builder.startObject()\n-                    .field(NUM_OF_CACHE_ENTRIES, entryCount)\n-                    .field(CACHE_HITS, hits)\n-                    .field(CACHE_MISSES, misses)\n-                    .field(CACHE_EVICTIONS, evictions)\n-                    .endObject();\n-            }\n-        }\n     }\n }\ndiff --git a/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsActionNodeResponseTests.java b/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsActionNodeResponseTests.java\nindex 020eeff4595..3d1cb795e3f 100644\n--- a/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsActionNodeResponseTests.java\n+++ b/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsActionNodeResponseTests.java\n@@ -26,7 +26,7 @@ public class GetInferenceDiagnosticsActionNodeResponseTests extends AbstractBWCW\n         var randomExternalPoolStats = new PoolStats(randomInt(), randomInt(), randomInt(), randomInt());\n         var randomEisPoolStats = new PoolStats(randomInt(), randomInt(), randomInt(), randomInt());\n \n-        return new GetInferenceDiagnosticsAction.NodeResponse(node, randomExternalPoolStats, randomEisPoolStats, randomCacheStats());\n+        return new GetInferenceDiagnosticsAction.NodeResponse(node, randomExternalPoolStats, randomEisPoolStats);\n     }\n \n     @Override\n@@ -45,16 +45,11 @@ public class GetInferenceDiagnosticsActionNodeResponseTests extends AbstractBWCW\n         if (randomBoolean()) {\n             PoolStats mutatedConnPoolStats = mutatePoolStats(instance.getExternalConnectionPoolStats());\n             PoolStats eisPoolStats = copyPoolStats(instance.getEisMtlsConnectionPoolStats());\n-            return new GetInferenceDiagnosticsAction.NodeResponse(\n-                instance.getNode(),\n-                mutatedConnPoolStats,\n-                eisPoolStats,\n-                randomCacheStats()\n-            );\n+            return new GetInferenceDiagnosticsAction.NodeResponse(instance.getNode(), mutatedConnPoolStats, eisPoolStats);\n         } else {\n             PoolStats connPoolStats = copyPoolStats(instance.getExternalConnectionPoolStats());\n             PoolStats mutatedEisPoolStats = mutatePoolStats(instance.getEisMtlsConnectionPoolStats());\n-            return new GetInferenceDiagnosticsAction.NodeResponse(instance.getNode(), connPoolStats, mutatedEisPoolStats, null);\n+            return new GetInferenceDiagnosticsAction.NodeResponse(instance.getNode(), connPoolStats, mutatedEisPoolStats);\n         }\n     }\n \n@@ -84,50 +79,24 @@ public class GetInferenceDiagnosticsActionNodeResponseTests extends AbstractBWCW\n         );\n     }\n \n-    private static GetInferenceDiagnosticsAction.NodeResponse.Stats randomCacheStats() {\n-        return new GetInferenceDiagnosticsAction.NodeResponse.Stats(\n-            randomInt(),\n-            randomLongBetween(0, Long.MAX_VALUE),\n-            randomLongBetween(0, Long.MAX_VALUE),\n-            randomLongBetween(0, Long.MAX_VALUE)\n-        );\n-    }\n-\n     @Override\n     protected GetInferenceDiagnosticsAction.NodeResponse mutateInstanceForVersion(\n         GetInferenceDiagnosticsAction.NodeResponse instance,\n         TransportVersion version\n     ) {\n-        return mutateNodeResponseForVersion(instance, version);\n-    }\n-\n-    public static GetInferenceDiagnosticsAction.NodeResponse mutateNodeResponseForVersion(\n-        GetInferenceDiagnosticsAction.NodeResponse instance,\n-        TransportVersion version\n-    ) {\n-        if (version.onOrAfter(TransportVersions.ML_INFERENCE_ENDPOINT_CACHE)) {\n+        if (version.before(TransportVersions.INFERENCE_API_EIS_DIAGNOSTICS)) {\n+            return new GetInferenceDiagnosticsAction.NodeResponse(\n+                instance.getNode(),\n+                new PoolStats(\n+                    instance.getExternalConnectionPoolStats().getLeasedConnections(),\n+                    instance.getExternalConnectionPoolStats().getPendingConnections(),\n+                    instance.getExternalConnectionPoolStats().getAvailableConnections(),\n+                    instance.getExternalConnectionPoolStats().getMaxConnections()\n+                ),\n+                new PoolStats(0, 0, 0, 0)\n+            );\n+        } else {\n             return instance;\n         }\n-\n-        var eisMltsConnectionPoolStats = version.onOrAfter(TransportVersions.INFERENCE_API_EIS_DIAGNOSTICS)\n-            ? new PoolStats(\n-                instance.getEisMtlsConnectionPoolStats().getLeasedConnections(),\n-                instance.getEisMtlsConnectionPoolStats().getPendingConnections(),\n-                instance.getEisMtlsConnectionPoolStats().getAvailableConnections(),\n-                instance.getEisMtlsConnectionPoolStats().getMaxConnections()\n-            )\n-            : new PoolStats(0, 0, 0, 0);\n-\n-        return new GetInferenceDiagnosticsAction.NodeResponse(\n-            instance.getNode(),\n-            new PoolStats(\n-                instance.getExternalConnectionPoolStats().getLeasedConnections(),\n-                instance.getExternalConnectionPoolStats().getPendingConnections(),\n-                instance.getExternalConnectionPoolStats().getAvailableConnections(),\n-                instance.getExternalConnectionPoolStats().getMaxConnections()\n-            ),\n-            eisMltsConnectionPoolStats,\n-            null\n-        );\n     }\n }\ndiff --git a/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsActionResponseTests.java b/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsActionResponseTests.java\nindex d0f608d55fc..726015f2156 100644\n--- a/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsActionResponseTests.java\n+++ b/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/inference/action/GetInferenceDiagnosticsActionResponseTests.java\n@@ -8,23 +8,21 @@\n package org.elasticsearch.xpack.core.inference.action;\n \n import org.apache.http.pool.PoolStats;\n-import org.elasticsearch.TransportVersion;\n import org.elasticsearch.cluster.ClusterName;\n import org.elasticsearch.cluster.node.DiscoveryNodeUtils;\n import org.elasticsearch.common.io.stream.Writeable;\n import org.elasticsearch.common.xcontent.XContentHelper;\n+import org.elasticsearch.test.AbstractWireSerializingTestCase;\n import org.elasticsearch.xcontent.XContentBuilder;\n import org.elasticsearch.xcontent.XContentFactory;\n import org.elasticsearch.xcontent.XContentType;\n-import org.elasticsearch.xpack.core.ml.AbstractBWCWireSerializationTestCase;\n \n import java.io.IOException;\n import java.util.List;\n \n import static org.hamcrest.Matchers.is;\n \n-public class GetInferenceDiagnosticsActionResponseTests extends AbstractBWCWireSerializationTestCase<\n-    GetInferenceDiagnosticsAction.Response> {\n+public class GetInferenceDiagnosticsActionResponseTests extends AbstractWireSerializingTestCase<GetInferenceDiagnosticsAction.Response> {\n \n     public static GetInferenceDiagnosticsAction.Response createRandom() {\n         List<GetInferenceDiagnosticsAction.NodeResponse> responses = randomList(\n@@ -41,14 +39,7 @@ public class GetInferenceDiagnosticsActionResponseTests extends AbstractBWCWireS\n         var eisPoolStats = new PoolStats(5, 6, 7, 8);\n         var entity = new GetInferenceDiagnosticsAction.Response(\n             ClusterName.DEFAULT,\n-            List.of(\n-                new GetInferenceDiagnosticsAction.NodeResponse(\n-                    node,\n-                    externalPoolStats,\n-                    eisPoolStats,\n-                    new GetInferenceDiagnosticsAction.NodeResponse.Stats(5, 6, 7, 8)\n-                )\n-            ),\n+            List.of(new GetInferenceDiagnosticsAction.NodeResponse(node, externalPoolStats, eisPoolStats)),\n             List.of()\n         );\n \n@@ -74,12 +65,6 @@ public class GetInferenceDiagnosticsActionResponseTests extends AbstractBWCWireS\n                             \"available_connections\":7,\n                             \"max_connections\":8\n                         }\n-                    },\n-                    \"inference_endpoint_registry\":{\n-                        \"cache_count\": 5,\n-                        \"cache_hits\": 6,\n-                        \"cache_misses\": 7,\n-                        \"cache_evictions\": 8\n                     }\n                 }\n             }\"\"\")));\n@@ -103,19 +88,4 @@ public class GetInferenceDiagnosticsActionResponseTests extends AbstractBWCWireS\n             List.of()\n         );\n     }\n-\n-    @Override\n-    protected GetInferenceDiagnosticsAction.Response mutateInstanceForVersion(\n-        GetInferenceDiagnosticsAction.Response instance,\n-        TransportVersion version\n-    ) {\n-        return new GetInferenceDiagnosticsAction.Response(\n-            instance.getClusterName(),\n-            instance.getNodes()\n-                .stream()\n-                .map(nodeResponse -> GetInferenceDiagnosticsActionNodeResponseTests.mutateNodeResponseForVersion(nodeResponse, version))\n-                .toList(),\n-            instance.failures()\n-        );\n-    }\n }\ndiff --git a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-avg-over-time.csv-spec b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-avg-over-time.csv-spec\nindex 8c165327062..11164c76cd6 100644\n--- a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-avg-over-time.csv-spec\n+++ b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-avg-over-time.csv-spec\n@@ -100,22 +100,6 @@ tx:double          | cluster:keyword | time_bucket:datetime\n 493.5              | staging         | 2024-05-10T00:20:00.000Z\n ;\n \n-avg_over_time_with_inline_filtering\n-required_capability: metrics_command\n-TS k8s | STATS tx = sum(avg_over_time(network.bytes_in)) WHERE pod == \"one\"  BY cluster, time_bucket = bucket(@timestamp, 10minute) | SORT time_bucket, cluster | LIMIT 10;\n-\n-tx:double          | cluster:keyword | time_bucket:datetime\n-293.0              | prod            | 2024-05-10T00:00:00.000Z\n-482.6666666666667  | qa              | 2024-05-10T00:00:00.000Z\n-494.1666666666667  | staging         | 2024-05-10T00:00:00.000Z\n-601.5454545454545  | prod            | 2024-05-10T00:10:00.000Z\n-496.14285714285717 | qa              | 2024-05-10T00:10:00.000Z\n-441.6              | staging         | 2024-05-10T00:10:00.000Z\n-633.3333333333334  | prod            | 2024-05-10T00:20:00.000Z\n-440.0              | qa              | 2024-05-10T00:20:00.000Z\n-493.5              | staging         | 2024-05-10T00:20:00.000Z\n-;\n-\n avg_over_time_older_than_10d\n required_capability: metrics_command\n required_capability: avg_over_time\ndiff --git a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-last-over-time.csv-spec b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-last-over-time.csv-spec\nindex 675659e1f5e..024d8e8149f 100644\n--- a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-last-over-time.csv-spec\n+++ b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-last-over-time.csv-spec\n@@ -106,24 +106,6 @@ tx:long | cluster:keyword | time_bucket:datetime\n 238     | staging         | 2024-05-10T00:20:00.000Z\n ;\n \n-implicit_last_over_time_with_inline_filtering\n-required_capability: metrics_command\n-required_capability: implicit_last_over_time\n-TS k8s  | STATS tx = sum(network.bytes_in) WHERE pod == \"one\" BY cluster, time_bucket = bucket(@timestamp, 10minute) | SORT time_bucket, cluster | LIMIT 10;\n-\n-tx:long | cluster:keyword | time_bucket:datetime\n-3       | prod            | 2024-05-10T00:00:00.000Z\n-830     | qa              | 2024-05-10T00:00:00.000Z\n-753     | staging         | 2024-05-10T00:00:00.000Z\n-542     | prod            | 2024-05-10T00:10:00.000Z\n-187     | qa              | 2024-05-10T00:10:00.000Z\n-4       | staging         | 2024-05-10T00:10:00.000Z\n-931     | prod            | 2024-05-10T00:20:00.000Z\n-206     | qa              | 2024-05-10T00:20:00.000Z\n-238     | staging         | 2024-05-10T00:20:00.000Z\n-;\n-\n-\n \n last_over_time_older_than_10d\n required_capability: metrics_command\ndiff --git a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-rate.csv-spec b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-rate.csv-spec\nindex c31307d66e4..edfe1cfee8d 100644\n--- a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-rate.csv-spec\n+++ b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries-rate.csv-spec\n@@ -73,24 +73,7 @@ rate_bytes_in:double | cluster:keyword | time_bucket:datetime\n 2.210158359293873    | prod            | 2024-05-10T00:20:00.000Z\n 0.8955555555555565   | qa              | 2024-05-10T00:20:00.000Z\n 0.595                | staging         | 2024-05-10T00:20:00.000Z\n-;\n-\n-rate_with_inline_filtering\n-required_capability: metrics_command\n-TS k8s \n-| STATS rate_bytes_in = sum(rate(network.total_bytes_in)) WHERE pod == \"one\" BY cluster, time_bucket = bucket(@timestamp, 10minute)\n-| SORT time_bucket, cluster | LIMIT 10;\n \n-rate_bytes_in:double | cluster:keyword | time_bucket:datetime\n-4.0314581958195825   | prod            | 2024-05-10T00:00:00.000Z\n-9.955833333333333    | qa              | 2024-05-10T00:00:00.000Z\n-4.242445473251029    | staging         | 2024-05-10T00:00:00.000Z\n-11.188380281690138   | prod            | 2024-05-10T00:10:00.000Z\n-12.222592592592592   | qa              | 2024-05-10T00:10:00.000Z\n-3.050371490280777    | staging         | 2024-05-10T00:10:00.000Z\n-2.210158359293873    | prod            | 2024-05-10T00:20:00.000Z\n-0.8955555555555565   | qa              | 2024-05-10T00:20:00.000Z\n-0.595                | staging         | 2024-05-10T00:20:00.000Z\n ;\n \n eval_on_rate\ndiff --git a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries.csv-spec b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries.csv-spec\nindex 10c0d100d2a..08dfe22d3f3 100644\n--- a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries.csv-spec\n+++ b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/k8s-timeseries.csv-spec\n@@ -201,24 +201,6 @@ max(rate(network.total_bytes_in)):double | time_bucket:datetime     | cluster:ke\n 11.562737642585551                       | 2024-05-10T00:10:00.000Z | prod\n ;\n \n-maxRateWithInlineFilter\n-required_capability: metrics_command\n-TS k8s | STATS max_rate = max(rate(network.total_bytes_in)) WHERE cluster==\"prod\" BY cluster | SORT cluster;\n-\n-max_rate:double         | cluster:keyword\n-8.716707021791768       | prod\n-null                    | qa\n-null                    | staging\n-;\n-\n-maxRateWithPreFilter\n-required_capability: metrics_command\n-TS k8s | WHERE cluster==\"prod\" | STATS max_rate = max(rate(network.total_bytes_in)) BY cluster | SORT cluster;\n-\n-max_rate:double         | cluster:keyword\n-8.716707021791768       | prod\n-;\n-\n notEnoughSamples\n required_capability: metrics_command\n TS k8s | WHERE @timestamp <= \"2024-05-10T00:06:14.000Z\" | STATS max(rate(network.total_bytes_in)) BY pod, time_bucket = bucket(@timestamp,1minute) | SORT pod, time_bucket DESC | LIMIT 10;\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/expression/function/aggregate/AggregateFunction.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/expression/function/aggregate/AggregateFunction.java\nindex 225895e15a3..d59bf721ccf 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/expression/function/aggregate/AggregateFunction.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/expression/function/aggregate/AggregateFunction.java\n@@ -11,9 +11,7 @@ import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n import org.elasticsearch.xpack.esql.capabilities.PostAnalysisPlanVerificationAware;\n import org.elasticsearch.xpack.esql.common.Failures;\n-import org.elasticsearch.xpack.esql.core.expression.AttributeSet;\n import org.elasticsearch.xpack.esql.core.expression.Expression;\n-import org.elasticsearch.xpack.esql.core.expression.Expressions;\n import org.elasticsearch.xpack.esql.core.expression.Literal;\n import org.elasticsearch.xpack.esql.core.expression.TypeResolutions;\n import org.elasticsearch.xpack.esql.core.expression.function.Function;\n@@ -154,17 +152,6 @@ public abstract class AggregateFunction extends Function implements PostAnalysis\n         return (AggregateFunction) replaceChildren(CollectionUtils.combine(asList(field, filter), parameters));\n     }\n \n-    /**\n-     * Returns the set of input attributes required by this aggregate function, excluding those referenced by the filter.\n-     */\n-    public AttributeSet aggregateInputReferences() {\n-        if (hasFilter()) {\n-            return Expressions.references(CollectionUtils.combine(List.of(field), parameters));\n-        } else {\n-            return references();\n-        }\n-    }\n-\n     @Override\n     public int hashCode() {\n         // NB: the hashcode is currently used for key generation so\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/optimizer/rules/logical/TranslateTimeSeriesAggregate.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/optimizer/rules/logical/TranslateTimeSeriesAggregate.java\nindex f1726a40d91..72d31c24f48 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/optimizer/rules/logical/TranslateTimeSeriesAggregate.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/optimizer/rules/logical/TranslateTimeSeriesAggregate.java\n@@ -13,7 +13,6 @@ import org.elasticsearch.xpack.esql.core.expression.Alias;\n import org.elasticsearch.xpack.esql.core.expression.Attribute;\n import org.elasticsearch.xpack.esql.core.expression.Expression;\n import org.elasticsearch.xpack.esql.core.expression.Expressions;\n-import org.elasticsearch.xpack.esql.core.expression.Literal;\n import org.elasticsearch.xpack.esql.core.expression.MetadataAttribute;\n import org.elasticsearch.xpack.esql.core.expression.NamedExpression;\n import org.elasticsearch.xpack.esql.core.type.DataType;\n@@ -185,21 +184,7 @@ public final class TranslateTimeSeriesAggregate extends OptimizerRules.Optimizer\n         for (NamedExpression agg : aggregate.aggregates()) {\n             if (agg instanceof Alias alias && alias.child() instanceof AggregateFunction af) {\n                 Holder<Boolean> changed = new Holder<>(Boolean.FALSE);\n-                final Expression inlineFilter;\n-                if (af.hasFilter()) {\n-                    inlineFilter = af.filter();\n-                    af = af.withFilter(Literal.TRUE);\n-                } else {\n-                    inlineFilter = null;\n-                }\n                 Expression outerAgg = af.transformDown(TimeSeriesAggregateFunction.class, tsAgg -> {\n-                    if (inlineFilter != null) {\n-                        if (tsAgg.hasFilter() == false) {\n-                            throw new IllegalStateException(\"inline filter isn't propagated to time-series aggregation\");\n-                        }\n-                    } else if (tsAgg.hasFilter()) {\n-                        throw new IllegalStateException(\"unexpected inline filter in time-series aggregation\");\n-                    }\n                     changed.set(Boolean.TRUE);\n                     if (tsAgg instanceof Rate) {\n                         hasRateAggregates.set(Boolean.TRUE);\n@@ -216,28 +201,14 @@ public final class TranslateTimeSeriesAggregate extends OptimizerRules.Optimizer\n                     secondPassAggs.add(new Alias(alias.source(), alias.name(), outerAgg, agg.id()));\n                 } else {\n                     // TODO: reject over_time_aggregation only\n-                    final Expression aggField = af.field();\n-                    var tsAgg = new LastOverTime(af.source(), aggField, timestamp.get());\n-                    final AggregateFunction firstStageFn;\n-                    if (inlineFilter != null) {\n-                        firstStageFn = tsAgg.perTimeSeriesAggregation().withFilter(inlineFilter);\n-                    } else {\n-                        firstStageFn = tsAgg.perTimeSeriesAggregation();\n-                    }\n+                    var tsAgg = new LastOverTime(af.source(), af.field(), timestamp.get());\n+                    AggregateFunction firstStageFn = tsAgg.perTimeSeriesAggregation();\n                     Alias newAgg = timeSeriesAggs.computeIfAbsent(firstStageFn, k -> {\n                         Alias firstStageAlias = new Alias(tsAgg.source(), internalNames.next(tsAgg.functionName()), firstStageFn);\n                         firstPassAggs.add(firstStageAlias);\n                         return firstStageAlias;\n                     });\n-                    secondPassAggs.add((Alias) agg.transformUp(f -> f == aggField || f instanceof AggregateFunction, e -> {\n-                        if (e == aggField) {\n-                            return newAgg.toAttribute();\n-                        } else if (e instanceof AggregateFunction f) {\n-                            return f.withFilter(Literal.TRUE);\n-                        } else {\n-                            return e;\n-                        }\n-                    }));\n+                    secondPassAggs.add((Alias) agg.transformUp(f -> f == af.field(), f -> newAgg.toAttribute()));\n                 }\n             }\n         }\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java\nindex e3e1315f98a..e45fe2b0e81 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/AbstractPhysicalOperationProviders.java\n@@ -279,7 +279,7 @@ public abstract class AbstractPhysicalOperationProviders implements PhysicalOper\n                             }\n                         } else {\n                             // extra dependencies like TS ones (that require a timestamp)\n-                            for (Expression input : aggregateFunction.aggregateInputReferences()) {\n+                            for (Expression input : aggregateFunction.references()) {\n                                 Attribute attr = Expressions.attribute(input);\n                                 if (attr == null) {\n                                     throw new EsqlIllegalArgumentException(\ndiff --git a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/optimizer/LogicalPlanOptimizerTests.java b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/optimizer/LogicalPlanOptimizerTests.java\nindex 1a181fe805e..6aad2f0f99c 100644\n--- a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/optimizer/LogicalPlanOptimizerTests.java\n+++ b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/optimizer/LogicalPlanOptimizerTests.java\n@@ -7751,70 +7751,6 @@ public class LogicalPlanOptimizerTests extends AbstractLogicalPlanOptimizerTests\n         assertThat(Expressions.attribute(bucket.field()).name(), equalTo(\"@timestamp\"));\n     }\n \n-    public void testTranslateWithInlineFilter() {\n-        assumeTrue(\"requires metrics command\", EsqlCapabilities.Cap.METRICS_COMMAND.isEnabled());\n-        var query = \"\"\"\n-            TS k8s | STATS sum(last_over_time(network.bytes_in)) WHERE cluster == \"prod\" BY bucket(@timestamp, 1 minute)\n-            | LIMIT 10\n-            \"\"\";\n-        var plan = logicalOptimizer.optimize(metricsAnalyzer.analyze(parser.createStatement(query, EsqlTestUtils.TEST_CFG)));\n-        var limit = as(plan, Limit.class);\n-        Aggregate finalAgg = as(limit.child(), Aggregate.class);\n-        assertThat(finalAgg, not(instanceOf(TimeSeriesAggregate.class)));\n-        TimeSeriesAggregate aggsByTsid = as(finalAgg.child(), TimeSeriesAggregate.class);\n-        assertNotNull(aggsByTsid.timeBucket());\n-        assertThat(aggsByTsid.timeBucket().buckets().fold(FoldContext.small()), equalTo(Duration.ofMinutes(1)));\n-        Eval evalBucket = as(aggsByTsid.child(), Eval.class);\n-        assertThat(evalBucket.fields(), hasSize(1));\n-        EsRelation relation = as(evalBucket.child(), EsRelation.class);\n-        assertThat(relation.indexMode(), equalTo(IndexMode.STANDARD));\n-\n-        var sum = as(Alias.unwrap(finalAgg.aggregates().get(0)), Sum.class);\n-        assertFalse(sum.hasFilter());\n-\n-        LastOverTime lastOverTime = as(Alias.unwrap(aggsByTsid.aggregates().get(0)), LastOverTime.class);\n-        assertThat(Expressions.attribute(lastOverTime.field()).name(), equalTo(\"network.bytes_in\"));\n-        assertThat(Expressions.attribute(aggsByTsid.groupings().get(1)).id(), equalTo(evalBucket.fields().get(0).id()));\n-        Bucket bucket = as(Alias.unwrap(evalBucket.fields().get(0)), Bucket.class);\n-        assertThat(Expressions.attribute(bucket.field()).name(), equalTo(\"@timestamp\"));\n-        assertTrue(lastOverTime.hasFilter());\n-        assertThat(lastOverTime.filter(), instanceOf(Equals.class));\n-    }\n-\n-    public void testTranslateWithInlineFilterWithImplicitLastOverTime() {\n-        assumeTrue(\"requires metrics command\", EsqlCapabilities.Cap.METRICS_COMMAND.isEnabled());\n-        var query = \"\"\"\n-            TS k8s | STATS avg(network.bytes_in) WHERE cluster == \"prod\" BY bucket(@timestamp, 1 minute)\n-            | LIMIT 10\n-            \"\"\";\n-        var plan = logicalOptimizer.optimize(metricsAnalyzer.analyze(parser.createStatement(query, EsqlTestUtils.TEST_CFG)));\n-        var project = as(plan, Project.class);\n-        var eval = as(project.child(), Eval.class);\n-        var limit = as(eval.child(), Limit.class);\n-        Aggregate finalAgg = as(limit.child(), Aggregate.class);\n-        assertThat(finalAgg, not(instanceOf(TimeSeriesAggregate.class)));\n-        TimeSeriesAggregate aggsByTsid = as(finalAgg.child(), TimeSeriesAggregate.class);\n-        assertNotNull(aggsByTsid.timeBucket());\n-        assertThat(aggsByTsid.timeBucket().buckets().fold(FoldContext.small()), equalTo(Duration.ofMinutes(1)));\n-        Eval evalBucket = as(aggsByTsid.child(), Eval.class);\n-        assertThat(evalBucket.fields(), hasSize(1));\n-        EsRelation relation = as(evalBucket.child(), EsRelation.class);\n-        assertThat(relation.indexMode(), equalTo(IndexMode.STANDARD));\n-\n-        var sum = as(Alias.unwrap(finalAgg.aggregates().get(0)), Sum.class);\n-        assertFalse(sum.hasFilter());\n-        var count = as(Alias.unwrap(finalAgg.aggregates().get(1)), Count.class);\n-        assertFalse(count.hasFilter());\n-\n-        LastOverTime lastOverTime = as(Alias.unwrap(aggsByTsid.aggregates().get(0)), LastOverTime.class);\n-        assertThat(Expressions.attribute(lastOverTime.field()).name(), equalTo(\"network.bytes_in\"));\n-        assertThat(Expressions.attribute(aggsByTsid.groupings().get(1)).id(), equalTo(evalBucket.fields().get(0).id()));\n-        Bucket bucket = as(Alias.unwrap(evalBucket.fields().get(0)), Bucket.class);\n-        assertThat(Expressions.attribute(bucket.field()).name(), equalTo(\"@timestamp\"));\n-        assertTrue(lastOverTime.hasFilter());\n-        assertThat(lastOverTime.filter(), instanceOf(Equals.class));\n-    }\n-\n     public void testMvSortInvalidOrder() {\n         VerificationException e = expectThrows(VerificationException.class, () -> plan(\"\"\"\n             from test\ndiff --git a/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/IndexLifecycle.java b/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/IndexLifecycle.java\nindex 82ccefaeb4a..14958dd971f 100644\n--- a/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/IndexLifecycle.java\n+++ b/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/IndexLifecycle.java\n@@ -141,16 +141,6 @@ public class IndexLifecycle extends Plugin implements ActionPlugin, HealthPlugin\n     @Override\n     public Collection<?> createComponents(PluginServices services) {\n         final List<Object> components = new ArrayList<>();\n-        PutLifecycleMetadataService putLifecycleMetadataService = new PutLifecycleMetadataService(\n-            services.clusterService(),\n-            services.xContentRegistry(),\n-            services.client(),\n-            getLicenseState(),\n-            services.threadPool(),\n-            services.projectResolver()\n-        );\n-        components.add(putLifecycleMetadataService);\n-\n         ILMHistoryTemplateRegistry ilmTemplateRegistry = new ILMHistoryTemplateRegistry(\n             settings,\n             services.clusterService(),\ndiff --git a/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/PutLifecycleMetadataService.java b/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/PutLifecycleMetadataService.java\ndeleted file mode 100644\nindex d1ce5dbd296..00000000000\n--- a/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/PutLifecycleMetadataService.java\n+++ /dev/null\n@@ -1,324 +0,0 @@\n-/*\n- * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n- * or more contributor license agreements. Licensed under the Elastic License\n- * 2.0; you may not use this file except in compliance with the Elastic License\n- * 2.0.\n- */\n-\n-package org.elasticsearch.xpack.ilm;\n-\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.elasticsearch.action.ActionListener;\n-import org.elasticsearch.action.support.master.AcknowledgedResponse;\n-import org.elasticsearch.client.internal.Client;\n-import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n-import org.elasticsearch.cluster.ClusterState;\n-import org.elasticsearch.cluster.ClusterStateAckListener;\n-import org.elasticsearch.cluster.ClusterStateUpdateTask;\n-import org.elasticsearch.cluster.ProjectState;\n-import org.elasticsearch.cluster.SimpleBatchedAckListenerTaskExecutor;\n-import org.elasticsearch.cluster.metadata.ProjectId;\n-import org.elasticsearch.cluster.metadata.ProjectMetadata;\n-import org.elasticsearch.cluster.metadata.RepositoriesMetadata;\n-import org.elasticsearch.cluster.project.ProjectResolver;\n-import org.elasticsearch.cluster.service.ClusterService;\n-import org.elasticsearch.cluster.service.MasterServiceTaskQueue;\n-import org.elasticsearch.common.Priority;\n-import org.elasticsearch.core.Nullable;\n-import org.elasticsearch.core.SuppressForbidden;\n-import org.elasticsearch.core.Tuple;\n-import org.elasticsearch.license.XPackLicenseState;\n-import org.elasticsearch.reservedstate.ReservedClusterStateHandler;\n-import org.elasticsearch.threadpool.ThreadPool;\n-import org.elasticsearch.xcontent.NamedXContentRegistry;\n-import org.elasticsearch.xpack.core.ClientHelper;\n-import org.elasticsearch.xpack.core.ilm.IndexLifecycleMetadata;\n-import org.elasticsearch.xpack.core.ilm.LifecyclePolicy;\n-import org.elasticsearch.xpack.core.ilm.LifecyclePolicyMetadata;\n-import org.elasticsearch.xpack.core.ilm.Phase;\n-import org.elasticsearch.xpack.core.ilm.SearchableSnapshotAction;\n-import org.elasticsearch.xpack.core.ilm.WaitForSnapshotAction;\n-import org.elasticsearch.xpack.core.ilm.action.PutLifecycleRequest;\n-import org.elasticsearch.xpack.core.slm.SnapshotLifecycleMetadata;\n-import org.elasticsearch.xpack.ilm.action.ReservedLifecycleAction;\n-\n-import java.time.Instant;\n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedMap;\n-import java.util.TreeMap;\n-\n-import static org.elasticsearch.xpack.core.ilm.LifecycleOperationMetadata.currentILMMode;\n-import static org.elasticsearch.xpack.core.ilm.PhaseCacheManagement.updateIndicesForPolicy;\n-import static org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOT_FEATURE;\n-\n-public class PutLifecycleMetadataService {\n-\n-    private static final Logger logger = LogManager.getLogger(PutLifecycleMetadataService.class);\n-\n-    private final ClusterService clusterService;\n-    private final NamedXContentRegistry xContentRegistry;\n-    private final Client client;\n-    private final XPackLicenseState licenseState;\n-    private final ThreadPool threadPool;\n-    private final ProjectResolver projectResolver;\n-    private final MasterServiceTaskQueue<UpdateLifecyclePolicyTask> taskQueue;\n-\n-    public PutLifecycleMetadataService(\n-        ClusterService clusterService,\n-        NamedXContentRegistry xContentRegistry,\n-        Client client,\n-        XPackLicenseState licenseState,\n-        ThreadPool threadPool,\n-        ProjectResolver projectResolver\n-    ) {\n-        this.clusterService = clusterService;\n-        this.xContentRegistry = xContentRegistry;\n-        this.client = client;\n-        this.licenseState = licenseState;\n-        this.threadPool = threadPool;\n-        this.projectResolver = projectResolver;\n-        this.taskQueue = clusterService.createTaskQueue(\"ilm-put-lifecycle-queue\", Priority.NORMAL, new IlmLifecycleExecutor());\n-    }\n-\n-    @SuppressForbidden(reason = \"legacy usage of unbatched task\") // TODO add support for batching here\n-    private void submitUnbatchedTask(@SuppressWarnings(\"SameParameterValue\") String source, ClusterStateUpdateTask task) {\n-        clusterService.submitUnbatchedStateUpdateTask(source, task);\n-    }\n-\n-    public void addLifecycle(PutLifecycleRequest request, ClusterState state, ActionListener<AcknowledgedResponse> listener) {\n-        // headers from the thread context stored by the AuthenticationService to be shared between the\n-        // REST layer and the Transport layer here must be accessed within this thread and not in the\n-        // cluster state thread in the ClusterStateUpdateTask below since that thread does not share the\n-        // same context, and therefore does not have access to the appropriate security headers.\n-        Map<String, String> filteredHeaders = ClientHelper.getPersistableSafeSecurityHeaders(threadPool.getThreadContext(), state);\n-\n-        LifecyclePolicy.validatePolicyName(request.getPolicy().getName());\n-        request.getPolicy().maybeAddDeprecationWarningForFreezeAction(request.getPolicy().getName());\n-        ProjectMetadata projectMetadata = projectResolver.getProjectMetadata(state);\n-        {\n-            IndexLifecycleMetadata lifecycleMetadata = projectMetadata.custom(IndexLifecycleMetadata.TYPE, IndexLifecycleMetadata.EMPTY);\n-            LifecyclePolicyMetadata existingPolicy = lifecycleMetadata.getPolicyMetadatas().get(request.getPolicy().getName());\n-            // Make the request a no-op if the policy and filtered headers match exactly\n-            if (isNoopUpdate(existingPolicy, request.getPolicy(), filteredHeaders)) {\n-                listener.onResponse(AcknowledgedResponse.TRUE);\n-                return;\n-            }\n-        }\n-\n-        UpdateLifecyclePolicyTask putTask = new UpdateLifecyclePolicyTask(\n-            projectMetadata.id(),\n-            request,\n-            listener,\n-            licenseState,\n-            filteredHeaders,\n-            xContentRegistry,\n-            client\n-        );\n-        taskQueue.submitTask(\"put-lifecycle-\" + request.getPolicy().getName(), putTask, putTask.timeout());\n-    }\n-\n-    /**\n-     * Returns 'true' if the ILM policy is effectually the same (same policy and headers), and thus can be a no-op update.\n-     */\n-    public static boolean isNoopUpdate(\n-        @Nullable LifecyclePolicyMetadata existingPolicy,\n-        LifecyclePolicy newPolicy,\n-        Map<String, String> filteredHeaders\n-    ) {\n-        if (existingPolicy == null) {\n-            return false;\n-        } else {\n-            return newPolicy.equals(existingPolicy.getPolicy()) && filteredHeaders.equals(existingPolicy.getHeaders());\n-        }\n-    }\n-\n-    /**\n-     * Validate that the license level is compliant for searchable-snapshots, that any referenced snapshot\n-     * repositories exist, and that any referenced SLM policies exist.\n-     *\n-     * @param policy The lifecycle policy\n-     * @param state  The cluster state\n-     */\n-    private static void validatePrerequisites(LifecyclePolicy policy, ProjectState state, XPackLicenseState licenseState) {\n-        List<Phase> phasesWithSearchableSnapshotActions = policy.getPhases()\n-            .values()\n-            .stream()\n-            .filter(phase -> phase.getActions().containsKey(SearchableSnapshotAction.NAME))\n-            .toList();\n-        // check license level for searchable snapshots\n-        if (phasesWithSearchableSnapshotActions.isEmpty() == false\n-            && SEARCHABLE_SNAPSHOT_FEATURE.checkWithoutTracking(licenseState) == false) {\n-            throw new IllegalArgumentException(\n-                \"policy [\"\n-                    + policy.getName()\n-                    + \"] defines the [\"\n-                    + SearchableSnapshotAction.NAME\n-                    + \"] action but the current license is non-compliant for [searchable-snapshots]\"\n-            );\n-        }\n-        // make sure any referenced snapshot repositories exist\n-        for (Phase phase : phasesWithSearchableSnapshotActions) {\n-            SearchableSnapshotAction action = (SearchableSnapshotAction) phase.getActions().get(SearchableSnapshotAction.NAME);\n-            String repository = action.getSnapshotRepository();\n-            if (RepositoriesMetadata.get(state.cluster()).repository(repository) == null) {\n-                throw new IllegalArgumentException(\n-                    \"no such repository [\"\n-                        + repository\n-                        + \"], the snapshot repository \"\n-                        + \"referenced by the [\"\n-                        + SearchableSnapshotAction.NAME\n-                        + \"] action in the [\"\n-                        + phase.getName()\n-                        + \"] phase \"\n-                        + \"must exist before it can be referenced by an ILM policy\"\n-                );\n-            }\n-        }\n-\n-        List<Phase> phasesWithWaitForSnapshotActions = policy.getPhases()\n-            .values()\n-            .stream()\n-            .filter(phase -> phase.getActions().containsKey(WaitForSnapshotAction.NAME))\n-            .toList();\n-        // make sure any referenced snapshot lifecycle policies exist\n-        for (Phase phase : phasesWithWaitForSnapshotActions) {\n-            WaitForSnapshotAction action = (WaitForSnapshotAction) phase.getActions().get(WaitForSnapshotAction.NAME);\n-            String slmPolicy = action.getPolicy();\n-            if (state.metadata()\n-                .custom(SnapshotLifecycleMetadata.TYPE, SnapshotLifecycleMetadata.EMPTY)\n-                .getSnapshotConfigurations()\n-                .get(slmPolicy) == null) {\n-                throw new IllegalArgumentException(\n-                    \"no such snapshot lifecycle policy [\"\n-                        + slmPolicy\n-                        + \"], the snapshot lifecycle policy \"\n-                        + \"referenced by the [\"\n-                        + WaitForSnapshotAction.NAME\n-                        + \"] action in the [\"\n-                        + phase.getName()\n-                        + \"] phase \"\n-                        + \"must exist before it can be referenced by an ILM policy\"\n-                );\n-            }\n-        }\n-    }\n-\n-    public static class UpdateLifecyclePolicyTask extends AckedClusterStateUpdateTask {\n-        private final ProjectId projectId;\n-        private final PutLifecycleRequest request;\n-        private final XPackLicenseState licenseState;\n-        private final Map<String, String> filteredHeaders;\n-        private final NamedXContentRegistry xContentRegistry;\n-        private final Client client;\n-        private final boolean verboseLogging;\n-\n-        public UpdateLifecyclePolicyTask(\n-            ProjectId projectId,\n-            PutLifecycleRequest request,\n-            ActionListener<AcknowledgedResponse> listener,\n-            XPackLicenseState licenseState,\n-            Map<String, String> filteredHeaders,\n-            NamedXContentRegistry xContentRegistry,\n-            Client client\n-        ) {\n-            super(request, listener);\n-            this.projectId = projectId;\n-            this.request = request;\n-            this.licenseState = licenseState;\n-            this.filteredHeaders = filteredHeaders;\n-            this.xContentRegistry = xContentRegistry;\n-            this.client = client;\n-            this.verboseLogging = true;\n-        }\n-\n-        /**\n-         * Used by the {@link ReservedClusterStateHandler} for ILM\n-         * {@link ReservedLifecycleAction}\n-         * <p>\n-         * It disables verbose logging and has no filtered headers.\n-         */\n-        public UpdateLifecyclePolicyTask(\n-            ProjectId projectId,\n-            PutLifecycleRequest request,\n-            XPackLicenseState licenseState,\n-            NamedXContentRegistry xContentRegistry,\n-            Client client\n-        ) {\n-            super(request, null);\n-            this.projectId = projectId;\n-            this.request = request;\n-            this.licenseState = licenseState;\n-            this.filteredHeaders = Collections.emptyMap();\n-            this.xContentRegistry = xContentRegistry;\n-            this.client = client;\n-            this.verboseLogging = false;\n-        }\n-\n-        @Override\n-        public ClusterState execute(ClusterState currentState) throws Exception {\n-            var projectState = currentState.projectState(projectId);\n-            final IndexLifecycleMetadata currentMetadata = projectState.metadata()\n-                .custom(IndexLifecycleMetadata.TYPE, IndexLifecycleMetadata.EMPTY);\n-            final LifecyclePolicyMetadata existingPolicyMetadata = currentMetadata.getPolicyMetadatas().get(request.getPolicy().getName());\n-\n-            // Double-check for no-op in the state update task, in case it was changed/reset in the meantime\n-            if (isNoopUpdate(existingPolicyMetadata, request.getPolicy(), filteredHeaders)) {\n-                return currentState;\n-            }\n-\n-            validatePrerequisites(request.getPolicy(), projectState, licenseState);\n-\n-            long nextVersion = (existingPolicyMetadata == null) ? 1L : existingPolicyMetadata.getVersion() + 1L;\n-            SortedMap<String, LifecyclePolicyMetadata> newPolicies = new TreeMap<>(currentMetadata.getPolicyMetadatas());\n-            LifecyclePolicyMetadata lifecyclePolicyMetadata = new LifecyclePolicyMetadata(\n-                request.getPolicy(),\n-                filteredHeaders,\n-                nextVersion,\n-                Instant.now().toEpochMilli()\n-            );\n-            LifecyclePolicyMetadata oldPolicy = newPolicies.put(lifecyclePolicyMetadata.getName(), lifecyclePolicyMetadata);\n-            if (verboseLogging) {\n-                if (oldPolicy == null) {\n-                    logger.info(\"adding index lifecycle policy [{}]\", request.getPolicy().getName());\n-                } else {\n-                    logger.info(\"updating index lifecycle policy [{}]\", request.getPolicy().getName());\n-                }\n-            }\n-            IndexLifecycleMetadata newMetadata = new IndexLifecycleMetadata(newPolicies, currentILMMode(projectState.metadata()));\n-            ProjectMetadata newProjectMetadata = ProjectMetadata.builder(projectState.metadata())\n-                .putCustom(IndexLifecycleMetadata.TYPE, newMetadata)\n-                .build();\n-            ClusterState nonRefreshedState = ClusterState.builder(currentState).putProjectMetadata(newProjectMetadata).build();\n-            if (oldPolicy == null) {\n-                return nonRefreshedState;\n-            } else {\n-                try {\n-                    ProjectMetadata refreshedProjectMetadata = updateIndicesForPolicy(\n-                        newProjectMetadata,\n-                        xContentRegistry,\n-                        client,\n-                        oldPolicy.getPolicy(),\n-                        lifecyclePolicyMetadata,\n-                        licenseState\n-                    );\n-                    return ClusterState.builder(currentState).putProjectMetadata(refreshedProjectMetadata).build();\n-                } catch (Exception e) {\n-                    logger.warn(() -> \"unable to refresh indices phase JSON for updated policy [\" + oldPolicy.getName() + \"]\", e);\n-                    // Revert to the non-refreshed state\n-                    return nonRefreshedState;\n-                }\n-            }\n-        }\n-    }\n-\n-    private static class IlmLifecycleExecutor extends SimpleBatchedAckListenerTaskExecutor<UpdateLifecyclePolicyTask> {\n-        @Override\n-        public Tuple<ClusterState, ClusterStateAckListener> executeTask(UpdateLifecyclePolicyTask task, ClusterState clusterState)\n-            throws Exception {\n-            return Tuple.tuple(task.execute(clusterState), task);\n-        }\n-    }\n-}\ndiff --git a/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/ReservedLifecycleAction.java b/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/ReservedLifecycleAction.java\nindex 1435bc70cf8..180e0b2a975 100644\n--- a/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/ReservedLifecycleAction.java\n+++ b/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/ReservedLifecycleAction.java\n@@ -22,7 +22,6 @@ import org.elasticsearch.xcontent.XContentParserConfiguration;\n import org.elasticsearch.xpack.core.ilm.LifecyclePolicy;\n import org.elasticsearch.xpack.core.ilm.action.DeleteLifecycleAction;\n import org.elasticsearch.xpack.core.ilm.action.PutLifecycleRequest;\n-import org.elasticsearch.xpack.ilm.PutLifecycleMetadataService;\n \n import java.io.IOException;\n import java.util.ArrayList;\n@@ -87,7 +86,7 @@ public class ReservedLifecycleAction implements ReservedProjectStateHandler<List\n         ClusterState state = prevState.state();\n \n         for (var request : requests) {\n-            PutLifecycleMetadataService.UpdateLifecyclePolicyTask task = new PutLifecycleMetadataService.UpdateLifecyclePolicyTask(\n+            TransportPutLifecycleAction.UpdateLifecyclePolicyTask task = new TransportPutLifecycleAction.UpdateLifecyclePolicyTask(\n                 state.metadata().getProject(projectId).id(),\n                 request,\n                 licenseState,\ndiff --git a/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/TransportPutLifecycleAction.java b/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/TransportPutLifecycleAction.java\nindex 2aec6fdfc79..2af462e9a83 100644\n--- a/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/TransportPutLifecycleAction.java\n+++ b/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/ilm/action/TransportPutLifecycleAction.java\n@@ -7,26 +7,60 @@\n \n package org.elasticsearch.xpack.ilm.action;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.support.ActionFilters;\n import org.elasticsearch.action.support.master.AcknowledgedResponse;\n import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.internal.Client;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateAckListener;\n+import org.elasticsearch.cluster.ProjectState;\n+import org.elasticsearch.cluster.SimpleBatchedAckListenerTaskExecutor;\n import org.elasticsearch.cluster.block.ClusterBlockException;\n import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.ProjectId;\n+import org.elasticsearch.cluster.metadata.ProjectMetadata;\n+import org.elasticsearch.cluster.metadata.RepositoriesMetadata;\n+import org.elasticsearch.cluster.project.ProjectResolver;\n import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.cluster.service.MasterServiceTaskQueue;\n+import org.elasticsearch.common.Priority;\n import org.elasticsearch.common.util.concurrent.EsExecutors;\n+import org.elasticsearch.core.Nullable;\n+import org.elasticsearch.core.Tuple;\n import org.elasticsearch.injection.guice.Inject;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.reservedstate.ReservedClusterStateHandler;\n import org.elasticsearch.tasks.Task;\n import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.xpack.core.ClientHelper;\n import org.elasticsearch.xpack.core.ilm.IndexLifecycleMetadata;\n+import org.elasticsearch.xpack.core.ilm.LifecyclePolicy;\n+import org.elasticsearch.xpack.core.ilm.LifecyclePolicyMetadata;\n+import org.elasticsearch.xpack.core.ilm.Phase;\n+import org.elasticsearch.xpack.core.ilm.SearchableSnapshotAction;\n+import org.elasticsearch.xpack.core.ilm.WaitForSnapshotAction;\n import org.elasticsearch.xpack.core.ilm.action.ILMActions;\n import org.elasticsearch.xpack.core.ilm.action.PutLifecycleRequest;\n-import org.elasticsearch.xpack.ilm.PutLifecycleMetadataService;\n+import org.elasticsearch.xpack.core.slm.SnapshotLifecycleMetadata;\n \n+import java.time.Instant;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+import static org.elasticsearch.xpack.core.ilm.LifecycleOperationMetadata.currentILMMode;\n+import static org.elasticsearch.xpack.core.ilm.PhaseCacheManagement.updateIndicesForPolicy;\n+import static org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotsConstants.SEARCHABLE_SNAPSHOT_FEATURE;\n \n /**\n  * This class is responsible for bootstrapping {@link IndexLifecycleMetadata} into the cluster-state, as well\n@@ -34,7 +68,12 @@ import java.util.Set;\n  */\n public class TransportPutLifecycleAction extends TransportMasterNodeAction<PutLifecycleRequest, AcknowledgedResponse> {\n \n-    private final PutLifecycleMetadataService putLifecycleMetadataService;\n+    private static final Logger logger = LogManager.getLogger(TransportPutLifecycleAction.class);\n+    private final NamedXContentRegistry xContentRegistry;\n+    private final Client client;\n+    private final XPackLicenseState licenseState;\n+    private final ProjectResolver projectResolver;\n+    private final MasterServiceTaskQueue<UpdateLifecyclePolicyTask> taskQueue;\n \n     @Inject\n     public TransportPutLifecycleAction(\n@@ -42,7 +81,10 @@ public class TransportPutLifecycleAction extends TransportMasterNodeAction<PutLi\n         ClusterService clusterService,\n         ThreadPool threadPool,\n         ActionFilters actionFilters,\n-        PutLifecycleMetadataService putLifecycleMetadataService\n+        NamedXContentRegistry namedXContentRegistry,\n+        XPackLicenseState licenseState,\n+        Client client,\n+        ProjectResolver projectResolver\n     ) {\n         super(\n             ILMActions.PUT.name(),\n@@ -54,7 +96,11 @@ public class TransportPutLifecycleAction extends TransportMasterNodeAction<PutLi\n             AcknowledgedResponse::readFrom,\n             EsExecutors.DIRECT_EXECUTOR_SERVICE\n         );\n-        this.putLifecycleMetadataService = putLifecycleMetadataService;\n+        this.xContentRegistry = namedXContentRegistry;\n+        this.licenseState = licenseState;\n+        this.client = client;\n+        this.projectResolver = projectResolver;\n+        this.taskQueue = clusterService.createTaskQueue(\"ilm-put-lifecycle-queue\", Priority.NORMAL, new IlmLifecycleExecutor());\n     }\n \n     @Override\n@@ -64,7 +110,228 @@ public class TransportPutLifecycleAction extends TransportMasterNodeAction<PutLi\n         ClusterState state,\n         ActionListener<AcknowledgedResponse> listener\n     ) {\n-        putLifecycleMetadataService.addLifecycle(request, state, listener);\n+        // headers from the thread context stored by the AuthenticationService to be shared between the\n+        // REST layer and the Transport layer here must be accessed within this thread and not in the\n+        // cluster state thread in the ClusterStateUpdateTask below since that thread does not share the\n+        // same context, and therefore does not have access to the appropriate security headers.\n+        Map<String, String> filteredHeaders = ClientHelper.getPersistableSafeSecurityHeaders(threadPool.getThreadContext(), state);\n+\n+        LifecyclePolicy.validatePolicyName(request.getPolicy().getName());\n+        request.getPolicy().maybeAddDeprecationWarningForFreezeAction(request.getPolicy().getName());\n+\n+        ProjectMetadata projectMetadata = projectResolver.getProjectMetadata(state);\n+        IndexLifecycleMetadata lifecycleMetadata = projectMetadata.custom(IndexLifecycleMetadata.TYPE, IndexLifecycleMetadata.EMPTY);\n+        LifecyclePolicyMetadata existingPolicy = lifecycleMetadata.getPolicyMetadatas().get(request.getPolicy().getName());\n+        // Make the request a no-op if the policy and filtered headers match exactly\n+        if (isNoopUpdate(existingPolicy, request.getPolicy(), filteredHeaders)) {\n+            listener.onResponse(AcknowledgedResponse.TRUE);\n+            return;\n+        }\n+\n+        UpdateLifecyclePolicyTask putTask = new UpdateLifecyclePolicyTask(\n+            projectMetadata.id(),\n+            request,\n+            listener,\n+            licenseState,\n+            filteredHeaders,\n+            xContentRegistry,\n+            client\n+        );\n+        taskQueue.submitTask(\"put-lifecycle-\" + request.getPolicy().getName(), putTask, putTask.timeout());\n+    }\n+\n+    public static class UpdateLifecyclePolicyTask extends AckedClusterStateUpdateTask {\n+        private final ProjectId projectId;\n+        private final PutLifecycleRequest request;\n+        private final XPackLicenseState licenseState;\n+        private final Map<String, String> filteredHeaders;\n+        private final NamedXContentRegistry xContentRegistry;\n+        private final Client client;\n+        private final boolean verboseLogging;\n+\n+        public UpdateLifecyclePolicyTask(\n+            ProjectId projectId,\n+            PutLifecycleRequest request,\n+            ActionListener<AcknowledgedResponse> listener,\n+            XPackLicenseState licenseState,\n+            Map<String, String> filteredHeaders,\n+            NamedXContentRegistry xContentRegistry,\n+            Client client\n+        ) {\n+            super(request, listener);\n+            this.projectId = projectId;\n+            this.request = request;\n+            this.licenseState = licenseState;\n+            this.filteredHeaders = filteredHeaders;\n+            this.xContentRegistry = xContentRegistry;\n+            this.client = client;\n+            this.verboseLogging = true;\n+        }\n+\n+        /**\n+         * Used by the {@link ReservedClusterStateHandler} for ILM\n+         * {@link ReservedLifecycleAction}\n+         * <p>\n+         * It disables verbose logging and has no filtered headers.\n+         */\n+        UpdateLifecyclePolicyTask(\n+            ProjectId projectId,\n+            PutLifecycleRequest request,\n+            XPackLicenseState licenseState,\n+            NamedXContentRegistry xContentRegistry,\n+            Client client\n+        ) {\n+            super(request, null);\n+            this.projectId = projectId;\n+            this.request = request;\n+            this.licenseState = licenseState;\n+            this.filteredHeaders = Collections.emptyMap();\n+            this.xContentRegistry = xContentRegistry;\n+            this.client = client;\n+            this.verboseLogging = false;\n+        }\n+\n+        @Override\n+        public ClusterState execute(ClusterState clusterState) throws Exception {\n+            var projectState = clusterState.projectState(projectId);\n+            final IndexLifecycleMetadata currentMetadata = projectState.metadata()\n+                .custom(IndexLifecycleMetadata.TYPE, IndexLifecycleMetadata.EMPTY);\n+            final LifecyclePolicyMetadata existingPolicyMetadata = currentMetadata.getPolicyMetadatas().get(request.getPolicy().getName());\n+\n+            // Double-check for no-op in the state update task, in case it was changed/reset in the meantime\n+            if (isNoopUpdate(existingPolicyMetadata, request.getPolicy(), filteredHeaders)) {\n+                return clusterState;\n+            }\n+\n+            validatePrerequisites(request.getPolicy(), projectState, licenseState);\n+\n+            long nextVersion = (existingPolicyMetadata == null) ? 1L : existingPolicyMetadata.getVersion() + 1L;\n+            SortedMap<String, LifecyclePolicyMetadata> newPolicies = new TreeMap<>(currentMetadata.getPolicyMetadatas());\n+            LifecyclePolicyMetadata lifecyclePolicyMetadata = new LifecyclePolicyMetadata(\n+                request.getPolicy(),\n+                filteredHeaders,\n+                nextVersion,\n+                Instant.now().toEpochMilli()\n+            );\n+            LifecyclePolicyMetadata oldPolicy = newPolicies.put(lifecyclePolicyMetadata.getName(), lifecyclePolicyMetadata);\n+            if (verboseLogging) {\n+                if (oldPolicy == null) {\n+                    logger.info(\"adding index lifecycle policy [{}]\", request.getPolicy().getName());\n+                } else {\n+                    logger.info(\"updating index lifecycle policy [{}]\", request.getPolicy().getName());\n+                }\n+            }\n+            IndexLifecycleMetadata newMetadata = new IndexLifecycleMetadata(newPolicies, currentILMMode(projectState.metadata()));\n+            ProjectMetadata newProjectMetadata = ProjectMetadata.builder(projectState.metadata())\n+                .putCustom(IndexLifecycleMetadata.TYPE, newMetadata)\n+                .build();\n+            ClusterState nonRefreshedState = ClusterState.builder(clusterState).putProjectMetadata(newProjectMetadata).build();\n+            if (oldPolicy == null) {\n+                return nonRefreshedState;\n+            } else {\n+                try {\n+                    ProjectMetadata refreshedProjectMetadata = updateIndicesForPolicy(\n+                        newProjectMetadata,\n+                        xContentRegistry,\n+                        client,\n+                        oldPolicy.getPolicy(),\n+                        lifecyclePolicyMetadata,\n+                        licenseState\n+                    );\n+                    return ClusterState.builder(clusterState).putProjectMetadata(refreshedProjectMetadata).build();\n+                } catch (Exception e) {\n+                    logger.warn(() -> \"unable to refresh indices phase JSON for updated policy [\" + oldPolicy.getName() + \"]\", e);\n+                    // Revert to the non-refreshed state\n+                    return nonRefreshedState;\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Returns 'true' if the ILM policy is effectually the same (same policy and headers), and thus can be a no-op update.\n+     */\n+    static boolean isNoopUpdate(\n+        @Nullable LifecyclePolicyMetadata existingPolicy,\n+        LifecyclePolicy newPolicy,\n+        Map<String, String> filteredHeaders\n+    ) {\n+        if (existingPolicy == null) {\n+            return false;\n+        } else {\n+            return newPolicy.equals(existingPolicy.getPolicy()) && filteredHeaders.equals(existingPolicy.getHeaders());\n+        }\n+    }\n+\n+    /**\n+     * Validate that the license level is compliant for searchable-snapshots, that any referenced snapshot\n+     * repositories exist, and that any referenced SLM policies exist.\n+     *\n+     * @param policy The lifecycle policy\n+     * @param state The project state\n+     */\n+    private static void validatePrerequisites(LifecyclePolicy policy, ProjectState state, XPackLicenseState licenseState) {\n+        List<Phase> phasesWithSearchableSnapshotActions = policy.getPhases()\n+            .values()\n+            .stream()\n+            .filter(phase -> phase.getActions().containsKey(SearchableSnapshotAction.NAME))\n+            .toList();\n+        // check license level for searchable snapshots\n+        if (phasesWithSearchableSnapshotActions.isEmpty() == false\n+            && SEARCHABLE_SNAPSHOT_FEATURE.checkWithoutTracking(licenseState) == false) {\n+            throw new IllegalArgumentException(\n+                \"policy [\"\n+                    + policy.getName()\n+                    + \"] defines the [\"\n+                    + SearchableSnapshotAction.NAME\n+                    + \"] action but the current license is non-compliant for [searchable-snapshots]\"\n+            );\n+        }\n+        // make sure any referenced snapshot repositories exist\n+        for (Phase phase : phasesWithSearchableSnapshotActions) {\n+            SearchableSnapshotAction action = (SearchableSnapshotAction) phase.getActions().get(SearchableSnapshotAction.NAME);\n+            String repository = action.getSnapshotRepository();\n+            if (RepositoriesMetadata.get(state.cluster()).repository(repository) == null) {\n+                throw new IllegalArgumentException(\n+                    \"no such repository [\"\n+                        + repository\n+                        + \"], the snapshot repository \"\n+                        + \"referenced by the [\"\n+                        + SearchableSnapshotAction.NAME\n+                        + \"] action in the [\"\n+                        + phase.getName()\n+                        + \"] phase \"\n+                        + \"must exist before it can be referenced by an ILM policy\"\n+                );\n+            }\n+        }\n+\n+        List<Phase> phasesWithWaitForSnapshotActions = policy.getPhases()\n+            .values()\n+            .stream()\n+            .filter(phase -> phase.getActions().containsKey(WaitForSnapshotAction.NAME))\n+            .toList();\n+        // make sure any referenced snapshot lifecycle policies exist\n+        for (Phase phase : phasesWithWaitForSnapshotActions) {\n+            WaitForSnapshotAction action = (WaitForSnapshotAction) phase.getActions().get(WaitForSnapshotAction.NAME);\n+            String slmPolicy = action.getPolicy();\n+            if (state.metadata()\n+                .custom(SnapshotLifecycleMetadata.TYPE, SnapshotLifecycleMetadata.EMPTY)\n+                .getSnapshotConfigurations()\n+                .get(slmPolicy) == null) {\n+                throw new IllegalArgumentException(\n+                    \"no such snapshot lifecycle policy [\"\n+                        + slmPolicy\n+                        + \"], the snapshot lifecycle policy \"\n+                        + \"referenced by the [\"\n+                        + WaitForSnapshotAction.NAME\n+                        + \"] action in the [\"\n+                        + phase.getName()\n+                        + \"] phase \"\n+                        + \"must exist before it can be referenced by an ILM policy\"\n+                );\n+            }\n+        }\n     }\n \n     @Override\n@@ -81,4 +348,15 @@ public class TransportPutLifecycleAction extends TransportMasterNodeAction<PutLi\n     public Set<String> modifiedKeys(PutLifecycleRequest request) {\n         return Set.of(request.getPolicy().getName());\n     }\n+\n+    private static class IlmLifecycleExecutor extends SimpleBatchedAckListenerTaskExecutor<UpdateLifecyclePolicyTask> {\n+\n+        @Override\n+        public Tuple<ClusterState, ClusterStateAckListener> executeTask(UpdateLifecyclePolicyTask task, ClusterState clusterState)\n+            throws Exception {\n+            return Tuple.tuple(task.execute(clusterState), task);\n+        }\n+\n+    }\n+\n }\ndiff --git a/x-pack/plugin/ilm/src/test/java/org/elasticsearch/xpack/ilm/action/TransportPutLifecycleActionTests.java b/x-pack/plugin/ilm/src/test/java/org/elasticsearch/xpack/ilm/action/TransportPutLifecycleActionTests.java\nindex dffc8993144..2498846c949 100644\n--- a/x-pack/plugin/ilm/src/test/java/org/elasticsearch/xpack/ilm/action/TransportPutLifecycleActionTests.java\n+++ b/x-pack/plugin/ilm/src/test/java/org/elasticsearch/xpack/ilm/action/TransportPutLifecycleActionTests.java\n@@ -8,11 +8,15 @@\n package org.elasticsearch.xpack.ilm.action;\n \n import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.client.internal.Client;\n+import org.elasticsearch.cluster.project.ProjectResolver;\n import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.license.XPackLicenseState;\n import org.elasticsearch.test.ESTestCase;\n import org.elasticsearch.test.MockUtils;\n import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xcontent.NamedXContentRegistry;\n import org.elasticsearch.xcontent.XContentParser;\n import org.elasticsearch.xcontent.XContentParserConfiguration;\n import org.elasticsearch.xcontent.XContentType;\n@@ -20,7 +24,6 @@ import org.elasticsearch.xpack.core.ilm.LifecyclePolicy;\n import org.elasticsearch.xpack.core.ilm.LifecyclePolicyMetadata;\n import org.elasticsearch.xpack.core.ilm.action.PutLifecycleRequest;\n import org.elasticsearch.xpack.ilm.LifecyclePolicyTestsUtils;\n-import org.elasticsearch.xpack.ilm.PutLifecycleMetadataService;\n \n import java.util.Map;\n \n@@ -37,22 +40,24 @@ public class TransportPutLifecycleActionTests extends ESTestCase {\n \n         LifecyclePolicyMetadata existing = new LifecyclePolicyMetadata(policy1, headers1, randomNonNegativeLong(), randomNonNegativeLong());\n \n-        assertTrue(PutLifecycleMetadataService.isNoopUpdate(existing, policy1, headers1));\n-        assertFalse(PutLifecycleMetadataService.isNoopUpdate(existing, policy2, headers1));\n-        assertFalse(PutLifecycleMetadataService.isNoopUpdate(existing, policy1, headers2));\n-        assertFalse(PutLifecycleMetadataService.isNoopUpdate(null, policy1, headers1));\n+        assertTrue(TransportPutLifecycleAction.isNoopUpdate(existing, policy1, headers1));\n+        assertFalse(TransportPutLifecycleAction.isNoopUpdate(existing, policy2, headers1));\n+        assertFalse(TransportPutLifecycleAction.isNoopUpdate(existing, policy1, headers2));\n+        assertFalse(TransportPutLifecycleAction.isNoopUpdate(null, policy1, headers1));\n     }\n \n     public void testReservedStateHandler() throws Exception {\n         ThreadPool threadPool = mock(ThreadPool.class);\n         TransportService transportService = MockUtils.setupTransportServiceWithThreadpoolExecutor(threadPool);\n-        ClusterService mockClusterService = mock(ClusterService.class);\n         TransportPutLifecycleAction putAction = new TransportPutLifecycleAction(\n             transportService,\n-            mockClusterService,\n+            mock(ClusterService.class),\n             threadPool,\n             mock(ActionFilters.class),\n-            mock(PutLifecycleMetadataService.class)\n+            mock(NamedXContentRegistry.class),\n+            mock(XPackLicenseState.class),\n+            mock(Client.class),\n+            mock(ProjectResolver.class)\n         );\n         assertEquals(ReservedLifecycleAction.NAME, putAction.reservedStateHandlerName().get());\n \ndiff --git a/x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/CreateFromDeploymentIT.java b/x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/CreateFromDeploymentIT.java\nindex 48e1a74b7b4..9701fff2a57 100644\n--- a/x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/CreateFromDeploymentIT.java\n+++ b/x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/CreateFromDeploymentIT.java\n@@ -110,8 +110,6 @@ public class CreateFromDeploymentIT extends InferenceBaseRestTest {\n         var results = infer(inferenceId, List.of(\"washing machine\"));\n         assertNotNull(results.get(\"sparse_embedding\"));\n \n-        deleteModel(inferenceId);\n-\n         forceStopMlNodeDeployment(deploymentId);\n     }\n \n@@ -227,7 +225,6 @@ public class CreateFromDeploymentIT extends InferenceBaseRestTest {\n             )\n         );\n \n-        deleteModel(inferenceId);\n         forceStopMlNodeDeployment(deploymentId);\n     }\n \n@@ -269,7 +266,6 @@ public class CreateFromDeploymentIT extends InferenceBaseRestTest {\n             is(Map.of(\"num_allocations\", 2, \"num_threads\", 1, \"model_id\", modelId))\n         );\n \n-        deleteModel(inferenceId);\n         forceStopMlNodeDeployment(deploymentId);\n     }\n \n@@ -313,8 +309,6 @@ public class CreateFromDeploymentIT extends InferenceBaseRestTest {\n             )\n         );\n \n-        deleteModel(inferenceId);\n-        deleteModel(secondInferenceId);\n         forceStopMlNodeDeployment(deploymentId);\n     }\n \n@@ -337,7 +331,6 @@ public class CreateFromDeploymentIT extends InferenceBaseRestTest {\n             )\n         );\n \n-        deleteModel(inferenceId);\n         // Force stop will stop the deployment\n         forceStopMlNodeDeployment(deploymentId);\n     }\n@@ -365,6 +358,16 @@ public class CreateFromDeploymentIT extends InferenceBaseRestTest {\n             \"\"\", modelId, deploymentId);\n     }\n \n+    private String updatedEndpointConfig(int numAllocations) {\n+        return Strings.format(\"\"\"\n+            {\n+              \"service_settings\": {\n+                \"num_allocations\": %d\n+              }\n+            }\n+            \"\"\", numAllocations);\n+    }\n+\n     private Response startMlNodeDeploymemnt(String modelId, String deploymentId) throws IOException {\n         String endPoint = \"/_ml/trained_models/\"\n             + modelId\n@@ -410,6 +413,16 @@ public class CreateFromDeploymentIT extends InferenceBaseRestTest {\n         return client().performRequest(request);\n     }\n \n+    private Map<String, Object> updateMlNodeDeploymemnt(String deploymentId, String body) throws IOException {\n+        String endPoint = \"/_ml/trained_models/\" + deploymentId + \"/deployment/_update\";\n+\n+        Request request = new Request(\"POST\", endPoint);\n+        request.setJsonEntity(body);\n+        var response = client().performRequest(request);\n+        assertStatusOkOrCreated(response);\n+        return entityAsMap(response);\n+    }\n+\n     protected void stopMlNodeDeployment(String deploymentId) throws IOException {\n         String endpoint = \"/_ml/trained_models/\" + deploymentId + \"/deployment/_stop\";\n         Request request = new Request(\"POST\", endpoint);\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/InferenceNamedWriteablesProvider.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/InferenceNamedWriteablesProvider.java\nindex e7008c2292d..35b3977b704 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/InferenceNamedWriteablesProvider.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/InferenceNamedWriteablesProvider.java\n@@ -7,9 +7,6 @@\n \n package org.elasticsearch.xpack.inference;\n \n-import org.elasticsearch.cluster.AbstractNamedDiffable;\n-import org.elasticsearch.cluster.NamedDiff;\n-import org.elasticsearch.cluster.metadata.Metadata;\n import org.elasticsearch.common.io.stream.NamedWriteableRegistry;\n import org.elasticsearch.inference.ChunkingSettings;\n import org.elasticsearch.inference.EmptySecretSettings;\n@@ -34,7 +31,6 @@ import org.elasticsearch.xpack.inference.chunking.RecursiveChunkingSettings;\n import org.elasticsearch.xpack.inference.chunking.SentenceBoundaryChunkingSettings;\n import org.elasticsearch.xpack.inference.chunking.WordBoundaryChunkingSettings;\n import org.elasticsearch.xpack.inference.common.amazon.AwsSecretSettings;\n-import org.elasticsearch.xpack.inference.registry.ClearInferenceEndpointCacheAction;\n import org.elasticsearch.xpack.inference.services.ai21.completion.Ai21ChatCompletionServiceSettings;\n import org.elasticsearch.xpack.inference.services.alibabacloudsearch.AlibabaCloudSearchServiceSettings;\n import org.elasticsearch.xpack.inference.services.alibabacloudsearch.completion.AlibabaCloudSearchCompletionServiceSettings;\n@@ -613,24 +609,6 @@ public class InferenceNamedWriteablesProvider {\n                 ElasticRerankerServiceSettings::new\n             )\n         );\n-        namedWriteables.add(\n-            new NamedWriteableRegistry.Entry(\n-                Metadata.ProjectCustom.class,\n-                ClearInferenceEndpointCacheAction.InvalidateCacheMetadata.NAME,\n-                ClearInferenceEndpointCacheAction.InvalidateCacheMetadata::new\n-            )\n-        );\n-        namedWriteables.add(\n-            new NamedWriteableRegistry.Entry(\n-                NamedDiff.class,\n-                ClearInferenceEndpointCacheAction.InvalidateCacheMetadata.NAME,\n-                in -> AbstractNamedDiffable.readDiffFrom(\n-                    Metadata.ProjectCustom.class,\n-                    ClearInferenceEndpointCacheAction.InvalidateCacheMetadata.NAME,\n-                    in\n-                )\n-            )\n-        );\n     }\n \n     private static void addChunkingSettingsNamedWriteables(List<NamedWriteableRegistry.Entry> namedWriteables) {\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/InferencePlugin.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/InferencePlugin.java\nindex 8380ac1d87c..5e7198d75f4 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/InferencePlugin.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/InferencePlugin.java\n@@ -104,8 +104,6 @@ import org.elasticsearch.xpack.inference.rank.random.RandomRankRetrieverBuilder;\n import org.elasticsearch.xpack.inference.rank.textsimilarity.TextSimilarityRankBuilder;\n import org.elasticsearch.xpack.inference.rank.textsimilarity.TextSimilarityRankDoc;\n import org.elasticsearch.xpack.inference.rank.textsimilarity.TextSimilarityRankRetrieverBuilder;\n-import org.elasticsearch.xpack.inference.registry.ClearInferenceEndpointCacheAction;\n-import org.elasticsearch.xpack.inference.registry.InferenceEndpointRegistry;\n import org.elasticsearch.xpack.inference.registry.ModelRegistry;\n import org.elasticsearch.xpack.inference.registry.ModelRegistryMetadata;\n import org.elasticsearch.xpack.inference.rest.RestDeleteInferenceEndpointAction;\n@@ -240,8 +238,7 @@ public class InferencePlugin extends Plugin\n             new ActionHandler(GetInferenceDiagnosticsAction.INSTANCE, TransportGetInferenceDiagnosticsAction.class),\n             new ActionHandler(GetInferenceServicesAction.INSTANCE, TransportGetInferenceServicesAction.class),\n             new ActionHandler(UnifiedCompletionAction.INSTANCE, TransportUnifiedCompletionInferenceAction.class),\n-            new ActionHandler(GetRerankerWindowSizeAction.INSTANCE, TransportGetRerankerWindowSizeAction.class),\n-            new ActionHandler(ClearInferenceEndpointCacheAction.INSTANCE, ClearInferenceEndpointCacheAction.class)\n+            new ActionHandler(GetRerankerWindowSizeAction.INSTANCE, TransportGetRerankerWindowSizeAction.class)\n         );\n     }\n \n@@ -395,16 +392,6 @@ public class InferencePlugin extends Plugin\n         // Add binding for interface -> implementation\n         components.add(new PluginComponentBinding<>(InferenceServiceRateLimitCalculator.class, calculator));\n \n-        components.add(\n-            new InferenceEndpointRegistry(\n-                services.clusterService(),\n-                settings,\n-                modelRegistry.get(),\n-                serviceRegistry,\n-                services.projectResolver()\n-            )\n-        );\n-\n         return components;\n     }\n \n@@ -459,13 +446,6 @@ public class InferencePlugin extends Plugin\n                 ModelRegistryMetadata::fromXContent\n             )\n         );\n-        namedXContent.add(\n-            new NamedXContentRegistry.Entry(\n-                Metadata.ProjectCustom.class,\n-                new ParseField(ClearInferenceEndpointCacheAction.InvalidateCacheMetadata.NAME),\n-                ClearInferenceEndpointCacheAction.InvalidateCacheMetadata::fromXContent\n-            )\n-        );\n         return namedXContent;\n     }\n \n@@ -561,7 +541,6 @@ public class InferencePlugin extends Plugin\n         settings.add(SKIP_VALIDATE_AND_START);\n         settings.add(INDICES_INFERENCE_BATCH_SIZE);\n         settings.add(INFERENCE_QUERY_TIMEOUT);\n-        settings.addAll(InferenceEndpointRegistry.getSettingsDefinitions());\n         settings.addAll(ElasticInferenceServiceSettings.getSettingsDefinitions());\n         return Collections.unmodifiableSet(settings);\n     }\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/BaseTransportInferenceAction.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/BaseTransportInferenceAction.java\nindex 8e34cafa3e8..269e0f27fd4 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/BaseTransportInferenceAction.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/BaseTransportInferenceAction.java\n@@ -25,6 +25,7 @@ import org.elasticsearch.inference.InferenceServiceRegistry;\n import org.elasticsearch.inference.InferenceServiceResults;\n import org.elasticsearch.inference.Model;\n import org.elasticsearch.inference.TaskType;\n+import org.elasticsearch.inference.UnparsedModel;\n import org.elasticsearch.inference.telemetry.InferenceStats;\n import org.elasticsearch.license.LicenseUtils;\n import org.elasticsearch.license.XPackLicenseState;\n@@ -41,7 +42,7 @@ import org.elasticsearch.xpack.inference.InferencePlugin;\n import org.elasticsearch.xpack.inference.action.task.StreamingTaskManager;\n import org.elasticsearch.xpack.inference.common.InferenceServiceNodeLocalRateLimitCalculator;\n import org.elasticsearch.xpack.inference.common.InferenceServiceRateLimitCalculator;\n-import org.elasticsearch.xpack.inference.registry.InferenceEndpointRegistry;\n+import org.elasticsearch.xpack.inference.registry.ModelRegistry;\n import org.elasticsearch.xpack.inference.telemetry.InferenceTimer;\n \n import java.io.IOException;\n@@ -78,7 +79,7 @@ public abstract class BaseTransportInferenceAction<Request extends BaseInference\n     private static final String STREAMING_INFERENCE_TASK_TYPE = \"streaming_inference\";\n     private static final String STREAMING_TASK_ACTION = \"xpack/inference/streaming_inference[n]\";\n     private final XPackLicenseState licenseState;\n-    private final InferenceEndpointRegistry endpointRegistry;\n+    private final ModelRegistry modelRegistry;\n     private final InferenceServiceRegistry serviceRegistry;\n     private final InferenceStats inferenceStats;\n     private final StreamingTaskManager streamingTaskManager;\n@@ -93,7 +94,7 @@ public abstract class BaseTransportInferenceAction<Request extends BaseInference\n         TransportService transportService,\n         ActionFilters actionFilters,\n         XPackLicenseState licenseState,\n-        InferenceEndpointRegistry endpointRegistry,\n+        ModelRegistry modelRegistry,\n         InferenceServiceRegistry serviceRegistry,\n         InferenceStats inferenceStats,\n         StreamingTaskManager streamingTaskManager,\n@@ -104,7 +105,7 @@ public abstract class BaseTransportInferenceAction<Request extends BaseInference\n     ) {\n         super(inferenceActionName, transportService, actionFilters, requestReader, EsExecutors.DIRECT_EXECUTOR_SERVICE);\n         this.licenseState = licenseState;\n-        this.endpointRegistry = endpointRegistry;\n+        this.modelRegistry = modelRegistry;\n         this.serviceRegistry = serviceRegistry;\n         this.inferenceStats = inferenceStats;\n         this.streamingTaskManager = streamingTaskManager;\n@@ -115,9 +116,9 @@ public abstract class BaseTransportInferenceAction<Request extends BaseInference\n         this.random = Randomness.get();\n     }\n \n-    protected abstract boolean isInvalidTaskTypeForInferenceEndpoint(Request request, Model model);\n+    protected abstract boolean isInvalidTaskTypeForInferenceEndpoint(Request request, UnparsedModel unparsedModel);\n \n-    protected abstract ElasticsearchStatusException createInvalidTaskTypeException(Request request, Model model);\n+    protected abstract ElasticsearchStatusException createInvalidTaskTypeException(Request request, UnparsedModel unparsedModel);\n \n     protected abstract void doInference(\n         Model model,\n@@ -135,13 +136,13 @@ public abstract class BaseTransportInferenceAction<Request extends BaseInference\n \n         var timer = InferenceTimer.start();\n \n-        var getModelListener = ActionListener.wrap((Model model) -> {\n-            var serviceName = model.getConfigurations().getService();\n+        var getModelListener = ActionListener.wrap((UnparsedModel unparsedModel) -> {\n+            var serviceName = unparsedModel.service();\n \n             try {\n-                validateRequest(request, model);\n+                validateRequest(request, unparsedModel);\n             } catch (Exception e) {\n-                recordRequestDurationMetrics(model, timer, e);\n+                recordRequestDurationMetrics(unparsedModel, timer, e);\n                 listener.onFailure(e);\n                 return;\n             }\n@@ -161,9 +162,15 @@ public abstract class BaseTransportInferenceAction<Request extends BaseInference\n \n             var service = serviceRegistry.getService(serviceName).get();\n             var localNodeId = nodeClient.getLocalNodeId();\n-            var routingDecision = determineRouting(serviceName, request, model.getTaskType(), localNodeId);\n+            var routingDecision = determineRouting(serviceName, request, unparsedModel, localNodeId);\n \n             if (routingDecision.currentNodeShouldHandleRequest()) {\n+                var model = service.parsePersistedConfigWithSecrets(\n+                    unparsedModel.inferenceEntityId(),\n+                    unparsedModel.taskType(),\n+                    unparsedModel.settings(),\n+                    unparsedModel.secrets()\n+                );\n                 inferOnServiceWithMetrics(model, request, service, timer, localNodeId, listener);\n             } else {\n                 // Reroute request\n@@ -179,23 +186,28 @@ public abstract class BaseTransportInferenceAction<Request extends BaseInference\n             listener.onFailure(e);\n         });\n \n-        endpointRegistry.getEndpoint(request.getInferenceEntityId(), getModelListener);\n+        modelRegistry.getModelWithSecrets(request.getInferenceEntityId(), getModelListener);\n     }\n \n-    private void validateRequest(Request request, Model model) {\n-        var serviceName = model.getConfigurations().getService();\n+    private void validateRequest(Request request, UnparsedModel unparsedModel) {\n+        var serviceName = unparsedModel.service();\n         var requestTaskType = request.getTaskType();\n         var service = serviceRegistry.getService(serviceName);\n \n         validationHelper(service::isEmpty, () -> unknownServiceException(serviceName, request.getInferenceEntityId()));\n         validationHelper(\n-            () -> request.getTaskType().isAnyOrSame(model.getTaskType()) == false,\n-            () -> requestModelTaskTypeMismatchException(requestTaskType, model.getTaskType())\n+            () -> request.getTaskType().isAnyOrSame(unparsedModel.taskType()) == false,\n+            () -> requestModelTaskTypeMismatchException(requestTaskType, unparsedModel.taskType())\n+        );\n+        validationHelper(\n+            () -> isInvalidTaskTypeForInferenceEndpoint(request, unparsedModel),\n+            () -> createInvalidTaskTypeException(request, unparsedModel)\n         );\n-        validationHelper(() -> isInvalidTaskTypeForInferenceEndpoint(request, model), () -> createInvalidTaskTypeException(request, model));\n     }\n \n-    private NodeRoutingDecision determineRouting(String serviceName, Request request, TaskType modelTaskType, String localNodeId) {\n+    private NodeRoutingDecision determineRouting(String serviceName, Request request, UnparsedModel unparsedModel, String localNodeId) {\n+        var modelTaskType = unparsedModel.taskType();\n+\n         // Rerouting not supported or request was already rerouted\n         if (inferenceServiceRateLimitCalculator.isTaskTypeReroutingSupported(serviceName, modelTaskType) == false\n             || request.hasBeenRerouted()) {\n@@ -262,7 +274,7 @@ public abstract class BaseTransportInferenceAction<Request extends BaseInference\n         );\n     }\n \n-    private void recordRequestDurationMetrics(Model model, InferenceTimer timer, @Nullable Throwable t) {\n+    private void recordRequestDurationMetrics(UnparsedModel model, InferenceTimer timer, @Nullable Throwable t) {\n         Map<String, Object> metricAttributes = new HashMap<>();\n         metricAttributes.putAll(modelAttributes(model));\n         metricAttributes.putAll(responseAttributes(unwrapCause(t)));\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportGetInferenceDiagnosticsAction.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportGetInferenceDiagnosticsAction.java\nindex 0b11b4c1c69..1ddfd784676 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportGetInferenceDiagnosticsAction.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportGetInferenceDiagnosticsAction.java\n@@ -19,7 +19,6 @@ import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.transport.TransportService;\n import org.elasticsearch.xpack.core.inference.action.GetInferenceDiagnosticsAction;\n import org.elasticsearch.xpack.inference.external.http.HttpClientManager;\n-import org.elasticsearch.xpack.inference.registry.InferenceEndpointRegistry;\n \n import java.io.IOException;\n import java.util.List;\n@@ -35,7 +34,6 @@ public class TransportGetInferenceDiagnosticsAction extends TransportNodesAction\n     public record ClientManagers(HttpClientManager externalHttpClientManager, HttpClientManager eisMtlsHttpClientManager) {}\n \n     private final ClientManagers managers;\n-    private final InferenceEndpointRegistry inferenceEndpointRegistry;\n \n     @Inject\n     public TransportGetInferenceDiagnosticsAction(\n@@ -43,8 +41,7 @@ public class TransportGetInferenceDiagnosticsAction extends TransportNodesAction\n         ClusterService clusterService,\n         TransportService transportService,\n         ActionFilters actionFilters,\n-        ClientManagers managers,\n-        InferenceEndpointRegistry inferenceEndpointRegistry\n+        ClientManagers managers\n     ) {\n         super(\n             GetInferenceDiagnosticsAction.NAME,\n@@ -56,7 +53,6 @@ public class TransportGetInferenceDiagnosticsAction extends TransportNodesAction\n         );\n \n         this.managers = Objects.requireNonNull(managers);\n-        this.inferenceEndpointRegistry = Objects.requireNonNull(inferenceEndpointRegistry);\n     }\n \n     @Override\n@@ -83,22 +79,7 @@ public class TransportGetInferenceDiagnosticsAction extends TransportNodesAction\n         return new GetInferenceDiagnosticsAction.NodeResponse(\n             transportService.getLocalNode(),\n             managers.externalHttpClientManager().getPoolStats(),\n-            managers.eisMtlsHttpClientManager().getPoolStats(),\n-            cacheStats()\n+            managers.eisMtlsHttpClientManager().getPoolStats()\n         );\n     }\n-\n-    private GetInferenceDiagnosticsAction.NodeResponse.Stats cacheStats() {\n-        if (inferenceEndpointRegistry.cacheEnabled()) {\n-            var stats = inferenceEndpointRegistry.stats();\n-            return new GetInferenceDiagnosticsAction.NodeResponse.Stats(\n-                inferenceEndpointRegistry.cacheCount(),\n-                stats.getHits(),\n-                stats.getMisses(),\n-                stats.getEvictions()\n-            );\n-        } else {\n-            return null;\n-        }\n-    }\n }\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportInferenceAction.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportInferenceAction.java\nindex f0fb0ec8275..f14d679ba7d 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportInferenceAction.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportInferenceAction.java\n@@ -15,6 +15,7 @@ import org.elasticsearch.inference.InferenceService;\n import org.elasticsearch.inference.InferenceServiceRegistry;\n import org.elasticsearch.inference.InferenceServiceResults;\n import org.elasticsearch.inference.Model;\n+import org.elasticsearch.inference.UnparsedModel;\n import org.elasticsearch.inference.telemetry.InferenceStats;\n import org.elasticsearch.injection.guice.Inject;\n import org.elasticsearch.license.XPackLicenseState;\n@@ -23,7 +24,7 @@ import org.elasticsearch.transport.TransportService;\n import org.elasticsearch.xpack.core.inference.action.InferenceAction;\n import org.elasticsearch.xpack.inference.action.task.StreamingTaskManager;\n import org.elasticsearch.xpack.inference.common.InferenceServiceRateLimitCalculator;\n-import org.elasticsearch.xpack.inference.registry.InferenceEndpointRegistry;\n+import org.elasticsearch.xpack.inference.registry.ModelRegistry;\n \n public class TransportInferenceAction extends BaseTransportInferenceAction<InferenceAction.Request> {\n \n@@ -32,7 +33,7 @@ public class TransportInferenceAction extends BaseTransportInferenceAction<Infer\n         TransportService transportService,\n         ActionFilters actionFilters,\n         XPackLicenseState licenseState,\n-        InferenceEndpointRegistry inferenceEndpointRegistry,\n+        ModelRegistry modelRegistry,\n         InferenceServiceRegistry serviceRegistry,\n         InferenceStats inferenceStats,\n         StreamingTaskManager streamingTaskManager,\n@@ -45,7 +46,7 @@ public class TransportInferenceAction extends BaseTransportInferenceAction<Infer\n             transportService,\n             actionFilters,\n             licenseState,\n-            inferenceEndpointRegistry,\n+            modelRegistry,\n             serviceRegistry,\n             inferenceStats,\n             streamingTaskManager,\n@@ -57,12 +58,12 @@ public class TransportInferenceAction extends BaseTransportInferenceAction<Infer\n     }\n \n     @Override\n-    protected boolean isInvalidTaskTypeForInferenceEndpoint(InferenceAction.Request request, Model model) {\n+    protected boolean isInvalidTaskTypeForInferenceEndpoint(InferenceAction.Request request, UnparsedModel unparsedModel) {\n         return false;\n     }\n \n     @Override\n-    protected ElasticsearchStatusException createInvalidTaskTypeException(InferenceAction.Request request, Model model) {\n+    protected ElasticsearchStatusException createInvalidTaskTypeException(InferenceAction.Request request, UnparsedModel unparsedModel) {\n         return null;\n     }\n \ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportUnifiedCompletionInferenceAction.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportUnifiedCompletionInferenceAction.java\nindex 4fe5dd3a55a..d0eef677ca1 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportUnifiedCompletionInferenceAction.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/action/TransportUnifiedCompletionInferenceAction.java\n@@ -16,6 +16,7 @@ import org.elasticsearch.inference.InferenceServiceRegistry;\n import org.elasticsearch.inference.InferenceServiceResults;\n import org.elasticsearch.inference.Model;\n import org.elasticsearch.inference.TaskType;\n+import org.elasticsearch.inference.UnparsedModel;\n import org.elasticsearch.inference.telemetry.InferenceStats;\n import org.elasticsearch.injection.guice.Inject;\n import org.elasticsearch.license.XPackLicenseState;\n@@ -28,7 +29,7 @@ import org.elasticsearch.xpack.core.inference.action.UnifiedCompletionAction;\n import org.elasticsearch.xpack.core.inference.results.UnifiedChatCompletionException;\n import org.elasticsearch.xpack.inference.action.task.StreamingTaskManager;\n import org.elasticsearch.xpack.inference.common.InferenceServiceRateLimitCalculator;\n-import org.elasticsearch.xpack.inference.registry.InferenceEndpointRegistry;\n+import org.elasticsearch.xpack.inference.registry.ModelRegistry;\n \n import java.util.concurrent.Flow;\n \n@@ -39,7 +40,7 @@ public class TransportUnifiedCompletionInferenceAction extends BaseTransportInfe\n         TransportService transportService,\n         ActionFilters actionFilters,\n         XPackLicenseState licenseState,\n-        InferenceEndpointRegistry inferenceEndpointRegistry,\n+        ModelRegistry modelRegistry,\n         InferenceServiceRegistry serviceRegistry,\n         InferenceStats inferenceStats,\n         StreamingTaskManager streamingTaskManager,\n@@ -52,7 +53,7 @@ public class TransportUnifiedCompletionInferenceAction extends BaseTransportInfe\n             transportService,\n             actionFilters,\n             licenseState,\n-            inferenceEndpointRegistry,\n+            modelRegistry,\n             serviceRegistry,\n             inferenceStats,\n             streamingTaskManager,\n@@ -64,12 +65,15 @@ public class TransportUnifiedCompletionInferenceAction extends BaseTransportInfe\n     }\n \n     @Override\n-    protected boolean isInvalidTaskTypeForInferenceEndpoint(UnifiedCompletionAction.Request request, Model model) {\n-        return request.getTaskType().isAnyOrSame(TaskType.CHAT_COMPLETION) == false || model.getTaskType() != TaskType.CHAT_COMPLETION;\n+    protected boolean isInvalidTaskTypeForInferenceEndpoint(UnifiedCompletionAction.Request request, UnparsedModel unparsedModel) {\n+        return request.getTaskType().isAnyOrSame(TaskType.CHAT_COMPLETION) == false || unparsedModel.taskType() != TaskType.CHAT_COMPLETION;\n     }\n \n     @Override\n-    protected ElasticsearchStatusException createInvalidTaskTypeException(UnifiedCompletionAction.Request request, Model model) {\n+    protected ElasticsearchStatusException createInvalidTaskTypeException(\n+        UnifiedCompletionAction.Request request,\n+        UnparsedModel unparsedModel\n+    ) {\n         return new ElasticsearchStatusException(\n             \"Incompatible task_type for unified API, the requested type [{}] must be one of [{}]\",\n             RestStatus.BAD_REQUEST,\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/ClearInferenceEndpointCacheAction.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/ClearInferenceEndpointCacheAction.java\ndeleted file mode 100644\nindex 2ca6a8312db..00000000000\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/ClearInferenceEndpointCacheAction.java\n+++ /dev/null\n@@ -1,242 +0,0 @@\n-/*\n- * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n- * or more contributor license agreements. Licensed under the Elastic License\n- * 2.0; you may not use this file except in compliance with the Elastic License\n- * 2.0.\n- */\n-\n-package org.elasticsearch.xpack.inference.registry;\n-\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.elasticsearch.TransportVersion;\n-import org.elasticsearch.action.ActionListener;\n-import org.elasticsearch.action.ActionType;\n-import org.elasticsearch.action.support.ActionFilters;\n-import org.elasticsearch.action.support.master.AcknowledgedRequest;\n-import org.elasticsearch.action.support.master.AcknowledgedResponse;\n-import org.elasticsearch.action.support.master.AcknowledgedTransportMasterNodeAction;\n-import org.elasticsearch.cluster.AbstractNamedDiffable;\n-import org.elasticsearch.cluster.AckedBatchedClusterStateUpdateTask;\n-import org.elasticsearch.cluster.ClusterState;\n-import org.elasticsearch.cluster.ClusterStateAckListener;\n-import org.elasticsearch.cluster.SimpleBatchedAckListenerTaskExecutor;\n-import org.elasticsearch.cluster.block.ClusterBlockException;\n-import org.elasticsearch.cluster.block.ClusterBlockLevel;\n-import org.elasticsearch.cluster.metadata.Metadata;\n-import org.elasticsearch.cluster.metadata.ProjectId;\n-import org.elasticsearch.cluster.metadata.ProjectMetadata;\n-import org.elasticsearch.cluster.project.ProjectResolver;\n-import org.elasticsearch.cluster.service.ClusterService;\n-import org.elasticsearch.cluster.service.MasterServiceTaskQueue;\n-import org.elasticsearch.common.Priority;\n-import org.elasticsearch.common.collect.Iterators;\n-import org.elasticsearch.common.io.stream.StreamInput;\n-import org.elasticsearch.common.io.stream.StreamOutput;\n-import org.elasticsearch.common.util.concurrent.EsExecutors;\n-import org.elasticsearch.core.TimeValue;\n-import org.elasticsearch.core.Tuple;\n-import org.elasticsearch.injection.guice.Inject;\n-import org.elasticsearch.tasks.Task;\n-import org.elasticsearch.threadpool.ThreadPool;\n-import org.elasticsearch.transport.TransportService;\n-import org.elasticsearch.xcontent.ConstructingObjectParser;\n-import org.elasticsearch.xcontent.ParseField;\n-import org.elasticsearch.xcontent.ToXContent;\n-import org.elasticsearch.xcontent.XContentParser;\n-\n-import java.io.IOException;\n-import java.util.EnumSet;\n-import java.util.Iterator;\n-import java.util.Objects;\n-\n-import static org.elasticsearch.TransportVersions.ML_INFERENCE_ENDPOINT_CACHE;\n-\n-/**\n- * Clears the cache in {@link InferenceEndpointRegistry}.\n- * This uses the cluster state to broadcast the message to all nodes to clear their cache, which has guaranteed delivery.\n- * There are some edge cases where deletes can come from any node, for example ElasticInferenceServiceAuthorizationHandler and\n- * SemanticTextIndexOptionsIT will delete endpoints on whatever node is handling the request. So this must use a master node transport\n- * action so that the cluster updates can invalidate the cache, even though most requests will originate from the master node\n- * (e.g. when updating and deleting inference endpoints via REST).\n- */\n-public class ClearInferenceEndpointCacheAction extends AcknowledgedTransportMasterNodeAction<ClearInferenceEndpointCacheAction.Request> {\n-    private static final Logger log = LogManager.getLogger(ClearInferenceEndpointCacheAction.class);\n-    private static final String NAME = \"cluster:internal/xpack/inference/clear_inference_endpoint_cache\";\n-    public static final ActionType<AcknowledgedResponse> INSTANCE = new ActionType<>(NAME);\n-    private static final String TASK_QUEUE_NAME = \"inference-endpoint-cache-management\";\n-\n-    private final ProjectResolver projectResolver;\n-    private final InferenceEndpointRegistry inferenceEndpointRegistry;\n-    private final MasterServiceTaskQueue<RefreshCacheMetadataVersionTask> taskQueue;\n-\n-    @Inject\n-    public ClearInferenceEndpointCacheAction(\n-        TransportService transportService,\n-        ClusterService clusterService,\n-        ThreadPool threadPool,\n-        ActionFilters actionFilters,\n-        ProjectResolver projectResolver,\n-        InferenceEndpointRegistry inferenceEndpointRegistry\n-    ) {\n-        super(\n-            NAME,\n-            transportService,\n-            clusterService,\n-            threadPool,\n-            actionFilters,\n-            ClearInferenceEndpointCacheAction.Request::new,\n-            EsExecutors.DIRECT_EXECUTOR_SERVICE\n-        );\n-        this.projectResolver = projectResolver;\n-        this.inferenceEndpointRegistry = inferenceEndpointRegistry;\n-        this.taskQueue = clusterService.createTaskQueue(TASK_QUEUE_NAME, Priority.IMMEDIATE, new CacheMetadataUpdateTaskExecutor());\n-        clusterService.addListener(\n-            event -> event.state()\n-                .metadata()\n-                .projects()\n-                .values()\n-                .stream()\n-                .map(ProjectMetadata::id)\n-                .filter(id -> event.customMetadataChanged(id, InvalidateCacheMetadata.NAME))\n-                .peek(id -> log.trace(\"Inference endpoint cache on node [{}]\", () -> event.state().nodes().getLocalNodeId()))\n-                .forEach(inferenceEndpointRegistry::invalidateAll)\n-        );\n-    }\n-\n-    @Override\n-    protected void masterOperation(\n-        Task task,\n-        ClearInferenceEndpointCacheAction.Request request,\n-        ClusterState state,\n-        ActionListener<AcknowledgedResponse> listener\n-    ) {\n-        if (inferenceEndpointRegistry.cacheEnabled()) {\n-            taskQueue.submitTask(\"invalidateAll\", new RefreshCacheMetadataVersionTask(projectResolver.getProjectId(), listener), null);\n-        } else {\n-            listener.onResponse(AcknowledgedResponse.TRUE);\n-        }\n-    }\n-\n-    @Override\n-    protected ClusterBlockException checkBlock(ClearInferenceEndpointCacheAction.Request request, ClusterState state) {\n-        return state.blocks().globalBlockedException(projectResolver.getProjectId(), ClusterBlockLevel.METADATA_WRITE);\n-    }\n-\n-    public static class Request extends AcknowledgedRequest<ClearInferenceEndpointCacheAction.Request> {\n-        protected Request() {\n-            super(TRAPPY_IMPLICIT_DEFAULT_MASTER_NODE_TIMEOUT, DEFAULT_ACK_TIMEOUT);\n-        }\n-\n-        protected Request(StreamInput in) throws IOException {\n-            super(in);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return Objects.hashCode(ackTimeout());\n-        }\n-\n-        @Override\n-        public boolean equals(Object other) {\n-            if (other == this) return true;\n-            return other instanceof ClearInferenceEndpointCacheAction.Request that && Objects.equals(that.ackTimeout(), ackTimeout());\n-        }\n-    }\n-\n-    public static class InvalidateCacheMetadata extends AbstractNamedDiffable<Metadata.ProjectCustom> implements Metadata.ProjectCustom {\n-        public static final String NAME = \"inference-endpoint-cache-metadata\";\n-        private static final InvalidateCacheMetadata EMPTY = new InvalidateCacheMetadata(0L);\n-        private static final ParseField VERSION_FIELD = new ParseField(\"version\");\n-\n-        @SuppressWarnings(\"unchecked\")\n-        private static final ConstructingObjectParser<InvalidateCacheMetadata, Void> PARSER = new ConstructingObjectParser<>(\n-            NAME,\n-            true,\n-            args -> new InvalidateCacheMetadata((long) args[0])\n-        );\n-\n-        static {\n-            PARSER.declareLong(ConstructingObjectParser.constructorArg(), VERSION_FIELD);\n-        }\n-\n-        public static InvalidateCacheMetadata fromXContent(XContentParser parser) {\n-            return PARSER.apply(parser, null);\n-        }\n-\n-        public static InvalidateCacheMetadata fromMetadata(ProjectMetadata projectMetadata) {\n-            InvalidateCacheMetadata metadata = projectMetadata.custom(NAME);\n-            return metadata == null ? EMPTY : metadata;\n-        }\n-\n-        private final long version;\n-\n-        private InvalidateCacheMetadata(long version) {\n-            this.version = version;\n-        }\n-\n-        public InvalidateCacheMetadata(StreamInput in) throws IOException {\n-            this(in.readVLong());\n-        }\n-\n-        public InvalidateCacheMetadata bumpVersion() {\n-            return new InvalidateCacheMetadata(version < Long.MAX_VALUE ? version + 1 : 1L);\n-        }\n-\n-        @Override\n-        public EnumSet<Metadata.XContentContext> context() {\n-            return Metadata.ALL_CONTEXTS;\n-        }\n-\n-        @Override\n-        public TransportVersion getMinimalSupportedVersion() {\n-            return ML_INFERENCE_ENDPOINT_CACHE;\n-        }\n-\n-        @Override\n-        public String getWriteableName() {\n-            return NAME;\n-        }\n-\n-        @Override\n-        public void writeTo(StreamOutput out) throws IOException {\n-            out.writeVLong(version);\n-        }\n-\n-        @Override\n-        public Iterator<? extends ToXContent> toXContentChunked(ToXContent.Params ignored) {\n-            return Iterators.single(((builder, params) -> builder.field(VERSION_FIELD.getPreferredName(), version)));\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return Objects.hashCode(version);\n-        }\n-\n-        @Override\n-        public boolean equals(Object other) {\n-            if (other == this) return true;\n-            return other instanceof InvalidateCacheMetadata that && that.version == this.version;\n-        }\n-    }\n-\n-    private static class RefreshCacheMetadataVersionTask extends AckedBatchedClusterStateUpdateTask {\n-        private final ProjectId projectId;\n-\n-        private RefreshCacheMetadataVersionTask(ProjectId projectId, ActionListener<AcknowledgedResponse> listener) {\n-            super(TimeValue.THIRTY_SECONDS, listener);\n-            this.projectId = projectId;\n-        }\n-    }\n-\n-    private static class CacheMetadataUpdateTaskExecutor extends SimpleBatchedAckListenerTaskExecutor<RefreshCacheMetadataVersionTask> {\n-        @Override\n-        public Tuple<ClusterState, ClusterStateAckListener> executeTask(RefreshCacheMetadataVersionTask task, ClusterState clusterState) {\n-            var projectMetadata = clusterState.metadata().getProject(task.projectId);\n-            var currentMetadata = InvalidateCacheMetadata.fromMetadata(projectMetadata);\n-            var updatedMetadata = currentMetadata.bumpVersion();\n-            var newProjectMetadata = ProjectMetadata.builder(projectMetadata).putCustom(InvalidateCacheMetadata.NAME, updatedMetadata);\n-            return new Tuple<>(ClusterState.builder(clusterState).putProjectMetadata(newProjectMetadata).build(), task);\n-        }\n-    }\n-}\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/InferenceEndpointRegistry.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/InferenceEndpointRegistry.java\ndeleted file mode 100644\nindex 46d93e8d404..00000000000\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/InferenceEndpointRegistry.java\n+++ /dev/null\n@@ -1,149 +0,0 @@\n-/*\n- * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n- * or more contributor license agreements. Licensed under the Elastic License\n- * 2.0; you may not use this file except in compliance with the Elastic License\n- * 2.0.\n- */\n-\n-package org.elasticsearch.xpack.inference.registry;\n-\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.elasticsearch.ResourceNotFoundException;\n-import org.elasticsearch.action.ActionListener;\n-import org.elasticsearch.cluster.metadata.ProjectId;\n-import org.elasticsearch.cluster.project.ProjectResolver;\n-import org.elasticsearch.cluster.service.ClusterService;\n-import org.elasticsearch.common.cache.Cache;\n-import org.elasticsearch.common.cache.CacheBuilder;\n-import org.elasticsearch.common.settings.Setting;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.core.TimeValue;\n-import org.elasticsearch.inference.InferenceServiceRegistry;\n-import org.elasticsearch.inference.Model;\n-\n-import java.util.Collection;\n-import java.util.List;\n-\n-/**\n- * A registry that assembles and caches Inference Endpoints, {@link Model}, for reuse.\n- * Models are high read and minimally written, where changes only occur during updates and deletes.\n- * The cache is invalidated via the {@link ClearInferenceEndpointCacheAction} so that every node gets the invalidation\n- * message.\n- */\n-public class InferenceEndpointRegistry {\n-\n-    private static final Setting<Boolean> INFERENCE_ENDPOINT_CACHE_ENABLED = Setting.boolSetting(\n-        \"xpack.inference.endpoint.cache.enabled\",\n-        true,\n-        Setting.Property.NodeScope,\n-        Setting.Property.Dynamic\n-    );\n-\n-    private static final Setting<Integer> INFERENCE_ENDPOINT_CACHE_WEIGHT = Setting.intSetting(\n-        \"xpack.inference.endpoint.cache.weight\",\n-        25,\n-        Setting.Property.NodeScope\n-    );\n-\n-    private static final Setting<TimeValue> INFERENCE_ENDPOINT_CACHE_EXPIRY = Setting.timeSetting(\n-        \"xpack.inference.endpoint.cache.expiry_time\",\n-        TimeValue.timeValueMinutes(15),\n-        TimeValue.timeValueMinutes(1),\n-        TimeValue.timeValueHours(1),\n-        Setting.Property.NodeScope\n-    );\n-\n-    public static Collection<? extends Setting<?>> getSettingsDefinitions() {\n-        return List.of(INFERENCE_ENDPOINT_CACHE_ENABLED, INFERENCE_ENDPOINT_CACHE_WEIGHT, INFERENCE_ENDPOINT_CACHE_EXPIRY);\n-    }\n-\n-    private static final Logger log = LogManager.getLogger(InferenceEndpointRegistry.class);\n-    private static final Cache.Stats EMPTY = new Cache.Stats(0, 0, 0);\n-    private final ModelRegistry modelRegistry;\n-    private final InferenceServiceRegistry serviceRegistry;\n-    private final ProjectResolver projectResolver;\n-    private final Cache<InferenceIdAndProject, Model> cache;\n-    private volatile boolean cacheEnabled;\n-\n-    public InferenceEndpointRegistry(\n-        ClusterService clusterService,\n-        Settings settings,\n-        ModelRegistry modelRegistry,\n-        InferenceServiceRegistry serviceRegistry,\n-        ProjectResolver projectResolver\n-    ) {\n-        this.modelRegistry = modelRegistry;\n-        this.serviceRegistry = serviceRegistry;\n-        this.projectResolver = projectResolver;\n-        this.cache = CacheBuilder.<InferenceIdAndProject, Model>builder()\n-            .setMaximumWeight(INFERENCE_ENDPOINT_CACHE_WEIGHT.get(settings))\n-            .setExpireAfterWrite(INFERENCE_ENDPOINT_CACHE_EXPIRY.get(settings))\n-            .build();\n-        this.cacheEnabled = INFERENCE_ENDPOINT_CACHE_ENABLED.get(settings);\n-\n-        clusterService.getClusterSettings()\n-            .addSettingsUpdateConsumer(INFERENCE_ENDPOINT_CACHE_ENABLED, enabled -> this.cacheEnabled = enabled);\n-    }\n-\n-    public void getEndpoint(String inferenceEntityId, ActionListener<Model> listener) {\n-        var key = new InferenceIdAndProject(inferenceEntityId, projectResolver.getProjectId());\n-        var cachedModel = cacheEnabled ? cache.get(key) : null;\n-        if (cachedModel != null) {\n-            log.trace(\"Retrieved [{}] from cache.\", inferenceEntityId);\n-            listener.onResponse(cachedModel);\n-        } else {\n-            loadFromIndex(key, listener);\n-        }\n-    }\n-\n-    void invalidateAll(ProjectId projectId) {\n-        if (cacheEnabled) {\n-            var cacheKeys = cache.keys().iterator();\n-            while (cacheKeys.hasNext()) {\n-                if (cacheKeys.next().projectId.equals(projectId)) {\n-                    cacheKeys.remove();\n-                }\n-            }\n-        }\n-    }\n-\n-    private void loadFromIndex(InferenceIdAndProject idAndProject, ActionListener<Model> listener) {\n-        modelRegistry.getModelWithSecrets(idAndProject.inferenceEntityId(), listener.delegateFailureAndWrap((l, unparsedModel) -> {\n-            var service = serviceRegistry.getService(unparsedModel.service())\n-                .orElseThrow(\n-                    () -> new ResourceNotFoundException(\n-                        \"Unknown service [{}] for model [{}]\",\n-                        unparsedModel.service(),\n-                        idAndProject.inferenceEntityId()\n-                    )\n-                );\n-\n-            var model = service.parsePersistedConfigWithSecrets(\n-                unparsedModel.inferenceEntityId(),\n-                unparsedModel.taskType(),\n-                unparsedModel.settings(),\n-                unparsedModel.secrets()\n-            );\n-\n-            if (cacheEnabled) {\n-                cache.put(idAndProject, model);\n-            }\n-            l.onResponse(model);\n-        }));\n-    }\n-\n-    public Cache.Stats stats() {\n-        return cacheEnabled ? cache.stats() : EMPTY;\n-    }\n-\n-    public int cacheCount() {\n-        return cacheEnabled ? cache.count() : 0;\n-    }\n-\n-    public boolean cacheEnabled() {\n-        return cacheEnabled;\n-    }\n-\n-    private record InferenceIdAndProject(String inferenceEntityId, ProjectId projectId) {}\n-}\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/ModelRegistry.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/ModelRegistry.java\nindex 7cd1cf5999d..fe7c4a9395c 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/ModelRegistry.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/registry/ModelRegistry.java\n@@ -614,7 +614,6 @@ public class ModelRegistry implements ClusterStateListener {\n             } else {\n                 // since updating the secrets was successful, we can remove the lock and respond to the final listener\n                 preventDeletionLock.remove(inferenceEntityId);\n-                refreshInferenceEndpointCache();\n                 finalListener.onResponse(true);\n             }\n         }).<BulkResponse>andThen((subListener, configResponse) -> {\n@@ -845,10 +844,7 @@ public class ModelRegistry implements ClusterStateListener {\n         client.execute(\n             DeleteByQueryAction.INSTANCE,\n             request,\n-            ActionListener.runAfter(\n-                getDeleteModelClusterStateListener(inferenceEntityIds, updateClusterState, listener),\n-                this::refreshInferenceEndpointCache\n-            )\n+            getDeleteModelClusterStateListener(inferenceEntityIds, updateClusterState, listener)\n         );\n     }\n \n@@ -903,17 +899,6 @@ public class ModelRegistry implements ClusterStateListener {\n         };\n     }\n \n-    private void refreshInferenceEndpointCache() {\n-        client.execute(\n-            ClearInferenceEndpointCacheAction.INSTANCE,\n-            new ClearInferenceEndpointCacheAction.Request(),\n-            ActionListener.wrap(\n-                ignored -> logger.debug(\"Successfully refreshed inference endpoint cache.\"),\n-                e -> logger.atDebug().withThrowable(e).log(\"Failed to refresh inference endpoint cache.\")\n-            )\n-        );\n-    }\n-\n     private static DeleteByQueryRequest createDeleteRequest(Set<String> inferenceEntityIds) {\n         DeleteByQueryRequest request = new DeleteByQueryRequest().setAbortOnVersionConflict(false);\n         request.indices(InferenceIndex.INDEX_PATTERN, InferenceSecretsIndex.INDEX_PATTERN);\ndiff --git a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/BaseTransportInferenceActionTestCase.java b/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/BaseTransportInferenceActionTestCase.java\nindex 47053b7cbe5..812cd1e3c6d 100644\n--- a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/BaseTransportInferenceActionTestCase.java\n+++ b/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/BaseTransportInferenceActionTestCase.java\n@@ -20,6 +20,7 @@ import org.elasticsearch.inference.InferenceServiceResults;\n import org.elasticsearch.inference.Model;\n import org.elasticsearch.inference.ModelConfigurations;\n import org.elasticsearch.inference.TaskType;\n+import org.elasticsearch.inference.UnparsedModel;\n import org.elasticsearch.inference.telemetry.InferenceStats;\n import org.elasticsearch.inference.telemetry.InferenceStatsTests;\n import org.elasticsearch.license.MockLicenseState;\n@@ -33,10 +34,11 @@ import org.elasticsearch.xpack.core.inference.action.InferenceAction;\n import org.elasticsearch.xpack.inference.InferencePlugin;\n import org.elasticsearch.xpack.inference.action.task.StreamingTaskManager;\n import org.elasticsearch.xpack.inference.common.InferenceServiceRateLimitCalculator;\n-import org.elasticsearch.xpack.inference.registry.InferenceEndpointRegistry;\n+import org.elasticsearch.xpack.inference.registry.ModelRegistry;\n import org.junit.Before;\n import org.mockito.ArgumentCaptor;\n \n+import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.Flow;\n@@ -57,7 +59,7 @@ import static org.mockito.Mockito.when;\n \n public abstract class BaseTransportInferenceActionTestCase<Request extends BaseInferenceActionRequest> extends ESTestCase {\n     private MockLicenseState licenseState;\n-    private InferenceEndpointRegistry inferenceEndpointRegistry;\n+    private ModelRegistry modelRegistry;\n     private StreamingTaskManager streamingTaskManager;\n     private BaseTransportInferenceAction<Request> action;\n     private ThreadPool threadPool;\n@@ -85,7 +87,7 @@ public abstract class BaseTransportInferenceActionTestCase<Request extends BaseI\n         transportService = mock();\n         inferenceServiceRateLimitCalculator = mock();\n         licenseState = mock();\n-        inferenceEndpointRegistry = mock();\n+        modelRegistry = mock();\n         serviceRegistry = mock();\n         inferenceStats = InferenceStatsTests.mockInferenceStats();\n         streamingTaskManager = mock();\n@@ -94,7 +96,7 @@ public abstract class BaseTransportInferenceActionTestCase<Request extends BaseI\n             transportService,\n             actionFilters,\n             licenseState,\n-            inferenceEndpointRegistry,\n+            modelRegistry,\n             serviceRegistry,\n             inferenceStats,\n             streamingTaskManager,\n@@ -111,7 +113,7 @@ public abstract class BaseTransportInferenceActionTestCase<Request extends BaseI\n         TransportService transportService,\n         ActionFilters actionFilters,\n         MockLicenseState licenseState,\n-        InferenceEndpointRegistry inferenceEndpointRegistry,\n+        ModelRegistry modelRegistry,\n         InferenceServiceRegistry serviceRegistry,\n         InferenceStats inferenceStats,\n         StreamingTaskManager streamingTaskManager,\n@@ -130,7 +132,7 @@ public abstract class BaseTransportInferenceActionTestCase<Request extends BaseI\n             ActionListener<?> listener = ans.getArgument(1);\n             listener.onFailure(expectedException);\n             return null;\n-        }).when(inferenceEndpointRegistry).getEndpoint(any(), any());\n+        }).when(modelRegistry).getModelWithSecrets(any(), any());\n \n         doExecute(taskType);\n \n@@ -166,7 +168,7 @@ public abstract class BaseTransportInferenceActionTestCase<Request extends BaseI\n     }\n \n     public void testMetricsAfterMissingService() {\n-        mockInferenceEndpointRegistry(taskType);\n+        mockModelRegistry(taskType);\n \n         when(serviceRegistry.getService(any())).thenReturn(Optional.empty());\n \n@@ -188,18 +190,19 @@ public abstract class BaseTransportInferenceActionTestCase<Request extends BaseI\n         }));\n     }\n \n-    protected void mockInferenceEndpointRegistry(TaskType expectedTaskType) {\n+    protected void mockModelRegistry(TaskType expectedTaskType) {\n+        var unparsedModel = new UnparsedModel(inferenceId, expectedTaskType, serviceId, Map.of(), Map.of());\n         doAnswer(ans -> {\n-            ActionListener<Model> listener = ans.getArgument(1);\n-            listener.onResponse(mockModel(expectedTaskType));\n+            ActionListener<UnparsedModel> listener = ans.getArgument(1);\n+            listener.onResponse(unparsedModel);\n             return null;\n-        }).when(inferenceEndpointRegistry).getEndpoint(any(), any());\n+        }).when(modelRegistry).getModelWithSecrets(any(), any());\n     }\n \n     public void testMetricsAfterUnknownTaskType() {\n         var modelTaskType = TaskType.RERANK;\n         var requestTaskType = TaskType.SPARSE_EMBEDDING;\n-        mockInferenceEndpointRegistry(modelTaskType);\n+        mockModelRegistry(modelTaskType);\n         when(serviceRegistry.getService(any())).thenReturn(Optional.of(mock()));\n \n         var listener = doExecute(requestTaskType);\n@@ -360,7 +363,7 @@ public abstract class BaseTransportInferenceActionTestCase<Request extends BaseI\n \n         when(threadPool.getThreadContext()).thenReturn(threadContext);\n \n-        mockInferenceEndpointRegistry(taskType);\n+        mockModelRegistry(taskType);\n         mockService(listener -> listener.onResponse(mock()));\n \n         Request request = createRequest();\n@@ -428,25 +431,30 @@ public abstract class BaseTransportInferenceActionTestCase<Request extends BaseI\n             listenerAction.accept(ans.getArgument(3));\n             return null;\n         }).when(service).unifiedCompletionInfer(any(), any(), any(), any());\n-        mockInferenceEndpointRegistry(taskType);\n-        when(serviceRegistry.getService(any())).thenReturn(Optional.of(service));\n+        mockModelAndServiceRegistry(service);\n     }\n \n     protected Model mockModel() {\n-        return mockModel(taskType);\n-    }\n-\n-    protected Model mockModel(TaskType expectedTaskType) {\n         Model model = mock();\n         ModelConfigurations modelConfigurations = mock();\n         when(modelConfigurations.getService()).thenReturn(serviceId);\n         when(model.getConfigurations()).thenReturn(modelConfigurations);\n-        when(model.getTaskType()).thenReturn(expectedTaskType);\n+        when(model.getTaskType()).thenReturn(taskType);\n         when(model.getServiceSettings()).thenReturn(mock());\n-        when(model.getInferenceEntityId()).thenReturn(inferenceId);\n         return model;\n     }\n \n+    protected void mockModelAndServiceRegistry(InferenceService service) {\n+        var unparsedModel = new UnparsedModel(inferenceId, taskType, serviceId, Map.of(), Map.of());\n+        doAnswer(ans -> {\n+            ActionListener<UnparsedModel> listener = ans.getArgument(1);\n+            listener.onResponse(unparsedModel);\n+            return null;\n+        }).when(modelRegistry).getModelWithSecrets(any(), any());\n+\n+        when(serviceRegistry.getService(any())).thenReturn(Optional.of(service));\n+    }\n+\n     protected void mockValidLicenseState() {\n         when(licenseState.isAllowed(InferencePlugin.INFERENCE_API_FEATURE)).thenReturn(true);\n     }\ndiff --git a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/TransportInferenceActionTests.java b/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/TransportInferenceActionTests.java\nindex dd0a1b95223..547078d93ac 100644\n--- a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/TransportInferenceActionTests.java\n+++ b/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/TransportInferenceActionTests.java\n@@ -22,7 +22,7 @@ import org.elasticsearch.xpack.core.inference.action.InferenceAction;\n import org.elasticsearch.xpack.inference.action.task.StreamingTaskManager;\n import org.elasticsearch.xpack.inference.common.InferenceServiceRateLimitCalculator;\n import org.elasticsearch.xpack.inference.common.RateLimitAssignment;\n-import org.elasticsearch.xpack.inference.registry.InferenceEndpointRegistry;\n+import org.elasticsearch.xpack.inference.registry.ModelRegistry;\n \n import java.util.List;\n \n@@ -49,7 +49,7 @@ public class TransportInferenceActionTests extends BaseTransportInferenceActionT\n         TransportService transportService,\n         ActionFilters actionFilters,\n         MockLicenseState licenseState,\n-        InferenceEndpointRegistry inferenceEndpointRegistry,\n+        ModelRegistry modelRegistry,\n         InferenceServiceRegistry serviceRegistry,\n         InferenceStats inferenceStats,\n         StreamingTaskManager streamingTaskManager,\n@@ -61,7 +61,7 @@ public class TransportInferenceActionTests extends BaseTransportInferenceActionT\n             transportService,\n             actionFilters,\n             licenseState,\n-            inferenceEndpointRegistry,\n+            modelRegistry,\n             serviceRegistry,\n             inferenceStats,\n             streamingTaskManager,\ndiff --git a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/TransportUnifiedCompletionActionTests.java b/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/TransportUnifiedCompletionActionTests.java\nindex 0b05509acaf..9e6f4a62609 100644\n--- a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/TransportUnifiedCompletionActionTests.java\n+++ b/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/action/TransportUnifiedCompletionActionTests.java\n@@ -20,7 +20,7 @@ import org.elasticsearch.xpack.core.inference.action.UnifiedCompletionAction;\n import org.elasticsearch.xpack.core.inference.results.UnifiedChatCompletionException;\n import org.elasticsearch.xpack.inference.action.task.StreamingTaskManager;\n import org.elasticsearch.xpack.inference.common.InferenceServiceRateLimitCalculator;\n-import org.elasticsearch.xpack.inference.registry.InferenceEndpointRegistry;\n+import org.elasticsearch.xpack.inference.registry.ModelRegistry;\n \n import java.util.Optional;\n \n@@ -45,7 +45,7 @@ public class TransportUnifiedCompletionActionTests extends BaseTransportInferenc\n         TransportService transportService,\n         ActionFilters actionFilters,\n         MockLicenseState licenseState,\n-        InferenceEndpointRegistry inferenceEndpointRegistry,\n+        ModelRegistry modelRegistry,\n         InferenceServiceRegistry serviceRegistry,\n         InferenceStats inferenceStats,\n         StreamingTaskManager streamingTaskManager,\n@@ -57,7 +57,7 @@ public class TransportUnifiedCompletionActionTests extends BaseTransportInferenc\n             transportService,\n             actionFilters,\n             licenseState,\n-            inferenceEndpointRegistry,\n+            modelRegistry,\n             serviceRegistry,\n             inferenceStats,\n             streamingTaskManager,\n@@ -75,7 +75,7 @@ public class TransportUnifiedCompletionActionTests extends BaseTransportInferenc\n     public void testThrows_IncompatibleTaskTypeException_WhenUsingATextEmbeddingInferenceEndpoint() {\n         var modelTaskType = TaskType.TEXT_EMBEDDING;\n         var requestTaskType = TaskType.TEXT_EMBEDDING;\n-        mockInferenceEndpointRegistry(modelTaskType);\n+        mockModelRegistry(modelTaskType);\n         when(serviceRegistry.getService(any())).thenReturn(Optional.of(mock()));\n \n         var listener = doExecute(requestTaskType);\n@@ -100,7 +100,7 @@ public class TransportUnifiedCompletionActionTests extends BaseTransportInferenc\n     public void testThrows_IncompatibleTaskTypeException_WhenUsingRequestIsAny_ModelIsTextEmbedding() {\n         var modelTaskType = TaskType.ANY;\n         var requestTaskType = TaskType.TEXT_EMBEDDING;\n-        mockInferenceEndpointRegistry(modelTaskType);\n+        mockModelRegistry(modelTaskType);\n         when(serviceRegistry.getService(any())).thenReturn(Optional.of(mock()));\n \n         var listener = doExecute(requestTaskType);\n@@ -123,7 +123,7 @@ public class TransportUnifiedCompletionActionTests extends BaseTransportInferenc\n     }\n \n     public void testMetricsAfterUnifiedInferSuccess_WithRequestTaskTypeAny() {\n-        mockInferenceEndpointRegistry(TaskType.COMPLETION);\n+        mockModelRegistry(TaskType.COMPLETION);\n         mockService(listener -> listener.onResponse(mock()));\n \n         var listener = doExecute(TaskType.ANY);\ndiff --git a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/registry/ClearInferenceEndpointCacheActionTests.java b/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/registry/ClearInferenceEndpointCacheActionTests.java\ndeleted file mode 100644\nindex f5e134c7089..00000000000\n--- a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/registry/ClearInferenceEndpointCacheActionTests.java\n+++ /dev/null\n@@ -1,128 +0,0 @@\n-/*\n- * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n- * or more contributor license agreements. Licensed under the Elastic License\n- * 2.0; you may not use this file except in compliance with the Elastic License\n- * 2.0.\n- */\n-\n-package org.elasticsearch.xpack.inference.registry;\n-\n-import org.elasticsearch.action.support.PlainActionFuture;\n-import org.elasticsearch.action.support.master.AcknowledgedResponse;\n-import org.elasticsearch.common.bytes.BytesReference;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.core.TimeValue;\n-import org.elasticsearch.inference.InputType;\n-import org.elasticsearch.inference.TaskType;\n-import org.elasticsearch.license.LicenseSettings;\n-import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.test.ESSingleNodeTestCase;\n-import org.elasticsearch.xcontent.XContentBuilder;\n-import org.elasticsearch.xcontent.XContentFactory;\n-import org.elasticsearch.xcontent.XContentType;\n-import org.elasticsearch.xpack.core.inference.action.GetInferenceDiagnosticsAction;\n-import org.elasticsearch.xpack.core.inference.action.InferenceAction;\n-import org.elasticsearch.xpack.core.inference.action.PutInferenceModelAction;\n-import org.elasticsearch.xpack.inference.LocalStateInferencePlugin;\n-import org.elasticsearch.xpack.inference.mock.TestSparseInferenceServiceExtension;\n-\n-import java.io.IOException;\n-import java.util.Collection;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.TimeUnit;\n-\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.hasSize;\n-\n-public class ClearInferenceEndpointCacheActionTests extends ESSingleNodeTestCase {\n-    private static final TimeValue TIMEOUT = new TimeValue(30, TimeUnit.SECONDS);\n-    private static final String INFERENCE_ENDPOINT_ID = \"1\";\n-\n-    @Override\n-    protected Collection<Class<? extends Plugin>> getPlugins() {\n-        return List.of(LocalStateInferencePlugin.class);\n-    }\n-\n-    @Override\n-    protected Settings nodeSettings() {\n-        return Settings.builder().put(LicenseSettings.SELF_GENERATED_LICENSE_TYPE.getKey(), \"trial\").build();\n-    }\n-\n-    public void testCacheEviction() throws Exception {\n-        storeGoodEndpoint();\n-        invokeEndpoint();\n-\n-        var stats = cacheStats();\n-        assertThat(stats.entryCount(), equalTo(1));\n-        assertThat(stats.hits(), equalTo(0L));\n-        assertThat(stats.misses(), equalTo(1L));\n-        assertThat(stats.evictions(), equalTo(0L));\n-\n-        var listener = new PlainActionFuture<AcknowledgedResponse>();\n-        clusterAdmin().execute(ClearInferenceEndpointCacheAction.INSTANCE, new ClearInferenceEndpointCacheAction.Request(), listener);\n-        assertTrue(listener.actionGet(TIMEOUT).isAcknowledged());\n-\n-        assertBusy(() -> {\n-            var nextStats = cacheStats();\n-            assertThat(nextStats.entryCount(), equalTo(0));\n-            assertThat(nextStats.hits(), equalTo(0L));\n-            assertThat(nextStats.misses(), equalTo(1L));\n-            assertThat(nextStats.evictions(), equalTo(1L));\n-        }, 10, TimeUnit.SECONDS);\n-\n-        invokeEndpoint();\n-        stats = cacheStats();\n-        assertThat(stats.entryCount(), equalTo(1));\n-        assertThat(stats.hits(), equalTo(0L));\n-        assertThat(stats.misses(), equalTo(2L));\n-        assertThat(stats.evictions(), equalTo(1L));\n-    }\n-\n-    private void storeGoodEndpoint() throws IOException {\n-        final BytesReference content;\n-        try (XContentBuilder builder = XContentFactory.jsonBuilder()) {\n-            builder.startObject();\n-            builder.field(\"service\", TestSparseInferenceServiceExtension.TestInferenceService.NAME);\n-            builder.field(\"service_settings\", Map.of(\"model\", \"model\", \"api_key\", \"1234\"));\n-            builder.endObject();\n-\n-            content = BytesReference.bytes(builder);\n-        }\n-\n-        var request = new PutInferenceModelAction.Request(\n-            TaskType.SPARSE_EMBEDDING,\n-            INFERENCE_ENDPOINT_ID,\n-            content,\n-            XContentType.JSON,\n-            TEST_REQUEST_TIMEOUT\n-        );\n-        client().execute(PutInferenceModelAction.INSTANCE, request).actionGet(TIMEOUT);\n-    }\n-\n-    private void invokeEndpoint() {\n-        client().execute(\n-            InferenceAction.INSTANCE,\n-            new InferenceAction.Request(\n-                TaskType.SPARSE_EMBEDDING,\n-                INFERENCE_ENDPOINT_ID,\n-                null,\n-                null,\n-                null,\n-                List.of(\"hello\"),\n-                null,\n-                InputType.INTERNAL_SEARCH,\n-                TIMEOUT,\n-                false\n-            )\n-        ).actionGet(TIMEOUT);\n-    }\n-\n-    private GetInferenceDiagnosticsAction.NodeResponse.Stats cacheStats() {\n-        var diagnostics = client().execute(GetInferenceDiagnosticsAction.INSTANCE, new GetInferenceDiagnosticsAction.Request())\n-            .actionGet(TIMEOUT);\n-\n-        assertThat(diagnostics.getNodes(), hasSize(1));\n-        return diagnostics.getNodes().getFirst().getInferenceEndpointRegistryStats();\n-    }\n-}\ndiff --git a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/registry/InferenceEndpointRegistryTests.java b/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/registry/InferenceEndpointRegistryTests.java\ndeleted file mode 100644\nindex b172f0e264c..00000000000\n--- a/x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/registry/InferenceEndpointRegistryTests.java\n+++ /dev/null\n@@ -1,104 +0,0 @@\n-/*\n- * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n- * or more contributor license agreements. Licensed under the Elastic License\n- * 2.0; you may not use this file except in compliance with the Elastic License\n- * 2.0.\n- */\n-\n-package org.elasticsearch.xpack.inference.registry;\n-\n-import org.elasticsearch.ResourceNotFoundException;\n-import org.elasticsearch.action.support.PlainActionFuture;\n-import org.elasticsearch.core.TimeValue;\n-import org.elasticsearch.inference.Model;\n-import org.elasticsearch.inference.TaskType;\n-import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.test.ESSingleNodeTestCase;\n-import org.elasticsearch.xpack.inference.LocalStateInferencePlugin;\n-import org.elasticsearch.xpack.inference.mock.AbstractTestInferenceService;\n-import org.elasticsearch.xpack.inference.mock.TestSparseInferenceServiceExtension;\n-import org.junit.Before;\n-\n-import java.util.Collection;\n-import java.util.List;\n-import java.util.concurrent.TimeUnit;\n-\n-import static org.elasticsearch.xpack.inference.registry.ModelRegistryTests.assertStoreModel;\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.is;\n-import static org.hamcrest.Matchers.sameInstance;\n-\n-public class InferenceEndpointRegistryTests extends ESSingleNodeTestCase {\n-    private static final TimeValue TIMEOUT = new TimeValue(30, TimeUnit.SECONDS);\n-\n-    private InferenceEndpointRegistry inferenceEndpointRegistry;\n-    private ModelRegistry registry;\n-\n-    @Override\n-    protected Collection<Class<? extends Plugin>> getPlugins() {\n-        return List.of(LocalStateInferencePlugin.class);\n-    }\n-\n-    @Before\n-    public void createComponents() {\n-        inferenceEndpointRegistry = node().injector().getInstance(InferenceEndpointRegistry.class);\n-        registry = node().injector().getInstance(ModelRegistry.class);\n-    }\n-\n-    public void testGetThrowsResourceNotFoundWhenNoHitsReturned() {\n-        assertThat(\n-            getEndpointException(\"this is not found\", ResourceNotFoundException.class).getMessage(),\n-            is(\"Inference endpoint not found [this is not found]\")\n-        );\n-    }\n-\n-    private <T extends Exception> Exception getEndpointException(String id, Class<T> expectedExceptionClass) {\n-        var listener = new PlainActionFuture<Model>();\n-        inferenceEndpointRegistry.getEndpoint(id, listener);\n-        return expectThrows(expectedExceptionClass, () -> listener.actionGet(TIMEOUT));\n-    }\n-\n-    public void testGetModel() {\n-        var expectedEndpoint = storeWorkingEndpoint(\"1\");\n-        var actualEndpoint = getEndpoint(\"1\");\n-        assertThat(actualEndpoint, equalTo(expectedEndpoint));\n-        assertThat(getEndpoint(\"1\"), sameInstance(actualEndpoint));\n-    }\n-\n-    private Model storeWorkingEndpoint(String id) {\n-        var expectedEndpoint = new AbstractTestInferenceService.TestServiceModel(\n-            id,\n-            TaskType.SPARSE_EMBEDDING,\n-            \"test_service\",\n-            new TestSparseInferenceServiceExtension.TestServiceSettings(\"model\", null, false),\n-            new AbstractTestInferenceService.TestTaskSettings(randomInt(3)),\n-            new AbstractTestInferenceService.TestSecretSettings(\"secret\")\n-        );\n-        assertStoreModel(registry, expectedEndpoint);\n-        return expectedEndpoint;\n-    }\n-\n-    private Model getEndpoint(String id) {\n-        var listener = new PlainActionFuture<Model>();\n-        inferenceEndpointRegistry.getEndpoint(id, listener);\n-        return listener.actionGet(TIMEOUT);\n-    }\n-\n-    public void testGetModelWithUnknownService() {\n-        var id = \"ahhhh\";\n-        var expectedEndpoint = new AbstractTestInferenceService.TestServiceModel(\n-            id,\n-            TaskType.SPARSE_EMBEDDING,\n-            \"hello\",\n-            new TestSparseInferenceServiceExtension.TestServiceSettings(\"model\", null, false),\n-            new AbstractTestInferenceService.TestTaskSettings(randomInt(3)),\n-            new AbstractTestInferenceService.TestSecretSettings(\"secret\")\n-        );\n-        assertStoreModel(registry, expectedEndpoint);\n-\n-        assertThat(\n-            getEndpointException(id, ResourceNotFoundException.class).getMessage(),\n-            equalTo(\"Unknown service [hello] for model [ahhhh]\")\n-        );\n-    }\n-}\ndiff --git a/x-pack/plugin/ml/qa/native-multi-node-tests/src/javaRestTest/java/org/elasticsearch/xpack/ml/integration/MlNativeIntegTestCase.java b/x-pack/plugin/ml/qa/native-multi-node-tests/src/javaRestTest/java/org/elasticsearch/xpack/ml/integration/MlNativeIntegTestCase.java\nindex bfce552b094..86aadcb0ec1 100644\n--- a/x-pack/plugin/ml/qa/native-multi-node-tests/src/javaRestTest/java/org/elasticsearch/xpack/ml/integration/MlNativeIntegTestCase.java\n+++ b/x-pack/plugin/ml/qa/native-multi-node-tests/src/javaRestTest/java/org/elasticsearch/xpack/ml/integration/MlNativeIntegTestCase.java\n@@ -20,7 +20,6 @@ import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n import org.elasticsearch.action.support.master.AcknowledgedResponse;\n import org.elasticsearch.client.internal.Client;\n import org.elasticsearch.client.internal.node.NodeClient;\n-import org.elasticsearch.cluster.AbstractNamedDiffable;\n import org.elasticsearch.cluster.ClusterModule;\n import org.elasticsearch.cluster.ClusterState;\n import org.elasticsearch.cluster.NamedDiff;\n@@ -100,7 +99,6 @@ import org.elasticsearch.xpack.core.security.authc.TokenMetadata;\n import org.elasticsearch.xpack.esql.core.plugin.EsqlCorePlugin;\n import org.elasticsearch.xpack.esql.plugin.EsqlPlugin;\n import org.elasticsearch.xpack.ilm.IndexLifecycle;\n-import org.elasticsearch.xpack.inference.registry.ClearInferenceEndpointCacheAction;\n import org.elasticsearch.xpack.inference.registry.ModelRegistryMetadata;\n import org.elasticsearch.xpack.ml.LocalStateMachineLearning;\n import org.elasticsearch.xpack.ml.autoscaling.MlScalingReason;\n@@ -438,24 +436,6 @@ abstract class MlNativeIntegTestCase extends ESIntegTestCase {\n                 new NamedWriteableRegistry.Entry(Metadata.ProjectCustom.class, ModelRegistryMetadata.TYPE, ModelRegistryMetadata::new)\n             );\n             entries.add(new NamedWriteableRegistry.Entry(NamedDiff.class, ModelRegistryMetadata.TYPE, ModelRegistryMetadata::readDiffFrom));\n-            entries.add(\n-                new NamedWriteableRegistry.Entry(\n-                    Metadata.ProjectCustom.class,\n-                    ClearInferenceEndpointCacheAction.InvalidateCacheMetadata.NAME,\n-                    ClearInferenceEndpointCacheAction.InvalidateCacheMetadata::new\n-                )\n-            );\n-            entries.add(\n-                new NamedWriteableRegistry.Entry(\n-                    NamedDiff.class,\n-                    ClearInferenceEndpointCacheAction.InvalidateCacheMetadata.NAME,\n-                    in -> AbstractNamedDiffable.readDiffFrom(\n-                        Metadata.ProjectCustom.class,\n-                        ClearInferenceEndpointCacheAction.InvalidateCacheMetadata.NAME,\n-                        in\n-                    )\n-                )\n-            );\n \n             // Retrieve the cluster state from a random node, and serialize and deserialize it.\n             final ClusterStateResponse clusterStateResponse = client().admin()\ndiff --git a/x-pack/plugin/security/qa/operator-privileges-tests/src/javaRestTest/java/org/elasticsearch/xpack/security/operator/Constants.java b/x-pack/plugin/security/qa/operator-privileges-tests/src/javaRestTest/java/org/elasticsearch/xpack/security/operator/Constants.java\nindex f2634a19d10..da9a81898de 100644\n--- a/x-pack/plugin/security/qa/operator-privileges-tests/src/javaRestTest/java/org/elasticsearch/xpack/security/operator/Constants.java\n+++ b/x-pack/plugin/security/qa/operator-privileges-tests/src/javaRestTest/java/org/elasticsearch/xpack/security/operator/Constants.java\n@@ -174,7 +174,6 @@ public class Constants {\n         \"cluster:admin/xpack/enrich/get\",\n         \"cluster:admin/xpack/enrich/put\",\n         \"cluster:admin/xpack/enrich/reindex\",\n-        \"cluster:internal/xpack/inference/clear_inference_endpoint_cache\",\n         \"cluster:admin/xpack/inference/delete\",\n         \"cluster:admin/xpack/inference/put\",\n         \"cluster:admin/xpack/inference/update\","
}