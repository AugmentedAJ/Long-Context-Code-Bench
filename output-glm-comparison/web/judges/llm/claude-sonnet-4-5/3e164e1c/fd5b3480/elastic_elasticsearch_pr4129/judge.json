{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 4129,
  "base_commit": "f7d5d1e5c9a04117f780a62cac970dbe24ddebf7",
  "head_commit": "77239a76f81879742d21004454691d2dfc6499b4",
  "judge_mode": "llm",
  "judge_model": "claude-sonnet-4-5",
  "scores": {
    "correctness": -0.7,
    "completeness": -0.6,
    "code_reuse": -0.3,
    "best_practices": -0.4,
    "unsolicited_docs": 1.0
  },
  "aggregate": -0.2,
  "rationale": "The agent's implementation has several critical issues: (1) It always adds CharTermAttribute in the constructor, whereas ground truth conditionally checks for it in reset() and only creates it if needed, which is important for compatibility. (2) The ByteTermAttribute interface and implementation are not properly refactored - ground truth removes setBytesRef() method, changes bytes field to final, adds toUTF16() method, and modifies clear() and copyTo() methods, but the agent makes none of these changes. (3) The agent misses the complete refactoring of ByteTermAttributeImpl which is central to the fix. (4) Missing the percolation API test (testSuggestFieldWithPercolateApi) which validates the fix works with the Percolator API. (5) Missing the testSuggestTokenFilterProperlyDelegateInputStream test. (6) The agent removes the ByteTermAttrToCharTermAttrFilter test helper but doesn't add the new test that verifies proper delegation. (7) Import consolidation using wildcards in ground truth is missing. The agent's approach of always setting CharTermAttribute could cause performance issues and doesn't match the lazy initialization pattern in ground truth.",
  "edit_run_id": "fd5b3480",
  "judge_run_id": "3e164e1c",
  "ground_truth_patch": "diff --git a/src/main/java/org/elasticsearch/search/suggest/completion/CompletionTokenStream.java b/src/main/java/org/elasticsearch/search/suggest/completion/CompletionTokenStream.java\nindex 260bd57246d..5bf98145850 100644\n--- a/src/main/java/org/elasticsearch/search/suggest/completion/CompletionTokenStream.java\n+++ b/src/main/java/org/elasticsearch/search/suggest/completion/CompletionTokenStream.java\n@@ -19,12 +19,8 @@\n package org.elasticsearch.search.suggest.completion;\n \n import org.apache.lucene.analysis.TokenStream;\n-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;\n-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n-import org.apache.lucene.util.AttributeImpl;\n-import org.apache.lucene.util.BytesRef;\n-import org.apache.lucene.util.IntsRef;\n+import org.apache.lucene.analysis.tokenattributes.*;\n+import org.apache.lucene.util.*;\n import org.apache.lucene.util.fst.Util;\n \n import java.io.IOException;\n@@ -36,9 +32,10 @@ import java.util.Set;\n  */\n public final class CompletionTokenStream extends TokenStream {\n \n-    private final PayloadAttribute payloadAttr = addAttribute(PayloadAttribute.class);;\n+    private final PayloadAttribute payloadAttr = addAttribute(PayloadAttribute.class);\n     private final PositionIncrementAttribute posAttr = addAttribute(PositionIncrementAttribute.class);\n-    private final ByteTermAttribute bytesAtt = addAttribute(ByteTermAttribute.class);\n+    private final ByteTermAttribute bytesAtt = addAttribute(ByteTermAttribute.class);;\n+\n \n     private final TokenStream input;\n     private BytesRef payload;\n@@ -46,9 +43,11 @@ public final class CompletionTokenStream extends TokenStream {\n     private ToFiniteStrings toFiniteStrings;\n     private int posInc = -1;\n     private static final int MAX_PATHS = 256;\n-    private final BytesRef scratch = new BytesRef();\n+    private CharTermAttribute charTermAttribute;\n \n     public CompletionTokenStream(TokenStream input, BytesRef payload, ToFiniteStrings toFiniteStrings) throws IOException {\n+        // Don't call the super(input) ctor - this is a true delegate and has a new attribute source since we consume\n+        // the input stream entirely in toFiniteStrings(input)\n         this.input = input;\n         this.payload = payload;\n         this.toFiniteStrings = toFiniteStrings;\n@@ -74,8 +73,11 @@ public final class CompletionTokenStream extends TokenStream {\n              * produced. Multi Fields have the same surface form and therefore sum up\n              */\n             posInc = 0;\n-            Util.toBytesRef(finiteStrings.next(), scratch); // now we have UTF-8\n-            bytesAtt.setBytesRef(scratch);\n+            Util.toBytesRef(finiteStrings.next(), bytesAtt.getBytesRef()); // now we have UTF-8\n+            if (charTermAttribute != null) {\n+                charTermAttribute.setLength(0);\n+                charTermAttribute.append(bytesAtt.toUTF16());\n+            }\n             if (payload != null) {\n                 payloadAttr.setPayload(this.payload);\n             }\n@@ -107,16 +109,23 @@ public final class CompletionTokenStream extends TokenStream {\n     @Override\n     public void reset() throws IOException {\n         super.reset();\n+        if (hasAttribute(CharTermAttribute.class)) {\n+            // we only create this if we really need it to safe the UTF-8 to UTF-16 conversion\n+            charTermAttribute = getAttribute(CharTermAttribute.class);\n+        }\n         finiteStrings = null;\n         posInc = -1;\n     }\n \n     public interface ByteTermAttribute extends TermToBytesRefAttribute {\n-        public void setBytesRef(BytesRef bytes);\n+        // marker interface\n+\n+        public CharSequence toUTF16();\n     }\n \n     public static final class ByteTermAttributeImpl extends AttributeImpl implements ByteTermAttribute, TermToBytesRefAttribute {\n-        private BytesRef bytes;\n+        private final BytesRef bytes = new BytesRef();\n+        private CharsRef charsRef;\n \n         @Override\n         public int fillBytesRef() {\n@@ -128,19 +137,24 @@ public final class CompletionTokenStream extends TokenStream {\n             return bytes;\n         }\n \n-        @Override\n-        public void setBytesRef(BytesRef bytes) {\n-            this.bytes = bytes;\n-        }\n-\n         @Override\n         public void clear() {\n+            bytes.length = 0;\n         }\n \n         @Override\n         public void copyTo(AttributeImpl target) {\n             ByteTermAttributeImpl other = (ByteTermAttributeImpl) target;\n-            other.bytes = bytes;\n+            other.bytes.copyBytes(bytes);\n+        }\n+\n+        @Override\n+        public CharSequence toUTF16() {\n+            if (charsRef == null) {\n+                charsRef = new CharsRef();\n+            }\n+            UnicodeUtil.UTF8toUTF16(bytes, charsRef);\n+            return charsRef;\n         }\n     }\n }\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java b/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java\nindex 1d518ce4e23..c49695e472e 100644\n--- a/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java\n+++ b/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java\n@@ -28,14 +28,17 @@ import org.elasticsearch.action.admin.indices.segments.IndexShardSegments;\n import org.elasticsearch.action.admin.indices.segments.ShardSegments;\n import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;\n import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.action.percolate.PercolateResponse;\n import org.elasticsearch.action.search.SearchPhaseExecutionException;\n import org.elasticsearch.action.suggest.SuggestResponse;\n+import org.elasticsearch.client.Requests;\n import org.elasticsearch.common.settings.ImmutableSettings;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.xcontent.XContentBuilder;\n import org.elasticsearch.index.mapper.MapperException;\n import org.elasticsearch.index.mapper.MapperParsingException;\n import org.elasticsearch.index.mapper.core.CompletionFieldMapper;\n+import org.elasticsearch.percolator.PercolatorService;\n import org.elasticsearch.search.sort.FieldSortBuilder;\n import org.elasticsearch.search.suggest.completion.CompletionStats;\n import org.elasticsearch.search.suggest.completion.CompletionSuggestion;\n@@ -56,6 +59,7 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF\n import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;\n import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\n import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;\n import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;\n import static org.hamcrest.Matchers.*;\n \n@@ -89,6 +93,36 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n         assertSuggestionsNotInOrder(\"t\", \"The Prodigy\", \"Turbonegro\", \"Turbonegro Get it on\", \"The Prodigy Firestarter\");\n     }\n \n+    @Test\n+    public void testSuggestFieldWithPercolateApi() throws Exception {\n+        createIndexAndMapping();\n+        String[][] input = {{\"Foo Fighters\"}, {\"Foo Fighters\"}, {\"Foo Fighters\"}, {\"Foo Fighters\"},\n+                {\"Generator\", \"Foo Fighters Generator\"}, {\"Learn to Fly\", \"Foo Fighters Learn to Fly\"},\n+                {\"The Prodigy\"}, {\"The Prodigy\"}, {\"The Prodigy\"}, {\"Firestarter\", \"The Prodigy Firestarter\"},\n+                {\"Turbonegro\"}, {\"Turbonegro\"}, {\"Get it on\", \"Turbonegro Get it on\"}}; // work with frequencies\n+        for (int i = 0; i < input.length; i++) {\n+            client().prepareIndex(INDEX, TYPE, \"\" + i)\n+                    .setSource(jsonBuilder()\n+                            .startObject().startObject(FIELD)\n+                            .startArray(\"input\").value(input[i]).endArray()\n+                            .endObject()\n+                            .endObject()\n+                    )\n+                    .execute().actionGet();\n+        }\n+\n+        client().prepareIndex(INDEX, PercolatorService.TYPE_NAME, \"4\")\n+                .setSource(jsonBuilder().startObject().field(\"query\", matchAllQuery()).endObject())\n+                .execute().actionGet();\n+\n+        refresh();\n+\n+        PercolateResponse response = client().preparePercolate().setIndices(INDEX).setDocumentType(TYPE)\n+                .setGetRequest(Requests.getRequest(INDEX).type(TYPE).id(\"1\"))\n+                .execute().actionGet();\n+        assertThat(response.getCount(), equalTo(1l));\n+    }\n+\n     @Test\n     public void testBasicPrefixSuggestion() throws Exception {\n         createIndexAndMapping();\ndiff --git a/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTest.java b/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTest.java\nindex d5ebfa5e19b..974652edc86 100644\n--- a/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTest.java\n+++ b/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTest.java\n@@ -25,10 +25,7 @@ import org.apache.lucene.analysis.core.SimpleAnalyzer;\n import org.apache.lucene.analysis.synonym.SynonymFilter;\n import org.apache.lucene.analysis.synonym.SynonymMap;\n import org.apache.lucene.analysis.synonym.SynonymMap.Builder;\n-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\n-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;\n-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;\n+import org.apache.lucene.analysis.tokenattributes.*;\n import org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester;\n import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.CharsRef;\n@@ -42,6 +39,8 @@ import java.io.IOException;\n import java.io.StringReader;\n import java.util.Set;\n \n+import static org.hamcrest.Matchers.equalTo;\n+\n public class CompletionTokenStreamTest extends ElasticsearchTokenStreamTestCase {\n \n     final XAnalyzingSuggester suggester = new XAnalyzingSuggester(new SimpleAnalyzer(TEST_VERSION_CURRENT));\n@@ -143,12 +142,36 @@ public class CompletionTokenStreamTest extends ElasticsearchTokenStreamTestCase\n         suggestTokenStream.close();\n \n     }\n-    \n+\n+    @Test\n+    public void testSuggestTokenFilterProperlyDelegateInputStream() throws Exception {\n+        TokenStream tokenStream = new MockTokenizer(new StringReader(\"mykeyword\"), MockTokenizer.WHITESPACE, true);\n+        BytesRef payload = new BytesRef(\"Surface keyword|friggin payload|10\");\n+        TokenStream suggestTokenStream = new ByteTermAttrToCharTermAttrFilter(new CompletionTokenStream(tokenStream, payload, new CompletionTokenStream.ToFiniteStrings() {\n+            @Override\n+            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {\n+                return suggester.toFiniteStrings(suggester.getTokenStreamToAutomaton(), stream);\n+            }\n+        }));\n+        TermToBytesRefAttribute termAtt = suggestTokenStream.getAttribute(TermToBytesRefAttribute.class);\n+        BytesRef ref = termAtt.getBytesRef();\n+        assertNotNull(ref);\n+        suggestTokenStream.reset();\n+\n+        while (suggestTokenStream.incrementToken()) {\n+            termAtt.fillBytesRef();\n+            assertThat(ref.utf8ToString(), equalTo(\"mykeyword\"));\n+        }\n+        suggestTokenStream.end();\n+        suggestTokenStream.close();\n+    }\n+\n+\n     public final static class ByteTermAttrToCharTermAttrFilter extends TokenFilter {\n-        private CharTermAttribute attr = addAttribute(CharTermAttribute.class);\n         private ByteTermAttribute byteAttr = addAttribute(ByteTermAttribute.class);\n         private PayloadAttribute payload = addAttribute(PayloadAttribute.class);\n         private TypeAttribute type = addAttribute(TypeAttribute.class);\n+        private CharTermAttribute charTermAttribute = addAttribute(CharTermAttribute.class);\n         protected ByteTermAttrToCharTermAttrFilter(TokenStream input) {\n             super(input);\n         }\n@@ -157,13 +180,12 @@ public class CompletionTokenStreamTest extends ElasticsearchTokenStreamTestCase\n         public boolean incrementToken() throws IOException {\n             if (input.incrementToken()) {\n                 BytesRef bytesRef = byteAttr.getBytesRef();\n-                attr.append(bytesRef.utf8ToString());\n                 // we move them over so we can assert them more easily in the tests\n-                type.setType(payload.getPayload().utf8ToString()); \n+                type.setType(payload.getPayload().utf8ToString());\n                 return true;\n             }\n             return false;\n         }\n-        \n+\n     }\n }"
}