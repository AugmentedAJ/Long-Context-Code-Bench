{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 4253,
  "base_commit": "9f5d01ca4cc06bf7affdb5a656dc838c383e0c4c",
  "head_commit": "6e1a04b3706e131ebac1a4be25d665d3c23f159f",
  "judge_mode": "llm",
  "judge_model": "claude-sonnet-4-5",
  "scores": {
    "correctness": -0.9,
    "completeness": -0.95,
    "code_reuse": -0.8,
    "best_practices": -0.7,
    "unsolicited_docs": 1.0
  },
  "aggregate": -0.4700000000000001,
  "rationale": "The agent's diff fundamentally misunderstands the task. The ground truth updates the completion suggest mechanism to incorporate Lucene trunk changes including: (1) adding unicode_aware parameter to fuzzy suggester with backwards-compatible postings format handling, (2) updating XAnalyzingSuggester/XFuzzySuggester constructors to accept sepLabel/payloadSep/endByte parameters, (3) adding CODEC_VERSION_LATEST and version-based deserialization, (4) extensive test updates, and (5) documentation changes. The agent instead added a count field and getCount() method to XAnalyzingSuggester with serialization/deserialization logic - a completely different feature not mentioned in the task or ground truth. None of the required changes are present: no unicode_aware parameter, no constructor signature updates, no backwards-compatible codec version handling, no documentation updates, no test file creation (AnalyzingCompletionLookupProviderV1.java), and no modifications to AnalyzingCompletionLookupProvider, CompletionSuggestParser, CompletionSuggestionContext, or CompletionSuggestionFuzzyBuilder. The agent's changes would not achieve the task objective of incorporating Lucene trunk updates. This represents a near-total failure to implement the requested functionality.",
  "edit_run_id": "7ad89f22",
  "judge_run_id": "3e164e1c",
  "ground_truth_patch": "diff --git a/docs/reference/search/suggesters/completion-suggest.asciidoc b/docs/reference/search/suggesters/completion-suggest.asciidoc\nindex 74a4aa8ed57..3b8068c6fff 100644\n--- a/docs/reference/search/suggesters/completion-suggest.asciidoc\n+++ b/docs/reference/search/suggesters/completion-suggest.asciidoc\n@@ -218,6 +218,11 @@ The following parameters are supported:\n     Minimum length of the input, which is not\n     checked for fuzzy alternatives, defaults to `1`\n \n+`unicode_aware`::\n+    Sets all are measurements (like edit distance,\n+    transpositions and lengths) in unicode code points\n+    (actual letters) instead of bytes.\n+\n NOTE: If you want to stick with the default values, but\n       still use fuzzy, you can either use `fuzzy: {}`\n       or `fuzzy: true`.\ndiff --git a/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java b/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java\nindex f107d17e83d..5dc327d9b4a 100644\n--- a/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java\n+++ b/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java\n@@ -26,6 +26,8 @@ import org.apache.lucene.search.suggest.InputIterator;\n import org.apache.lucene.search.suggest.Lookup;\n import org.apache.lucene.search.suggest.Sort;\n import org.apache.lucene.store.*;\n+import org.apache.lucene.store.DataInput;\n+import org.apache.lucene.store.DataOutput;\n import org.apache.lucene.util.*;\n import org.apache.lucene.util.automaton.*;\n import org.apache.lucene.util.fst.*;\n@@ -34,10 +36,7 @@ import org.apache.lucene.util.fst.PairOutputs.Pair;\n import org.apache.lucene.util.fst.Util.MinResult;\n import org.elasticsearch.common.collect.HppcMaps;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.io.InputStream;\n-import java.io.OutputStream;\n+import java.io.*;\n import java.util.*;\n \n /**\n@@ -53,8 +52,9 @@ import java.util.*;\n  * then the partial text \"ghost chr...\" could see the\n  * suggestion \"The Ghost of Christmas Past\".  Note that\n  * position increments MUST NOT be preserved for this example\n- * to work, so you should call\n- * {@link #setPreservePositionIncrements(boolean) setPreservePositionIncrements(false)}.\n+ * to work, so you should call the constructor with\n+ * <code>preservePositionIncrements</code> parameter set to\n+ * false\n  *\n  * <p>\n  * If SynonymFilter is used to map wifi and wireless network to\n@@ -124,24 +124,24 @@ public class XAnalyzingSuggester extends Lookup {\n   private final boolean preserveSep;\n \n   /** Include this flag in the options parameter to {@link\n-   *  #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,FST,boolean,int)} to always\n+   *  #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int)} to always\n    *  return the exact match first, regardless of score.  This\n    *  has no performance impact but could result in\n    *  low-quality suggestions. */\n   public static final int EXACT_FIRST = 1;\n \n   /** Include this flag in the options parameter to {@link\n-   *  #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,FST,boolean,int)} to preserve\n+   *  #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int)} to preserve\n    *  token separators when matching. */\n   public static final int PRESERVE_SEP = 2;\n \n   /** Represents the separation between tokens, if\n    *  PRESERVE_SEP was specified */\n-  private static final int SEP_LABEL = 0xFF; \n+  public static final int SEP_LABEL = '\\u001F';\n \n   /** Marks end of the analyzed input and start of dedup\n    *  byte. */\n-  private static final int END_BYTE = 0x0;\n+  public static final int END_BYTE = 0x0;\n \n   /** Maximum number of dup surface forms (different surface\n    *  forms for the same analyzed form). */\n@@ -160,27 +160,31 @@ public class XAnalyzingSuggester extends Lookup {\n \n   private boolean hasPayloads;\n \n-  private static final int PAYLOAD_SEP = '\\u001f';\n+  private final int sepLabel;\n+  private final int payloadSep;\n+  private final int endByte;\n+\n+  public static final int PAYLOAD_SEP = '\\u001f';\n \n   /** Whether position holes should appear in the automaton. */\n   private boolean preservePositionIncrements;\n \n   /**\n-   * Calls {@link #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,FST,boolean,int)\n+   * Calls {@link #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int)\n    * AnalyzingSuggester(analyzer, analyzer, EXACT_FIRST |\n    * PRESERVE_SEP, 256, -1)}\n    */\n   public XAnalyzingSuggester(Analyzer analyzer) {\n-    this(analyzer, analyzer, EXACT_FIRST | PRESERVE_SEP, 256, -1, null, false, 0);\n+    this(analyzer, analyzer, EXACT_FIRST | PRESERVE_SEP, 256, -1, true, null, false, 0, SEP_LABEL, PAYLOAD_SEP, END_BYTE);\n   }\n \n   /**\n-   * Calls {@link #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,FST,boolean,int)\n+   * Calls {@link #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int)\n    * AnalyzingSuggester(indexAnalyzer, queryAnalyzer, EXACT_FIRST |\n    * PRESERVE_SEP, 256, -1)}\n    */\n   public XAnalyzingSuggester(Analyzer indexAnalyzer, Analyzer queryAnalyzer) {\n-    this(indexAnalyzer, queryAnalyzer, EXACT_FIRST | PRESERVE_SEP, 256, -1, null, false, 0);\n+    this(indexAnalyzer, queryAnalyzer, EXACT_FIRST | PRESERVE_SEP, 256, -1, true, null, false, 0, SEP_LABEL, PAYLOAD_SEP, END_BYTE);\n   }\n \n   /**\n@@ -199,8 +203,9 @@ public class XAnalyzingSuggester extends Lookup {\n    *   to expand from the analyzed form.  Set this to -1 for\n    *   no limit.\n    */\n-  public XAnalyzingSuggester(Analyzer indexAnalyzer, Analyzer queryAnalyzer, int options, int maxSurfaceFormsPerAnalyzedForm, int maxGraphExpansions\n-          , FST<Pair<Long, BytesRef>> fst, boolean hasPayloads, int maxAnalyzedPathsForOneInput) { \n+  public XAnalyzingSuggester(Analyzer indexAnalyzer, Analyzer queryAnalyzer, int options, int maxSurfaceFormsPerAnalyzedForm, int maxGraphExpansions,\n+                             boolean preservePositionIncrements, FST<Pair<Long, BytesRef>> fst, boolean hasPayloads, int maxAnalyzedPathsForOneInput,\n+                             int sepLabel, int payloadSep, int endByte) {\n       // SIMON EDIT: I added fst, hasPayloads and maxAnalyzedPathsForOneInput \n     this.indexAnalyzer = indexAnalyzer;\n     this.queryAnalyzer = queryAnalyzer;\n@@ -226,16 +231,13 @@ public class XAnalyzingSuggester extends Lookup {\n     }\n     this.maxGraphExpansions = maxGraphExpansions;\n     this.maxAnalyzedPathsForOneInput = maxAnalyzedPathsForOneInput;\n-    this.preservePositionIncrements = true;\n-  }\n-\n-  /** Whether to take position holes (position increment > 1) into account when\n-   *  building the automaton, <code>true</code> by default. */\n-  public void setPreservePositionIncrements(boolean preservePositionIncrements) {\n     this.preservePositionIncrements = preservePositionIncrements;\n+    this.sepLabel = sepLabel;\n+    this.payloadSep = payloadSep;\n+    this.endByte = endByte;\n   }\n \n-    /** Returns byte size of the underlying FST. */\n+  /** Returns byte size of the underlying FST. */\n   public long sizeInBytes() {\n     return fst == null ? 0 : fst.sizeInBytes();\n   }\n@@ -251,7 +253,7 @@ public class XAnalyzingSuggester extends Lookup {\n \n   // Replaces SEP with epsilon or remaps them if\n   // we were asked to preserve them:\n-  private static void replaceSep(Automaton a, boolean preserveSep) {\n+  private static void replaceSep(Automaton a, boolean preserveSep, int replaceSep) {\n \n     State[] states = a.getNumberedStates();\n \n@@ -265,7 +267,7 @@ public class XAnalyzingSuggester extends Lookup {\n         if (t.getMin() == TokenStreamToAutomaton.POS_SEP) {\n           if (preserveSep) {\n             // Remap to SEP_LABEL:\n-            newTransitions.add(new Transition(SEP_LABEL, t.getDest()));\n+            newTransitions.add(new Transition(replaceSep, t.getDest()));\n           } else {\n             copyDestTransitions(state, t.getDest(), newTransitions);\n             a.setDeterministic(false);\n@@ -289,21 +291,30 @@ public class XAnalyzingSuggester extends Lookup {\n     }\n   }\n \n+  protected Automaton convertAutomaton(Automaton a) {\n+    return a;\n+  }\n+\n   /** Just escapes the 0xff byte (which we still for SEP). */\n   private static final class  EscapingTokenStreamToAutomaton extends TokenStreamToAutomaton {\n \n     final BytesRef spare = new BytesRef();\n+    private char sepLabel;\n+\n+    public EscapingTokenStreamToAutomaton(char sepLabel) {\n+      this.sepLabel = sepLabel;\n+    }\n \n     @Override\n     protected BytesRef changeToken(BytesRef in) {\n       int upto = 0;\n       for(int i=0;i<in.length;i++) {\n         byte b = in.bytes[in.offset+i];\n-        if (b == (byte) SEP_LABEL) {\n+        if (b == (byte) sepLabel) {\n           if (spare.bytes.length == upto) {\n             spare.grow(upto+2);\n           }\n-          spare.bytes[upto++] = (byte) SEP_LABEL;\n+          spare.bytes[upto++] = (byte) sepLabel;\n           spare.bytes[upto++] = b;\n         } else {\n           if (spare.bytes.length == upto) {\n@@ -321,7 +332,7 @@ public class XAnalyzingSuggester extends Lookup {\n   public TokenStreamToAutomaton getTokenStreamToAutomaton() {\n     final TokenStreamToAutomaton tsta;\n     if (preserveSep) {\n-      tsta = new EscapingTokenStreamToAutomaton();\n+      tsta = new EscapingTokenStreamToAutomaton((char) sepLabel);\n     } else {\n       // When we're not preserving sep, we don't steal 0xff\n       // byte, so we don't need to do any escaping:\n@@ -387,7 +398,7 @@ public class XAnalyzingSuggester extends Lookup {\n       }\n       return scratchA.compareTo(scratchB);\n     }\n-  };\n+  }\n \n   @Override\n   public void build(InputIterator iterator) throws IOException {\n@@ -454,7 +465,7 @@ public class XAnalyzingSuggester extends Lookup {\n \n           if (hasPayloads) {\n             for(int i=0;i<surfaceForm.length;i++) {\n-              if (surfaceForm.bytes[i] == PAYLOAD_SEP) {\n+              if (surfaceForm.bytes[i] == payloadSep) {\n                 throw new IllegalArgumentException(\"surface form cannot contain unit separator character U+001F; this character is reserved\");\n               }\n             }\n@@ -558,7 +569,7 @@ public class XAnalyzingSuggester extends Lookup {\n           int payloadLength = scratch.length - payloadOffset;\n           BytesRef br = new BytesRef(surface.length + 1 + payloadLength);\n           System.arraycopy(surface.bytes, surface.offset, br.bytes, 0, surface.length);\n-          br.bytes[surface.length] = PAYLOAD_SEP;\n+          br.bytes[surface.length] = (byte) payloadSep;\n           System.arraycopy(scratch.bytes, payloadOffset, br.bytes, surface.length+1, payloadLength);\n           br.length = br.bytes.length;\n           builder.add(scratchInts, outputs.newPair(cost, br));\n@@ -566,8 +577,10 @@ public class XAnalyzingSuggester extends Lookup {\n       }\n       fst = builder.finish();\n \n-      //Util.dotToFile(fst, \"/tmp/suggest.dot\");\n-      \n+      //PrintWriter pw = new PrintWriter(\"/tmp/out.dot\");\n+      //Util.toDot(fst, pw, true, true);\n+      //pw.close();\n+\n       success = true;\n     } finally {\n       if (success) {\n@@ -616,7 +629,7 @@ public class XAnalyzingSuggester extends Lookup {\n     if (hasPayloads) {\n       int sepIndex = -1;\n       for(int i=0;i<output2.length;i++) {\n-        if (output2.bytes[output2.offset+i] == PAYLOAD_SEP) {\n+        if (output2.bytes[output2.offset+i] == payloadSep) {\n           sepIndex = i;\n           break;\n         }\n@@ -649,7 +662,7 @@ public class XAnalyzingSuggester extends Lookup {\n           return false;\n         }\n       }\n-      return output2.bytes[output2.offset + key.length] == PAYLOAD_SEP;\n+      return output2.bytes[output2.offset + key.length] == payloadSep;\n     } else {\n       return key.bytesEquals(output2);\n     }\n@@ -667,6 +680,14 @@ public class XAnalyzingSuggester extends Lookup {\n     }\n \n     //System.out.println(\"lookup key=\" + key + \" num=\" + num);\n+    for (int i = 0; i < key.length(); i++) {\n+      if (key.charAt(i) == 0x1E) {\n+        throw new IllegalArgumentException(\"lookup key cannot contain HOLE character U+001E; this character is reserved\");\n+      }\n+      if (key.charAt(i) == 0x1F) {\n+        throw new IllegalArgumentException(\"lookup key cannot contain unit separator character U+001F; this character is reserved\");\n+      }\n+    }\n     final BytesRef utf8Key = new BytesRef(key);\n     try {\n \n@@ -688,13 +709,13 @@ public class XAnalyzingSuggester extends Lookup {\n \n       final List<LookupResult> results = new ArrayList<LookupResult>();\n \n-      List<FSTUtil.Path<Pair<Long,BytesRef>>> prefixPaths = FSTUtil.intersectPrefixPaths(lookupAutomaton, fst);\n+      List<FSTUtil.Path<Pair<Long,BytesRef>>> prefixPaths = FSTUtil.intersectPrefixPaths(convertAutomaton(lookupAutomaton), fst);\n \n       if (exactFirst) {\n \n         int count = 0;\n         for (FSTUtil.Path<Pair<Long,BytesRef>> path : prefixPaths) {\n-          if (fst.findTargetArc(END_BYTE, path.fstNode, scratchArc, bytesReader) != null) {\n+          if (fst.findTargetArc(endByte, path.fstNode, scratchArc, bytesReader) != null) {\n             // This node has END_BYTE arc leaving, meaning it's an\n             // \"exact\" match:\n             count++;\n@@ -712,7 +733,7 @@ public class XAnalyzingSuggester extends Lookup {\n         // pruned our exact match from one of these nodes\n         // ...:\n         for (FSTUtil.Path<Pair<Long,BytesRef>> path : prefixPaths) {\n-          if (fst.findTargetArc(END_BYTE, path.fstNode, scratchArc, bytesReader) != null) {\n+          if (fst.findTargetArc(endByte, path.fstNode, scratchArc, bytesReader) != null) {\n             // This node has END_BYTE arc leaving, meaning it's an\n             // \"exact\" match:\n             searcher.addStartPaths(scratchArc, fst.outputs.add(path.output, scratchArc.output), false, path.input);\n@@ -820,13 +841,12 @@ public class XAnalyzingSuggester extends Lookup {\n     throws IOException {\n     return prefixPaths;\n   }\n-  \n-  final Set<IntsRef> toFiniteStrings(final BytesRef surfaceForm, final TokenStreamToAutomaton ts2a) throws IOException {\n-      // Analyze surface form:\n+\n+  public final Set<IntsRef> toFiniteStrings(final BytesRef surfaceForm, final TokenStreamToAutomaton ts2a) throws IOException {\n+    // Analyze surface form:\n     TokenStream ts = indexAnalyzer.tokenStream(\"\", surfaceForm.utf8ToString());\n     return toFiniteStrings(ts2a, ts);\n   }\n-  \n   public final Set<IntsRef> toFiniteStrings(final TokenStreamToAutomaton ts2a, TokenStream ts) throws IOException {\n       // Analyze surface form:\n \n@@ -836,7 +856,7 @@ public class XAnalyzingSuggester extends Lookup {\n       Automaton automaton = ts2a.toAutomaton(ts);\n       ts.close();\n \n-      replaceSep(automaton, preserveSep);\n+      replaceSep(automaton, preserveSep, sepLabel);\n \n       assert SpecialOperations.isFinite(automaton);\n \n@@ -862,7 +882,7 @@ public class XAnalyzingSuggester extends Lookup {\n     // This way we could eg differentiate \"net\" from \"net \",\n     // which we can't today...\n \n-    replaceSep(automaton, preserveSep);\n+    replaceSep(automaton, preserveSep, sepLabel);\n \n     // TODO: we can optimize this somewhat by determinizing\n     // while we convert\n@@ -903,7 +923,6 @@ public class XAnalyzingSuggester extends Lookup {\n   \n     public static class XBuilder {\n         private Builder<Pair<Long, BytesRef>> builder;\n-        BytesRef previousAnalyzed = null;\n         private int maxSurfaceFormsPerAnalyzedForm;\n         private IntsRef scratchInts = new IntsRef();\n         private final PairOutputs<Long, BytesRef> outputs;\n@@ -912,8 +931,10 @@ public class XAnalyzingSuggester extends Lookup {\n         private final SurfaceFormAndPayload[] surfaceFormsAndPayload;\n         private int count;\n         private ObjectIntOpenHashMap<BytesRef> seenSurfaceForms = HppcMaps.Object.Integer.ensureNoNullKeys(256, 0.75f);\n+        private int payloadSep;\n \n-        public XBuilder(int maxSurfaceFormsPerAnalyzedForm, boolean hasPayloads) {\n+        public XBuilder(int maxSurfaceFormsPerAnalyzedForm, boolean hasPayloads, int payloadSep) {\n+            this.payloadSep = payloadSep;\n             this.outputs = new PairOutputs<Long, BytesRef>(PositiveIntOutputs.getSingleton(), ByteSequenceOutputs.getSingleton());\n             this.builder = new Builder<Pair<Long, BytesRef>>(FST.INPUT_TYPE.BYTE1, outputs);\n             this.maxSurfaceFormsPerAnalyzedForm = maxSurfaceFormsPerAnalyzedForm;\n@@ -983,7 +1004,7 @@ public class XAnalyzingSuggester extends Lookup {\n                 int len = surface.length + 1 + payload.length;\n                 final BytesRef br = new BytesRef(len);\n                 System.arraycopy(surface.bytes, surface.offset, br.bytes, 0, surface.length);\n-                br.bytes[surface.length] = PAYLOAD_SEP;\n+                br.bytes[surface.length] = (byte) payloadSep;\n                 System.arraycopy(payload.bytes, payload.offset, br.bytes, surface.length + 1, payload.length);\n                 br.length = len;\n                 payloadRef = br;\ndiff --git a/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java b/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java\nindex 4ea297cb232..d364d29fa12 100644\n--- a/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java\n+++ b/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java\n@@ -19,6 +19,7 @@\n package org.apache.lucene.search.suggest.analyzing;\n \n import org.apache.lucene.analysis.Analyzer;\n+import org.apache.lucene.analysis.TokenStreamToAutomaton;\n import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.IntsRef;\n import org.apache.lucene.util.automaton.*;\n@@ -48,6 +49,9 @@ import java.util.Set;\n  * #DEFAULT_NON_FUZZY_PREFIX} byte is not allowed to be\n  * edited.  We allow up to 1 (@link\n  * #DEFAULT_MAX_EDITS} edit.\n+ * If {@link #unicodeAware} parameter in the constructor is set to true, maxEdits,\n+ * minFuzzyLength, transpositions and nonFuzzyPrefix are measured in Unicode code\n+ * points (actual letters) instead of bytes.*\n  *\n  * <p>\n  * NOTE: This suggester does not boost suggestions that\n@@ -60,12 +64,22 @@ import java.util.Set;\n  * like synonyms to keep the complexity of the prefix intersection low for good\n  * lookup performance. At index time, complex analyzers can safely be used.\n  * </p>\n+ *\n+ * @lucene.experimental\n  */\n public final class XFuzzySuggester extends XAnalyzingSuggester {\n     private final int maxEdits;\n     private final boolean transpositions;\n     private final int nonFuzzyPrefix;\n     private final int minFuzzyLength;\n+    private final boolean unicodeAware;\n+\n+    /**\n+     *  Measure maxEdits, minFuzzyLength, transpositions and nonFuzzyPrefix\n+     *  parameters in Unicode code points (actual letters)\n+     *  instead of bytes.\n+     */\n+    public static final boolean DEFAULT_UNICODE_AWARE = false;\n \n     /**\n      * The default minimum length of the key passed to {@link\n@@ -108,7 +122,7 @@ public final class XFuzzySuggester extends XAnalyzingSuggester {\n      */\n     public XFuzzySuggester(Analyzer indexAnalyzer, Analyzer queryAnalyzer) {\n         this(indexAnalyzer, queryAnalyzer, EXACT_FIRST | PRESERVE_SEP, 256, -1, DEFAULT_MAX_EDITS, DEFAULT_TRANSPOSITIONS,\n-                DEFAULT_NON_FUZZY_PREFIX, DEFAULT_MIN_FUZZY_LENGTH, null, false, 0);\n+                DEFAULT_NON_FUZZY_PREFIX, DEFAULT_MIN_FUZZY_LENGTH, DEFAULT_UNICODE_AWARE, null, false, 0, SEP_LABEL, PAYLOAD_SEP, END_BYTE);\n \n     }\n \n@@ -133,11 +147,15 @@ public final class XFuzzySuggester extends XAnalyzingSuggester {\n      *        Levenshtein algorithm.\n      * @param nonFuzzyPrefix length of common (non-fuzzy) prefix (see default {@link #DEFAULT_NON_FUZZY_PREFIX}\n      * @param minFuzzyLength minimum length of lookup key before any edits are allowed (see default {@link #DEFAULT_MIN_FUZZY_LENGTH})\n+     * @param sepLabel separation label\n+     * @param payloadSep payload separator byte\n+     * @param endByte end byte marker byte\n      */\n     public XFuzzySuggester(Analyzer indexAnalyzer, Analyzer queryAnalyzer, int options, int maxSurfaceFormsPerAnalyzedForm, int maxGraphExpansions,\n-                           int maxEdits, boolean transpositions, int nonFuzzyPrefix, int minFuzzyLength,\n-                           FST<PairOutputs.Pair<Long, BytesRef>> fst, boolean hasPayloads, int maxAnalyzedPathsForOneInput) {\n-        super(indexAnalyzer, queryAnalyzer, options, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions, fst, hasPayloads, maxAnalyzedPathsForOneInput);\n+                           int maxEdits, boolean transpositions, int nonFuzzyPrefix, int minFuzzyLength, boolean unicodeAware,\n+                           FST<PairOutputs.Pair<Long, BytesRef>> fst, boolean hasPayloads, int maxAnalyzedPathsForOneInput,\n+                           int sepLabel, int payloadSep, int endByte) {\n+        super(indexAnalyzer, queryAnalyzer, options, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions, true, fst, hasPayloads, maxAnalyzedPathsForOneInput, sepLabel, payloadSep, endByte);\n         if (maxEdits < 0 || maxEdits > LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE) {\n             throw new IllegalArgumentException(\"maxEdits must be between 0 and \" + LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE);\n         }\n@@ -152,6 +170,7 @@ public final class XFuzzySuggester extends XAnalyzingSuggester {\n         this.transpositions = transpositions;\n         this.nonFuzzyPrefix = nonFuzzyPrefix;\n         this.minFuzzyLength = minFuzzyLength;\n+        this.unicodeAware = unicodeAware;\n     }\n \n     @Override\n@@ -170,7 +189,7 @@ public final class XFuzzySuggester extends XAnalyzingSuggester {\n         // \"compete\") ... in which case I think the wFST needs\n         // to be log weights or something ...\n \n-        Automaton levA = toLevenshteinAutomata(lookupAutomaton);\n+        Automaton levA = convertAutomaton(toLevenshteinAutomata(lookupAutomaton));\n     /*\n       Writer w = new OutputStreamWriter(new FileOutputStream(\"out.dot\"), \"UTF-8\");\n       w.write(levA.toDot());\n@@ -180,6 +199,24 @@ public final class XFuzzySuggester extends XAnalyzingSuggester {\n         return FSTUtil.intersectPrefixPaths(levA, fst);\n     }\n \n+    @Override\n+    protected Automaton convertAutomaton(Automaton a) {\n+      if (unicodeAware) {\n+        Automaton utf8automaton = new UTF32ToUTF8().convert(a);\n+        BasicOperations.determinize(utf8automaton);\n+        return utf8automaton;\n+      } else {\n+        return a;\n+      }\n+    }\n+\n+    @Override\n+    public TokenStreamToAutomaton getTokenStreamToAutomaton() {\n+      final TokenStreamToAutomaton tsta = super.getTokenStreamToAutomaton();\n+      tsta.setUnicodeArcs(unicodeAware);\n+      return tsta;\n+    }\n+\n     Automaton toLevenshteinAutomata(Automaton automaton) {\n         final Set<IntsRef> ref = SpecialOperations.getFiniteStrings(automaton, -1);\n         Automaton subs[] = new Automaton[ref.size()];\n@@ -197,7 +234,7 @@ public final class XFuzzySuggester extends XAnalyzingSuggester {\n                 // to allow the trailing dedup bytes to be\n                 // edited... but then 0 byte is \"in general\" allowed\n                 // on input (but not in UTF8).\n-                LevenshteinAutomata lev = new LevenshteinAutomata(ints, 255, transpositions);\n+                LevenshteinAutomata lev = new LevenshteinAutomata(ints, unicodeAware ? Character.MAX_CODE_POINT : 255, transpositions);\n                 Automaton levAutomaton = lev.toAutomaton(maxEdits);\n                 Automaton combined = BasicOperations.concatenate(Arrays.asList(prefix, levAutomaton));\n                 combined.setDeterministic(true); // its like the special case in concatenate itself, except we cloneExpanded already\ndiff --git a/src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java b/src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java\nindex 7cb0a9d1838..b13356df642 100644\n--- a/src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java\n+++ b/src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java\n@@ -55,7 +55,8 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n     private static final int MAX_GRAPH_EXPANSIONS = -1;\n \n     public static final String CODEC_NAME = \"analyzing\";\n-    public static final int CODEC_VERSION = 1;\n+    public static final int CODEC_VERSION_START = 1;\n+    public static final int CODEC_VERSION_LATEST = 2;\n \n     private boolean preserveSep;\n     private boolean preservePositionIncrements;\n@@ -73,8 +74,7 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n         int options = preserveSep ? XAnalyzingSuggester.PRESERVE_SEP : 0;\n         // needs to fixed in the suggester first before it can be supported\n         //options |= exactFirst ? XAnalyzingSuggester.EXACT_FIRST : 0;\n-        prototype = new XAnalyzingSuggester(null, null, options, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions, null, false, 1);\n-        prototype.setPreservePositionIncrements(preservePositionIncrements);\n+        prototype = new XAnalyzingSuggester(null, null, options, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions, preservePositionIncrements, null, false, 1, XAnalyzingSuggester.SEP_LABEL, XAnalyzingSuggester.PAYLOAD_SEP, XAnalyzingSuggester.END_BYTE);\n     }\n \n     @Override\n@@ -84,7 +84,7 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n \n     @Override\n     public FieldsConsumer consumer(final IndexOutput output) throws IOException {\n-        CodecUtil.writeHeader(output, CODEC_NAME, CODEC_VERSION);\n+        CodecUtil.writeHeader(output, CODEC_NAME, CODEC_VERSION_LATEST);\n         return new FieldsConsumer() {\n             private Map<FieldInfo, Long> fieldOffsets = new HashMap<FieldInfo, Long>();\n \n@@ -111,7 +111,7 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n             public TermsConsumer addField(final FieldInfo field) throws IOException {\n \n                 return new TermsConsumer() {\n-                    final XAnalyzingSuggester.XBuilder builder = new XAnalyzingSuggester.XBuilder(maxSurfaceFormsPerAnalyzedForm, hasPayloads);\n+                    final XAnalyzingSuggester.XBuilder builder = new XAnalyzingSuggester.XBuilder(maxSurfaceFormsPerAnalyzedForm, hasPayloads, XAnalyzingSuggester.PAYLOAD_SEP);\n                     final CompletionPostingsConsumer postingsConsumer = new CompletionPostingsConsumer(AnalyzingCompletionLookupProvider.this, builder);\n \n                     @Override\n@@ -156,6 +156,9 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n                             options |= hasPayloads ? SERIALIZE_HAS_PAYLOADS : 0;\n                             options |= preservePositionIncrements ? SERIALIZE_PRESERVE_POSITION_INCREMENTS : 0;\n                             output.writeVInt(options);\n+                            output.writeVInt(XAnalyzingSuggester.SEP_LABEL);\n+                            output.writeVInt(XAnalyzingSuggester.END_BYTE);\n+                            output.writeVInt(XAnalyzingSuggester.PAYLOAD_SEP);\n                         }\n                     }\n                 };\n@@ -200,7 +203,7 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n \n     @Override\n     public LookupFactory load(IndexInput input) throws IOException {\n-        CodecUtil.checkHeader(input, CODEC_NAME, CODEC_VERSION, CODEC_VERSION);\n+        int version = CodecUtil.checkHeader(input, CODEC_NAME, CODEC_VERSION_START, CODEC_VERSION_LATEST);\n         final Map<String, AnalyzingSuggestHolder> lookupMap = new HashMap<String, AnalyzingSuggestHolder>();\n         input.seek(input.length() - 8);\n         long metaPointer = input.readLong();\n@@ -225,8 +228,23 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n             boolean preserveSep = (options & SERIALIZE_PRESERVE_SEPERATORS) != 0;\n             boolean hasPayloads = (options & SERIALIZE_HAS_PAYLOADS) != 0;\n             boolean preservePositionIncrements = (options & SERIALIZE_PRESERVE_POSITION_INCREMENTS) != 0;\n-            lookupMap.put(entry.getValue(), new AnalyzingSuggestHolder(preserveSep, preservePositionIncrements, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions,\n-                    hasPayloads, maxAnalyzedPathsForOneInput, fst));\n+\n+            // first version did not include these three fields, so fall back to old default (before the analyzingsuggester\n+            // was updated in Lucene, so we cannot use the suggester defaults)\n+            int sepLabel, payloadSep, endByte;\n+            if (version == CODEC_VERSION_START) {\n+                sepLabel = 0xFF;\n+                payloadSep = '\\u001f';\n+                endByte = 0x0;\n+            } else {\n+                sepLabel = input.readVInt();\n+                endByte = input.readVInt();\n+                payloadSep = input.readVInt();\n+            }\n+\n+            AnalyzingSuggestHolder holder = new AnalyzingSuggestHolder(preserveSep, preservePositionIncrements, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions,\n+                    hasPayloads, maxAnalyzedPathsForOneInput, fst, sepLabel, payloadSep, endByte);\n+            lookupMap.put(entry.getValue(), holder);\n         }\n         return new LookupFactory() {\n             @Override\n@@ -242,17 +260,16 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n                     suggester = new XFuzzySuggester(mapper.indexAnalyzer(), mapper.searchAnalyzer(), flags,\n                             analyzingSuggestHolder.maxSurfaceFormsPerAnalyzedForm, analyzingSuggestHolder.maxGraphExpansions,\n                             suggestionContext.getFuzzyEditDistance(), suggestionContext.isFuzzyTranspositions(),\n-                            suggestionContext.getFuzzyPrefixLength(), suggestionContext.getFuzzyMinLength(),\n+                            suggestionContext.getFuzzyPrefixLength(), suggestionContext.getFuzzyMinLength(), suggestionContext.isFuzzyUnicodeAware(),\n                             analyzingSuggestHolder.fst, analyzingSuggestHolder.hasPayloads,\n-                            analyzingSuggestHolder.maxAnalyzedPathsForOneInput);\n+                            analyzingSuggestHolder.maxAnalyzedPathsForOneInput, analyzingSuggestHolder.sepLabel, analyzingSuggestHolder.payloadSep, analyzingSuggestHolder.endByte);\n \n                 } else {\n                     suggester = new XAnalyzingSuggester(mapper.indexAnalyzer(), mapper.searchAnalyzer(), flags,\n                             analyzingSuggestHolder.maxSurfaceFormsPerAnalyzedForm, analyzingSuggestHolder.maxGraphExpansions,\n-                            analyzingSuggestHolder.fst, analyzingSuggestHolder.hasPayloads,\n-                            analyzingSuggestHolder.maxAnalyzedPathsForOneInput);\n+                            analyzingSuggestHolder.preservePositionIncrements, analyzingSuggestHolder.fst, analyzingSuggestHolder.hasPayloads,\n+                            analyzingSuggestHolder.maxAnalyzedPathsForOneInput, analyzingSuggestHolder.sepLabel, analyzingSuggestHolder.payloadSep, analyzingSuggestHolder.endByte);\n                 }\n-                suggester.setPreservePositionIncrements(analyzingSuggestHolder.preservePositionIncrements);\n                 return suggester;\n             }\n \n@@ -280,6 +297,11 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n \n                 return new CompletionStats(sizeInBytes, completionFields);\n             }\n+\n+            @Override\n+            AnalyzingSuggestHolder getAnalyzingSuggestHolder(FieldMapper<?> mapper) {\n+                return lookupMap.get(mapper.names().indexName());\n+            }\n         };\n     }\n \n@@ -291,9 +313,16 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n         final boolean hasPayloads;\n         final int maxAnalyzedPathsForOneInput;\n         final FST<Pair<Long, BytesRef>> fst;\n+        final int sepLabel;\n+        final int payloadSep;\n+        final int endByte;\n \n         public AnalyzingSuggestHolder(boolean preserveSep, boolean preservePositionIncrements, int maxSurfaceFormsPerAnalyzedForm, int maxGraphExpansions,\n                                       boolean hasPayloads, int maxAnalyzedPathsForOneInput, FST<Pair<Long, BytesRef>> fst) {\n+            this(preserveSep, preservePositionIncrements, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions, hasPayloads, maxAnalyzedPathsForOneInput, fst, XAnalyzingSuggester.SEP_LABEL, XAnalyzingSuggester.PAYLOAD_SEP, XAnalyzingSuggester.END_BYTE);\n+        }\n+\n+        public AnalyzingSuggestHolder(boolean preserveSep, boolean preservePositionIncrements, int maxSurfaceFormsPerAnalyzedForm, int maxGraphExpansions, boolean hasPayloads, int maxAnalyzedPathsForOneInput, FST<Pair<Long, BytesRef>> fst, int sepLabel, int payloadSep, int endByte) {\n             this.preserveSep = preserveSep;\n             this.preservePositionIncrements = preservePositionIncrements;\n             this.maxSurfaceFormsPerAnalyzedForm = maxSurfaceFormsPerAnalyzedForm;\n@@ -301,8 +330,10 @@ public class AnalyzingCompletionLookupProvider extends CompletionLookupProvider\n             this.hasPayloads = hasPayloads;\n             this.maxAnalyzedPathsForOneInput = maxAnalyzedPathsForOneInput;\n             this.fst = fst;\n+            this.sepLabel = sepLabel;\n+            this.payloadSep = payloadSep;\n+            this.endByte = endByte;\n         }\n-\n     }\n \n     @Override\ndiff --git a/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java b/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java\nindex 646d1b54d89..5cdb064316b 100644\n--- a/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java\n+++ b/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java\n@@ -362,5 +362,6 @@ public class Completion090PostingsFormat extends PostingsFormat {\n     public static abstract class LookupFactory {\n         public abstract Lookup getLookup(FieldMapper<?> mapper, CompletionSuggestionContext suggestionContext);\n         public abstract CompletionStats stats(String ... fields);\n+        abstract AnalyzingCompletionLookupProvider.AnalyzingSuggestHolder getAnalyzingSuggestHolder(FieldMapper<?> mapper);\n     }\n }\ndiff --git a/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java b/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java\nindex b6ecf7c8777..6ba83579f7a 100644\n--- a/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java\n+++ b/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java\n@@ -68,6 +68,8 @@ public class CompletionSuggestParser implements SuggestContextParser {\n                             suggestion.setFuzzyMinLength(parser.intValue());\n                         } else if (\"prefix_length\".equals(fuzzyConfigName) || \"prefixLength\".equals(fuzzyConfigName)) {\n                             suggestion.setFuzzyPrefixLength(parser.intValue());\n+                        } else if (\"unicode_aware\".equals(fuzzyConfigName) || \"unicodeAware\".equals(fuzzyConfigName)) {\n+                            suggestion.setFuzzyUnicodeAware(parser.booleanValue());\n                         }\n                     }\n                 }\ndiff --git a/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionContext.java b/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionContext.java\nindex b986c0000a9..f52965c7abd 100644\n--- a/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionContext.java\n+++ b/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionContext.java\n@@ -34,6 +34,7 @@ public class CompletionSuggestionContext extends SuggestionSearchContext.Suggest\n     private int fuzzyMinLength = XFuzzySuggester.DEFAULT_MIN_FUZZY_LENGTH;\n     private int fuzzyPrefixLength = XFuzzySuggester.DEFAULT_NON_FUZZY_PREFIX;\n     private boolean fuzzy = false;\n+    private boolean fuzzyUnicodeAware = XFuzzySuggester.DEFAULT_UNICODE_AWARE;\n \n     public CompletionSuggestionContext(Suggester suggester) {\n         super(suggester);\n@@ -86,4 +87,12 @@ public class CompletionSuggestionContext extends SuggestionSearchContext.Suggest\n     public boolean isFuzzy() {\n         return fuzzy;\n     }\n+\n+    public void setFuzzyUnicodeAware(boolean fuzzyUnicodeAware) {\n+        this.fuzzyUnicodeAware = fuzzyUnicodeAware;\n+    }\n+\n+    public boolean isFuzzyUnicodeAware() {\n+        return fuzzyUnicodeAware;\n+    }\n }\ndiff --git a/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionFuzzyBuilder.java b/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionFuzzyBuilder.java\nindex 7bda3f98591..f4b520fd4df 100644\n--- a/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionFuzzyBuilder.java\n+++ b/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionFuzzyBuilder.java\n@@ -38,6 +38,7 @@ public class CompletionSuggestionFuzzyBuilder extends SuggestBuilder.SuggestionB\n     private boolean fuzzyTranspositions = XFuzzySuggester.DEFAULT_TRANSPOSITIONS;\n     private int fuzzyMinLength = XFuzzySuggester.DEFAULT_MIN_FUZZY_LENGTH;\n     private int fuzzyPrefixLength = XFuzzySuggester.DEFAULT_NON_FUZZY_PREFIX;\n+    private boolean unicodeAware = XFuzzySuggester.DEFAULT_UNICODE_AWARE;\n \n     public int getFuzzyEditDistance() {\n         return fuzzyEditDistance;\n@@ -75,6 +76,15 @@ public class CompletionSuggestionFuzzyBuilder extends SuggestBuilder.SuggestionB\n         return this;\n     }\n \n+    public boolean isUnicodeAware() {\n+        return unicodeAware;\n+    }\n+\n+    public CompletionSuggestionFuzzyBuilder setUnicodeAware(boolean unicodeAware) {\n+        this.unicodeAware = unicodeAware;\n+        return this;\n+    }\n+\n     @Override\n     protected XContentBuilder innerToXContent(XContentBuilder builder, ToXContent.Params params) throws IOException {\n         builder.startObject(\"fuzzy\");\n@@ -91,6 +101,9 @@ public class CompletionSuggestionFuzzyBuilder extends SuggestBuilder.SuggestionB\n         if (fuzzyPrefixLength != XFuzzySuggester.DEFAULT_NON_FUZZY_PREFIX) {\n             builder.field(\"prefix_length\", fuzzyPrefixLength);\n         }\n+        if (unicodeAware != XFuzzySuggester.DEFAULT_UNICODE_AWARE) {\n+            builder.field(\"unicode_aware\", unicodeAware);\n+        }\n \n         builder.endObject();\n         return builder;\ndiff --git a/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java b/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java\nindex d942c765700..3346730a453 100644\n--- a/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java\n+++ b/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java\n@@ -68,10 +68,11 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n     private final String INDEX = RandomStrings.randomAsciiOfLength(getRandom(), 10).toLowerCase(Locale.ROOT);\n     private final String TYPE = RandomStrings.randomAsciiOfLength(getRandom(), 10).toLowerCase(Locale.ROOT);\n     private final String FIELD = RandomStrings.randomAsciiOfLength(getRandom(), 10).toLowerCase(Locale.ROOT);\n+    private final CompletionMappingBuilder completionMappingBuilder = new CompletionMappingBuilder();\n \n     @Test\n     public void testSimple() throws Exception {\n-        createIndexAndMapping();\n+        createIndexAndMapping(completionMappingBuilder);\n         String[][] input = {{\"Foo Fighters\"}, {\"Foo Fighters\"}, {\"Foo Fighters\"}, {\"Foo Fighters\"},\n                 {\"Generator\", \"Foo Fighters Generator\"}, {\"Learn to Fly\", \"Foo Fighters Learn to Fly\"},\n                 {\"The Prodigy\"}, {\"The Prodigy\"}, {\"The Prodigy\"}, {\"Firestarter\", \"The Prodigy Firestarter\"},\n@@ -95,7 +96,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testSuggestFieldWithPercolateApi() throws Exception {\n-        createIndexAndMapping();\n+        createIndexAndMapping(completionMappingBuilder);\n         String[][] input = {{\"Foo Fighters\"}, {\"Foo Fighters\"}, {\"Foo Fighters\"}, {\"Foo Fighters\"},\n                 {\"Generator\", \"Foo Fighters Generator\"}, {\"Learn to Fly\", \"Foo Fighters Learn to Fly\"},\n                 {\"The Prodigy\"}, {\"The Prodigy\"}, {\"The Prodigy\"}, {\"Firestarter\", \"The Prodigy Firestarter\"},\n@@ -125,7 +126,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testBasicPrefixSuggestion() throws Exception {\n-        createIndexAndMapping();\n+        completionMappingBuilder.payloads(true);\n+        createIndexAndMapping(completionMappingBuilder);\n         for (int i = 0; i < 2; i++) {\n             createData(i == 0);\n             assertSuggestions(\"f\", \"Firestarter - The Prodigy\", \"Foo Fighters\", \"Generator - Foo Fighters\", \"Learn to Fly - Foo Fighters\");\n@@ -137,7 +139,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatWeightsAreWorking() throws Exception {\n-        createIndexAndMapping();\n+        createIndexAndMapping(completionMappingBuilder);\n \n         List<String> similarNames = Lists.newArrayList(\"the\", \"The Prodigy\", \"The Verve\", \"The the\");\n         // the weight is 1000 divided by string length, so the results are easy to to check\n@@ -157,7 +159,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatWeightMustBeAnInteger() throws Exception {\n-        createIndexAndMapping();\n+        createIndexAndMapping(completionMappingBuilder);\n \n         try {\n             client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n@@ -174,7 +176,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatInputCanBeAStringInsteadOfAnArray() throws Exception {\n-        createIndexAndMapping();\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -190,7 +192,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatPayloadsAreArbitraryJsonObjects() throws Exception {\n-        createIndexAndMapping();\n+        completionMappingBuilder.payloads(true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -223,7 +226,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testPayloadAsNumeric() throws Exception {\n-        createIndexAndMapping();\n+        completionMappingBuilder.payloads(true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -250,7 +254,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testPayloadAsString() throws Exception {\n-        createIndexAndMapping();\n+        completionMappingBuilder.payloads(true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -277,7 +282,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test(expected = MapperException.class)\n     public void testThatExceptionIsThrownWhenPayloadsAreDisabledButInIndexRequest() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", false, false, true);\n+        completionMappingBuilder.payloads(false);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -290,7 +296,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testDisabledPreserveSeperators() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", true, false, true);\n+        completionMappingBuilder.preserveSeparators(false);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -313,7 +320,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testEnabledPreserveSeperators() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", true, true, true);\n+        completionMappingBuilder.preserveSeparators(true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -334,7 +342,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatMultipleInputsAreSupported() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", false, false, true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -351,7 +359,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatShortSyntaxIsWorking() throws Exception {\n-        createIndexAndMapping();\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startArray(FIELD)\n@@ -368,7 +376,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n     @Test\n     public void testThatDisablingPositionIncrementsWorkForStopwords() throws Exception {\n         // analyzer which removes stopwords... so may not be the simple one\n-        createIndexAndMapping(\"classic\", \"classic\", false, false, false);\n+        completionMappingBuilder.searchAnalyzer(\"classic\").indexAnalyzer(\"classic\").preservePositionIncrements(false);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -389,7 +398,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n                 .putArray(\"analysis.analyzer.suggest_analyzer_synonyms.filter\", \"standard\", \"lowercase\", \"my_synonyms\")\n                 .put(\"analysis.filter.my_synonyms.type\", \"synonym\")\n                 .putArray(\"analysis.filter.my_synonyms.synonyms\", \"foo,renamed\");\n-        createIndexAndMappingAndSettings(settingsBuilder, \"suggest_analyzer_synonyms\", \"suggest_analyzer_synonyms\", false, false, true);\n+        completionMappingBuilder.searchAnalyzer(\"suggest_analyzer_synonyms\").indexAnalyzer(\"suggest_analyzer_synonyms\");\n+        createIndexAndMappingAndSettings(settingsBuilder, completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -451,7 +461,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatFuzzySuggesterWorks() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", true, true, true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -474,7 +484,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatFuzzySuggesterSupportsEditDistances() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", true, true, true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -499,7 +509,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatFuzzySuggesterSupportsTranspositions() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", true, true, true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -522,7 +532,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatFuzzySuggesterSupportsMinPrefixLength() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", true, true, true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -545,7 +555,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatFuzzySuggesterSupportsNonPrefixLength() throws Exception {\n-        createIndexAndMapping(\"simple\", \"simple\", true, true, true);\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -566,6 +576,36 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n         assertSuggestions(suggestResponse, false, \"foo\", \"Nirvana\");\n     }\n \n+    @Test\n+    public void testThatFuzzySuggesterIsUnicodeAware() throws Exception {\n+        createIndexAndMapping(completionMappingBuilder);\n+\n+        client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n+                .startObject().startObject(FIELD)\n+                .startArray(\"input\").value(\"\").endArray()\n+                .endObject().endObject()\n+        ).get();\n+\n+        refresh();\n+\n+        // suggestion with a character, which needs unicode awareness\n+        CompletionSuggestionFuzzyBuilder completionSuggestionBuilder =\n+                new CompletionSuggestionFuzzyBuilder(\"foo\").field(FIELD).text(\"\").size(10).setUnicodeAware(true);\n+\n+        SuggestResponse suggestResponse = client().prepareSuggest(INDEX).addSuggestion(completionSuggestionBuilder).execute().actionGet();\n+        assertSuggestions(suggestResponse, false, \"foo\", \"\");\n+\n+        // removing unicode awareness leads to no result\n+        completionSuggestionBuilder.setUnicodeAware(false);\n+        suggestResponse = client().prepareSuggest(INDEX).addSuggestion(completionSuggestionBuilder).execute().actionGet();\n+        assertSuggestions(suggestResponse, false, \"foo\");\n+\n+        // increasing edit distance instead of unicode awareness works again, as this is only a single character\n+        completionSuggestionBuilder.setFuzzyEditDistance(2);\n+        suggestResponse = client().prepareSuggest(INDEX).addSuggestion(completionSuggestionBuilder).execute().actionGet();\n+        assertSuggestions(suggestResponse, false, \"foo\", \"\");\n+    }\n+\n     @Test\n     public void testThatStatsAreWorking() throws Exception {\n         String otherField = \"testOtherField\";\n@@ -612,7 +652,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test\n     public void testThatSortingOnCompletionFieldReturnsUsefulException() throws Exception {\n-        createIndexAndMapping();\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -638,7 +678,11 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n                 .put(\"index.analysis.filter.suggest_stop_filter.type\", \"stop\")\n                 .put(\"index.analysis.filter.suggest_stop_filter.remove_trailing\", false);\n \n-        createIndexAndMappingAndSettings(settingsBuilder, \"simple\", \"stoptest\", true, true, true);\n+        CompletionMappingBuilder completionMappingBuilder = new CompletionMappingBuilder();\n+        completionMappingBuilder.preserveSeparators(true).preservePositionIncrements(true);\n+        completionMappingBuilder.searchAnalyzer(\"stoptest\");\n+        completionMappingBuilder.indexAnalyzer(\"simple\");\n+        createIndexAndMappingAndSettings(settingsBuilder, completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().field(FIELD, \"Feed trolls\").endObject()\n@@ -650,8 +694,11 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n         refresh();\n \n+        assertSuggestions(\"f\", \"Feed the trolls\", \"Feed trolls\");\n+        assertSuggestions(\"fe\", \"Feed the trolls\", \"Feed trolls\");\n+        assertSuggestions(\"fee\", \"Feed the trolls\", \"Feed trolls\");\n+        assertSuggestions(\"feed\", \"Feed the trolls\", \"Feed trolls\");\n         assertSuggestions(\"feed t\", \"Feed the trolls\", \"Feed trolls\");\n-        assertSuggestions(\"feed th\", \"Feed the trolls\");\n         assertSuggestions(\"feed the\", \"Feed the trolls\");\n         // stop word complete, gets ignored on query time, makes it \"feed\" only\n         assertSuggestions(\"feed the \", \"Feed the trolls\", \"Feed trolls\");\n@@ -661,7 +708,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test(expected = MapperParsingException.class)\n     public void testThatIndexingInvalidFieldsInCompletionFieldResultsInException() throws Exception {\n-        createIndexAndMapping();\n+        CompletionMappingBuilder completionMappingBuilder = new CompletionMappingBuilder();\n+        createIndexAndMapping(completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -730,11 +778,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n         return names;\n     }\n \n-    private void createIndexAndMapping() throws IOException {\n-        createIndexAndMapping(\"simple\", \"simple\", true, false, true);\n-    }\n-\n-    private void createIndexAndMappingAndSettings(Settings.Builder settingsBuilder, String indexAnalyzer, String searchAnalyzer, boolean payloads, boolean preserveSeparators, boolean preservePositionIncrements) throws IOException {\n+    private void createIndexAndMappingAndSettings(Settings.Builder settingsBuilder, CompletionMappingBuilder completionMappingBuilder) throws IOException {\n         client().admin().indices().prepareCreate(INDEX)\n                 .setSettings(settingsBuilder)\n                 .get();\n@@ -742,11 +786,11 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n                 .startObject(TYPE).startObject(\"properties\")\n                 .startObject(FIELD)\n                 .field(\"type\", \"completion\")\n-                .field(\"index_analyzer\", indexAnalyzer)\n-                .field(\"search_analyzer\", searchAnalyzer)\n-                .field(\"payloads\", payloads)\n-                .field(\"preserve_separators\", preserveSeparators)\n-                .field(\"preserve_position_increments\", preservePositionIncrements)\n+                .field(\"index_analyzer\", completionMappingBuilder.indexAnalyzer)\n+                .field(\"search_analyzer\", completionMappingBuilder.searchAnalyzer)\n+                .field(\"payloads\", completionMappingBuilder.payloads)\n+                .field(\"preserve_separators\", completionMappingBuilder.preserveSeparators)\n+                .field(\"preserve_position_increments\", completionMappingBuilder.preservePositionIncrements)\n                 .endObject()\n                 .endObject().endObject()\n                 .endObject())\n@@ -755,8 +799,8 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n         ensureYellow();\n     }\n \n-    private void createIndexAndMapping(String indexAnalyzer, String searchAnalyzer, boolean payloads, boolean preserveSeparators, boolean preservePositionIncrements) throws IOException {\n-        createIndexAndMappingAndSettings(createDefaultSettings(), indexAnalyzer, searchAnalyzer, payloads, preserveSeparators, preservePositionIncrements);\n+    private void createIndexAndMapping(CompletionMappingBuilder completionMappingBuilder) throws IOException {\n+        createIndexAndMappingAndSettings(createDefaultSettings(), completionMappingBuilder);\n     }\n \n     private ImmutableSettings.Builder createDefaultSettings() {\n@@ -808,7 +852,7 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n \n     @Test // see #3555\n     public void testPrunedSegments() throws IOException {\n-        createIndexAndMappingAndSettings(settingsBuilder().put(SETTING_NUMBER_OF_SHARDS, 1).put(SETTING_NUMBER_OF_REPLICAS, 0), \"standard\", \"standard\", false, false, false);\n+        createIndexAndMappingAndSettings(settingsBuilder().put(SETTING_NUMBER_OF_SHARDS, 1).put(SETTING_NUMBER_OF_REPLICAS, 0), completionMappingBuilder);\n \n         client().prepareIndex(INDEX, TYPE, \"1\").setSource(jsonBuilder()\n                 .startObject().startObject(FIELD)\n@@ -935,4 +979,33 @@ public class CompletionSuggestSearchTests extends ElasticsearchIntegrationTest {\n         }\n         return new String(charArray);\n     }\n+\n+    private static class CompletionMappingBuilder {\n+        private String searchAnalyzer = \"simple\";\n+        private String indexAnalyzer = \"simple\";\n+        private Boolean payloads = getRandom().nextBoolean();\n+        private Boolean preserveSeparators = getRandom().nextBoolean();\n+        private Boolean preservePositionIncrements = getRandom().nextBoolean();\n+\n+        public CompletionMappingBuilder searchAnalyzer(String searchAnalyzer) {\n+            this.searchAnalyzer = searchAnalyzer;\n+            return this;\n+        }\n+        public CompletionMappingBuilder indexAnalyzer(String indexAnalyzer) {\n+            this.indexAnalyzer = indexAnalyzer;\n+            return this;\n+        }\n+        public CompletionMappingBuilder payloads(Boolean payloads) {\n+            this.payloads = payloads;\n+            return this;\n+        }\n+        public CompletionMappingBuilder preserveSeparators(Boolean preserveSeparators) {\n+            this.preserveSeparators = preserveSeparators;\n+            return this;\n+        }\n+        public CompletionMappingBuilder preservePositionIncrements(Boolean preservePositionIncrements) {\n+            this.preservePositionIncrements = preservePositionIncrements;\n+            return this;\n+        }\n+    }\n }\ndiff --git a/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java b/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java\nnew file mode 100644\nindex 00000000000..e866790c0cd\n--- /dev/null\n+++ b/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java\n@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to ElasticSearch and Shay Banon under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. ElasticSearch licenses this\n+ * file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.suggest.completion;\n+\n+import com.carrotsearch.hppc.ObjectLongOpenHashMap;\n+import org.apache.lucene.analysis.TokenStream;\n+import org.apache.lucene.codecs.*;\n+import org.apache.lucene.index.FieldInfo;\n+import org.apache.lucene.search.suggest.Lookup;\n+import org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester;\n+import org.apache.lucene.search.suggest.analyzing.XFuzzySuggester;\n+import org.apache.lucene.store.IndexInput;\n+import org.apache.lucene.store.IndexOutput;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.IOUtils;\n+import org.apache.lucene.util.IntsRef;\n+import org.apache.lucene.util.fst.ByteSequenceOutputs;\n+import org.apache.lucene.util.fst.FST;\n+import org.apache.lucene.util.fst.PairOutputs;\n+import org.apache.lucene.util.fst.PairOutputs.Pair;\n+import org.apache.lucene.util.fst.PositiveIntOutputs;\n+import org.elasticsearch.common.regex.Regex;\n+import org.elasticsearch.index.mapper.FieldMapper;\n+import org.elasticsearch.search.suggest.completion.Completion090PostingsFormat.CompletionLookupProvider;\n+import org.elasticsearch.search.suggest.completion.Completion090PostingsFormat.LookupFactory;\n+import org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider.AnalyzingSuggestHolder;\n+\n+import java.io.IOException;\n+import java.util.*;\n+/**\n+ * This is an older implementation of the AnalyzingCompletionLookupProvider class\n+ * We use this to test for backwards compatibility in our tests, namely\n+ * CompletionPostingsFormatTest\n+ * This ensures upgrades between versions work smoothly\n+ */\n+public class AnalyzingCompletionLookupProviderV1 extends CompletionLookupProvider {\n+\n+    // for serialization\n+    public static final int SERIALIZE_PRESERVE_SEPERATORS = 1;\n+    public static final int SERIALIZE_HAS_PAYLOADS = 2;\n+    public static final int SERIALIZE_PRESERVE_POSITION_INCREMENTS = 4;\n+\n+    private static final int MAX_SURFACE_FORMS_PER_ANALYZED_FORM = 256;\n+    private static final int MAX_GRAPH_EXPANSIONS = -1;\n+\n+    public static final String CODEC_NAME = \"analyzing\";\n+    public static final int CODEC_VERSION = 1;\n+\n+    private boolean preserveSep;\n+    private boolean preservePositionIncrements;\n+    private int maxSurfaceFormsPerAnalyzedForm;\n+    private int maxGraphExpansions;\n+    private boolean hasPayloads;\n+    private final XAnalyzingSuggester prototype;\n+\n+    // important, these are the settings from the old xanalyzingsuggester\n+    public static final int SEP_LABEL = 0xFF;\n+    public static final int END_BYTE = 0x0;\n+    public static final int PAYLOAD_SEP = '\\u001f';\n+\n+    public AnalyzingCompletionLookupProviderV1(boolean preserveSep, boolean exactFirst, boolean preservePositionIncrements, boolean hasPayloads) {\n+        this.preserveSep = preserveSep;\n+        this.preservePositionIncrements = preservePositionIncrements;\n+        this.hasPayloads = hasPayloads;\n+        this.maxSurfaceFormsPerAnalyzedForm = MAX_SURFACE_FORMS_PER_ANALYZED_FORM;\n+        this.maxGraphExpansions = MAX_GRAPH_EXPANSIONS;\n+        int options = preserveSep ? XAnalyzingSuggester.PRESERVE_SEP : 0;\n+        // needs to fixed in the suggester first before it can be supported\n+        //options |= exactFirst ? XAnalyzingSuggester.EXACT_FIRST : 0;\n+        prototype = new XAnalyzingSuggester(null, null, options, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions, preservePositionIncrements,\n+                null, false, 1, SEP_LABEL, PAYLOAD_SEP, END_BYTE);\n+    }\n+\n+    @Override\n+    public String getName() {\n+        return \"analyzing\";\n+    }\n+\n+    @Override\n+    public FieldsConsumer consumer(final IndexOutput output) throws IOException {\n+        CodecUtil.writeHeader(output, CODEC_NAME, CODEC_VERSION);\n+        return new FieldsConsumer() {\n+            private Map<FieldInfo, Long> fieldOffsets = new HashMap<FieldInfo, Long>();\n+\n+            @Override\n+            public void close() throws IOException {\n+                try { /*\n+                       * write the offsets per field such that we know where\n+                       * we need to load the FSTs from\n+                       */\n+                    long pointer = output.getFilePointer();\n+                    output.writeVInt(fieldOffsets.size());\n+                    for (Map.Entry<FieldInfo, Long> entry : fieldOffsets.entrySet()) {\n+                        output.writeString(entry.getKey().name);\n+                        output.writeVLong(entry.getValue());\n+                    }\n+                    output.writeLong(pointer);\n+                    output.flush();\n+                } finally {\n+                    IOUtils.close(output);\n+                }\n+            }\n+\n+            @Override\n+            public TermsConsumer addField(final FieldInfo field) throws IOException {\n+\n+                return new TermsConsumer() {\n+                    final XAnalyzingSuggester.XBuilder builder = new XAnalyzingSuggester.XBuilder(maxSurfaceFormsPerAnalyzedForm, hasPayloads, PAYLOAD_SEP);\n+                    final CompletionPostingsConsumer postingsConsumer = new CompletionPostingsConsumer(AnalyzingCompletionLookupProviderV1.this, builder);\n+\n+                    @Override\n+                    public PostingsConsumer startTerm(BytesRef text) throws IOException {\n+                        builder.startTerm(text);\n+                        return postingsConsumer;\n+                    }\n+\n+                    @Override\n+                    public Comparator<BytesRef> getComparator() throws IOException {\n+                        return BytesRef.getUTF8SortedAsUnicodeComparator();\n+                    }\n+\n+                    @Override\n+                    public void finishTerm(BytesRef text, TermStats stats) throws IOException {\n+                        builder.finishTerm(stats.docFreq); // use  doc freq as a fallback\n+                    }\n+\n+                    @Override\n+                    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {\n+                        /*\n+                         * Here we are done processing the field and we can\n+                         * buid the FST and write it to disk.\n+                         */\n+                        FST<Pair<Long, BytesRef>> build = builder.build();\n+                        assert build != null || docCount == 0 : \"the FST is null but docCount is != 0 actual value: [\" + docCount + \"]\";\n+                        /*\n+                         * it's possible that the FST is null if we have 2 segments that get merged\n+                         * and all docs that have a value in this field are deleted. This will cause\n+                         * a consumer to be created but it doesn't consume any values causing the FSTBuilder\n+                         * to return null.\n+                         */\n+                        if (build != null) {\n+                            fieldOffsets.put(field, output.getFilePointer());\n+                            build.save(output);\n+                            /* write some more meta-info */\n+                            output.writeVInt(postingsConsumer.getMaxAnalyzedPathsForOneInput());\n+                            output.writeVInt(maxSurfaceFormsPerAnalyzedForm);\n+                            output.writeInt(maxGraphExpansions); // can be negative\n+                            int options = 0;\n+                            options |= preserveSep ? SERIALIZE_PRESERVE_SEPERATORS : 0;\n+                            options |= hasPayloads ? SERIALIZE_HAS_PAYLOADS : 0;\n+                            options |= preservePositionIncrements ? SERIALIZE_PRESERVE_POSITION_INCREMENTS : 0;\n+                            output.writeVInt(options);\n+                        }\n+                    }\n+                };\n+            }\n+        };\n+    }\n+\n+    private static final class CompletionPostingsConsumer extends PostingsConsumer {\n+        private final SuggestPayload spare = new SuggestPayload();\n+        private AnalyzingCompletionLookupProviderV1 analyzingSuggestLookupProvider;\n+        private XAnalyzingSuggester.XBuilder builder;\n+        private int maxAnalyzedPathsForOneInput = 0;\n+\n+        public CompletionPostingsConsumer(AnalyzingCompletionLookupProviderV1 analyzingSuggestLookupProvider, XAnalyzingSuggester.XBuilder builder) {\n+            this.analyzingSuggestLookupProvider = analyzingSuggestLookupProvider;\n+            this.builder = builder;\n+        }\n+\n+        @Override\n+        public void startDoc(int docID, int freq) throws IOException {\n+        }\n+\n+        @Override\n+        public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {\n+            analyzingSuggestLookupProvider.parsePayload(payload, spare);\n+            builder.addSurface(spare.surfaceForm, spare.payload, spare.weight);\n+            // multi fields have the same surface form so we sum up here\n+            maxAnalyzedPathsForOneInput = Math.max(maxAnalyzedPathsForOneInput, position + 1);\n+        }\n+\n+        @Override\n+        public void finishDoc() throws IOException {\n+        }\n+\n+        public int getMaxAnalyzedPathsForOneInput() {\n+            return maxAnalyzedPathsForOneInput;\n+        }\n+    }\n+\n+    ;\n+\n+\n+    @Override\n+    public LookupFactory load(IndexInput input) throws IOException {\n+        CodecUtil.checkHeader(input, CODEC_NAME, CODEC_VERSION, CODEC_VERSION);\n+        final Map<String, AnalyzingSuggestHolder> lookupMap = new HashMap<String, AnalyzingSuggestHolder>();\n+        input.seek(input.length() - 8);\n+        long metaPointer = input.readLong();\n+        input.seek(metaPointer);\n+        int numFields = input.readVInt();\n+\n+        Map<Long, String> meta = new TreeMap<Long, String>();\n+        for (int i = 0; i < numFields; i++) {\n+            String name = input.readString();\n+            long offset = input.readVLong();\n+            meta.put(offset, name);\n+        }\n+\n+        for (Map.Entry<Long, String> entry : meta.entrySet()) {\n+            input.seek(entry.getKey());\n+            FST<Pair<Long, BytesRef>> fst = new FST<Pair<Long, BytesRef>>(input, new PairOutputs<Long, BytesRef>(\n+                    PositiveIntOutputs.getSingleton(), ByteSequenceOutputs.getSingleton()));\n+            int maxAnalyzedPathsForOneInput = input.readVInt();\n+            int maxSurfaceFormsPerAnalyzedForm = input.readVInt();\n+            int maxGraphExpansions = input.readInt();\n+            int options = input.readVInt();\n+            boolean preserveSep = (options & SERIALIZE_PRESERVE_SEPERATORS) != 0;\n+            boolean hasPayloads = (options & SERIALIZE_HAS_PAYLOADS) != 0;\n+            boolean preservePositionIncrements = (options & SERIALIZE_PRESERVE_POSITION_INCREMENTS) != 0;\n+            lookupMap.put(entry.getValue(), new AnalyzingSuggestHolder(preserveSep, preservePositionIncrements, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions,\n+                    hasPayloads, maxAnalyzedPathsForOneInput, fst));\n+        }\n+        return new LookupFactory() {\n+            @Override\n+            public Lookup getLookup(FieldMapper<?> mapper, CompletionSuggestionContext suggestionContext) {\n+                AnalyzingSuggestHolder analyzingSuggestHolder = lookupMap.get(mapper.names().indexName());\n+                if (analyzingSuggestHolder == null) {\n+                    return null;\n+                }\n+                int flags = analyzingSuggestHolder.preserveSep ? XAnalyzingSuggester.PRESERVE_SEP : 0;\n+\n+                XAnalyzingSuggester suggester;\n+                if (suggestionContext.isFuzzy()) {\n+                    suggester = new XFuzzySuggester(mapper.indexAnalyzer(), mapper.searchAnalyzer(), flags,\n+                            analyzingSuggestHolder.maxSurfaceFormsPerAnalyzedForm, analyzingSuggestHolder.maxGraphExpansions,\n+                            suggestionContext.getFuzzyEditDistance(), suggestionContext.isFuzzyTranspositions(),\n+                            suggestionContext.getFuzzyPrefixLength(), suggestionContext.getFuzzyMinLength(), false,\n+                            analyzingSuggestHolder.fst, analyzingSuggestHolder.hasPayloads,\n+                            analyzingSuggestHolder.maxAnalyzedPathsForOneInput, SEP_LABEL, PAYLOAD_SEP, END_BYTE);\n+\n+                } else {\n+                    suggester = new XAnalyzingSuggester(mapper.indexAnalyzer(), mapper.searchAnalyzer(), flags,\n+                            analyzingSuggestHolder.maxSurfaceFormsPerAnalyzedForm, analyzingSuggestHolder.maxGraphExpansions,\n+                            analyzingSuggestHolder.preservePositionIncrements,\n+                            analyzingSuggestHolder.fst, analyzingSuggestHolder.hasPayloads,\n+                            analyzingSuggestHolder.maxAnalyzedPathsForOneInput, SEP_LABEL, PAYLOAD_SEP, END_BYTE);\n+                }\n+                return suggester;\n+            }\n+\n+            @Override\n+            public CompletionStats stats(String... fields) {\n+                long sizeInBytes = 0;\n+                ObjectLongOpenHashMap<String> completionFields = null;\n+                if (fields != null  && fields.length > 0) {\n+                    completionFields = new ObjectLongOpenHashMap<String>(fields.length);\n+                }\n+\n+                for (Map.Entry<String, AnalyzingSuggestHolder> entry : lookupMap.entrySet()) {\n+                    sizeInBytes += entry.getValue().fst.sizeInBytes();\n+                    if (fields == null || fields.length == 0) {\n+                        continue;\n+                    }\n+                    for (String field : fields) {\n+                        // support for getting fields by regex as in fielddata\n+                        if (Regex.simpleMatch(field, entry.getKey())) {\n+                            long fstSize = entry.getValue().fst.sizeInBytes();\n+                            completionFields.addTo(field, fstSize);\n+                        }\n+                    }\n+                }\n+\n+                return new CompletionStats(sizeInBytes, completionFields);\n+            }\n+            @Override\n+            AnalyzingSuggestHolder getAnalyzingSuggestHolder(FieldMapper<?> mapper) {\n+                return lookupMap.get(mapper.names().indexName());\n+            }\n+        };\n+    }\n+\n+    /*\n+    // might be readded when we change the current impl, right now not needed\n+    static class AnalyzingSuggestHolder {\n+        final boolean preserveSep;\n+        final boolean preservePositionIncrements;\n+        final int maxSurfaceFormsPerAnalyzedForm;\n+        final int maxGraphExpansions;\n+        final boolean hasPayloads;\n+        final int maxAnalyzedPathsForOneInput;\n+        final FST<Pair<Long, BytesRef>> fst;\n+\n+        public AnalyzingSuggestHolder(boolean preserveSep, boolean preservePositionIncrements, int maxSurfaceFormsPerAnalyzedForm, int maxGraphExpansions,\n+                                      boolean hasPayloads, int maxAnalyzedPathsForOneInput, FST<Pair<Long, BytesRef>> fst) {\n+            this.preserveSep = preserveSep;\n+            this.preservePositionIncrements = preservePositionIncrements;\n+            this.maxSurfaceFormsPerAnalyzedForm = maxSurfaceFormsPerAnalyzedForm;\n+            this.maxGraphExpansions = maxGraphExpansions;\n+            this.hasPayloads = hasPayloads;\n+            this.maxAnalyzedPathsForOneInput = maxAnalyzedPathsForOneInput;\n+            this.fst = fst;\n+        }\n+\n+    }\n+    */\n+\n+    @Override\n+    public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {\n+        return prototype.toFiniteStrings(prototype.getTokenStreamToAutomaton(), stream);\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/src/test/java/org/elasticsearch/search/suggest/CompletionPostingsFormatTest.java b/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java\nsimilarity index 78%\nrename from src/test/java/org/elasticsearch/search/suggest/CompletionPostingsFormatTest.java\nrename to src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java\nindex 18249dc1c54..8f0660a5f66 100644\n--- a/src/test/java/org/elasticsearch/search/suggest/CompletionPostingsFormatTest.java\n+++ b/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java\n@@ -17,8 +17,9 @@\n  * under the License.\n  */\n \n-package org.elasticsearch.search.suggest;\n+package org.elasticsearch.search.suggest.completion;\n \n+import com.google.common.collect.Lists;\n import org.apache.lucene.analysis.standard.StandardAnalyzer;\n import org.apache.lucene.codecs.*;\n import org.apache.lucene.document.Document;\n@@ -30,10 +31,7 @@ import org.apache.lucene.search.suggest.Lookup;\n import org.apache.lucene.search.suggest.Lookup.LookupResult;\n import org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester;\n import org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester;\n-import org.apache.lucene.store.IOContext;\n-import org.apache.lucene.store.IndexInput;\n-import org.apache.lucene.store.IndexOutput;\n-import org.apache.lucene.store.RAMDirectory;\n+import org.apache.lucene.store.*;\n import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.LineFileDocs;\n import org.elasticsearch.index.analysis.NamedAnalyzer;\n@@ -42,10 +40,8 @@ import org.elasticsearch.index.codec.postingsformat.PostingsFormatProvider;\n import org.elasticsearch.index.codec.postingsformat.PreBuiltPostingsFormatProvider;\n import org.elasticsearch.index.mapper.FieldMapper.Names;\n import org.elasticsearch.index.mapper.core.CompletionFieldMapper;\n-import org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider;\n-import org.elasticsearch.search.suggest.completion.Completion090PostingsFormat;\n+import org.elasticsearch.search.suggest.SuggestUtils;\n import org.elasticsearch.search.suggest.completion.Completion090PostingsFormat.LookupFactory;\n-import org.elasticsearch.search.suggest.completion.CompletionSuggestionContext;\n import org.elasticsearch.test.ElasticsearchTestCase;\n import org.junit.Test;\n \n@@ -56,37 +52,22 @@ import java.util.HashMap;\n import java.util.List;\n \n import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n \n public class CompletionPostingsFormatTest extends ElasticsearchTestCase {\n \n     @Test\n     public void testCompletionPostingsFormat() throws IOException {\n-        AnalyzingCompletionLookupProvider provider = new AnalyzingCompletionLookupProvider(true, false, true, true);\n-        RAMDirectory dir = new RAMDirectory();\n-        IndexOutput output = dir.createOutput(\"foo.txt\", IOContext.DEFAULT);\n-        FieldsConsumer consumer = provider.consumer(output);\n-        FieldInfo fieldInfo = new FieldInfo(\"foo\", true, 1, false, true, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n-                DocValuesType.SORTED, DocValuesType.BINARY, new HashMap<String, String>());\n-        TermsConsumer addField = consumer.addField(fieldInfo);\n+        AnalyzingCompletionLookupProviderV1 providerV1 = new AnalyzingCompletionLookupProviderV1(true, false, true, true);\n+        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);\n+        List<Completion090PostingsFormat.CompletionLookupProvider> providers = Lists.newArrayList(providerV1, currentProvider);\n \n-        PostingsConsumer postingsConsumer = addField.startTerm(new BytesRef(\"foofightersgenerator\"));\n-        postingsConsumer.startDoc(0, 1);\n-        postingsConsumer.addPosition(256 - 2, provider.buildPayload(new BytesRef(\"Generator - Foo Fighters\"), 9, new BytesRef(\"id:10\")), 0,\n-                1);\n-        postingsConsumer.finishDoc();\n-        addField.finishTerm(new BytesRef(\"foofightersgenerator\"), new TermStats(1, 1));\n-        addField.startTerm(new BytesRef(\"generator\"));\n-        postingsConsumer.startDoc(0, 1);\n-        postingsConsumer.addPosition(256 - 1, provider.buildPayload(new BytesRef(\"Generator - Foo Fighters\"), 9, new BytesRef(\"id:10\")), 0,\n-                1);\n-        postingsConsumer.finishDoc();\n-        addField.finishTerm(new BytesRef(\"generator\"), new TermStats(1, 1));\n-        addField.finish(1, 1, 1);\n-        consumer.close();\n-        output.close();\n+        Completion090PostingsFormat.CompletionLookupProvider randomProvider = providers.get(getRandom().nextInt(providers.size()));\n+        RAMDirectory dir = new RAMDirectory();\n+        writeData(dir, randomProvider);\n \n         IndexInput input = dir.openInput(\"foo.txt\", IOContext.DEFAULT);\n-        LookupFactory load = provider.load(input);\n+        LookupFactory load = currentProvider.load(input);\n         PostingsFormatProvider format = new PreBuiltPostingsFormatProvider(new ElasticSearch090PostingsFormat());\n         NamedAnalyzer analyzer = new NamedAnalyzer(\"foo\", new StandardAnalyzer(TEST_VERSION_CURRENT));\n         Lookup lookup = load.getLookup(new CompletionFieldMapper(new Names(\"foo\"), analyzer, analyzer, format, null, true, true, true, Integer.MAX_VALUE), new CompletionSuggestionContext(null));\n@@ -96,6 +77,43 @@ public class CompletionPostingsFormatTest extends ElasticsearchTestCase {\n         dir.close();\n     }\n \n+    @Test\n+    public void testProviderBackwardCompatibilityForVersion1() throws IOException {\n+        AnalyzingCompletionLookupProviderV1 providerV1 = new AnalyzingCompletionLookupProviderV1(true, false, true, true);\n+        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);\n+\n+        RAMDirectory dir = new RAMDirectory();\n+        writeData(dir, providerV1);\n+\n+        IndexInput input = dir.openInput(\"foo.txt\", IOContext.DEFAULT);\n+        LookupFactory load = currentProvider.load(input);\n+        PostingsFormatProvider format = new PreBuiltPostingsFormatProvider(new ElasticSearch090PostingsFormat());\n+        NamedAnalyzer analyzer = new NamedAnalyzer(\"foo\", new StandardAnalyzer(TEST_VERSION_CURRENT));\n+        AnalyzingCompletionLookupProvider.AnalyzingSuggestHolder analyzingSuggestHolder = load.getAnalyzingSuggestHolder(new CompletionFieldMapper(new Names(\"foo\"), analyzer, analyzer, format, null, true, true, true, Integer.MAX_VALUE));\n+        assertThat(analyzingSuggestHolder.sepLabel, is(AnalyzingCompletionLookupProviderV1.SEP_LABEL));\n+        assertThat(analyzingSuggestHolder.payloadSep, is(AnalyzingCompletionLookupProviderV1.PAYLOAD_SEP));\n+        assertThat(analyzingSuggestHolder.endByte, is(AnalyzingCompletionLookupProviderV1.END_BYTE));\n+        dir.close();\n+    }\n+\n+    @Test\n+    public void testProviderVersion2() throws IOException {\n+        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);\n+\n+        RAMDirectory dir = new RAMDirectory();\n+        writeData(dir, currentProvider);\n+\n+        IndexInput input = dir.openInput(\"foo.txt\", IOContext.DEFAULT);\n+        LookupFactory load = currentProvider.load(input);\n+        PostingsFormatProvider format = new PreBuiltPostingsFormatProvider(new ElasticSearch090PostingsFormat());\n+        NamedAnalyzer analyzer = new NamedAnalyzer(\"foo\", new StandardAnalyzer(TEST_VERSION_CURRENT));\n+        AnalyzingCompletionLookupProvider.AnalyzingSuggestHolder analyzingSuggestHolder = load.getAnalyzingSuggestHolder(new CompletionFieldMapper(new Names(\"foo\"), analyzer, analyzer, format, null, true, true, true, Integer.MAX_VALUE));\n+        assertThat(analyzingSuggestHolder.sepLabel, is(XAnalyzingSuggester.SEP_LABEL));\n+        assertThat(analyzingSuggestHolder.payloadSep, is(XAnalyzingSuggester.PAYLOAD_SEP));\n+        assertThat(analyzingSuggestHolder.endByte, is(XAnalyzingSuggester.END_BYTE));\n+        dir.close();\n+    }\n+\n     @Test\n     public void testDuellCompletions() throws IOException, NoSuchFieldException, SecurityException, IllegalArgumentException,\n             IllegalAccessException {\n@@ -105,8 +123,7 @@ public class CompletionPostingsFormatTest extends ElasticsearchTestCase {\n         final int options = preserveSeparators ? AnalyzingSuggester.PRESERVE_SEP : 0;\n \n         XAnalyzingSuggester reference = new XAnalyzingSuggester(new StandardAnalyzer(TEST_VERSION_CURRENT), new StandardAnalyzer(\n-                TEST_VERSION_CURRENT), options, 256, -1, null, false, 1);\n-        reference.setPreservePositionIncrements(preservePositionIncrements);\n+                TEST_VERSION_CURRENT), options, 256, -1, preservePositionIncrements, null, false, 1, XAnalyzingSuggester.SEP_LABEL, XAnalyzingSuggester.PAYLOAD_SEP, XAnalyzingSuggester.END_BYTE);\n         LineFileDocs docs = new LineFileDocs(getRandom());\n         int num = atLeast(150);\n         final String[] titles = new String[num];\n@@ -283,4 +300,28 @@ public class CompletionPostingsFormatTest extends ElasticsearchTestCase {\n     }\n \n     // TODO ADD more unittests\n+    private void writeData(Directory dir, Completion090PostingsFormat.CompletionLookupProvider provider) throws IOException {\n+        IndexOutput output = dir.createOutput(\"foo.txt\", IOContext.DEFAULT);\n+        FieldsConsumer consumer = provider.consumer(output);\n+        FieldInfo fieldInfo = new FieldInfo(\"foo\", true, 1, false, true, true, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS,\n+                DocValuesType.SORTED, DocValuesType.BINARY, new HashMap<String, String>());\n+        TermsConsumer addField = consumer.addField(fieldInfo);\n+\n+        PostingsConsumer postingsConsumer = addField.startTerm(new BytesRef(\"foofightersgenerator\"));\n+        postingsConsumer.startDoc(0, 1);\n+        postingsConsumer.addPosition(256 - 2, provider.buildPayload(new BytesRef(\"Generator - Foo Fighters\"), 9, new BytesRef(\"id:10\")), 0,\n+                1);\n+        postingsConsumer.finishDoc();\n+        addField.finishTerm(new BytesRef(\"foofightersgenerator\"), new TermStats(1, 1));\n+        addField.startTerm(new BytesRef(\"generator\"));\n+        postingsConsumer.startDoc(0, 1);\n+        postingsConsumer.addPosition(256 - 1, provider.buildPayload(new BytesRef(\"Generator - Foo Fighters\"), 9, new BytesRef(\"id:10\")), 0,\n+                1);\n+        postingsConsumer.finishDoc();\n+        addField.finishTerm(new BytesRef(\"generator\"), new TermStats(1, 1));\n+        addField.finish(1, 1, 1);\n+        consumer.close();\n+        output.close();\n+\n+    }\n }"
}