{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 114869,
  "base_commit": "2aec12c17383de5da35664d9160904b668944364",
  "head_commit": "5964c569ec8c5dd2e492db6db87556c160cfad7a",
  "task_instructions": "You are working on a codebase. Your task is to make the necessary code changes to accomplish the following:\n\nStandardize error code when bulk body is invalid\n\nCurrently the incremental and non-incremental bulk variations will\r\nreturn different error codes when the json body provided is invalid.\r\nThis commit ensures both version return status code 400. Additionally,\r\nthis renames the incremental rest tests to bulk tests and ensures that\r\nall tests work with both bulk api versions. We set these tests to\r\nrandomize which version of the api we test each run.\n\nPlease make all necessary code changes to complete this task.",
  "ground_truth_diff": "diff --git a/docs/changelog/114869.yaml b/docs/changelog/114869.yaml\nnew file mode 100644\nindex 000000000000..755418e7ce4d\n--- /dev/null\n+++ b/docs/changelog/114869.yaml\n@@ -0,0 +1,5 @@\n+pr: 114869\n+summary: Standardize error code when bulk body is invalid\n+area: CRUD\n+type: bug\n+issues: []\ndiff --git a/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java b/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/BulkRestIT.java\nsimilarity index 81%\nrename from qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\nrename to qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/BulkRestIT.java\nindex da0501169627..369d0824bdb2 100644\n--- a/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\n+++ b/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/BulkRestIT.java\n@@ -9,6 +9,8 @@\n \n package org.elasticsearch.http;\n \n+import org.apache.http.entity.ByteArrayEntity;\n+import org.apache.http.entity.ContentType;\n import org.elasticsearch.action.bulk.IncrementalBulkService;\n import org.elasticsearch.client.Request;\n import org.elasticsearch.client.Response;\n@@ -19,24 +21,30 @@ import org.elasticsearch.test.ESIntegTestCase;\n import org.elasticsearch.xcontent.json.JsonXContent;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.util.List;\n import java.util.Map;\n \n+import static org.elasticsearch.rest.RestStatus.BAD_REQUEST;\n import static org.elasticsearch.rest.RestStatus.OK;\n import static org.hamcrest.CoreMatchers.containsString;\n import static org.hamcrest.Matchers.equalTo;\n \n @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE, supportsDedicatedMasters = false, numDataNodes = 2, numClientNodes = 0)\n-public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n+public class BulkRestIT extends HttpSmokeTestCase {\n \n     @Override\n     protected Settings nodeSettings(int nodeOrdinal, Settings otherSettings) {\n         return Settings.builder()\n             .put(super.nodeSettings(nodeOrdinal, otherSettings))\n-            .put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), true)\n+            .put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), seventyFivePercentOfTheTime())\n             .build();\n     }\n \n+    private static boolean seventyFivePercentOfTheTime() {\n+        return (randomBoolean() && randomBoolean()) == false;\n+    }\n+\n     public void testBulkUriMatchingDoesNotMatchBulkCapabilitiesApi() throws IOException {\n         Request request = new Request(\"GET\", \"/_capabilities?method=GET&path=%2F_bulk&capabilities=failure_store_status&pretty\");\n         Response response = getRestClient().performRequest(request);\n@@ -51,6 +59,26 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n         assertThat(responseException.getMessage(), containsString(\"request body is required\"));\n     }\n \n+    public void testBulkInvalidIndexNameString() throws IOException {\n+        Request request = new Request(\"POST\", \"/_bulk\");\n+\n+        byte[] bytes1 = \"{\\\"create\\\":{\\\"_index\\\":\\\"\".getBytes(StandardCharsets.UTF_8);\n+        byte[] bytes2 = new byte[] { (byte) 0xfe, (byte) 0xfe, (byte) 0xff, (byte) 0xff };\n+        byte[] bytes3 = \"\\\",\\\"_id\\\":\\\"1\\\"}}\\n{\\\"field\\\":1}\\n\\r\\n\".getBytes(StandardCharsets.UTF_8);\n+        byte[] bulkBody = new byte[bytes1.length + bytes2.length + bytes3.length];\n+        System.arraycopy(bytes1, 0, bulkBody, 0, bytes1.length);\n+        System.arraycopy(bytes2, 0, bulkBody, bytes1.length, bytes2.length);\n+        System.arraycopy(bytes3, 0, bulkBody, bytes1.length + bytes2.length, bytes3.length);\n+\n+        request.setEntity(new ByteArrayEntity(bulkBody, ContentType.APPLICATION_JSON));\n+\n+        ResponseException responseException = expectThrows(ResponseException.class, () -> getRestClient().performRequest(request));\n+        assertThat(responseException.getResponse().getStatusLine().getStatusCode(), equalTo(BAD_REQUEST.getStatus()));\n+        assertThat(responseException.getMessage(), containsString(\"could not parse bulk request body\"));\n+        assertThat(responseException.getMessage(), containsString(\"json_parse_exception\"));\n+        assertThat(responseException.getMessage(), containsString(\"Invalid UTF-8\"));\n+    }\n+\n     public void testBulkRequestBodyImproperlyTerminated() throws IOException {\n         Request request = new Request(randomBoolean() ? \"POST\" : \"PUT\", \"/_bulk\");\n         // missing final line of the bulk body. cannot process\n@@ -61,10 +89,10 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n         );\n         ResponseException responseException = expectThrows(ResponseException.class, () -> getRestClient().performRequest(request));\n         assertEquals(400, responseException.getResponse().getStatusLine().getStatusCode());\n-        assertThat(responseException.getMessage(), containsString(\"could not parse bulk request body\"));\n+        assertThat(responseException.getMessage(), containsString(\"The bulk request must be terminated by a newline\"));\n     }\n \n-    public void testIncrementalBulk() throws IOException {\n+    public void testBulkRequest() throws IOException {\n         Request createRequest = new Request(\"PUT\", \"/index_name\");\n         createRequest.setJsonEntity(\"\"\"\n             {\n@@ -81,7 +109,6 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n \n         Request firstBulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n \n-        // index documents for the rollup job\n         String bulkBody = \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\"\n             + \"{\\\"field\\\":1}\\n\"\n             + \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}}\\n\"\n@@ -113,7 +140,6 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n \n         Request firstBulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n \n-        // index documents for the rollup job\n         String bulkBody = \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\"\n             + \"{\\\"field\\\":1}\\n\"\n             + \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}}\\n\"\n@@ -137,7 +163,7 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n         }\n     }\n \n-    public void testIncrementalMalformed() throws IOException {\n+    public void testMalformedActionLineBulk() throws IOException {\n         Request createRequest = new Request(\"PUT\", \"/index_name\");\n         createRequest.setJsonEntity(\"\"\"\n             {\n@@ -154,7 +180,6 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n \n         Request bulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n \n-        // index documents for the rollup job\n         final StringBuilder bulk = new StringBuilder();\n         bulk.append(\"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\"}}\\n\");\n         bulk.append(\"{\\\"field\\\":1}\\n\");\n@@ -170,7 +195,6 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n     private static void sendLargeBulk() throws IOException {\n         Request bulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n \n-        // index documents for the rollup job\n         final StringBuilder bulk = new StringBuilder();\n         bulk.append(\"{\\\"delete\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\");\n         int updates = 0;\ndiff --git a/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java b/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\nindex 03694c7442d4..1e80e6de60d6 100644\n--- a/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\n+++ b/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\n@@ -104,19 +104,23 @@ public class RestBulkAction extends BaseRestHandler {\n             boolean defaultRequireDataStream = request.paramAsBoolean(DocWriteRequest.REQUIRE_DATA_STREAM, false);\n             bulkRequest.timeout(request.paramAsTime(\"timeout\", BulkShardRequest.DEFAULT_TIMEOUT));\n             bulkRequest.setRefreshPolicy(request.param(\"refresh\"));\n-            bulkRequest.add(\n-                request.requiredContent(),\n-                defaultIndex,\n-                defaultRouting,\n-                defaultFetchSourceContext,\n-                defaultPipeline,\n-                defaultRequireAlias,\n-                defaultRequireDataStream,\n-                defaultListExecutedPipelines,\n-                allowExplicitIndex,\n-                request.getXContentType(),\n-                request.getRestApiVersion()\n-            );\n+            try {\n+                bulkRequest.add(\n+                    request.requiredContent(),\n+                    defaultIndex,\n+                    defaultRouting,\n+                    defaultFetchSourceContext,\n+                    defaultPipeline,\n+                    defaultRequireAlias,\n+                    defaultRequireDataStream,\n+                    defaultListExecutedPipelines,\n+                    allowExplicitIndex,\n+                    request.getXContentType(),\n+                    request.getRestApiVersion()\n+                );\n+            } catch (Exception e) {\n+                return channel -> new RestToXContentListener<>(channel).onFailure(parseFailureException(e));\n+            }\n \n             return channel -> client.bulk(bulkRequest, new RestRefCountedChunkedToXContentListener<>(channel));\n         } else {\n@@ -127,6 +131,15 @@ public class RestBulkAction extends BaseRestHandler {\n         }\n     }\n \n+    private static Exception parseFailureException(Exception e) {\n+        if (e instanceof IllegalArgumentException) {\n+            return e;\n+        } else {\n+            // TODO: Maybe improve in follow-up to be XContentParseException and include line number and column\n+            return new ElasticsearchParseException(\"could not parse bulk request body\", e);\n+        }\n+    }\n+\n     static class ChunkHandler implements BaseRestHandler.RequestBodyChunkConsumer {\n \n         private final boolean allowExplicitIndex;\n@@ -219,9 +232,7 @@ public class RestBulkAction extends BaseRestHandler {\n \n                 } catch (Exception e) {\n                     shortCircuit();\n-                    new RestToXContentListener<>(channel).onFailure(\n-                        new ElasticsearchParseException(\"could not parse bulk request body\", e)\n-                    );\n+                    new RestToXContentListener<>(channel).onFailure(parseFailureException(e));\n                     return;\n                 }\n             }",
  "judge_mode": "comparative",
  "judge_model": "openai/claude-sonnet-4-5-20250929",
  "test_label": "v0",
  "agent_results": [
    {
      "runner": "auggie",
      "model": "sonnet4.5",
      "edit_run_id": "a9463435",
      "status": "success",
      "elapsed_ms": 292719,
      "patch_unified": "diff --git a/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java b/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\ndeleted file mode 100644\nindex da050116..00000000\n--- a/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\n+++ /dev/null\n@@ -1,201 +0,0 @@\n-/*\n- * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n- * or more contributor license agreements. Licensed under the \"Elastic License\n- * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n- * Public License v 1\"; you may not use this file except in compliance with, at\n- * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n- * License v3.0 only\", or the \"Server Side Public License, v 1\".\n- */\n-\n-package org.elasticsearch.http;\n-\n-import org.elasticsearch.action.bulk.IncrementalBulkService;\n-import org.elasticsearch.client.Request;\n-import org.elasticsearch.client.Response;\n-import org.elasticsearch.client.ResponseException;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.common.xcontent.XContentHelper;\n-import org.elasticsearch.test.ESIntegTestCase;\n-import org.elasticsearch.xcontent.json.JsonXContent;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.Map;\n-\n-import static org.elasticsearch.rest.RestStatus.OK;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.Matchers.equalTo;\n-\n-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE, supportsDedicatedMasters = false, numDataNodes = 2, numClientNodes = 0)\n-public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n-\n-    @Override\n-    protected Settings nodeSettings(int nodeOrdinal, Settings otherSettings) {\n-        return Settings.builder()\n-            .put(super.nodeSettings(nodeOrdinal, otherSettings))\n-            .put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), true)\n-            .build();\n-    }\n-\n-    public void testBulkUriMatchingDoesNotMatchBulkCapabilitiesApi() throws IOException {\n-        Request request = new Request(\"GET\", \"/_capabilities?method=GET&path=%2F_bulk&capabilities=failure_store_status&pretty\");\n-        Response response = getRestClient().performRequest(request);\n-        assertEquals(200, response.getStatusLine().getStatusCode());\n-    }\n-\n-    public void testBulkMissingBody() throws IOException {\n-        Request request = new Request(randomBoolean() ? \"POST\" : \"PUT\", \"/_bulk\");\n-        request.setJsonEntity(\"\");\n-        ResponseException responseException = expectThrows(ResponseException.class, () -> getRestClient().performRequest(request));\n-        assertEquals(400, responseException.getResponse().getStatusLine().getStatusCode());\n-        assertThat(responseException.getMessage(), containsString(\"request body is required\"));\n-    }\n-\n-    public void testBulkRequestBodyImproperlyTerminated() throws IOException {\n-        Request request = new Request(randomBoolean() ? \"POST\" : \"PUT\", \"/_bulk\");\n-        // missing final line of the bulk body. cannot process\n-        request.setJsonEntity(\n-            \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\"\n-                + \"{\\\"field\\\":1}\\n\"\n-                + \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}\"\n-        );\n-        ResponseException responseException = expectThrows(ResponseException.class, () -> getRestClient().performRequest(request));\n-        assertEquals(400, responseException.getResponse().getStatusLine().getStatusCode());\n-        assertThat(responseException.getMessage(), containsString(\"could not parse bulk request body\"));\n-    }\n-\n-    public void testIncrementalBulk() throws IOException {\n-        Request createRequest = new Request(\"PUT\", \"/index_name\");\n-        createRequest.setJsonEntity(\"\"\"\n-            {\n-              \"settings\": {\n-                \"index\": {\n-                  \"number_of_shards\": 1,\n-                  \"number_of_replicas\": 1,\n-                  \"write.wait_for_active_shards\": 2\n-                }\n-              }\n-            }\"\"\");\n-        final Response indexCreatedResponse = getRestClient().performRequest(createRequest);\n-        assertThat(indexCreatedResponse.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        Request firstBulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n-\n-        // index documents for the rollup job\n-        String bulkBody = \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\"\n-            + \"{\\\"field\\\":1}\\n\"\n-            + \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}}\\n\"\n-            + \"{\\\"field\\\":1}\\n\"\n-            + \"\\r\\n\";\n-\n-        firstBulkRequest.setJsonEntity(bulkBody);\n-\n-        final Response indexSuccessFul = getRestClient().performRequest(firstBulkRequest);\n-        assertThat(indexSuccessFul.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        sendLargeBulk();\n-    }\n-\n-    public void testBulkWithIncrementalDisabled() throws IOException {\n-        Request createRequest = new Request(\"PUT\", \"/index_name\");\n-        createRequest.setJsonEntity(\"\"\"\n-            {\n-              \"settings\": {\n-                \"index\": {\n-                  \"number_of_shards\": 1,\n-                  \"number_of_replicas\": 1,\n-                  \"write.wait_for_active_shards\": 2\n-                }\n-              }\n-            }\"\"\");\n-        final Response indexCreatedResponse = getRestClient().performRequest(createRequest);\n-        assertThat(indexCreatedResponse.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        Request firstBulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n-\n-        // index documents for the rollup job\n-        String bulkBody = \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\"\n-            + \"{\\\"field\\\":1}\\n\"\n-            + \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}}\\n\"\n-            + \"{\\\"field\\\":1}\\n\"\n-            + \"\\r\\n\";\n-\n-        firstBulkRequest.setJsonEntity(bulkBody);\n-\n-        final Response indexSuccessFul = getRestClient().performRequest(firstBulkRequest);\n-        assertThat(indexSuccessFul.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        updateClusterSettings(Settings.builder().put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), false));\n-\n-        internalCluster().getInstances(IncrementalBulkService.class).forEach(i -> i.setForTests(false));\n-\n-        try {\n-            sendLargeBulk();\n-        } finally {\n-            internalCluster().getInstances(IncrementalBulkService.class).forEach(i -> i.setForTests(true));\n-            updateClusterSettings(Settings.builder().put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), (String) null));\n-        }\n-    }\n-\n-    public void testIncrementalMalformed() throws IOException {\n-        Request createRequest = new Request(\"PUT\", \"/index_name\");\n-        createRequest.setJsonEntity(\"\"\"\n-            {\n-              \"settings\": {\n-                \"index\": {\n-                  \"number_of_shards\": 1,\n-                  \"number_of_replicas\": 1,\n-                  \"write.wait_for_active_shards\": 2\n-                }\n-              }\n-            }\"\"\");\n-        final Response indexCreatedResponse = getRestClient().performRequest(createRequest);\n-        assertThat(indexCreatedResponse.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        Request bulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n-\n-        // index documents for the rollup job\n-        final StringBuilder bulk = new StringBuilder();\n-        bulk.append(\"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\"}}\\n\");\n-        bulk.append(\"{\\\"field\\\":1}\\n\");\n-        bulk.append(\"{}\\n\");\n-        bulk.append(\"\\r\\n\");\n-\n-        bulkRequest.setJsonEntity(bulk.toString());\n-\n-        expectThrows(ResponseException.class, () -> getRestClient().performRequest(bulkRequest));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static void sendLargeBulk() throws IOException {\n-        Request bulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n-\n-        // index documents for the rollup job\n-        final StringBuilder bulk = new StringBuilder();\n-        bulk.append(\"{\\\"delete\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\");\n-        int updates = 0;\n-        for (int i = 0; i < 1000; i++) {\n-            bulk.append(\"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\"}}\\n\");\n-            bulk.append(\"{\\\"field\\\":\").append(i).append(\"}\\n\");\n-            if (randomBoolean() && randomBoolean() && randomBoolean() && randomBoolean()) {\n-                ++updates;\n-                bulk.append(\"{\\\"update\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}}\\n\");\n-                bulk.append(\"{\\\"doc\\\":{\\\"field\\\":\").append(i).append(\"}}\\n\");\n-            }\n-        }\n-        bulk.append(\"\\r\\n\");\n-\n-        bulkRequest.setJsonEntity(bulk.toString());\n-\n-        final Response bulkResponse = getRestClient().performRequest(bulkRequest);\n-        assertThat(bulkResponse.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-        Map<String, Object> responseMap = XContentHelper.convertToMap(\n-            JsonXContent.jsonXContent,\n-            bulkResponse.getEntity().getContent(),\n-            true\n-        );\n-\n-        assertFalse((Boolean) responseMap.get(\"errors\"));\n-        assertThat(((List<Object>) responseMap.get(\"items\")).size(), equalTo(1001 + updates));\n-    }\n-}\ndiff --git a/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java b/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\nindex 03694c74..c8e693df 100644\n--- a/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\n+++ b/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\n@@ -104,19 +104,23 @@ public class RestBulkAction extends BaseRestHandler {\n             boolean defaultRequireDataStream = request.paramAsBoolean(DocWriteRequest.REQUIRE_DATA_STREAM, false);\n             bulkRequest.timeout(request.paramAsTime(\"timeout\", BulkShardRequest.DEFAULT_TIMEOUT));\n             bulkRequest.setRefreshPolicy(request.param(\"refresh\"));\n-            bulkRequest.add(\n-                request.requiredContent(),\n-                defaultIndex,\n-                defaultRouting,\n-                defaultFetchSourceContext,\n-                defaultPipeline,\n-                defaultRequireAlias,\n-                defaultRequireDataStream,\n-                defaultListExecutedPipelines,\n-                allowExplicitIndex,\n-                request.getXContentType(),\n-                request.getRestApiVersion()\n-            );\n+            try {\n+                bulkRequest.add(\n+                    request.requiredContent(),\n+                    defaultIndex,\n+                    defaultRouting,\n+                    defaultFetchSourceContext,\n+                    defaultPipeline,\n+                    defaultRequireAlias,\n+                    defaultRequireDataStream,\n+                    defaultListExecutedPipelines,\n+                    allowExplicitIndex,\n+                    request.getXContentType(),\n+                    request.getRestApiVersion()\n+                );\n+            } catch (Exception e) {\n+                throw new ElasticsearchParseException(\"could not parse bulk request body\", e);\n+            }\n \n             return channel -> client.bulk(bulkRequest, new RestRefCountedChunkedToXContentListener<>(channel));\n         } else {",
      "scores": {
        "correctness": -0.3,
        "completeness": -0.4,
        "code_reuse": 0.5,
        "best_practices": -0.2,
        "unsolicited_docs": 1.0
      },
      "aggregate": 0.12000000000000002,
      "rationale": "The agent made critical errors: (1) Deleted the entire test file instead of renaming it, losing all test content; (2) Used throw instead of return channel -> ... pattern in RestBulkAction, breaking the async handler contract; (3) Missing the parseFailureException helper method that handles IllegalArgumentException vs other exceptions differently; (4) Missing all test enhancements (new testBulkInvalidIndexNameString test, randomization logic, test method renames); (5) Missing changelog file. The only correct change was wrapping bulkRequest.add() in try-catch, but even that was implemented incorrectly.",
      "llm_rating": 0.22,
      "llm_summary": "Agent deleted IncrementalBulkRestIT.java entirely instead of renaming to BulkRestIT.java and added try-catch in RestBulkAction but used throw instead of return channel pattern, breaking async handler contract and missing parseFailureException helper method.",
      "errors": []
    },
    {
      "runner": "claude-code",
      "model": "claude-sonnet-4-5",
      "edit_run_id": "e9a2f06b",
      "status": "success",
      "elapsed_ms": 386673,
      "patch_unified": "diff --git a/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java b/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\nindex da050116..8c5d4ba0 100644\n--- a/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\n+++ b/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\n@@ -27,13 +27,15 @@ import static org.hamcrest.CoreMatchers.containsString;\n import static org.hamcrest.Matchers.equalTo;\n \n @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE, supportsDedicatedMasters = false, numDataNodes = 2, numClientNodes = 0)\n-public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n+public class BulkRestIT extends HttpSmokeTestCase {\n+\n+    private final boolean useIncrementalBulk = randomBoolean();\n \n     @Override\n     protected Settings nodeSettings(int nodeOrdinal, Settings otherSettings) {\n         return Settings.builder()\n             .put(super.nodeSettings(nodeOrdinal, otherSettings))\n-            .put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), true)\n+            .put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), useIncrementalBulk)\n             .build();\n     }\n \n@@ -64,7 +66,7 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n         assertThat(responseException.getMessage(), containsString(\"could not parse bulk request body\"));\n     }\n \n-    public void testIncrementalBulk() throws IOException {\n+    public void testBulk() throws IOException {\n         Request createRequest = new Request(\"PUT\", \"/index_name\");\n         createRequest.setJsonEntity(\"\"\"\n             {\n@@ -137,7 +139,7 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n         }\n     }\n \n-    public void testIncrementalMalformed() throws IOException {\n+    public void testBulkMalformed() throws IOException {\n         Request createRequest = new Request(\"PUT\", \"/index_name\");\n         createRequest.setJsonEntity(\"\"\"\n             {\n@@ -163,7 +165,9 @@ public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n \n         bulkRequest.setJsonEntity(bulk.toString());\n \n-        expectThrows(ResponseException.class, () -> getRestClient().performRequest(bulkRequest));\n+        ResponseException responseException = expectThrows(ResponseException.class, () -> getRestClient().performRequest(bulkRequest));\n+        assertEquals(400, responseException.getResponse().getStatusLine().getStatusCode());\n+        assertThat(responseException.getMessage(), containsString(\"could not parse bulk request body\"));\n     }\n \n     @SuppressWarnings(\"unchecked\")\ndiff --git a/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java b/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\nindex 03694c74..c8e693df 100644\n--- a/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\n+++ b/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\n@@ -104,19 +104,23 @@ public class RestBulkAction extends BaseRestHandler {\n             boolean defaultRequireDataStream = request.paramAsBoolean(DocWriteRequest.REQUIRE_DATA_STREAM, false);\n             bulkRequest.timeout(request.paramAsTime(\"timeout\", BulkShardRequest.DEFAULT_TIMEOUT));\n             bulkRequest.setRefreshPolicy(request.param(\"refresh\"));\n-            bulkRequest.add(\n-                request.requiredContent(),\n-                defaultIndex,\n-                defaultRouting,\n-                defaultFetchSourceContext,\n-                defaultPipeline,\n-                defaultRequireAlias,\n-                defaultRequireDataStream,\n-                defaultListExecutedPipelines,\n-                allowExplicitIndex,\n-                request.getXContentType(),\n-                request.getRestApiVersion()\n-            );\n+            try {\n+                bulkRequest.add(\n+                    request.requiredContent(),\n+                    defaultIndex,\n+                    defaultRouting,\n+                    defaultFetchSourceContext,\n+                    defaultPipeline,\n+                    defaultRequireAlias,\n+                    defaultRequireDataStream,\n+                    defaultListExecutedPipelines,\n+                    allowExplicitIndex,\n+                    request.getXContentType(),\n+                    request.getRestApiVersion()\n+                );\n+            } catch (Exception e) {\n+                throw new ElasticsearchParseException(\"could not parse bulk request body\", e);\n+            }\n \n             return channel -> client.bulk(bulkRequest, new RestRefCountedChunkedToXContentListener<>(channel));\n         } else {",
      "scores": {
        "correctness": 0.3,
        "completeness": -0.2,
        "code_reuse": 0.8,
        "best_practices": 0.0,
        "unsolicited_docs": 1.0
      },
      "aggregate": 0.38,
      "rationale": "The agent correctly renamed the test file and added try-catch error handling in RestBulkAction, but has several issues: (1) Uses instance variable for randomization instead of method-level randomization, which breaks test isolation; (2) Throws exception directly instead of returning channel callback with error handler, breaking the async pattern; (3) Missing the new testBulkInvalidIndexNameString test; (4) Missing changelog file; (5) Missing helper method parseFailureException and its logic to differentiate IllegalArgumentException; (6) Missing updates to ChunkHandler error handling. The core error standardization is partially implemented but the execution pattern is incorrect.",
      "llm_rating": 0.38,
      "llm_summary": "Renamed IncrementalBulkRestIT to BulkRestIT and added try-catch in RestBulkAction.prepareRequest but used instance variable for randomization breaking test isolation and threw exception directly instead of using async channel callback pattern required by ground truth.",
      "errors": []
    },
    {
      "runner": "factory",
      "model": "claude-sonnet-4-5-20250929",
      "edit_run_id": "074538b9",
      "status": "success",
      "elapsed_ms": 330405,
      "patch_unified": "diff --git a/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java b/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\ndeleted file mode 100644\nindex da050116..00000000\n--- a/qa/smoke-test-http/src/javaRestTest/java/org/elasticsearch/http/IncrementalBulkRestIT.java\n+++ /dev/null\n@@ -1,201 +0,0 @@\n-/*\n- * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n- * or more contributor license agreements. Licensed under the \"Elastic License\n- * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n- * Public License v 1\"; you may not use this file except in compliance with, at\n- * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n- * License v3.0 only\", or the \"Server Side Public License, v 1\".\n- */\n-\n-package org.elasticsearch.http;\n-\n-import org.elasticsearch.action.bulk.IncrementalBulkService;\n-import org.elasticsearch.client.Request;\n-import org.elasticsearch.client.Response;\n-import org.elasticsearch.client.ResponseException;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.common.xcontent.XContentHelper;\n-import org.elasticsearch.test.ESIntegTestCase;\n-import org.elasticsearch.xcontent.json.JsonXContent;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.Map;\n-\n-import static org.elasticsearch.rest.RestStatus.OK;\n-import static org.hamcrest.CoreMatchers.containsString;\n-import static org.hamcrest.Matchers.equalTo;\n-\n-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE, supportsDedicatedMasters = false, numDataNodes = 2, numClientNodes = 0)\n-public class IncrementalBulkRestIT extends HttpSmokeTestCase {\n-\n-    @Override\n-    protected Settings nodeSettings(int nodeOrdinal, Settings otherSettings) {\n-        return Settings.builder()\n-            .put(super.nodeSettings(nodeOrdinal, otherSettings))\n-            .put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), true)\n-            .build();\n-    }\n-\n-    public void testBulkUriMatchingDoesNotMatchBulkCapabilitiesApi() throws IOException {\n-        Request request = new Request(\"GET\", \"/_capabilities?method=GET&path=%2F_bulk&capabilities=failure_store_status&pretty\");\n-        Response response = getRestClient().performRequest(request);\n-        assertEquals(200, response.getStatusLine().getStatusCode());\n-    }\n-\n-    public void testBulkMissingBody() throws IOException {\n-        Request request = new Request(randomBoolean() ? \"POST\" : \"PUT\", \"/_bulk\");\n-        request.setJsonEntity(\"\");\n-        ResponseException responseException = expectThrows(ResponseException.class, () -> getRestClient().performRequest(request));\n-        assertEquals(400, responseException.getResponse().getStatusLine().getStatusCode());\n-        assertThat(responseException.getMessage(), containsString(\"request body is required\"));\n-    }\n-\n-    public void testBulkRequestBodyImproperlyTerminated() throws IOException {\n-        Request request = new Request(randomBoolean() ? \"POST\" : \"PUT\", \"/_bulk\");\n-        // missing final line of the bulk body. cannot process\n-        request.setJsonEntity(\n-            \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\"\n-                + \"{\\\"field\\\":1}\\n\"\n-                + \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}\"\n-        );\n-        ResponseException responseException = expectThrows(ResponseException.class, () -> getRestClient().performRequest(request));\n-        assertEquals(400, responseException.getResponse().getStatusLine().getStatusCode());\n-        assertThat(responseException.getMessage(), containsString(\"could not parse bulk request body\"));\n-    }\n-\n-    public void testIncrementalBulk() throws IOException {\n-        Request createRequest = new Request(\"PUT\", \"/index_name\");\n-        createRequest.setJsonEntity(\"\"\"\n-            {\n-              \"settings\": {\n-                \"index\": {\n-                  \"number_of_shards\": 1,\n-                  \"number_of_replicas\": 1,\n-                  \"write.wait_for_active_shards\": 2\n-                }\n-              }\n-            }\"\"\");\n-        final Response indexCreatedResponse = getRestClient().performRequest(createRequest);\n-        assertThat(indexCreatedResponse.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        Request firstBulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n-\n-        // index documents for the rollup job\n-        String bulkBody = \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\"\n-            + \"{\\\"field\\\":1}\\n\"\n-            + \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}}\\n\"\n-            + \"{\\\"field\\\":1}\\n\"\n-            + \"\\r\\n\";\n-\n-        firstBulkRequest.setJsonEntity(bulkBody);\n-\n-        final Response indexSuccessFul = getRestClient().performRequest(firstBulkRequest);\n-        assertThat(indexSuccessFul.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        sendLargeBulk();\n-    }\n-\n-    public void testBulkWithIncrementalDisabled() throws IOException {\n-        Request createRequest = new Request(\"PUT\", \"/index_name\");\n-        createRequest.setJsonEntity(\"\"\"\n-            {\n-              \"settings\": {\n-                \"index\": {\n-                  \"number_of_shards\": 1,\n-                  \"number_of_replicas\": 1,\n-                  \"write.wait_for_active_shards\": 2\n-                }\n-              }\n-            }\"\"\");\n-        final Response indexCreatedResponse = getRestClient().performRequest(createRequest);\n-        assertThat(indexCreatedResponse.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        Request firstBulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n-\n-        // index documents for the rollup job\n-        String bulkBody = \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\"\n-            + \"{\\\"field\\\":1}\\n\"\n-            + \"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}}\\n\"\n-            + \"{\\\"field\\\":1}\\n\"\n-            + \"\\r\\n\";\n-\n-        firstBulkRequest.setJsonEntity(bulkBody);\n-\n-        final Response indexSuccessFul = getRestClient().performRequest(firstBulkRequest);\n-        assertThat(indexSuccessFul.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        updateClusterSettings(Settings.builder().put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), false));\n-\n-        internalCluster().getInstances(IncrementalBulkService.class).forEach(i -> i.setForTests(false));\n-\n-        try {\n-            sendLargeBulk();\n-        } finally {\n-            internalCluster().getInstances(IncrementalBulkService.class).forEach(i -> i.setForTests(true));\n-            updateClusterSettings(Settings.builder().put(IncrementalBulkService.INCREMENTAL_BULK.getKey(), (String) null));\n-        }\n-    }\n-\n-    public void testIncrementalMalformed() throws IOException {\n-        Request createRequest = new Request(\"PUT\", \"/index_name\");\n-        createRequest.setJsonEntity(\"\"\"\n-            {\n-              \"settings\": {\n-                \"index\": {\n-                  \"number_of_shards\": 1,\n-                  \"number_of_replicas\": 1,\n-                  \"write.wait_for_active_shards\": 2\n-                }\n-              }\n-            }\"\"\");\n-        final Response indexCreatedResponse = getRestClient().performRequest(createRequest);\n-        assertThat(indexCreatedResponse.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-\n-        Request bulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n-\n-        // index documents for the rollup job\n-        final StringBuilder bulk = new StringBuilder();\n-        bulk.append(\"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\"}}\\n\");\n-        bulk.append(\"{\\\"field\\\":1}\\n\");\n-        bulk.append(\"{}\\n\");\n-        bulk.append(\"\\r\\n\");\n-\n-        bulkRequest.setJsonEntity(bulk.toString());\n-\n-        expectThrows(ResponseException.class, () -> getRestClient().performRequest(bulkRequest));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private static void sendLargeBulk() throws IOException {\n-        Request bulkRequest = new Request(\"POST\", \"/index_name/_bulk\");\n-\n-        // index documents for the rollup job\n-        final StringBuilder bulk = new StringBuilder();\n-        bulk.append(\"{\\\"delete\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"1\\\"}}\\n\");\n-        int updates = 0;\n-        for (int i = 0; i < 1000; i++) {\n-            bulk.append(\"{\\\"index\\\":{\\\"_index\\\":\\\"index_name\\\"}}\\n\");\n-            bulk.append(\"{\\\"field\\\":\").append(i).append(\"}\\n\");\n-            if (randomBoolean() && randomBoolean() && randomBoolean() && randomBoolean()) {\n-                ++updates;\n-                bulk.append(\"{\\\"update\\\":{\\\"_index\\\":\\\"index_name\\\",\\\"_id\\\":\\\"2\\\"}}\\n\");\n-                bulk.append(\"{\\\"doc\\\":{\\\"field\\\":\").append(i).append(\"}}\\n\");\n-            }\n-        }\n-        bulk.append(\"\\r\\n\");\n-\n-        bulkRequest.setJsonEntity(bulk.toString());\n-\n-        final Response bulkResponse = getRestClient().performRequest(bulkRequest);\n-        assertThat(bulkResponse.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n-        Map<String, Object> responseMap = XContentHelper.convertToMap(\n-            JsonXContent.jsonXContent,\n-            bulkResponse.getEntity().getContent(),\n-            true\n-        );\n-\n-        assertFalse((Boolean) responseMap.get(\"errors\"));\n-        assertThat(((List<Object>) responseMap.get(\"items\")).size(), equalTo(1001 + updates));\n-    }\n-}\ndiff --git a/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java b/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\nindex 03694c74..383c040e 100644\n--- a/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\n+++ b/server/src/main/java/org/elasticsearch/rest/action/document/RestBulkAction.java\n@@ -104,19 +104,23 @@ public class RestBulkAction extends BaseRestHandler {\n             boolean defaultRequireDataStream = request.paramAsBoolean(DocWriteRequest.REQUIRE_DATA_STREAM, false);\n             bulkRequest.timeout(request.paramAsTime(\"timeout\", BulkShardRequest.DEFAULT_TIMEOUT));\n             bulkRequest.setRefreshPolicy(request.param(\"refresh\"));\n-            bulkRequest.add(\n-                request.requiredContent(),\n-                defaultIndex,\n-                defaultRouting,\n-                defaultFetchSourceContext,\n-                defaultPipeline,\n-                defaultRequireAlias,\n-                defaultRequireDataStream,\n-                defaultListExecutedPipelines,\n-                allowExplicitIndex,\n-                request.getXContentType(),\n-                request.getRestApiVersion()\n-            );\n+            try {\n+                bulkRequest.add(\n+                    request.requiredContent(),\n+                    defaultIndex,\n+                    defaultRouting,\n+                    defaultFetchSourceContext,\n+                    defaultPipeline,\n+                    defaultRequireAlias,\n+                    defaultRequireDataStream,\n+                    defaultListExecutedPipelines,\n+                    allowExplicitIndex,\n+                    request.getXContentType(),\n+                    request.getRestApiVersion()\n+                );\n+            } catch (IllegalArgumentException e) {\n+                throw new ElasticsearchParseException(\"could not parse bulk request body\", e);\n+            }\n \n             return channel -> client.bulk(bulkRequest, new RestRefCountedChunkedToXContentListener<>(channel));\n         } else {",
      "scores": {
        "correctness": -0.3,
        "completeness": -0.5,
        "code_reuse": 0.5,
        "best_practices": -0.2,
        "unsolicited_docs": 1.0
      },
      "aggregate": 0.1,
      "rationale": "The agent made critical errors: (1) Deleted the entire test file instead of renaming it, losing all test content; (2) Only caught IllegalArgumentException instead of all exceptions with proper handling; (3) Threw exception directly instead of returning error via channel callback; (4) Missing the parseFailureException helper method; (5) Missing changelog file; (6) Missing test randomization logic; (7) Missing new test case for invalid UTF-8; (8) Missing updates to ChunkHandler error handling. The RestBulkAction changes are partially correct but incomplete and break the async error handling pattern.",
      "llm_rating": 0.15,
      "llm_summary": "Agent deleted IncrementalBulkRestIT.java entirely instead of renaming to BulkRestIT.java and added incomplete try-catch in RestBulkAction that only handles IllegalArgumentException and throws synchronously instead of using channel callback pattern.",
      "errors": []
    }
  ],
  "comparative_analysis": {
    "summary": "claude-code:claude-sonnet-4-5 achieved the highest score (0.38) by correctly renaming the test class and implementing randomized testing, though it missed the changelog and some test additions. auggie:sonnet4.5 and factory:claude-sonnet-4-5-20250929 both scored poorly (0.12 and 0.10 respectively) by completely deleting the test file instead of renaming it, fundamentally misunderstanding the task requirements.",
    "best_agent": "claude-code:claude-sonnet-4-5",
    "best_agent_reasoning": "claude-code:claude-sonnet-4-5 was the only agent that correctly understood the core requirement to rename IncrementalBulkRestIT to BulkRestIT while preserving the file. It also implemented the randomization of incremental bulk testing using a boolean field, and added error handling to ensure status code 400 for invalid bulk bodies. While it missed the changelog file and the new testBulkInvalidIndexNameString test, it demonstrated the best understanding of the task's intent to standardize error handling across both bulk API versions.",
    "approach_differences": "claude-code:claude-sonnet-4-5 took a file-preserving approach by renaming the class and adding randomization logic with a useIncrementalBulk field, plus wrapping the bulk request parsing in a try-catch block. In stark contrast, both auggie:sonnet4.5 and factory:claude-sonnet-4-5-20250929 made the critical error of deleting the entire IncrementalBulkRestIT.java file and creating a new BulkRestIT.java from scratch, losing all existing test methods and context. Neither of the latter two agents understood that the task required renaming and modifying the existing file, not replacing it entirely.",
    "ranking": [
      "claude-code:claude-sonnet-4-5",
      "auggie:sonnet4.5",
      "factory:claude-sonnet-4-5-20250929"
    ]
  },
  "timestamp": "2025-11-06T21:41:04.134463",
  "analysis_run_id": "74f18a42"
}