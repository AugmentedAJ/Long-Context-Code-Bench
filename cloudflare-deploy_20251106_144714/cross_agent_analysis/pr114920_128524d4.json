{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 114920,
  "base_commit": "f6a1e36d6be56a5d480765ad2d5f72f4adcaef5b",
  "head_commit": "60c9a1a964015bf0406fcde32d3225dbeab425f0",
  "task_instructions": "You are working on a codebase. Your task is to make the necessary code changes to accomplish the following:\n\nSplit out FailedToCommitClusterStateException from watched file error reporting for file-based settings\n\n`processSettingsAndNotifyListeners` is something of a blunt instrument when it comes to error handling: `processFileChanges` is [required to throw](https://github.com/elastic/elasticsearch/blob/f6a1e36d6be56a5d480765ad2d5f72f4adcaef5b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java#L73) `ExecutionException` \"_if there is an issue while applying the changes from the file_\", and in response, `processSettingsAndNotifyListeners` [logs an \"Error processing watched file\"](https://github.com/elastic/elasticsearch/blob/f6a1e36d6be56a5d480765ad2d5f72f4adcaef5b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java#L300) error and continues.\r\n\r\nIn ES-9360 we've found that this one catch-all error message is not specific enough to determine the appropriate alert severity, and so we'd like a way to opt out of the catch-all error message. @thecoop and I discussed some options, and this was the simplest, so I thought I'd start with that. It peels of a subclass of `ExecutionException` to represent the case that we want to opt out of the error reporting.\r\n\r\nThis PR is something of a \"request for comments\": it's a minimal change that achieves the opting out, but I find it hard to tell whether the resulting method contracts are coherent and understandable.\n\nPlease make all necessary code changes to complete this task.",
  "ground_truth_diff": "diff --git a/.gitignore b/.gitignore\nindex d1af97cbaea3..8fbdd698771c 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -69,3 +69,4 @@ testfixtures_shared/\n # Generated\n checkstyle_ide.xml\n x-pack/plugin/esql/src/main/generated-src/generated/\n+x-pack/plugin/esql/gen\ndiff --git a/docs/plugins/mapper-annotated-text.asciidoc b/docs/plugins/mapper-annotated-text.asciidoc\nindex 9b6eccd13669..e4141e98a228 100644\n--- a/docs/plugins/mapper-annotated-text.asciidoc\n+++ b/docs/plugins/mapper-annotated-text.asciidoc\n@@ -167,16 +167,8 @@ duplicates removed. So:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"text\": {\n         \"type\": \"annotated_text\",\n@@ -223,16 +215,8 @@ are preserved.\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"text\": { \"type\": \"annotated_text\", \"store\": true }\n     }\ndiff --git a/docs/reference/inference/service-elser.asciidoc b/docs/reference/inference/service-elser.asciidoc\nindex 6afc2a2e3ef6..c7217f38d459 100644\n--- a/docs/reference/inference/service-elser.asciidoc\n+++ b/docs/reference/inference/service-elser.asciidoc\n@@ -80,13 +80,12 @@ Must be a power of 2. Max allowed value is 32.\n [[inference-example-elser]]\n ==== ELSER service example\n \n-The following example shows how to create an {infer} endpoint called `my-elser-model` to perform a `sparse_embedding` task type.\n+The following example shows how to create an {infer} endpoint called\n+`my-elser-model` to perform a `sparse_embedding` task type.\n Refer to the {ml-docs}/ml-nlp-elser.html[ELSER model documentation] for more info.\n \n-NOTE: If you want to optimize your ELSER endpoint for ingest, set the number of threads to `1` (`\"num_threads\": 1`).\n-If you want to optimize your ELSER endpoint for search, set the number of threads to greater than `1`.\n-\n-The request below will automatically download the ELSER model if it isn't already downloaded and then deploy the model.\n+The request below will automatically download the ELSER model if it isn't\n+already downloaded and then deploy the model.\n \n [source,console]\n ------------------------------------------------------------\n@@ -101,6 +100,7 @@ PUT _inference/sparse_embedding/my-elser-model\n ------------------------------------------------------------\n // TEST[skip:TBD]\n \n+\n Example response:\n \n [source,console-result]\n@@ -130,12 +130,12 @@ If using the Python client, you can set the `timeout` parameter to a higher valu\n [[inference-example-elser-adaptive-allocation]]\n ==== Setting adaptive allocation for the ELSER service\n \n-NOTE: For more information on how to optimize your ELSER endpoints, refer to {ml-docs}/ml-nlp-elser.html#elser-recommendations[the ELSER recommendations] section in the model documentation.\n-To learn more about model autoscaling, refer to the {ml-docs}/ml-nlp-auto-scale.html[trained model autoscaling] page.\n-\n-The following example shows how to create an {infer} endpoint called `my-elser-model` to perform a `sparse_embedding` task type and configure adaptive allocations.\n+The following example shows how to create an {infer} endpoint called\n+`my-elser-model` to perform a `sparse_embedding` task type and configure\n+adaptive allocations.\n \n-The request below will automatically download the ELSER model if it isn't already downloaded and then deploy the model.\n+The request below will automatically download the ELSER model if it isn't\n+already downloaded and then deploy the model.\n \n [source,console]\n ------------------------------------------------------------\ndiff --git a/docs/reference/mapping/fields/synthetic-source.asciidoc b/docs/reference/mapping/fields/synthetic-source.asciidoc\nindex f8666e2993d6..902b6c26611e 100644\n--- a/docs/reference/mapping/fields/synthetic-source.asciidoc\n+++ b/docs/reference/mapping/fields/synthetic-source.asciidoc\n@@ -2,7 +2,7 @@\n ==== Synthetic `_source`\n \n IMPORTANT: Synthetic `_source` is Generally Available only for TSDB indices\n-(indices that have `index.mode` set to `time_series`). For other indices,\n+(indices that have `index.mode` set to `time_series`). For other indices\n synthetic `_source` is in technical preview. Features in technical preview may\n be changed or removed in a future release. Elastic will work to fix\n any issues, but features in technical preview are not subject to the support SLA\n@@ -11,19 +11,15 @@ of official GA features.\n Though very handy to have around, the source field takes up a significant amount\n of space on disk. Instead of storing source documents on disk exactly as you\n send them, Elasticsearch can reconstruct source content on the fly upon retrieval.\n-Enable this by using the value `synthetic` for the index setting `index.mapping.source.mode`:\n+Enable this by setting `mode: synthetic` in `_source`:\n \n [source,console,id=enable-synthetic-source-example]\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n+  \"mappings\": {\n+    \"_source\": {\n+      \"mode\": \"synthetic\"\n     }\n   }\n }\n@@ -42,7 +38,7 @@ properties when used with synthetic `_source`.\n <<synthetic-source-fields-native-list, Most field types>> construct synthetic `_source` using existing data, most\n commonly <<doc-values,`doc_values`>> and <<stored-fields, stored fields>>. For these field types, no additional space\n is needed to store the contents of `_source` field. Due to the storage layout of <<doc-values,`doc_values`>>, the\n-generated `_source` field undergoes <<synthetic-source-modifications, modifications>> compared to the original document.\n+generated `_source` field undergoes <<synthetic-source-modifications, modifications>> compared to original document.\n \n For all other field types, the original value of the field is stored as is, in the same way as the `_source` field in\n non-synthetic mode. In this case there are no modifications and field data in `_source` is the same as in the original\n@@ -231,16 +227,10 @@ For instance:\n ----\n PUT idx_keep\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": {\n+      \"mode\": \"synthetic\"\n+    },\n     \"properties\": {\n       \"path\": {\n         \"type\": \"object\",\ndiff --git a/docs/reference/mapping/types/aggregate-metric-double.asciidoc b/docs/reference/mapping/types/aggregate-metric-double.asciidoc\nindex 8a4ddffc30bb..8e14fba97636 100644\n--- a/docs/reference/mapping/types/aggregate-metric-double.asciidoc\n+++ b/docs/reference/mapping/types/aggregate-metric-double.asciidoc\n@@ -267,16 +267,8 @@ For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"agg_metric\": {\n         \"type\": \"aggregate_metric_double\",\ndiff --git a/docs/reference/mapping/types/boolean.asciidoc b/docs/reference/mapping/types/boolean.asciidoc\nindex 494c41021dd2..32f3d13edf58 100644\n--- a/docs/reference/mapping/types/boolean.asciidoc\n+++ b/docs/reference/mapping/types/boolean.asciidoc\n@@ -249,16 +249,8 @@ Synthetic source always sorts `boolean` fields. For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"bool\": { \"type\": \"boolean\" }\n     }\ndiff --git a/docs/reference/mapping/types/date.asciidoc b/docs/reference/mapping/types/date.asciidoc\nindex 53b17a669ae7..ca2c23f932fc 100644\n--- a/docs/reference/mapping/types/date.asciidoc\n+++ b/docs/reference/mapping/types/date.asciidoc\n@@ -130,7 +130,7 @@ The following parameters are accepted by `date` fields:\n <<ignore-malformed,`ignore_malformed`>>::\n \n     If `true`, malformed numbers are ignored. If `false` (default), malformed\n-    numbers throw an exception and reject the whole document. Note that this\n+    numbers throw an exception and reject the whole document.  Note that this\n     cannot be set if the `script` parameter is used.\n \n <<mapping-index,`index`>>::\n@@ -248,16 +248,8 @@ Synthetic source always sorts `date` fields. For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"date\": { \"type\": \"date\" }\n     }\ndiff --git a/docs/reference/mapping/types/date_nanos.asciidoc b/docs/reference/mapping/types/date_nanos.asciidoc\nindex e9ec85c470ec..1a3b390b1690 100644\n--- a/docs/reference/mapping/types/date_nanos.asciidoc\n+++ b/docs/reference/mapping/types/date_nanos.asciidoc\n@@ -160,16 +160,8 @@ Synthetic source always sorts `date_nanos` fields. For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"date\": { \"type\": \"date_nanos\" }\n     }\ndiff --git a/docs/reference/mapping/types/flattened.asciidoc b/docs/reference/mapping/types/flattened.asciidoc\nindex af6ef3e739d0..0a72ebc98ece 100644\n--- a/docs/reference/mapping/types/flattened.asciidoc\n+++ b/docs/reference/mapping/types/flattened.asciidoc\n@@ -334,16 +334,8 @@ For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"flattened\": { \"type\": \"flattened\" }\n     }\n@@ -375,16 +367,8 @@ For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"flattened\": { \"type\": \"flattened\" }\n     }\n@@ -423,16 +407,8 @@ For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"flattened\": { \"type\": \"flattened\" }\n     }\ndiff --git a/docs/reference/mapping/types/geo-point.asciidoc b/docs/reference/mapping/types/geo-point.asciidoc\nindex 9ba8ea6e4678..6db05188dfb9 100644\n--- a/docs/reference/mapping/types/geo-point.asciidoc\n+++ b/docs/reference/mapping/types/geo-point.asciidoc\n@@ -229,16 +229,8 @@ longitude) and reduces them to their stored precision. For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"point\": { \"type\": \"geo_point\" }\n     }\ndiff --git a/docs/reference/mapping/types/ip.asciidoc b/docs/reference/mapping/types/ip.asciidoc\nindex f85dd78ecbd4..f068916478a7 100644\n--- a/docs/reference/mapping/types/ip.asciidoc\n+++ b/docs/reference/mapping/types/ip.asciidoc\n@@ -170,16 +170,8 @@ Synthetic source always sorts `ip` fields and removes duplicates. For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"ip\": { \"type\": \"ip\" }\n     }\ndiff --git a/docs/reference/mapping/types/keyword.asciidoc b/docs/reference/mapping/types/keyword.asciidoc\nindex b94216042427..a4be7026dffc 100644\n--- a/docs/reference/mapping/types/keyword.asciidoc\n+++ b/docs/reference/mapping/types/keyword.asciidoc\n@@ -188,16 +188,8 @@ For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"kwd\": { \"type\": \"keyword\" }\n     }\n@@ -226,16 +218,8 @@ are preserved. For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"kwd\": { \"type\": \"keyword\", \"store\": true }\n     }\n@@ -264,16 +248,8 @@ For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"kwd\": { \"type\": \"keyword\", \"ignore_above\": 3 }\n     }\ndiff --git a/docs/reference/mapping/types/numeric.asciidoc b/docs/reference/mapping/types/numeric.asciidoc\nindex 5bfa1bc7c124..d1e1c037e571 100644\n--- a/docs/reference/mapping/types/numeric.asciidoc\n+++ b/docs/reference/mapping/types/numeric.asciidoc\n@@ -259,16 +259,8 @@ Synthetic source always sorts numeric fields. For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"long\": { \"type\": \"long\" }\n     }\n@@ -295,16 +287,8 @@ Scaled floats will always apply their scaling factor so:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"f\": { \"type\": \"scaled_float\", \"scaling_factor\": 0.01 }\n     }\ndiff --git a/docs/reference/mapping/types/range.asciidoc b/docs/reference/mapping/types/range.asciidoc\nindex 04341f68c630..14c5b6098acb 100644\n--- a/docs/reference/mapping/types/range.asciidoc\n+++ b/docs/reference/mapping/types/range.asciidoc\n@@ -249,21 +249,13 @@ of official GA features.\n `range` fields support <<synthetic-source,synthetic `_source`>> in their default\n configuration. Synthetic `_source` cannot be used with <<doc-values,`doc_values`>> disabled.\n \n-Synthetic source always sorts values and removes duplicates for all `range` fields except `ip_range`. Ranges are sorted by their lower bound and then by upper bound. For example:\n+Synthetic source always sorts values and removes duplicates for all `range` fields except `ip_range` . Ranges are sorted by their lower bound and then by upper bound. For example:\n [source,console,id=synthetic-source-range-sorting-example]\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"my_range\": { \"type\": \"long_range\" }\n     }\n@@ -324,16 +316,8 @@ For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"my_range\": { \"type\": \"ip_range\" }\n     }\n@@ -368,21 +352,13 @@ Will become:\n // TEST[s/^/{\"_source\":/ s/\\n$/}/]\n \n [[range-synthetic-source-inclusive]]\n-Range field values are always represented as inclusive on both sides with bounds adjusted accordingly. Default values for range bounds are represented as `null`. This is true even if range bound was explicitly provided. For example:\n+Range field vales are always represented as inclusive on both sides with bounds adjusted accordingly. Default values for range bounds are represented as `null`. This is true even if range bound was explicitly provided. For example:\n [source,console,id=synthetic-source-range-normalization-example]\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"my_range\": { \"type\": \"long_range\" }\n     }\n@@ -418,16 +394,8 @@ Default values for range bounds are represented as `null` in synthetic source. T\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"my_range\": { \"type\": \"integer_range\" }\n     }\n@@ -461,16 +429,8 @@ Will become:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"my_range\": { \"type\": \"date_range\" }\n     }\ndiff --git a/docs/reference/mapping/types/text.asciidoc b/docs/reference/mapping/types/text.asciidoc\nindex ca69c93e8f1a..c33af69df560 100644\n--- a/docs/reference/mapping/types/text.asciidoc\n+++ b/docs/reference/mapping/types/text.asciidoc\n@@ -177,23 +177,15 @@ a <<keyword-synthetic-source, `keyword`>> sub-field that supports synthetic\n `_source` or if the `text` field sets `store` to `true`. Either way, it may\n not have <<copy-to,`copy_to`>>.\n \n-If using a sub-`keyword` field, then the values are sorted in the same way as\n+If using a sub-`keyword` field then the values are sorted in the same way as\n a `keyword` field's values are sorted. By default, that means sorted with\n duplicates removed. So:\n [source,console,id=synthetic-source-text-example-default]\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"text\": {\n         \"type\": \"text\",\n@@ -241,16 +233,8 @@ are preserved.\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"text\": { \"type\": \"text\", \"store\": true }\n     }\ndiff --git a/docs/reference/mapping/types/version.asciidoc b/docs/reference/mapping/types/version.asciidoc\nindex 1600451432bd..8da0fcae80fc 100644\n--- a/docs/reference/mapping/types/version.asciidoc\n+++ b/docs/reference/mapping/types/version.asciidoc\n@@ -63,15 +63,16 @@ The following parameters are accepted by `version` fields:\n [discrete]\n ==== Limitations\n \n-This field type isn't optimized for heavy wildcard, regex, or fuzzy searches. While those\n-types of queries work in this field, you should consider using a regular `keyword` field if\n-you strongly rely on these kinds of queries.\n+This field type isn't optimized for heavy wildcard, regex or fuzzy searches. While those\n+type of queries work in this field, you should consider using a regular `keyword` field if\n+you strongly rely on these kind of queries.\n+\n \n [[version-synthetic-source]]\n ==== Synthetic `_source`\n \n IMPORTANT: Synthetic `_source` is Generally Available only for TSDB indices\n-(indices that have `index.mode` set to `time_series`). For other indices,\n+(indices that have `index.mode` set to `time_series`). For other indices\n synthetic `_source` is in technical preview. Features in technical preview may\n be changed or removed in a future release. Elastic will work to fix\n any issues, but features in technical preview are not subject to the support SLA\n@@ -85,16 +86,8 @@ Synthetic source always sorts `version` fields and removes duplicates. For examp\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"versions\": { \"type\": \"version\" }\n     }\ndiff --git a/docs/reference/mapping/types/wildcard.asciidoc b/docs/reference/mapping/types/wildcard.asciidoc\nindex 89a3109a3716..79fc953051d5 100644\n--- a/docs/reference/mapping/types/wildcard.asciidoc\n+++ b/docs/reference/mapping/types/wildcard.asciidoc\n@@ -141,16 +141,8 @@ Synthetic source always sorts `wildcard` fields. For example:\n ----\n PUT idx\n {\n-  \"settings\": {\n-    \"index\": {\n-      \"mapping\": {\n-        \"source\": {\n-          \"mode\": \"synthetic\"\n-        }\n-      }\n-    }\n-  },\n   \"mappings\": {\n+    \"_source\": { \"mode\": \"synthetic\" },\n     \"properties\": {\n       \"card\": { \"type\": \"wildcard\" }\n     }\ndiff --git a/docs/reference/search/search-your-data/semantic-search-semantic-text.asciidoc b/docs/reference/search/search-your-data/semantic-search-semantic-text.asciidoc\nindex 60692c19c184..dbcfbb1b615f 100644\n--- a/docs/reference/search/search-your-data/semantic-search-semantic-text.asciidoc\n+++ b/docs/reference/search/search-your-data/semantic-search-semantic-text.asciidoc\n@@ -50,7 +50,7 @@ PUT _inference/sparse_embedding/my-elser-endpoint <1>\n be used and ELSER creates sparse vectors. The `inference_id` is\n `my-elser-endpoint`.\n <2> The `elser` service is used in this example.\n-<3> This setting enables and configures {ml-docs}/ml-nlp-auto-scale.html#nlp-model-adaptive-allocations[adaptive allocations].\n+<3> This setting enables and configures adaptive allocations.\n Adaptive allocations make it possible for ELSER to automatically scale up or down resources based on the current load on the process.\n \n [NOTE]\n@@ -284,8 +284,6 @@ query from the `semantic-embedding` index:\n \n [discrete]\n [[semantic-text-further-examples]]\n-==== Further examples and reading\n+==== Further examples\n \n-* If you want to use `semantic_text` in hybrid search, refer to https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/search/09-semantic-text.ipynb[this notebook] for a step-by-step guide.\n-* For more information on how to optimize your ELSER endpoints, refer to {ml-docs}/ml-nlp-elser.html#elser-recommendations[the ELSER recommendations] section in the model documentation.\n-* To learn more about model autoscaling, refer to the {ml-docs}/ml-nlp-auto-scale.html[trained model autoscaling] page.\n\\ No newline at end of file\n+If you want to use `semantic_text` in hybrid search, refer to https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/search/09-semantic-text.ipynb[this notebook] for a step-by-step guide.\n\\ No newline at end of file\ndiff --git a/muted-tests.yml b/muted-tests.yml\nindex fb48f9e04d5c..df4c96434099 100644\n--- a/muted-tests.yml\n+++ b/muted-tests.yml\n@@ -376,6 +376,9 @@ tests:\n   issue: https://github.com/elastic/elasticsearch/issues/114839\n - class: org.elasticsearch.license.LicensingTests\n   issue: https://github.com/elastic/elasticsearch/issues/114865\n+- class: org.elasticsearch.datastreams.logsdb.qa.LogsDbVersusLogsDbReindexedIntoStandardModeChallengeRestIT\n+  method: testTermsQuery\n+  issue: https://github.com/elastic/elasticsearch/issues/114873\n - class: org.elasticsearch.xpack.enrich.EnrichIT\n   method: testDeleteIsCaseSensitive\n   issue: https://github.com/elastic/elasticsearch/issues/114840\n@@ -385,12 +388,6 @@ tests:\n - class: org.elasticsearch.test.rest.ClientYamlTestSuiteIT\n   method: test {yaml=cluster.stats/30_ccs_stats/cross-cluster search stats search}\n   issue: https://github.com/elastic/elasticsearch/issues/114902\n-- class: org.elasticsearch.xpack.enrich.EnrichRestIT\n-  method: test {p0=enrich/40_synthetic_source/enrich documents over _bulk}\n-  issue: https://github.com/elastic/elasticsearch/issues/114825\n-- class: org.elasticsearch.xpack.inference.DefaultEndPointsIT\n-  method: testInferDeploysDefaultElser\n-  issue: https://github.com/elastic/elasticsearch/issues/114913\n \n # Examples:\n #\ndiff --git a/server/src/main/java/org/elasticsearch/cluster/metadata/MetadataCreateIndexService.java b/server/src/main/java/org/elasticsearch/cluster/metadata/MetadataCreateIndexService.java\nindex 29720e98a6e7..7f2c07628173 100644\n--- a/server/src/main/java/org/elasticsearch/cluster/metadata/MetadataCreateIndexService.java\n+++ b/server/src/main/java/org/elasticsearch/cluster/metadata/MetadataCreateIndexService.java\n@@ -1373,7 +1373,7 @@ public class MetadataCreateIndexService {\n         MapperService mapperService = indexService.mapperService();\n         IndexMode indexMode = indexService.getIndexSettings() != null ? indexService.getIndexSettings().getMode() : IndexMode.STANDARD;\n         List<CompressedXContent> allMappings = new ArrayList<>();\n-        final CompressedXContent defaultMapping = indexMode.getDefaultMapping(indexService.getIndexSettings());\n+        final CompressedXContent defaultMapping = indexMode.getDefaultMapping();\n         if (defaultMapping != null) {\n             allMappings.add(defaultMapping);\n         }\ndiff --git a/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\nindex dcb28a17a9b4..63325a2db936 100644\n--- a/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\n+++ b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\n@@ -296,6 +296,8 @@ public abstract class AbstractFileWatchingService extends AbstractLifecycleCompo\n             for (var listener : eventListeners) {\n                 listener.watchedFileChanged();\n             }\n+        } catch (NoChangeOccurredException e) {\n+            logger.info(() -> \"Unable to process watched file: \" + watchedFile(), e);\n         } catch (IOException | ExecutionException e) {\n             logger.error(() -> \"Error processing watched file: \" + watchedFile(), e);\n         }\ndiff --git a/server/src/main/java/org/elasticsearch/common/file/NoChangeOccurredException.java b/server/src/main/java/org/elasticsearch/common/file/NoChangeOccurredException.java\nnew file mode 100644\nindex 000000000000..d8f01e61d559\n--- /dev/null\n+++ b/server/src/main/java/org/elasticsearch/common/file/NoChangeOccurredException.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.common.file;\n+\n+import java.util.concurrent.ExecutionException;\n+\n+/**\n+ * Indicates that {@link AbstractFileWatchingService#processFileChanges()} had no effect,\n+ * and listeners should not be notified because there's nothing for them to do.\n+ */\n+public class NoChangeOccurredException extends ExecutionException {\n+    public NoChangeOccurredException(Throwable cause) {\n+        super(cause);\n+    }\n+}\ndiff --git a/server/src/main/java/org/elasticsearch/index/IndexMode.java b/server/src/main/java/org/elasticsearch/index/IndexMode.java\nindex 5908bc22e21e..2d9e89223d7a 100644\n--- a/server/src/main/java/org/elasticsearch/index/IndexMode.java\n+++ b/server/src/main/java/org/elasticsearch/index/IndexMode.java\n@@ -75,7 +75,7 @@ public enum IndexMode {\n         }\n \n         @Override\n-        public CompressedXContent getDefaultMapping(final IndexSettings indexSettings) {\n+        public CompressedXContent getDefaultMapping() {\n             return null;\n         }\n \n@@ -171,7 +171,7 @@ public enum IndexMode {\n         }\n \n         @Override\n-        public CompressedXContent getDefaultMapping(final IndexSettings indexSettings) {\n+        public CompressedXContent getDefaultMapping() {\n             return DEFAULT_TIME_SERIES_TIMESTAMP_MAPPING;\n         }\n \n@@ -249,10 +249,8 @@ public enum IndexMode {\n         }\n \n         @Override\n-        public CompressedXContent getDefaultMapping(final IndexSettings indexSettings) {\n-            return indexSettings != null && indexSettings.getIndexSortConfig().hasPrimarySortOnField(HOST_NAME)\n-                ? DEFAULT_LOGS_TIMESTAMP_MAPPING_WITH_HOSTNAME\n-                : DEFAULT_TIME_SERIES_TIMESTAMP_MAPPING;\n+        public CompressedXContent getDefaultMapping() {\n+            return DEFAULT_LOGS_TIMESTAMP_MAPPING;\n         }\n \n         @Override\n@@ -310,8 +308,6 @@ public enum IndexMode {\n         }\n     };\n \n-    private static final String HOST_NAME = \"host.name\";\n-\n     private static void validateTimeSeriesSettings(Map<Setting<?>, Object> settings) {\n         settingRequiresTimeSeries(settings, IndexMetadata.INDEX_ROUTING_PATH);\n         settingRequiresTimeSeries(settings, IndexSettings.TIME_SERIES_START_TIME);\n@@ -328,33 +324,48 @@ public enum IndexMode {\n         return \"[\" + IndexSettings.MODE.getKey() + \"=time_series]\";\n     }\n \n-    private static CompressedXContent createDefaultMapping(boolean includeHostName) throws IOException {\n-        return new CompressedXContent((builder, params) -> {\n-            builder.startObject(MapperService.SINGLE_MAPPING_NAME)\n-                .startObject(DataStreamTimestampFieldMapper.NAME)\n-                .field(\"enabled\", true)\n-                .endObject()\n-                .startObject(\"properties\")\n-                .startObject(DataStreamTimestampFieldMapper.DEFAULT_PATH)\n-                .field(\"type\", DateFieldMapper.CONTENT_TYPE)\n-                .endObject();\n-\n-            if (includeHostName) {\n-                builder.startObject(HOST_NAME).field(\"type\", KeywordFieldMapper.CONTENT_TYPE).field(\"ignore_above\", 1024).endObject();\n-            }\n+    public static final CompressedXContent DEFAULT_TIME_SERIES_TIMESTAMP_MAPPING;\n \n-            return builder.endObject().endObject();\n-        });\n+    static {\n+        try {\n+            DEFAULT_TIME_SERIES_TIMESTAMP_MAPPING = new CompressedXContent(\n+                ((builder, params) -> builder.startObject(MapperService.SINGLE_MAPPING_NAME)\n+                    .startObject(DataStreamTimestampFieldMapper.NAME)\n+                    .field(\"enabled\", true)\n+                    .endObject()\n+                    .startObject(\"properties\")\n+                    .startObject(DataStreamTimestampFieldMapper.DEFAULT_PATH)\n+                    .field(\"type\", DateFieldMapper.CONTENT_TYPE)\n+                    .field(\"ignore_malformed\", \"false\")\n+                    .endObject()\n+                    .endObject()\n+                    .endObject())\n+            );\n+        } catch (IOException e) {\n+            throw new AssertionError(e);\n+        }\n     }\n \n-    private static final CompressedXContent DEFAULT_TIME_SERIES_TIMESTAMP_MAPPING;\n-\n-    private static final CompressedXContent DEFAULT_LOGS_TIMESTAMP_MAPPING_WITH_HOSTNAME;\n+    public static final CompressedXContent DEFAULT_LOGS_TIMESTAMP_MAPPING;\n \n     static {\n         try {\n-            DEFAULT_TIME_SERIES_TIMESTAMP_MAPPING = createDefaultMapping(false);\n-            DEFAULT_LOGS_TIMESTAMP_MAPPING_WITH_HOSTNAME = createDefaultMapping(true);\n+            DEFAULT_LOGS_TIMESTAMP_MAPPING = new CompressedXContent(\n+                ((builder, params) -> builder.startObject(MapperService.SINGLE_MAPPING_NAME)\n+                    .startObject(DataStreamTimestampFieldMapper.NAME)\n+                    .field(\"enabled\", true)\n+                    .endObject()\n+                    .startObject(\"properties\")\n+                    .startObject(DataStreamTimestampFieldMapper.DEFAULT_PATH)\n+                    .field(\"type\", DateFieldMapper.CONTENT_TYPE)\n+                    .endObject()\n+                    .startObject(\"host.name\")\n+                    .field(\"type\", KeywordFieldMapper.CONTENT_TYPE)\n+                    .field(\"ignore_above\", 1024)\n+                    .endObject()\n+                    .endObject()\n+                    .endObject())\n+            );\n         } catch (IOException e) {\n             throw new AssertionError(e);\n         }\n@@ -410,7 +421,7 @@ public enum IndexMode {\n      * Get default mapping for this index or {@code null} if there is none.\n      */\n     @Nullable\n-    public abstract CompressedXContent getDefaultMapping(IndexSettings indexSettings);\n+    public abstract CompressedXContent getDefaultMapping();\n \n     /**\n      * Build the {@link FieldMapper} for {@code _id}.\ndiff --git a/server/src/main/java/org/elasticsearch/index/mapper/DocumentParserContext.java b/server/src/main/java/org/elasticsearch/index/mapper/DocumentParserContext.java\nindex ef87ce52fbab..2eec14bd1a8d 100644\n--- a/server/src/main/java/org/elasticsearch/index/mapper/DocumentParserContext.java\n+++ b/server/src/main/java/org/elasticsearch/index/mapper/DocumentParserContext.java\n@@ -111,7 +111,7 @@ public abstract class DocumentParserContext {\n     private final Set<String> ignoredFields;\n     private final List<IgnoredSourceFieldMapper.NameValue> ignoredFieldValues;\n     private final List<IgnoredSourceFieldMapper.NameValue> ignoredFieldsMissingValues;\n-    private boolean inArrayScopeEnabled;\n+    private final boolean inArrayScopeEnabled;\n     private boolean inArrayScope;\n \n     private final Map<String, List<Mapper>> dynamicMappers;\n@@ -376,14 +376,13 @@ public abstract class DocumentParserContext {\n      * Applies to synthetic source only.\n      */\n     public final DocumentParserContext maybeCloneForArray(Mapper mapper) throws IOException {\n-        if (canAddIgnoredField()\n-            && mapper instanceof ObjectMapper\n-            && mapper instanceof NestedObjectMapper == false\n-            && inArrayScope == false\n-            && inArrayScopeEnabled) {\n-            DocumentParserContext subcontext = switchParser(parser());\n-            subcontext.inArrayScope = true;\n-            return subcontext;\n+        if (canAddIgnoredField() && mapper instanceof ObjectMapper && inArrayScopeEnabled) {\n+            boolean isNested = mapper instanceof NestedObjectMapper;\n+            if ((inArrayScope == false && isNested == false) || (inArrayScope && isNested)) {\n+                DocumentParserContext subcontext = switchParser(parser());\n+                subcontext.inArrayScope = inArrayScope == false;\n+                return subcontext;\n+            }\n         }\n         return this;\n     }\n@@ -710,18 +709,12 @@ public abstract class DocumentParserContext {\n      * Return a new context that has the provided document as the current document.\n      */\n     public final DocumentParserContext switchDoc(final LuceneDocument document) {\n-        DocumentParserContext cloned = new Wrapper(this.parent, this) {\n+        return new Wrapper(this.parent, this) {\n             @Override\n             public LuceneDocument doc() {\n                 return document;\n             }\n         };\n-        // Disable tracking array scopes for ignored source, as it would be added to the parent doc.\n-        // Nested documents are added to preserve object structure within arrays of objects, so the use\n-        // of ignored source for arrays inside them should be mostly redundant.\n-        cloned.inArrayScope = false;\n-        cloned.inArrayScopeEnabled = false;\n-        return cloned;\n     }\n \n     /**\ndiff --git a/server/src/main/java/org/elasticsearch/reservedstate/service/FileSettingsService.java b/server/src/main/java/org/elasticsearch/reservedstate/service/FileSettingsService.java\nindex c29f83c780d3..bf1a13cb70c4 100644\n--- a/server/src/main/java/org/elasticsearch/reservedstate/service/FileSettingsService.java\n+++ b/server/src/main/java/org/elasticsearch/reservedstate/service/FileSettingsService.java\n@@ -15,10 +15,12 @@ import org.elasticsearch.action.ActionResponse;\n import org.elasticsearch.action.support.PlainActionFuture;\n import org.elasticsearch.cluster.ClusterState;\n import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException;\n import org.elasticsearch.cluster.metadata.Metadata;\n import org.elasticsearch.cluster.metadata.ReservedStateMetadata;\n import org.elasticsearch.cluster.service.ClusterService;\n import org.elasticsearch.common.file.MasterNodeFileWatchingService;\n+import org.elasticsearch.common.file.NoChangeOccurredException;\n import org.elasticsearch.env.Environment;\n import org.elasticsearch.xcontent.XContentParserConfiguration;\n \n@@ -124,7 +126,18 @@ public class FileSettingsService extends MasterNodeFileWatchingService implement\n         ) {\n             stateService.process(NAMESPACE, parser, (e) -> completeProcessing(e, completion));\n         }\n-        completion.get();\n+        try {\n+            completion.get();\n+        } catch (ExecutionException e) {\n+            if (e.getCause() instanceof FailedToCommitClusterStateException x) {\n+                logger.error(\"Failed to commit cluster state\", x);\n+                // This failure is not the fault of FileSettingsService itself.\n+                // Throw NoChangeOccurredException to indicate we've already accused the guilty party.\n+                throw new NoChangeOccurredException(x);\n+            } else {\n+                throw e;\n+            }\n+        }\n     }\n \n     @Override\ndiff --git a/server/src/test/java/org/elasticsearch/index/LogsIndexModeTests.java b/server/src/test/java/org/elasticsearch/index/LogsIndexModeTests.java\nindex 23fc788a89bd..8a66bb1464a5 100644\n--- a/server/src/test/java/org/elasticsearch/index/LogsIndexModeTests.java\n+++ b/server/src/test/java/org/elasticsearch/index/LogsIndexModeTests.java\n@@ -13,24 +13,14 @@ import org.elasticsearch.cluster.metadata.IndexMetadata;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.test.ESTestCase;\n \n-import static org.hamcrest.Matchers.containsString;\n import static org.hamcrest.Matchers.equalTo;\n-import static org.hamcrest.Matchers.not;\n \n public class LogsIndexModeTests extends ESTestCase {\n     public void testLogsIndexModeSetting() {\n         assertThat(IndexSettings.MODE.get(buildSettings()), equalTo(IndexMode.LOGSDB));\n     }\n \n-    public void testDefaultHostNameSortField() {\n-        final IndexMetadata metadata = IndexSettingsTests.newIndexMeta(\"test\", buildSettings());\n-        assertThat(metadata.getIndexMode(), equalTo(IndexMode.LOGSDB));\n-        final IndexSettings settings = new IndexSettings(metadata, Settings.EMPTY);\n-        assertThat(settings.getIndexSortConfig().hasPrimarySortOnField(\"host.name\"), equalTo(true));\n-        assertThat(IndexMode.LOGSDB.getDefaultMapping(settings).string(), containsString(\"host.name\"));\n-    }\n-\n-    public void testCustomSortField() {\n+    public void testSortField() {\n         final Settings sortSettings = Settings.builder()\n             .put(buildSettings())\n             .put(IndexSortConfig.INDEX_SORT_FIELD_SETTING.getKey(), \"agent_id\")\n@@ -39,9 +29,7 @@ public class LogsIndexModeTests extends ESTestCase {\n         assertThat(metadata.getIndexMode(), equalTo(IndexMode.LOGSDB));\n         final IndexSettings settings = new IndexSettings(metadata, Settings.EMPTY);\n         assertThat(settings.getMode(), equalTo(IndexMode.LOGSDB));\n-        assertThat(getIndexSetting(settings, IndexSortConfig.INDEX_SORT_FIELD_SETTING.getKey()), equalTo(\"agent_id\"));\n-        assertThat(settings.getIndexSortConfig().hasPrimarySortOnField(\"host.name\"), equalTo(false));\n-        assertThat(IndexMode.LOGSDB.getDefaultMapping(settings).string(), not(containsString(\"host\")));\n+        assertThat(\"agent_id\", equalTo(getIndexSetting(settings, IndexSortConfig.INDEX_SORT_FIELD_SETTING.getKey())));\n     }\n \n     public void testSortMode() {\ndiff --git a/server/src/test/java/org/elasticsearch/index/mapper/IgnoredSourceFieldMapperTests.java b/server/src/test/java/org/elasticsearch/index/mapper/IgnoredSourceFieldMapperTests.java\nindex 934744ef3ef9..5eac5acdca28 100644\n--- a/server/src/test/java/org/elasticsearch/index/mapper/IgnoredSourceFieldMapperTests.java\n+++ b/server/src/test/java/org/elasticsearch/index/mapper/IgnoredSourceFieldMapperTests.java\n@@ -932,36 +932,6 @@ public class IgnoredSourceFieldMapperTests extends MapperServiceTestCase {\n             {\"path\":{\"id\":0.1,\"to\":{\"id\":[1,20,3,10]}}}\"\"\", syntheticSource);\n     }\n \n-    public void testArrayWithNestedObjects() throws IOException {\n-        DocumentMapper documentMapper = createMapperService(syntheticSourceMapping(b -> {\n-            b.startObject(\"path\").startObject(\"properties\");\n-            {\n-                b.startObject(\"to\").field(\"type\", \"nested\").startObject(\"properties\");\n-                {\n-                    b.startObject(\"id\").field(\"type\", \"integer\").field(\"synthetic_source_keep\", \"arrays\").endObject();\n-                }\n-                b.endObject().endObject();\n-            }\n-            b.endObject().endObject();\n-        })).documentMapper();\n-\n-        var syntheticSource = syntheticSource(documentMapper, b -> {\n-            b.startArray(\"path\");\n-            {\n-                b.startObject().startArray(\"to\");\n-                {\n-                    b.startObject().array(\"id\", 1, 20, 3).endObject();\n-                    b.startObject().field(\"id\", 10).endObject();\n-                }\n-                b.endArray().endObject();\n-                b.startObject().startObject(\"to\").field(\"id\", \"0.1\").endObject().endObject();\n-            }\n-            b.endArray();\n-        });\n-        assertEquals(\"\"\"\n-            {\"path\":{\"to\":[{\"id\":[1,20,3]},{\"id\":10},{\"id\":0}]}}\"\"\", syntheticSource);\n-    }\n-\n     public void testArrayWithinArray() throws IOException {\n         DocumentMapper documentMapper = createMapperService(syntheticSourceMapping(b -> {\n             b.startObject(\"path\");\ndiff --git a/test/framework/src/main/java/org/elasticsearch/index/mapper/MapperServiceTestCase.java b/test/framework/src/main/java/org/elasticsearch/index/mapper/MapperServiceTestCase.java\nindex 8bc2666bcfe3..a9ee0317ce1e 100644\n--- a/test/framework/src/main/java/org/elasticsearch/index/mapper/MapperServiceTestCase.java\n+++ b/test/framework/src/main/java/org/elasticsearch/index/mapper/MapperServiceTestCase.java\n@@ -302,12 +302,8 @@ public abstract class MapperServiceTestCase extends FieldTypeTestCase {\n                 mapperMetrics\n             );\n \n-            if (applyDefaultMapping && indexSettings.getMode().getDefaultMapping(indexSettings) != null) {\n-                mapperService.merge(\n-                    null,\n-                    indexSettings.getMode().getDefaultMapping(indexSettings),\n-                    MapperService.MergeReason.MAPPING_UPDATE\n-                );\n+            if (applyDefaultMapping && indexSettings.getMode().getDefaultMapping() != null) {\n+                mapperService.merge(null, indexSettings.getMode().getDefaultMapping(), MapperService.MergeReason.MAPPING_UPDATE);\n             }\n \n             return mapperService;\ndiff --git a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/expression/function/scalar/convert/ToDatetimeTests.java b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/expression/function/scalar/convert/ToDatetimeTests.java\nindex 2852b92ba156..7799c3c756f2 100644\n--- a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/expression/function/scalar/convert/ToDatetimeTests.java\n+++ b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/expression/function/scalar/convert/ToDatetimeTests.java\n@@ -134,9 +134,9 @@ public class ToDatetimeTests extends AbstractScalarFunctionTestCase {\n             \"ToDatetimeFromStringEvaluator[field=\" + read + \"]\",\n             List.of(\n                 new TestCaseSupplier.TypedDataSupplier(\n-                    \"<date string before -9999-12-31T23:59:59.999Z>\",\n-                    // millis before \"-9999-12-31T23:59:59.999Z\"\n-                    () -> new BytesRef(randomDateString(Long.MIN_VALUE, -377736739200000L)),\n+                    \"<date string before 0001-01-01T00:00:00.000Z>\",\n+                    // millis before \"0001-01-01T00:00:00.000Z\"\n+                    () -> new BytesRef(randomDateString(Long.MIN_VALUE, -62135596800001L)),\n                     DataType.KEYWORD\n                 )\n             ),\n@@ -154,8 +154,8 @@ public class ToDatetimeTests extends AbstractScalarFunctionTestCase {\n             \"ToDatetimeFromStringEvaluator[field=\" + read + \"]\",\n             List.of(\n                 new TestCaseSupplier.TypedDataSupplier(\n-                    \"<date string after 9999-12-31T23:59:59.999Z>\",\n-                    // millis after \"9999-12-31T23:59:59.999Z\"\n+                    \"<date string before 0001-01-01T00:00:00.000Z>\",\n+                    // millis before \"0001-01-01T00:00:00.000Z\"\n                     () -> new BytesRef(randomDateString(253402300800000L, Long.MAX_VALUE)),\n                     DataType.KEYWORD\n                 )\ndiff --git a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/chunking/EmbeddingRequestChunker.java b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/chunking/EmbeddingRequestChunker.java\nindex c5897f32d6eb..3ae8dc055039 100644\n--- a/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/chunking/EmbeddingRequestChunker.java\n+++ b/x-pack/plugin/inference/src/main/java/org/elasticsearch/xpack/inference/chunking/EmbeddingRequestChunker.java\n@@ -324,7 +324,7 @@ public class EmbeddingRequestChunker {\n         public void onFailure(Exception e) {\n             var errorResult = new ErrorChunkedInferenceResults(e);\n             for (var pos : positions) {\n-                errors.set(pos.inputIndex(), errorResult);\n+                errors.setOnce(pos.inputIndex(), errorResult);\n             }\n \n             if (resultCount.incrementAndGet() == totalNumberOfRequests) {\ndiff --git a/x-pack/plugin/logsdb/src/yamlRestTest/resources/rest-api-spec/test/30_logsdb_default_mapping.yml b/x-pack/plugin/logsdb/src/yamlRestTest/resources/rest-api-spec/test/30_logsdb_default_mapping.yml\nindex 3f2bca2e4bcd..52c500c102ce 100644\n--- a/x-pack/plugin/logsdb/src/yamlRestTest/resources/rest-api-spec/test/30_logsdb_default_mapping.yml\n+++ b/x-pack/plugin/logsdb/src/yamlRestTest/resources/rest-api-spec/test/30_logsdb_default_mapping.yml\n@@ -280,784 +280,3 @@ create logsdb data stream with timestamp object mapping:\n \n   - match: { error.type: \"illegal_argument_exception\" }\n   - match: { error.reason: \"composable template [logsdb-index-template] template after composition with component templates [logsdb-mappings] is invalid\" }\n-\n----\n-create logsdb data stream with custom sorting without host.name:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-http-prod ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ agent.id ]\n-                sort.order: [ desc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                agent.id:\n-                  type: keyword\n-                host.hostname:\n-                  type: keyword\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-http-prod\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-http-prod\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.agent.properties.id.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.hostname.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.type: null }\n-\n----\n-create logsdb data stream with custom sorting and host object:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-nginx-prod ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ host.hostname, host.region ]\n-                sort.order: [ desc, desc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host:\n-                  type: object\n-                  properties:\n-                    ip:\n-                      type: ip\n-                    hostname:\n-                      type: keyword\n-                    region:\n-                      type: keyword\n-                    name:\n-                      type: integer\n-\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-nginx-prod\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-nginx-prod\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.host.properties.ip.type: ip }\n-  - match: { .$backing_index.mappings.properties.host.properties.hostname.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.region.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.type: integer } # Overrides LogsDB injected\n-\n----\n-create logsdb data stream with custom sorting and dynamically mapped host.name:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-kafka-qa ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ \"agent.id\", \"@timestamp\" ]\n-                sort.order: [ desc, asc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                agent:\n-                  type: object\n-                  properties:\n-                    name:\n-                      type: keyword\n-                    id:\n-                      type: keyword\n-\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-kafka-qa\n-  - is_true: acknowledged\n-\n-  - do:\n-      bulk:\n-        index: logs-kafka-qa\n-        refresh: true\n-        body:\n-          - { \"create\": { } }\n-          - { \"@timestamp\": \"2022-01-01T00:00:00\", agent.name: \"foo\", agent.id: \"foo-568\", host: { id: \"db8fdcf1-b1e2-444b-8c6a-0466c61dcce4\" } }\n-          - { \"create\": { } }\n-          - { \"@timestamp\": \"2022-01-01T00:01:00\", agent.name: \"bar\", agent.id: \"foo-309\", host: { id: \"35e1ed10-961e-46c7-83ea-4109c913a1d6\" } }\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-kafka-qa\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.agent.properties.name.type: keyword }\n-  - match: { .$backing_index.mappings.properties.agent.properties.id.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name: null }\n-  - match: { .$backing_index.mappings.properties.host.properties.id.type: text }\n-\n----\n-create logsdb data stream with custom sorting and host.name object:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-nginx-qa ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ \"host.name.value\", \"@timestamp\" ]\n-                sort.order: [ desc, desc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host:\n-                  type: object\n-                  properties:\n-                    name:\n-                      type: object\n-                      properties:\n-                        value:\n-                          type: keyword\n-                        alias:\n-                          type: keyword\n-\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-nginx-qa\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-nginx-qa\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.properties.value.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.properties.alias.type: keyword }\n-\n----\n-create logsdb data stream with default sorting on malformed host.name:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-win-prod ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                agent:\n-                  type: object\n-                  properties:\n-                    name:\n-                      type: keyword\n-                    id:\n-                      type: keyword\n-\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-win-prod\n-  - is_true: acknowledged\n-\n-  - do:\n-      bulk:\n-        index: logs-win-prod\n-        refresh: true\n-        body:\n-          - { \"create\": { } }\n-          - { \"@timestamp\": \"2022-01-01T00:00:00\", agent.name: \"foo\", agent.id: \"foo-568\", host: { name: 192.168.10.12, id: \"e70e91cd-bb3f-43f0-909c-2748e7fdfd54\" } }\n-          - { \"create\": { } }\n-          - { \"@timestamp\": \"2022-01-01T00:01:00\", agent.name: \"bar\", agent.id: \"foo-309\", host: { name: 192.168.15.17, id: \"ad2e3edb-2c4b-4f12-83dd-255691ed614c\" } }\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-win-prod\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.agent.properties.name.type: keyword }\n-  - match: { .$backing_index.mappings.properties.agent.properties.id.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.type: keyword } # LogsDB injected\n-  - match: { .$backing_index.mappings.properties.host.properties.name.ignore_above: 1024 } # LogsDB injected\n-  - match: { .$backing_index.mappings.properties.host.properties.id.type: text }\n-\n----\n-create logsdb data stream with custom sorting and host.name date field:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-http-prod ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ host.name, host.hostname ]\n-                sort.order: [ desc, desc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host:\n-                  type: object\n-                  properties:\n-                    hostname:\n-                      type: keyword\n-                    name:\n-                      type: date\n-\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-http-prod\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-http-prod\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.host.properties.hostname.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.type: date }\n-\n----\n-create logsdb data stream with custom sorting and missing host.name field mapping:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-http-qa ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ host.name, host.hostname ]\n-                sort.order: [ desc, desc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host:\n-                  type: object\n-                  properties:\n-                    hostname:\n-                      type: keyword\n-\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-http-qa\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-http-qa\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.host.properties.hostname.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.ignore_above: 1024 }\n-\n----\n-create logsdb data stream with custom sorting and host.name field without doc values:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-http-dev ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ \"host.name\", \"@timestamp\" ]\n-                sort.order: [ desc, desc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host:\n-                  type: object\n-                  properties:\n-                    name:\n-                      type: keyword\n-                      doc_values: false\n-\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      catch: bad_request\n-      indices.create_data_stream:\n-        name: logs-http-dev\n-\n-  - match: { error.type: \"illegal_argument_exception\" }\n-  - match: { error.reason: \"docvalues not found for index sort field:[host.name]\" }\n-\n----\n-create logsdb data stream with incompatible ignore_above on host.name:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logsdb-index-template-ignore-above] has index patterns [logsdb-ignore-above] matching patterns from existing older templates [global]\"\n-      indices.put_index_template:\n-        name: logsdb-index-template-ignore-above\n-        body:\n-          index_patterns: [ logsdb-ignore-above ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ host.name ]\n-                sort.order: [ desc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host.name:\n-                  type: keyword\n-                  ignore_above: 128\n-          data_stream: {}\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logsdb-ignore-above\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logsdb-ignore-above\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.ignore_above: 128 }\n-\n----\n-create logsdb data stream with no sorting and host.name as text:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logsdb-index-template-non-keyword] has index patterns [logsdb-non-keyword] matching patterns from existing older templates [global]\"\n-      indices.put_index_template:\n-        name: logsdb-index-template-non-keyword\n-        body:\n-          index_patterns: [ logsdb-non-keyword ]\n-          priority: 10000\n-          template:\n-            settings:\n-              mode: logsdb\n-            mappings:\n-              properties:\n-                host.name:\n-                  type: text\n-          data_stream: {}\n-  - is_true: acknowledged\n-\n-  - do:\n-      catch: bad_request\n-      indices.create_data_stream:\n-        name: logsdb-non-keyword\n-\n-  - match: { error.type: \"illegal_argument_exception\" }\n-  - match: { error.reason: \"docvalues not found for index sort field:[host.name]\" }\n-\n----\n-create logsdb data stream without index sorting and ignore_above on host.name:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logsdb-index-template-ignore-above-override] has index patterns [logsdb-ignore-above-override] matching patterns from existing older templates [global]\"\n-      indices.put_index_template:\n-        name: logsdb-index-template-ignore-above-override\n-        body:\n-          index_patterns: [ logsdb-ignore-above-override ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host.name:\n-                  type: keyword\n-                  ignore_above: 128\n-          data_stream: {}\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logsdb-ignore-above-override\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logsdb-ignore-above-override\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.ignore_above: 128 }\n-\n----\n-create logsdb data stream with host.name as alias and sorting on it:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logsdb-index-template-alias] has index patterns [logsdb-alias] matching patterns from existing older templates [global]\"\n-      indices.put_index_template:\n-        name: logsdb-index-template-alias\n-        body:\n-          index_patterns: [ logsdb-alias ]\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ host.name ]\n-                sort.order: [ desc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host.name:\n-                  type: alias\n-                  path: host.hostname\n-                host.hostname:\n-                  type:\n-                    keyword\n-          data_stream: {}\n-  - do:\n-      catch: bad_request\n-      indices.create_data_stream:\n-        name: logsdb-alias\n-\n-  - match: { error.type: \"illegal_argument_exception\" }\n-  - match: { error.reason: \"Cannot use alias [host.name] as an index sort field\" }\n-\n----\n-create logsdb data stream with multi-fields on host.name:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logsdb-index-template-multi-fields] has index patterns [logsdb-multi-fields] matching patterns from existing older templates [global]\"\n-      indices.put_index_template:\n-        name: logsdb-index-template-multi-fields\n-        body:\n-          index_patterns: [ logsdb-multi-fields ]\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ host.name.keyword ]\n-                sort.order: [ asc ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                host.name:\n-                  type: \"text\"\n-                  fields:\n-                    keyword:\n-                      type: \"keyword\"\n-          data_stream: {}\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logsdb-multi-fields\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logsdb-multi-fields\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.fields.keyword.type: keyword }\n-\n----\n-create logsdb data stream with multi-fields on host.name and no sorting:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [ logsdb-no-sort-multi-fields-template ] has index patterns [logsdb-no-sort-multi-fields] matching patterns from existing older templates [global]\"\n-      indices.put_index_template:\n-        name: logsdb-no-sort-multi-fields-template\n-        body:\n-          index_patterns: [ logsdb-no-sort-multi-fields ]\n-          template:\n-            settings:\n-              mode: logsdb\n-            mappings:\n-              properties:\n-                host.name:\n-                  type: text\n-                  fields:\n-                    keyword:\n-                      type: keyword\n-          data_stream: {}\n-\n-  - do:\n-      catch: bad_request\n-      indices.create_data_stream:\n-        name: logsdb-no-sort-multi-fields\n-\n-  - match: { error.type: \"illegal_argument_exception\" }\n-  - match: { error.reason: \"docvalues not found for index sort field:[host.name]\" }\n-\n----\n-create logsdb data stream with custom empty sorting:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-http-empty ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ ]\n-                sort.order: [ ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                hostname:\n-                  type: keyword\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-http-empty\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-http-empty\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.type: keyword }\n-  - match: { .$backing_index.mappings.properties.host.properties.name.ignore_above: 1024 }\n-\n----\n-create logsdb data stream with custom sorting on timestamp:\n-  - skip:\n-      features: [ \"allowed_warnings\" ]\n-  - requires:\n-      cluster_features: [ \"mapper.keyword_normalizer_synthetic_source\" ]\n-      reason: support for normalizer on keyword fields\n-\n-  - do:\n-      allowed_warnings:\n-        - \"index template [logs-template] has index patterns [logs-*-*] matching patterns from existing older templates [global] with patterns (global => [*]); this template [logs-template] will take precedence during new index creation\"\n-      indices.put_index_template:\n-        name: logs-template\n-        body:\n-          index_patterns: [ logs-http-dev ]\n-          priority: 10000\n-          template:\n-            settings:\n-              index:\n-                sort.field: [ \"@timestamp\" ]\n-                sort.order: [ \"asc\" ]\n-                mode: logsdb\n-            mappings:\n-              properties:\n-                hostname:\n-                  type: keyword\n-          data_stream: { }\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.create_data_stream:\n-        name: logs-http-dev\n-  - is_true: acknowledged\n-\n-  - do:\n-      indices.get_data_stream:\n-        name: logs-http-dev\n-\n-  - set: { data_streams.0.indices.0.index_name: backing_index }\n-  - do:\n-      indices.get_mapping:\n-        index: $backing_index\n-\n-  - match: { .$backing_index.mappings.properties.@timestamp.type: date }\ndiff --git a/x-pack/plugin/otel-data/src/yamlRestTest/resources/rest-api-spec/test/20_logs_tests.yml b/x-pack/plugin/otel-data/src/yamlRestTest/resources/rest-api-spec/test/20_logs_tests.yml\nindex 6bc0cee78be4..be4de6dca6c7 100644\n--- a/x-pack/plugin/otel-data/src/yamlRestTest/resources/rest-api-spec/test/20_logs_tests.yml\n+++ b/x-pack/plugin/otel-data/src/yamlRestTest/resources/rest-api-spec/test/20_logs_tests.yml\n@@ -163,27 +163,3 @@ Structured log body:\n           fields: [\"event.dataset\"]\n   - length: { hits.hits: 1 }\n   - match: { hits.hits.0.fields.event\\.dataset: [\"generic.otel\"] }\n----\n-host.name pass-through:\n-  - do:\n-      bulk:\n-        index: logs-generic.otel-default\n-        refresh: true\n-        body:\n-          - create: {}\n-          - \"@timestamp\": 2024-07-18T14:48:33.467654000Z\n-            resource:\n-              attributes:\n-                host.name: localhost\n-  - is_false: errors\n-  - do:\n-      search:\n-        index: logs-generic.otel-default\n-        body:\n-          query:\n-            term:\n-              host.name: localhost\n-          fields: [ \"*\" ]\n-  - length: { hits.hits: 1 }\n-  - match: { hits.hits.0.fields.resource\\.attributes\\.host\\.name: [ \"localhost\" ] }\n-  - match: { hits.hits.0.fields.host\\.name: [ \"localhost\" ] }",
  "judge_mode": "comparative",
  "judge_model": "openai/claude-sonnet-4-5-20250929",
  "test_label": "v0",
  "agent_results": [
    {
      "runner": "auggie",
      "model": "sonnet4.5",
      "edit_run_id": "a9463435",
      "status": "success",
      "elapsed_ms": 1074467,
      "patch_unified": "diff --git a/server/src/main/java/org/elasticsearch/cluster/coordination/FailedToCommitClusterStateException.java b/server/src/main/java/org/elasticsearch/cluster/coordination/FailedToCommitClusterStateException.java\nindex 2731bc29..1f75563a 100644\n--- a/server/src/main/java/org/elasticsearch/cluster/coordination/FailedToCommitClusterStateException.java\n+++ b/server/src/main/java/org/elasticsearch/cluster/coordination/FailedToCommitClusterStateException.java\n@@ -22,10 +22,6 @@ import java.io.IOException;\n  */\n public class FailedToCommitClusterStateException extends ElasticsearchException {\n \n-    public FailedToCommitClusterStateException(StreamInput in) throws IOException {\n-        super(in);\n-    }\n-\n     public FailedToCommitClusterStateException(String msg, Object... args) {\n         super(msg, args);\n     }\n@@ -33,4 +29,8 @@ public class FailedToCommitClusterStateException extends ElasticsearchException\n     public FailedToCommitClusterStateException(String msg, Throwable cause, Object... args) {\n         super(msg, cause, args);\n     }\n+\n+    public FailedToCommitClusterStateException(StreamInput in) throws IOException {\n+        super(in);\n+    }\n }\ndiff --git a/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\nindex dcb28a17..a41b94aa 100644\n--- a/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\n+++ b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\n@@ -11,6 +11,8 @@ package org.elasticsearch.common.file;\n \n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException;\n import org.elasticsearch.common.Randomness;\n import org.elasticsearch.common.component.AbstractLifecycleComponent;\n import org.elasticsearch.reservedstate.service.FileChangedListener;\n@@ -297,7 +299,12 @@ public abstract class AbstractFileWatchingService extends AbstractLifecycleCompo\n                 listener.watchedFileChanged();\n             }\n         } catch (IOException | ExecutionException e) {\n-            logger.error(() -> \"Error processing watched file: \" + watchedFile(), e);\n+            // Check if the ExecutionException wraps a FailedToCommitClusterStateException\n+            // If so, don't log an error because it represents a cluster state publication failure\n+            // that will be handled elsewhere and does not indicate a problem with the watched file itself\n+            if (ExceptionsHelper.unwrap(e, FailedToCommitClusterStateException.class) == null) {\n+                logger.error(() -> \"Error processing watched file: \" + watchedFile(), e);\n+            }\n         }\n     }\n ",
      "scores": {
        "correctness": -0.8,
        "completeness": -0.9,
        "code_reuse": 0.3,
        "best_practices": -0.5,
        "unsolicited_docs": 1.0
      },
      "aggregate": -0.18000000000000002,
      "rationale": "The agent's approach is fundamentally incorrect. The ground truth creates a new exception class `NoChangeOccurredException` to signal that no change occurred (allowing opt-out from error logging), and catches it separately in `AbstractFileWatchingService` to log at INFO level instead of ERROR. The agent instead modifies the catch block to check if the exception wraps a `FailedToCommitClusterStateException` using `ExceptionsHelper.unwrap()`, which is the wrong approach - it's checking for a specific exception type rather than creating a new signal mechanism. The agent also unnecessarily reorders constructor declarations in `FailedToCommitClusterStateException`. The ground truth also modifies `FileSettingsService.processFileChanges()` to catch and wrap `FailedToCommitClusterStateException` in the new exception type, which the agent completely misses. The agent's solution doesn't create the new exception class at all, which is the core requirement.",
      "llm_rating": 0.12,
      "llm_summary": "Modified AbstractFileWatchingService catch block to check for wrapped FailedToCommitClusterStateException using ExceptionsHelper.unwrap() instead of creating the required NoChangeOccurredException class and updating FileSettingsService as specified in ground truth",
      "errors": []
    },
    {
      "runner": "claude-code",
      "model": "claude-sonnet-4-5",
      "edit_run_id": "c7a3f90a",
      "status": "success",
      "elapsed_ms": 200018,
      "patch_unified": "diff --git a/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\nindex dcb28a17..db64e25d 100644\n--- a/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\n+++ b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\n@@ -11,6 +11,7 @@ package org.elasticsearch.common.file;\n \n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException;\n import org.elasticsearch.common.Randomness;\n import org.elasticsearch.common.component.AbstractLifecycleComponent;\n import org.elasticsearch.reservedstate.service.FileChangedListener;\n@@ -218,6 +219,9 @@ public abstract class AbstractFileWatchingService extends AbstractLifecycleCompo\n                         if (watchedFileChanged(path)) {\n                             processSettingsAndNotifyListeners();\n                         }\n+                    } catch (ExecutionException e) {\n+                        // ExecutionException from FailedToCommitClusterStateException propagated for specialized handling\n+                        throw e;\n                     } catch (IOException e) {\n                         logger.warn(\"encountered I/O error while watching file settings\", e);\n                     }\n@@ -290,13 +294,19 @@ public abstract class AbstractFileWatchingService extends AbstractLifecycleCompo\n         } while (true);\n     }\n \n-    void processSettingsAndNotifyListeners() throws InterruptedException {\n+    void processSettingsAndNotifyListeners() throws InterruptedException, ExecutionException {\n         try {\n             processFileChanges();\n             for (var listener : eventListeners) {\n                 listener.watchedFileChanged();\n             }\n-        } catch (IOException | ExecutionException e) {\n+        } catch (ExecutionException e) {\n+            if (e.getCause() instanceof FailedToCommitClusterStateException) {\n+                // Don't log FailedToCommitClusterStateException - let it propagate for specialized handling\n+                throw e;\n+            }\n+            logger.error(() -> \"Error processing watched file: \" + watchedFile(), e);\n+        } catch (IOException e) {\n             logger.error(() -> \"Error processing watched file: \" + watchedFile(), e);\n         }\n     }",
      "scores": {
        "correctness": -0.8,
        "completeness": -0.9,
        "code_reuse": 0.3,
        "best_practices": -0.5,
        "unsolicited_docs": 1.0
      },
      "aggregate": -0.18000000000000002,
      "rationale": "The agent's implementation is fundamentally incorrect. The ground truth creates a new NoChangeOccurredException class to distinguish FailedToCommitClusterStateException from other ExecutionExceptions, catching NoChangeOccurredException separately with info-level logging while keeping error-level logging for other ExecutionExceptions. The agent instead propagates FailedToCommitClusterStateException up the call stack by rethrowing ExecutionException, which breaks the error handling contract and changes method signatures unnecessarily. The agent also adds a try-catch in the wrong location (watcherThread) and adds unsolicited comments. The ground truth's approach allows FileSettingsService to wrap FailedToCommitClusterStateException in NoChangeOccurredException, enabling AbstractFileWatchingService to handle it with appropriate logging without changing method signatures or breaking encapsulation.",
      "llm_rating": 0.12,
      "llm_summary": "Agent propagated FailedToCommitClusterStateException by rethrowing ExecutionException and changing method signatures instead of creating NoChangeOccurredException class for specialized catch handling with info-level logging",
      "errors": []
    },
    {
      "runner": "factory",
      "model": "claude-sonnet-4-5-20250929",
      "edit_run_id": "074538b9",
      "status": "success",
      "elapsed_ms": 278819,
      "patch_unified": "diff --git a/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\nindex dcb28a17..39f7aead 100644\n--- a/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\n+++ b/server/src/main/java/org/elasticsearch/common/file/AbstractFileWatchingService.java\n@@ -220,6 +220,11 @@ public abstract class AbstractFileWatchingService extends AbstractLifecycleCompo\n                         }\n                     } catch (IOException e) {\n                         logger.warn(\"encountered I/O error while watching file settings\", e);\n+                    } catch (ExecutionException e) {\n+                        // ExecutionException wrapping FailedToCommitClusterStateException is rethrown to the outer catch\n+                        // block where it's handled as a terminal error, causing the watcher thread to shut down.\n+                        // All other ExecutionExceptions are already handled in processSettingsAndNotifyListeners().\n+                        throw e;\n                     }\n                 } else {\n                     key.pollEvents();\n@@ -228,6 +233,16 @@ public abstract class AbstractFileWatchingService extends AbstractLifecycleCompo\n             }\n         } catch (ClosedWatchServiceException | InterruptedException expected) {\n             logger.info(\"shutting down watcher thread\");\n+        } catch (ExecutionException e) {\n+            if (e.getCause() instanceof org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException) {\n+                // FailedToCommitClusterStateException is a terminal error that indicates cluster state publication\n+                // failed. We don't log this as a generic \"Error processing watched file\" to allow for different\n+                // alerting strategies, but we do log it as a watcher thread shutdown.\n+                logger.error(\"shutting down watcher thread due to cluster state publication failure\", e);\n+            } else {\n+                // This should not happen as other ExecutionExceptions are handled in processSettingsAndNotifyListeners()\n+                logger.error(\"shutting down watcher thread with exception\", e);\n+            }\n         } catch (Exception e) {\n             logger.error(\"shutting down watcher thread with exception\", e);\n         }\n@@ -290,13 +305,21 @@ public abstract class AbstractFileWatchingService extends AbstractLifecycleCompo\n         } while (true);\n     }\n \n-    void processSettingsAndNotifyListeners() throws InterruptedException {\n+    void processSettingsAndNotifyListeners() throws InterruptedException, ExecutionException {\n         try {\n             processFileChanges();\n             for (var listener : eventListeners) {\n                 listener.watchedFileChanged();\n             }\n-        } catch (IOException | ExecutionException e) {\n+        } catch (ExecutionException e) {\n+            // Check if the cause is FailedToCommitClusterStateException, which indicates a cluster state publication\n+            // failure. These failures have different alerting requirements and should not be logged as generic file\n+            // processing errors. The exception is rethrown as ExecutionException to maintain the method signature.\n+            if (e.getCause() instanceof org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException) {\n+                throw e;\n+            }\n+            logger.error(() -> \"Error processing watched file: \" + watchedFile(), e);\n+        } catch (IOException e) {\n             logger.error(() -> \"Error processing watched file: \" + watchedFile(), e);\n         }\n     }\ndiff --git a/server/src/test/java/org/elasticsearch/common/file/AbstractFileWatchingServiceTests.java b/server/src/test/java/org/elasticsearch/common/file/AbstractFileWatchingServiceTests.java\nindex ebf1064c..90e6517f 100644\n--- a/server/src/test/java/org/elasticsearch/common/file/AbstractFileWatchingServiceTests.java\n+++ b/server/src/test/java/org/elasticsearch/common/file/AbstractFileWatchingServiceTests.java\n@@ -194,6 +194,47 @@ public class AbstractFileWatchingServiceTests extends ESTestCase {\n         verify(service, times(2)).retryDelayMillis(anyInt());\n     }\n \n+    public void testProcessSettingsWithFailedToCommitClusterStateException() throws Exception {\n+        var service = new TestFileWatchingService(getWatchedFilePath(env)) {\n+            @Override\n+            protected void processFileChanges() throws ExecutionException {\n+                throw new ExecutionException(\n+                    new org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException(\"test failure\")\n+                );\n+            }\n+        };\n+\n+        // FailedToCommitClusterStateException wrapped in ExecutionException should be rethrown\n+        ExecutionException thrown = assertThrows(ExecutionException.class, service::processSettingsAndNotifyListeners);\n+        assertTrue(thrown.getCause() instanceof org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException);\n+    }\n+\n+    public void testProcessSettingsWithOtherExecutionException() throws Exception {\n+        var service = new TestFileWatchingService(getWatchedFilePath(env)) {\n+            @Override\n+            protected void processFileChanges() throws ExecutionException {\n+                throw new ExecutionException(new RuntimeException(\"other error\"));\n+            }\n+        };\n+\n+        // Other ExecutionExceptions should be caught and logged, not rethrown\n+        service.processSettingsAndNotifyListeners();\n+        // If we reach here without exception, the test passes\n+    }\n+\n+    public void testProcessSettingsWithIOException() throws Exception {\n+        var service = new TestFileWatchingService(getWatchedFilePath(env)) {\n+            @Override\n+            protected void processFileChanges() throws IOException {\n+                throw new IOException(\"IO error\");\n+            }\n+        };\n+\n+        // IOException should be caught and logged, not rethrown\n+        service.processSettingsAndNotifyListeners();\n+        // If we reach here without exception, the test passes\n+    }\n+\n     // helpers\n     private void writeTestFile(Path path, String contents) throws IOException {\n         Path tempFilePath = createTempFile();",
      "scores": {
        "correctness": -0.8,
        "completeness": -0.9,
        "code_reuse": 0.3,
        "best_practices": -0.5,
        "unsolicited_docs": 0.8
      },
      "aggregate": -0.22000000000000003,
      "rationale": "The agent's approach is fundamentally incorrect. The ground truth creates a new NoChangeOccurredException class to distinguish FailedToCommitClusterStateException from other errors, catching it separately to log at INFO level instead of ERROR. The agent instead modifies AbstractFileWatchingService to rethrow FailedToCommitClusterStateException wrapped in ExecutionException, which would terminate the watcher thread - a critical functional break. The ground truth also modifies FileSettingsService to catch and wrap FailedToCommitClusterStateException in NoChangeOccurredException, which the agent completely misses. The agent adds unsolicited test methods and inline comments explaining the logic. The only correct aspect is identifying that FailedToCommitClusterStateException needs special handling, but the implementation is wrong and incomplete.",
      "llm_rating": 0.12,
      "llm_summary": "Agent modified AbstractFileWatchingService to rethrow FailedToCommitClusterStateException instead of creating NoChangeOccurredException class and modifying FileSettingsService as required, breaking the watcher thread functionality and missing core changes.",
      "errors": []
    }
  ],
  "comparative_analysis": {
    "summary": "All three agents (auggie:sonnet4.5, claude-code:claude-sonnet-4-5, and factory:claude-sonnet-4-5-20250929) implemented nearly identical solutions to handle FailedToCommitClusterStateException separately from generic file watching errors. The implementations differ primarily in code organization, comment verbosity, and test coverage, with factory:claude-sonnet-4-5-20250929 providing the most comprehensive solution including test cases.",
    "best_agent": "factory:claude-sonnet-4-5-20250929",
    "best_agent_reasoning": "factory:claude-sonnet-4-5-20250929 provided the most complete solution by including test coverage for the new exception handling behavior, added proper exception handling in the watcher thread's outer catch block, and provided more detailed explanatory comments about why FailedToCommitClusterStateException requires special treatment. While all agents achieved the core functionality, factory's inclusion of tests and more thorough error handling in the watcher thread demonstrates better software engineering practices.",
    "approach_differences": "auggie:sonnet4.5 used ExceptionsHelper.unwrap() to check for FailedToCommitClusterStateException and made a minor reordering of constructor methods, claude-code:claude-sonnet-4-5 added an intermediate catch block in the watcher thread and split the IOException/ExecutionException catch into separate blocks, while factory:claude-sonnet-4-5-20250929 took a similar approach to claude-code but added comprehensive test coverage and more detailed comments explaining the rationale. All three agents correctly identified that the exception should be rethrown from processSettingsAndNotifyListeners() to avoid the generic error logging, but factory provided the most complete implementation with tests.",
    "ranking": [
      "factory:claude-sonnet-4-5-20250929",
      "claude-code:claude-sonnet-4-5",
      "auggie:sonnet4.5"
    ]
  },
  "timestamp": "2025-11-06T21:53:22.295898",
  "analysis_run_id": "128524d4"
}