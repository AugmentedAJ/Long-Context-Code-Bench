{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 114911,
  "base_commit": "f6a1e36d6be56a5d480765ad2d5f72f4adcaef5b",
  "head_commit": "d95609c4fca7f356a7e7ab808ce8d9caa4f0cac1",
  "task_instructions": "You are working on a codebase. Your task is to make the necessary code changes to accomplish the following:\n\nEnhance empty queue conditional in slicing logic\n\nWith recent changes in Lucene 9.12 around not forking execution when not necessary (see https://github.com/apache/lucene/pull/13472), we have removed the search worker thread pool in #111099. The worker thread pool had unlimited queue, and we feared that we couuld have much more queueing on the search thread pool if we execute segment level searches on the same thread pool as the shard level searches, because every shard search would take up to a thread per slice when executing the query phase.\r\n\r\nWe have then introduced an additional conditional to stop parallelizing when there is a queue. That is perhaps a bit extreme, as it's a decision made when creating the searcher, while a queue may no longer be there once the search is executing. This has caused some benchmarks regressions, given that having a queue may be a transient scenario, especially with short-lived segment searches being queued up. We may end up disabling inter-segment concurrency more aggressively than we would want, penalizing requests that do benefit from concurrency. At the same time, we do want to have some kind of protection against rejections of shard searches that would be caused by excessive slicing. When the queue is above a certain size, we can turn off the slicing and effectively disable inter-segment concurrency. With this commit we set that threshold to be the number of threads in the search pool.\n\nPlease make all necessary code changes to complete this task.",
  "ground_truth_diff": "diff --git a/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java b/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\nindex 1521b17a8176..8ac35f7c40ca 100644\n--- a/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\n+++ b/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\n@@ -291,7 +291,7 @@ final class DefaultSearchContext extends SearchContext {\n         ToLongFunction<String> fieldCardinality\n     ) {\n         return executor instanceof ThreadPoolExecutor tpe\n-            && tpe.getQueue().isEmpty()\n+            && tpe.getQueue().size() <= tpe.getMaximumPoolSize()\n             && isParallelCollectionSupportedForResults(resultsType, request.source(), fieldCardinality, enableQueryPhaseParallelCollection)\n                 ? tpe.getMaximumPoolSize()\n                 : 1;\ndiff --git a/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java b/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\nindex 0e4945e8bb8d..a474c1dc38c5 100644\n--- a/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\n+++ b/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\n@@ -81,6 +81,7 @@ import java.io.IOException;\n import java.util.UUID;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.function.Function;\n import java.util.function.Supplier;\n import java.util.function.ToLongFunction;\n@@ -507,10 +508,10 @@ public class DefaultSearchContextTests extends MapperServiceTestCase {\n         }\n     }\n \n-    public void testDetermineMaximumNumberOfSlices() {\n+    private static ShardSearchRequest createParallelRequest() {\n         IndexShard indexShard = mock(IndexShard.class);\n         when(indexShard.shardId()).thenReturn(new ShardId(\"index\", \"uuid\", 0));\n-        ShardSearchRequest parallelReq = new ShardSearchRequest(\n+        return new ShardSearchRequest(\n             OriginalIndices.NONE,\n             new SearchRequest().allowPartialSearchResults(randomBoolean()),\n             indexShard.shardId(),\n@@ -521,69 +522,74 @@ public class DefaultSearchContextTests extends MapperServiceTestCase {\n             System.currentTimeMillis(),\n             null\n         );\n-        ShardSearchRequest singleSliceReq = new ShardSearchRequest(\n-            OriginalIndices.NONE,\n-            new SearchRequest().allowPartialSearchResults(randomBoolean())\n-                .source(new SearchSourceBuilder().sort(SortBuilders.fieldSort(FieldSortBuilder.DOC_FIELD_NAME))),\n-            indexShard.shardId(),\n-            0,\n-            1,\n-            AliasFilter.EMPTY,\n-            1f,\n-            System.currentTimeMillis(),\n-            null\n-        );\n-        int executorPoolSize = randomIntBetween(1, 100);\n-        ExecutorService threadPoolExecutor = EsExecutors.newFixed(\n-            \"test\",\n-            executorPoolSize,\n-            0,\n-            Thread::new,\n-            new ThreadContext(Settings.EMPTY),\n-            EsExecutors.TaskTrackingConfig.DO_NOT_TRACK\n-        );\n-        ExecutorService notThreadPoolExecutor = Executors.newWorkStealingPool();\n-        ToLongFunction<String> fieldCardinality = name -> -1;\n \n+    }\n+\n+    public void testDetermineMaximumNumberOfSlicesNoExecutor() {\n+        ToLongFunction<String> fieldCardinality = name -> { throw new UnsupportedOperationException(); };\n         assertEquals(\n-            executorPoolSize,\n+            1,\n             DefaultSearchContext.determineMaximumNumberOfSlices(\n-                threadPoolExecutor,\n-                parallelReq,\n+                null,\n+                createParallelRequest(),\n                 SearchService.ResultsType.DFS,\n-                true,\n+                randomBoolean(),\n                 fieldCardinality\n             )\n         );\n         assertEquals(\n-            executorPoolSize,\n+            1,\n             DefaultSearchContext.determineMaximumNumberOfSlices(\n-                threadPoolExecutor,\n-                singleSliceReq,\n-                SearchService.ResultsType.DFS,\n-                true,\n+                null,\n+                createParallelRequest(),\n+                SearchService.ResultsType.QUERY,\n+                randomBoolean(),\n                 fieldCardinality\n             )\n         );\n+    }\n+\n+    public void testDetermineMaximumNumberOfSlicesNotThreadPoolExecutor() {\n+        ExecutorService notThreadPoolExecutor = Executors.newWorkStealingPool();\n+        ToLongFunction<String> fieldCardinality = name -> { throw new UnsupportedOperationException(); };\n         assertEquals(\n             1,\n-            DefaultSearchContext.determineMaximumNumberOfSlices(null, parallelReq, SearchService.ResultsType.DFS, true, fieldCardinality)\n+            DefaultSearchContext.determineMaximumNumberOfSlices(\n+                notThreadPoolExecutor,\n+                createParallelRequest(),\n+                SearchService.ResultsType.DFS,\n+                randomBoolean(),\n+                fieldCardinality\n+            )\n         );\n         assertEquals(\n-            executorPoolSize,\n+            1,\n             DefaultSearchContext.determineMaximumNumberOfSlices(\n-                threadPoolExecutor,\n-                parallelReq,\n+                notThreadPoolExecutor,\n+                createParallelRequest(),\n                 SearchService.ResultsType.QUERY,\n-                true,\n+                randomBoolean(),\n                 fieldCardinality\n             )\n         );\n+    }\n+\n+    public void testDetermineMaximumNumberOfSlicesEnableQueryPhaseParallelCollection() {\n+        int executorPoolSize = randomIntBetween(1, 100);\n+        ThreadPoolExecutor threadPoolExecutor = EsExecutors.newFixed(\n+            \"test\",\n+            executorPoolSize,\n+            0,\n+            Thread::new,\n+            new ThreadContext(Settings.EMPTY),\n+            EsExecutors.TaskTrackingConfig.DO_NOT_TRACK\n+        );\n+        ToLongFunction<String> fieldCardinality = name -> -1;\n         assertEquals(\n-            1,\n+            executorPoolSize,\n             DefaultSearchContext.determineMaximumNumberOfSlices(\n                 threadPoolExecutor,\n-                singleSliceReq,\n+                createParallelRequest(),\n                 SearchService.ResultsType.QUERY,\n                 true,\n                 fieldCardinality\n@@ -592,54 +598,133 @@ public class DefaultSearchContextTests extends MapperServiceTestCase {\n         assertEquals(\n             1,\n             DefaultSearchContext.determineMaximumNumberOfSlices(\n-                notThreadPoolExecutor,\n-                parallelReq,\n-                SearchService.ResultsType.DFS,\n-                true,\n+                threadPoolExecutor,\n+                createParallelRequest(),\n+                SearchService.ResultsType.QUERY,\n+                false,\n                 fieldCardinality\n             )\n         );\n-\n         assertEquals(\n             executorPoolSize,\n             DefaultSearchContext.determineMaximumNumberOfSlices(\n                 threadPoolExecutor,\n-                parallelReq,\n+                createParallelRequest(),\n                 SearchService.ResultsType.DFS,\n-                false,\n+                randomBoolean(),\n                 fieldCardinality\n             )\n         );\n-        assertEquals(\n+    }\n+\n+    public void testDetermineMaximumNumberOfSlicesSingleSortByField() {\n+        IndexShard indexShard = mock(IndexShard.class);\n+        when(indexShard.shardId()).thenReturn(new ShardId(\"index\", \"uuid\", 0));\n+        ShardSearchRequest singleSliceReq = new ShardSearchRequest(\n+            OriginalIndices.NONE,\n+            new SearchRequest().allowPartialSearchResults(randomBoolean())\n+                .source(new SearchSourceBuilder().sort(SortBuilders.fieldSort(FieldSortBuilder.DOC_FIELD_NAME))),\n+            indexShard.shardId(),\n+            0,\n             1,\n-            DefaultSearchContext.determineMaximumNumberOfSlices(null, parallelReq, SearchService.ResultsType.DFS, false, fieldCardinality)\n+            AliasFilter.EMPTY,\n+            1f,\n+            System.currentTimeMillis(),\n+            null\n         );\n+        ToLongFunction<String> fieldCardinality = name -> { throw new UnsupportedOperationException(); };\n+        int executorPoolSize = randomIntBetween(1, 100);\n+        ThreadPoolExecutor threadPoolExecutor = EsExecutors.newFixed(\n+            \"test\",\n+            executorPoolSize,\n+            0,\n+            Thread::new,\n+            new ThreadContext(Settings.EMPTY),\n+            EsExecutors.TaskTrackingConfig.DO_NOT_TRACK\n+        );\n+        // DFS concurrency does not rely on slices, hence it kicks in regardless of the request (supportsParallelCollection is not called)\n         assertEquals(\n-            1,\n+            executorPoolSize,\n             DefaultSearchContext.determineMaximumNumberOfSlices(\n                 threadPoolExecutor,\n-                parallelReq,\n-                SearchService.ResultsType.QUERY,\n-                false,\n+                singleSliceReq,\n+                SearchService.ResultsType.DFS,\n+                true,\n                 fieldCardinality\n             )\n         );\n-        assertEquals(\n-            1,\n-            DefaultSearchContext.determineMaximumNumberOfSlices(null, parallelReq, SearchService.ResultsType.QUERY, false, fieldCardinality)\n-        );\n         assertEquals(\n             1,\n             DefaultSearchContext.determineMaximumNumberOfSlices(\n-                notThreadPoolExecutor,\n-                parallelReq,\n-                SearchService.ResultsType.DFS,\n-                false,\n+                threadPoolExecutor,\n+                singleSliceReq,\n+                SearchService.ResultsType.QUERY,\n+                true,\n                 fieldCardinality\n             )\n         );\n     }\n \n+    public void testDetermineMaximumNumberOfSlicesWithQueue() {\n+        int executorPoolSize = randomIntBetween(1, 100);\n+        ThreadPoolExecutor threadPoolExecutor = EsExecutors.newFixed(\n+            \"test\",\n+            executorPoolSize,\n+            1000,\n+            Thread::new,\n+            new ThreadContext(Settings.EMPTY),\n+            EsExecutors.TaskTrackingConfig.DO_NOT_TRACK\n+        );\n+        ToLongFunction<String> fieldCardinality = name -> { throw new UnsupportedOperationException(); };\n+\n+        for (int i = 0; i < executorPoolSize; i++) {\n+            assertTrue(threadPoolExecutor.getQueue().offer(() -> {}));\n+            assertEquals(\n+                executorPoolSize,\n+                DefaultSearchContext.determineMaximumNumberOfSlices(\n+                    threadPoolExecutor,\n+                    createParallelRequest(),\n+                    SearchService.ResultsType.DFS,\n+                    true,\n+                    fieldCardinality\n+                )\n+            );\n+            assertEquals(\n+                executorPoolSize,\n+                DefaultSearchContext.determineMaximumNumberOfSlices(\n+                    threadPoolExecutor,\n+                    createParallelRequest(),\n+                    SearchService.ResultsType.QUERY,\n+                    true,\n+                    fieldCardinality\n+                )\n+            );\n+        }\n+        for (int i = 0; i < 100; i++) {\n+            assertTrue(threadPoolExecutor.getQueue().offer(() -> {}));\n+            assertEquals(\n+                1,\n+                DefaultSearchContext.determineMaximumNumberOfSlices(\n+                    threadPoolExecutor,\n+                    createParallelRequest(),\n+                    SearchService.ResultsType.DFS,\n+                    true,\n+                    fieldCardinality\n+                )\n+            );\n+            assertEquals(\n+                1,\n+                DefaultSearchContext.determineMaximumNumberOfSlices(\n+                    threadPoolExecutor,\n+                    createParallelRequest(),\n+                    SearchService.ResultsType.QUERY,\n+                    true,\n+                    fieldCardinality\n+                )\n+            );\n+        }\n+    }\n+\n     public void testIsParallelCollectionSupportedForResults() {\n         SearchSourceBuilder searchSourceBuilderOrNull = randomBoolean() ? null : new SearchSourceBuilder();\n         ToLongFunction<String> fieldCardinality = name -> -1;",
  "judge_mode": "comparative",
  "judge_model": "openai/claude-sonnet-4-5-20250929",
  "test_label": "v0",
  "agent_results": [
    {
      "runner": "auggie",
      "model": "sonnet4.5",
      "edit_run_id": "a9463435",
      "status": "success",
      "elapsed_ms": 644997,
      "patch_unified": "diff --git a/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java b/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\nindex 1521b17a..8ac35f7c 100644\n--- a/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\n+++ b/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\n@@ -291,7 +291,7 @@ final class DefaultSearchContext extends SearchContext {\n         ToLongFunction<String> fieldCardinality\n     ) {\n         return executor instanceof ThreadPoolExecutor tpe\n-            && tpe.getQueue().isEmpty()\n+            && tpe.getQueue().size() <= tpe.getMaximumPoolSize()\n             && isParallelCollectionSupportedForResults(resultsType, request.source(), fieldCardinality, enableQueryPhaseParallelCollection)\n                 ? tpe.getMaximumPoolSize()\n                 : 1;\ndiff --git a/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java b/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\nindex 0e4945e8..61ae9608 100644\n--- a/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\n+++ b/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\n@@ -79,6 +79,7 @@ import org.elasticsearch.xcontent.XContentBuilder;\n \n import java.io.IOException;\n import java.util.UUID;\n+import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.function.Function;\n@@ -640,6 +641,114 @@ public class DefaultSearchContextTests extends MapperServiceTestCase {\n         );\n     }\n \n+    public void testDetermineMaximumNumberOfSlicesWithQueueSize() throws InterruptedException {\n+        IndexShard indexShard = mock(IndexShard.class);\n+        when(indexShard.shardId()).thenReturn(new ShardId(\"index\", \"uuid\", 0));\n+        ShardSearchRequest parallelReq = new ShardSearchRequest(\n+            OriginalIndices.NONE,\n+            new SearchRequest().allowPartialSearchResults(randomBoolean()),\n+            indexShard.shardId(),\n+            0,\n+            1,\n+            AliasFilter.EMPTY,\n+            1f,\n+            System.currentTimeMillis(),\n+            null\n+        );\n+        ToLongFunction<String> fieldCardinality = name -> -1;\n+\n+        int executorPoolSize = randomIntBetween(2, 10);\n+        // Create a thread pool executor with a queue\n+        ExecutorService threadPoolExecutor = EsExecutors.newFixed(\n+            \"test\",\n+            executorPoolSize,\n+            executorPoolSize * 2, // queue size is 2x pool size\n+            Thread::new,\n+            new ThreadContext(Settings.EMPTY),\n+            EsExecutors.TaskTrackingConfig.DO_NOT_TRACK\n+        );\n+\n+        try {\n+            // With empty queue, slicing should be enabled\n+            assertEquals(\n+                executorPoolSize,\n+                DefaultSearchContext.determineMaximumNumberOfSlices(\n+                    threadPoolExecutor,\n+                    parallelReq,\n+                    SearchService.ResultsType.DFS,\n+                    true,\n+                    fieldCardinality\n+                )\n+            );\n+\n+            // Fill the queue with tasks up to the pool size\n+            CountDownLatch blockLatch = new CountDownLatch(1);\n+            CountDownLatch startLatch = new CountDownLatch(executorPoolSize);\n+            for (int i = 0; i < executorPoolSize; i++) {\n+                threadPoolExecutor.execute(() -> {\n+                    startLatch.countDown();\n+                    try {\n+                        blockLatch.await();\n+                    } catch (InterruptedException e) {\n+                        Thread.currentThread().interrupt();\n+                    }\n+                });\n+            }\n+            // Wait for all threads to start\n+            startLatch.await();\n+\n+            // Add tasks to the queue up to pool size\n+            for (int i = 0; i < executorPoolSize; i++) {\n+                threadPoolExecutor.execute(() -> {\n+                    try {\n+                        blockLatch.await();\n+                    } catch (InterruptedException e) {\n+                        Thread.currentThread().interrupt();\n+                    }\n+                });\n+            }\n+\n+            // With queue size == pool size, slicing should still be enabled\n+            assertEquals(\n+                executorPoolSize,\n+                DefaultSearchContext.determineMaximumNumberOfSlices(\n+                    threadPoolExecutor,\n+                    parallelReq,\n+                    SearchService.ResultsType.DFS,\n+                    true,\n+                    fieldCardinality\n+                )\n+            );\n+\n+            // Add one more task to exceed the pool size threshold\n+            threadPoolExecutor.execute(() -> {\n+                try {\n+                    blockLatch.await();\n+                } catch (InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            });\n+\n+            // With queue size > pool size, slicing should be disabled\n+            assertEquals(\n+                1,\n+                DefaultSearchContext.determineMaximumNumberOfSlices(\n+                    threadPoolExecutor,\n+                    parallelReq,\n+                    SearchService.ResultsType.DFS,\n+                    true,\n+                    fieldCardinality\n+                )\n+            );\n+\n+            // Release all tasks\n+            blockLatch.countDown();\n+        } finally {\n+            threadPoolExecutor.shutdown();\n+            threadPoolExecutor.awaitTermination(10, java.util.concurrent.TimeUnit.SECONDS);\n+        }\n+    }\n+\n     public void testIsParallelCollectionSupportedForResults() {\n         SearchSourceBuilder searchSourceBuilderOrNull = randomBoolean() ? null : new SearchSourceBuilder();\n         ToLongFunction<String> fieldCardinality = name -> -1;",
      "scores": {
        "correctness": 1.0,
        "completeness": 0.3,
        "code_reuse": 0.5,
        "best_practices": 0.7,
        "unsolicited_docs": 1.0
      },
      "aggregate": 0.7,
      "rationale": "The agent correctly implements the core logic change in DefaultSearchContext.java, changing the queue condition from isEmpty() to size() <= getMaximumPoolSize(), which matches the ground truth perfectly. However, the agent's test refactoring differs significantly from ground truth: it adds only one new test method (testDetermineMaximumNumberOfSlicesWithQueueSize) using CountDownLatch for thread synchronization, while ground truth refactors the existing test into multiple focused test methods (testDetermineMaximumNumberOfSlicesNoExecutor, testDetermineMaximumNumberOfSlicesNotThreadPoolExecutor, etc.) and adds a simpler testDetermineMaximumNumberOfSlicesWithQueue that directly manipulates the queue without thread blocking. The agent's test is more complex with thread coordination but tests the same behavior. The agent also misses the createParallelRequest() helper method extraction and the comprehensive test reorganization present in ground truth. The missing ThreadPoolExecutor import in the agent's diff is a minor issue.",
      "llm_rating": 0.6,
      "llm_summary": "Changed queue condition from isEmpty() to size() <= getMaximumPoolSize() in DefaultSearchContext.java and added a single complex test using CountDownLatch for thread synchronization, but missed the ground truth's test refactoring into multiple focused methods and the simpler queue manipulation approach.",
      "errors": []
    },
    {
      "runner": "claude-code",
      "model": "claude-sonnet-4-5",
      "edit_run_id": "c7a3f90a",
      "status": "success",
      "elapsed_ms": 207845,
      "patch_unified": "diff --git a/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java b/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\nindex 1521b17a..8ac35f7c 100644\n--- a/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\n+++ b/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\n@@ -291,7 +291,7 @@ final class DefaultSearchContext extends SearchContext {\n         ToLongFunction<String> fieldCardinality\n     ) {\n         return executor instanceof ThreadPoolExecutor tpe\n-            && tpe.getQueue().isEmpty()\n+            && tpe.getQueue().size() <= tpe.getMaximumPoolSize()\n             && isParallelCollectionSupportedForResults(resultsType, request.source(), fieldCardinality, enableQueryPhaseParallelCollection)\n                 ? tpe.getMaximumPoolSize()\n                 : 1;\ndiff --git a/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java b/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java\nindex 9164fd88..a468b857 100644\n--- a/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java\n+++ b/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java\n@@ -746,11 +746,11 @@ public class TransportTermsEnumAction extends HandledTransportAction<TermsEnumRe\n         if (request.shardIds().size() == 0) {\n             listener.onResponse(NodeTermsEnumResponse.empty(request.nodeId()));\n         } else {\n-            // Use the search threadpool if its queue is empty\n+            // Use the search threadpool if its queue size is below the pool size threshold\n             assert transportService.getThreadPool().executor(ThreadPool.Names.SEARCH) instanceof EsThreadPoolExecutor\n                 : \"SEARCH threadpool must be an instance of ThreadPoolExecutor\";\n             EsThreadPoolExecutor ex = (EsThreadPoolExecutor) transportService.getThreadPool().executor(ThreadPool.Names.SEARCH);\n-            final Executor executor = ex.getQueue().size() == 0 ? ex : shardExecutor;\n+            final Executor executor = ex.getQueue().size() <= ex.getMaximumPoolSize() ? ex : shardExecutor;\n             executor.execute(ActionRunnable.supply(listener, () -> dataNodeOperation(request)));\n         }\n     }",
      "scores": {
        "correctness": 0.5,
        "completeness": -0.3,
        "code_reuse": 1.0,
        "best_practices": 0.8,
        "unsolicited_docs": 1.0
      },
      "aggregate": 0.6,
      "rationale": "The agent correctly modified the core logic in DefaultSearchContext.java, changing the queue check from isEmpty() to size() <= getMaximumPoolSize(), which matches the ground truth. However, it completely missed all the test changes in DefaultSearchContextTests.java, which represent a significant portion of the ground truth diff (refactoring existing tests and adding a new comprehensive test for queue behavior). The agent also made an additional unsolicited change to TransportTermsEnumAction.java that was not in the ground truth, though this change is consistent with the same pattern and appears reasonable. The core functional change is correct, but the missing test coverage is a major omission.",
      "llm_rating": 0.52,
      "llm_summary": "Modified DefaultSearchContext.java queue conditional from isEmpty() to size() <= getMaximumPoolSize() and applied same pattern to TransportTermsEnumAction.java, but omitted all test refactoring and new test cases from DefaultSearchContextTests.java",
      "errors": []
    },
    {
      "runner": "factory",
      "model": "claude-sonnet-4-5-20250929",
      "edit_run_id": "074538b9",
      "status": "success",
      "elapsed_ms": 259476,
      "patch_unified": "diff --git a/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java b/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\nindex 1521b17a..8ac35f7c 100644\n--- a/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\n+++ b/server/src/main/java/org/elasticsearch/search/DefaultSearchContext.java\n@@ -291,7 +291,7 @@ final class DefaultSearchContext extends SearchContext {\n         ToLongFunction<String> fieldCardinality\n     ) {\n         return executor instanceof ThreadPoolExecutor tpe\n-            && tpe.getQueue().isEmpty()\n+            && tpe.getQueue().size() <= tpe.getMaximumPoolSize()\n             && isParallelCollectionSupportedForResults(resultsType, request.source(), fieldCardinality, enableQueryPhaseParallelCollection)\n                 ? tpe.getMaximumPoolSize()\n                 : 1;\ndiff --git a/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java b/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\nindex 0e4945e8..a863b7e0 100644\n--- a/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\n+++ b/server/src/test/java/org/elasticsearch/search/DefaultSearchContextTests.java\n@@ -79,6 +79,7 @@ import org.elasticsearch.xcontent.XContentBuilder;\n \n import java.io.IOException;\n import java.util.UUID;\n+import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.function.Function;\n@@ -640,6 +641,113 @@ public class DefaultSearchContextTests extends MapperServiceTestCase {\n         );\n     }\n \n+    public void testDetermineMaximumNumberOfSlicesWithQueueSize() throws InterruptedException {\n+        IndexShard indexShard = mock(IndexShard.class);\n+        when(indexShard.shardId()).thenReturn(new ShardId(\"index\", \"uuid\", 0));\n+        ShardSearchRequest parallelReq = new ShardSearchRequest(\n+            OriginalIndices.NONE,\n+            new SearchRequest().allowPartialSearchResults(randomBoolean()),\n+            indexShard.shardId(),\n+            0,\n+            1,\n+            AliasFilter.EMPTY,\n+            1f,\n+            System.currentTimeMillis(),\n+            null\n+        );\n+        int executorPoolSize = randomIntBetween(2, 10);\n+        int queueSize = executorPoolSize + randomIntBetween(1, 10);\n+        // Create an executor with queue capacity\n+        ExecutorService threadPoolExecutor = EsExecutors.newFixed(\n+            \"test\",\n+            executorPoolSize,\n+            queueSize,\n+            Thread::new,\n+            new ThreadContext(Settings.EMPTY),\n+            EsExecutors.TaskTrackingConfig.DO_NOT_TRACK\n+        );\n+        ToLongFunction<String> fieldCardinality = name -> -1;\n+\n+        // Queue is empty, should allow parallelization\n+        assertEquals(\n+            executorPoolSize,\n+            DefaultSearchContext.determineMaximumNumberOfSlices(\n+                threadPoolExecutor,\n+                parallelReq,\n+                SearchService.ResultsType.DFS,\n+                true,\n+                fieldCardinality\n+            )\n+        );\n+\n+        // Use a latch to block all threads so tasks queue up\n+        final CountDownLatch blockingLatch = new CountDownLatch(1);\n+        final CountDownLatch readyLatch = new CountDownLatch(executorPoolSize);\n+        \n+        // Fill the thread pool with blocked tasks\n+        for (int i = 0; i < executorPoolSize; i++) {\n+            threadPoolExecutor.submit(() -> {\n+                try {\n+                    readyLatch.countDown();\n+                    blockingLatch.await();\n+                } catch (InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            });\n+        }\n+        \n+        // Wait for all threads to be occupied\n+        readyLatch.await();\n+\n+        // Now add tasks to the queue up to the threshold (equal to pool size)\n+        for (int i = 0; i < executorPoolSize; i++) {\n+            threadPoolExecutor.submit(() -> {\n+                try {\n+                    blockingLatch.await();\n+                } catch (InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            });\n+        }\n+\n+        // Queue size is at the threshold (equal to pool size), should still allow parallelization\n+        assertEquals(\n+            executorPoolSize,\n+            DefaultSearchContext.determineMaximumNumberOfSlices(\n+                threadPoolExecutor,\n+                parallelReq,\n+                SearchService.ResultsType.DFS,\n+                true,\n+                fieldCardinality\n+            )\n+        );\n+\n+        // Add one more task to exceed the threshold\n+        threadPoolExecutor.submit(() -> {\n+            try {\n+                blockingLatch.await();\n+            } catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+            }\n+        });\n+\n+        // Queue size exceeds the threshold, should disable parallelization\n+        assertEquals(\n+            1,\n+            DefaultSearchContext.determineMaximumNumberOfSlices(\n+                threadPoolExecutor,\n+                parallelReq,\n+                SearchService.ResultsType.DFS,\n+                true,\n+                fieldCardinality\n+            )\n+        );\n+\n+        // Release all threads and clean up\n+        blockingLatch.countDown();\n+        threadPoolExecutor.shutdown();\n+    }\n+\n     public void testIsParallelCollectionSupportedForResults() {\n         SearchSourceBuilder searchSourceBuilderOrNull = randomBoolean() ? null : new SearchSourceBuilder();\n         ToLongFunction<String> fieldCardinality = name -> -1;",
      "scores": {
        "correctness": 1.0,
        "completeness": 0.4,
        "code_reuse": 0.5,
        "best_practices": 0.7,
        "unsolicited_docs": 1.0
      },
      "aggregate": 0.72,
      "rationale": "The agent correctly implements the core logic change in DefaultSearchContext.java, changing the queue condition from isEmpty() to size() <= getMaximumPoolSize(), which matches the ground truth perfectly. However, the agent's test approach differs significantly: instead of refactoring existing tests into multiple focused test methods (testDetermineMaximumNumberOfSlicesNoExecutor, testDetermineMaximumNumberOfSlicesNotThreadPoolExecutor, etc.) and adding a comprehensive testDetermineMaximumNumberOfSlicesWithQueue that tests both threshold and above-threshold scenarios, the agent adds only a single new test method (testDetermineMaximumNumberOfSlicesWithQueueSize) that uses CountDownLatch for thread blocking. The ground truth refactors the existing testDetermineMaximumNumberOfSlices into 5 separate focused tests plus adds a new comprehensive queue test, while the agent leaves the original test intact and adds one additional test. The agent's test is more complex (using latches) but less comprehensive than the ground truth's approach of simply offering tasks to the queue. The agent misses the test refactoring entirely and the helper method createParallelRequest(), resulting in code duplication.",
      "llm_rating": 0.62,
      "llm_summary": "Changed queue condition from isEmpty() to size() <= getMaximumPoolSize() in DefaultSearchContext.java correctly but added only one new test using CountDownLatch instead of refactoring existing tests into multiple focused methods and adding comprehensive queue threshold testing as in ground truth",
      "errors": []
    }
  ],
  "comparative_analysis": {
    "summary": "All three agents correctly implemented the core logic change from `isEmpty()` to `size() <= getMaximumPoolSize()` in DefaultSearchContext.java. The key differentiator was test coverage: auggie:sonnet4.5 and factory:claude-sonnet-4-5-20250929 added comprehensive tests for the queue size threshold behavior, while claude-code:claude-sonnet-4-5 made an unrelated change to TransportTermsEnumAction.java instead of adding tests.",
    "best_agent": "auggie:sonnet4.5",
    "best_agent_reasoning": "auggie:sonnet4.5 achieved the highest aggregate score (0.70) by correctly implementing the main logic change and adding thorough test coverage that validates the queue size threshold behavior. The agent created a test that verifies slicing is enabled when queue size equals the pool size and disabled when it exceeds the threshold, directly testing the new conditional logic.",
    "approach_differences": "All agents made the identical core change to DefaultSearchContext.java. However, auggie:sonnet4.5 and factory:claude-sonnet-4-5-20250929 both added new test methods (testDetermineMaximumNumberOfSlicesWithQueueSize) that use CountDownLatch to simulate queue conditions and verify the threshold behavior. In contrast, claude-code:claude-sonnet-4-5 applied the same queue size logic to a different file (TransportTermsEnumAction.java) rather than adding tests, which was not part of the expected changes and resulted in negative completeness scoring.",
    "ranking": [
      "auggie:sonnet4.5",
      "factory:claude-sonnet-4-5-20250929",
      "claude-code:claude-sonnet-4-5"
    ]
  },
  "timestamp": "2025-11-06T21:50:53.038455",
  "analysis_run_id": "0bf2e216"
}