{"timestamp": 1762415358.977737, "event": "agent_start", "runner": "factory", "model": "claude-sonnet-4-5-20250929", "command": ["droid", "exec", "--skip-permissions-unsafe", "You are working on a codebase. Your task is to make the necessary code changes to accomplish the following:\n\n[DOCS] Updates adaptive allocations reference docs\n\n## Overview\r\n\r\nAdaptive allocations can scale down to `0` allocations. This PR removes a sentence that stated that it is not possible.\n\nPlease make all necessary code changes to complete this task.", "--output-format", "stream-json", "--model", "claude-sonnet-4-5-20250929"], "workspace": "/var/folders/1t/tn_3vh3x0nv_snwb7xrv97th0000gn/T/tmpj1glbd_x/workspace", "timeout_s": 7200}
{"timestamp": 1762415450.8927512, "event": "agent_run", "stdout": "{\"type\":\"system\",\"subtype\":\"init\",\"cwd\":\"/private/var/folders/1t/tn_3vh3x0nv_snwb7xrv97th0000gn/T/tmpj1glbd_x/workspace\",\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\",\"tools\":[\"Read\",\"LS\",\"Execute\",\"Edit\",\"ApplyPatch\",\"Grep\",\"Glob\",\"Create\",\"ExitSpecMode\",\"WebSearch\",\"TodoWrite\",\"FetchUrl\",\"slack_post_message\",\"GenerateDroid\"],\"model\":\"claude-sonnet-4-5-20250929\"}\n{\"type\":\"message\",\"role\":\"user\",\"id\":\"63c8405a-6941-4dd7-bbde-bf6c316f646f\",\"text\":\"You are working on a codebase. Your task is to make the necessary code changes to accomplish the following:\\n\\n[DOCS] Updates adaptive allocations reference docs\\n\\n## Overview\\r\\n\\r\\nAdaptive allocations can scale down to `0` allocations. This PR removes a sentence that stated that it is not possible.\\n\\nPlease make all necessary code changes to complete this task.\",\"timestamp\":1762415360335,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"771f1183-ed2b-48dd-8440-777b6d933351\",\"text\":\"I'll help you update the adaptive allocations reference documentation to remove the sentence stating that it cannot scale down to 0 allocations. Let me start by exploring the codebase to locate the relevant documentation.\",\"timestamp\":1762415369355,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01AR3582KMMF6GyCMxorAgzL\",\"messageId\":\"771f1183-ed2b-48dd-8440-777b6d933351\",\"toolId\":\"TodoWrite\",\"toolName\":\"TodoWrite\",\"parameters\":{\"todos\":[{\"id\":\"1\",\"content\":\"Search for adaptive allocations documentation files\",\"status\":\"in_progress\",\"priority\":\"high\"},{\"id\":\"2\",\"content\":\"Locate the sentence about not being able to scale down to 0 allocations\",\"status\":\"pending\",\"priority\":\"high\"},{\"id\":\"3\",\"content\":\"Remove or update the incorrect sentence\",\"status\":\"pending\",\"priority\":\"high\"},{\"id\":\"4\",\"content\":\"Verify the changes are correct\",\"status\":\"pending\",\"priority\":\"medium\"}]},\"timestamp\":1762415369355,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_011Sfqf7Wsjd68zsgb18F3QB\",\"messageId\":\"771f1183-ed2b-48dd-8440-777b6d933351\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"adaptive allocations\",\"case_insensitive\":true,\"output_mode\":\"file_paths\"},\"timestamp\":1762415369355,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01RgfD3DCnDt8Gh2Th1ynZwE\",\"messageId\":\"771f1183-ed2b-48dd-8440-777b6d933351\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"scale down to 0\",\"case_insensitive\":true,\"output_mode\":\"content\",\"context\":3},\"timestamp\":1762415369355,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_0125jtmD49Szxnd11yVBGK4X\",\"messageId\":\"771f1183-ed2b-48dd-8440-777b6d933351\",\"toolId\":\"Glob\",\"toolName\":\"Glob\",\"parameters\":{\"patterns\":[\"**/adaptive*.md\",\"**/adaptive*.asciidoc\",\"**/adaptive*.adoc\"]},\"timestamp\":1762415369355,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01AR3582KMMF6GyCMxorAgzL\",\"messageId\":\"a17a20d3-ef17-4cdc-b8aa-94933ddc3902\",\"toolId\":\"TodoWrite\",\"isError\":false,\"value\":\"TODO List Updated\",\"timestamp\":1762415370153,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_011Sfqf7Wsjd68zsgb18F3QB\",\"messageId\":\"a17a20d3-ef17-4cdc-b8aa-94933ddc3902\",\"toolId\":\"Grep\",\"isError\":false,\"value\":\"./x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerServiceTests.java\\n./x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/assignment/TrainedModelAssignmentClusterService.java\\n./x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerService.java\\n./x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScaler.java\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/TrainedModelAssignment.java\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/action/StartTrainedModelDeploymentAction.java\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/action/UpdateTrainedModelDeploymentAction.java\\n./docs/reference/inference/service-elser.asciidoc\\n./docs/reference/ml/trained-models/apis/update-trained-model-deployment.asciidoc\\n./docs/reference/ml/trained-models/apis/start-trained-model-deployment.asciidoc\\n./docs/reference/ml/ml-shared.asciidoc\\n./docs/reference/search/search-your-data/semantic-search-semantic-text.asciidoc\\n./docs/reference/search/search-your-data/semantic-text-hybrid-search\\n./docs/changelog/110630.yaml\",\"timestamp\":1762415370153,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01RgfD3DCnDt8Gh2Th1ynZwE\",\"messageId\":\"a17a20d3-ef17-4cdc-b8aa-94933ddc3902\",\"toolId\":\"Grep\",\"isError\":false,\"value\":\"./x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerTests.java-    }\\n./x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerTests.java-\\n./x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerTests.java-    public void testAutoscaling_scaleDownToZeroAllocations() {\\n./x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerTests.java:        int scaleDownAfterInactivitySeconds = 60 * 15; // scale down to 0 after 15 minutes\\n./x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerTests.java-        AdaptiveAllocationsScaler adaptiveAllocationsScaler = new AdaptiveAllocationsScaler(\\n./x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerTests.java-            \\\"test-deployment\\\",\\n./x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/adaptiveallocations/AdaptiveAllocationsScalerTests.java-            1,\",\"timestamp\":1762415370153,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_0125jtmD49Szxnd11yVBGK4X\",\"messageId\":\"a17a20d3-ef17-4cdc-b8aa-94933ddc3902\",\"toolId\":\"Glob\",\"isError\":false,\"value\":\"No matching files found\",\"timestamp\":1762415370153,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"7dad1dd6-14f6-496b-8734-6415f6fb62b9\",\"text\":\"Good! I found several documentation files. Let me search for documentation that mentions scaling down to 0 or mentions that it's not possible:\",\"timestamp\":1762415376699,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_0165LmLM1oLEDuVvirYQKEUz\",\"messageId\":\"7dad1dd6-14f6-496b-8734-6415f6fb62b9\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"not.*scale.*down.*0|cannot.*scale.*0|can.*not.*scale.*0\",\"case_insensitive\":true,\"output_mode\":\"content\",\"context\":5,\"glob_pattern\":\"*.asciidoc\"},\"timestamp\":1762415376699,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01AtFYBsVyKLBT7rw5uhBxLh\",\"messageId\":\"7dad1dd6-14f6-496b-8734-6415f6fb62b9\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"scale.*zero|zero.*allocation\",\"case_insensitive\":true,\"output_mode\":\"content\",\"context\":5,\"glob_pattern\":\"*.asciidoc\"},\"timestamp\":1762415376699,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_0165LmLM1oLEDuVvirYQKEUz\",\"messageId\":\"e1cfeff3-2300-4fd1-b172-349ec2aef525\",\"toolId\":\"Grep\",\"isError\":false,\"value\":\"No matches found\",\"timestamp\":1762415376895,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01AtFYBsVyKLBT7rw5uhBxLh\",\"messageId\":\"e1cfeff3-2300-4fd1-b172-349ec2aef525\",\"toolId\":\"Grep\",\"isError\":false,\"value\":\"./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-The Normalize Aggregation supports multiple methods to transform the bucket values. Each method definition will use\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-the following original set of bucket values as examples: `[5, 5, 10, 50, 10, 20]`.\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-_rescale_0_1_::\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc:                This method rescales the data such that the minimum number is zero, and the maximum number is 1, with the rest normalized\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-                linearly in-between.\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-                x' = (x - min_x) / (max_x - min_x)\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-                [0, 0, .1111, 1, .1111, .3333]\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-_rescale_0_100_::\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc:                This method rescales the data such that the minimum number is zero, and the maximum number is 100, with the rest normalized\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-                linearly in-between.\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-                x' = 100 * (x - min_x) / (max_x - min_x)\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-\\n./docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc-                [0, 0, 11.11, 100, 11.11, 33.33]\",\"timestamp\":1762415376895,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01WAiFiGEkDNmQ5Vxt1fejvR\",\"messageId\":\"3bb70396-e683-4551-aadd-50e0deeb60b8\",\"toolId\":\"\",\"isError\":false,\"value\":\"[role=\\\"xpack\\\"]\\n[[start-trained-model-deployment]]\\n= Start trained model deployment API\\n[subs=\\\"attributes\\\"]\\n++++\\n<titleabbrev>Start trained model deployment</titleabbrev>\\n++++\\n\\nStarts a new trained model deployment.\\n\\n[[start-trained-model-deployment-request]]\\n== {api-request-title}\\n\\n`POST _ml/trained_models/<model_id>/deployment/_start`\\n\\n[[start-trained-model-deployment-prereq]]\\n== {api-prereq-title}\\nRequires the `manage_ml` cluster privilege. This privilege is included in the\\n`machine_learning_admin` built-in role.\\n\\n[[start-trained-model-deployment-desc]]\\n== {api-description-title}\\n\\nCurrently only `pytorch` models are supported for deployment. Once deployed\\nthe model can be used by the <<inference-processor,{infer-cap} processor>>\\nin an ingest pipeline or directly in the <<infer-trained-model>> API.\\n\\nA model can be deployed multiple times by using deployment IDs. A deployment ID\\nmust be unique and should not match any other deployment ID or model ID, unless\\nit is the same as the ID of the model being deployed. If `deployment_id` is not\\nset, it defaults to the `model_id`.\\n\\nYou can enable adaptive allocations to automatically scale model allocations up\\nand down based on the actual resource requirement of the processes.\\n\\nManually scaling inference performance can be achieved by setting the parameters\\n`number_of_allocations` and `threads_per_allocation`.\\n\\nIncreasing `threads_per_allocation` means more threads are used when an\\ninference request is processed on a node. This can improve inference speed for\\ncertain models. It may also result in improvement to throughput.\\n\\nIncreasing `number_of_allocations` means more threads are used to process\\nmultiple inference requests in parallel resulting in throughput improvement.\\nEach model allocation uses a number of threads defined by\\n`threads_per_allocation`.\\n\\nModel allocations are distributed across {ml} nodes. All allocations assigned to\\na node share the same copy of the model in memory. To avoid thread\\noversubscription which is detrimental to performance, model allocations are\\ndistributed in such a way that the total number of used threads does not surpass\\nthe node's allocated processors.\\n\\n[[start-trained-model-deployment-path-params]]\\n== {api-path-parms-title}\\n\\n`<model_id>`::\\n(Required, string)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=model-id]\\n\\n[[start-trained-model-deployment-query-params]]\\n== {api-query-parms-title}\\n\\n`deployment_id`::\\n(Optional, string)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=deployment-id]\\n+\\n--\\nDefaults to `model_id`.\\n--\\n\\n`timeout`::\\n(Optional, time)\\nControls the amount of time to wait for the model to deploy. Defaults to 30\\nseconds.\\n\\n`wait_for`::\\n(Optional, string)\\nSpecifies the allocation status to wait for before returning. Defaults to\\n`started`. The value `starting` indicates deployment is starting but not yet on\\nany node. The value `started` indicates the model has started on at least one\\nnode. The value `fully_allocated` indicates the deployment has started on all\\nvalid nodes.\\n\\n[[start-trained-model-deployment-request-body]]\\n== {api-request-body-title}\\n\\n`adaptive_allocations`::\\n(Optional, object)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation]\\n\\n`enabled`:::\\n(Optional, Boolean)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-enabled]\\n\\n`max_number_of_allocations`:::\\n(Optional, integer)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-max-number]\\n\\n`min_number_of_allocations`:::\\n(Optional, integer)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-min-number]\\n\\n`cache_size`::\\n(Optional, <<byte-units,byte value>>)\\nThe inference cache size (in memory outside the JVM heap) per node for the\\nmodel. In serverless, the cache is disabled by default. Otherwise, the default value is the size of the model as reported by the\\n`model_size_bytes` field in the <<get-trained-models-stats>>. To disable the\\ncache, `0b` can be provided.\\n\\n`number_of_allocations`::\\n(Optional, integer)\\nThe total number of allocations this model is assigned across {ml} nodes.\\nIncreasing this value generally increases the throughput. Defaults to `1`.\\nIf `adaptive_allocations` is enabled, do not set this value, because it's automatically set.\\n\\n`priority`::\\n(Optional, string)\\nThe priority of the deployment. The default value is `normal`.\\nThere are two priority settings:\\n+\\n--\\n* `normal`: Use this for deployments in production. The deployment allocations\\nare distributed so that node processors are not oversubscribed.\\n* `low`: Use this for testing model functionality. The intention is that these\\ndeployments are not sent a high volume of input. The deployment is required to\\nhave a single allocation with just one thread. Low priority deployments may be\\nassigned on nodes that already utilize all their processors but will be given a\\nlower CPU priority than normal deployments. Low priority deployments may be\\nunassigned in order to satisfy more allocations of normal priority deployments.\\n--\\n\\nWARNING: Heavy usage of low priority deployments may impact performance of\\nnormal priority deployments.\\n\\n`queue_capacity`::\\n(Optional, integer)\\nControls how many inference requests are allowed in the queue at a time.\\nEvery machine learning node in the cluster where the model can be allocated\\nhas a queue of this size; when the number of requests exceeds the total value,\\nnew requests are rejected with a 429 error. Defaults to 1024. Max allowed value\\nis 1000000.\\n\\n`threads_per_allocation`::\\n(Optional, integer)\\nSets the number of threads used by each model allocation during inference. This\\ngenerally increases the speed per inference request. The inference process is a\\ncompute-bound process; `threads_per_allocations` must not exceed the number of\\navailable allocated processors per node. Defaults to 1. Must be a power of 2.\\nMax allowed value is 32.\\n\\n\\n[[start-trained-model-deployment-example]]\\n== {api-examples-title}\\n\\nThe following example starts a new deployment for a\\n`elastic__distilbert-base-uncased-finetuned-conll03-english` trained model:\\n\\n[source,console]\\n--------------------------------------------------\\nPOST _ml/trained_models/elastic__distilbert-base-uncased-finetuned-conll03-english/deployment/_start?wait_for=started&timeout=1m\\n--------------------------------------------------\\n// TEST[skip:TBD]\\n\\nThe API returns the following results:\\n\\n[source,console-result]\\n----\\n{\\n    \\\"assignment\\\": {\\n        \\\"task_parameters\\\": {\\n            \\\"model_id\\\": \\\"elastic__distilbert-base-uncased-finetuned-conll03-english\\\",\\n            \\\"model_bytes\\\": 265632637,\\n            \\\"threads_per_allocation\\\" : 1,\\n            \\\"number_of_allocations\\\" : 1,\\n            \\\"queue_capacity\\\" : 1024,\\n            \\\"priority\\\": \\\"normal\\\"\\n        },\\n        \\\"routing_table\\\": {\\n            \\\"uckeG3R8TLe2MMNBQ6AGrw\\\": {\\n                \\\"routing_state\\\": \\\"started\\\",\\n                \\\"reason\\\": \\\"\\\"\\n            }\\n        },\\n        \\\"assignment_state\\\": \\\"started\\\",\\n        \\\"start_time\\\": \\\"2022-11-02T11:50:34.766591Z\\\"\\n    }\\n}\\n----\\n\\n\\n[[start-trained-model-deployment-deployment-id-example]]\\n=== Using deployment IDs\\n\\nThe following example starts a new deployment for the `my_model` trained model\\nwith the ID `my_model_for_ingest`. The deployment ID an be used in {infer} API\\ncalls or in {infer} processors.\\n\\n[source,console]\\n--------------------------------------------------\\nPOST _ml/trained_models/my_model/deployment/_start?deployment_id=my_model_for_ingest\\n--------------------------------------------------\\n// TEST[skip:TBD]\\n\\nThe `my_model` trained model can be deployed again with a different ID:\\n\\n[source,console]\\n--------------------------------------------------\\nPOST _ml/trained_models/my_model/deployment/_start?deployment_id=my_model_for_search\\n--------------------------------------------------\\n// TEST[skip:TBD]\\n\\n\\n[[start-trained-model-deployment-adaptive-allocation-example]]\\n=== Setting adaptive allocations\\n\\nThe following example starts a new deployment of the `my_model` trained model\\nwith the ID `my_model_for_search` and enables adaptive allocations with the\\nminimum number of 3 allocations and the maximum number of 10. \\n\\n[source,console]\\n--------------------------------------------------\\nPOST _ml/trained_models/my_model/deployment/_start?deployment_id=my_model_for_search\\n{\\n  \\\"adaptive_allocations\\\": {\\n    \\\"enabled\\\": true,\\n    \\\"min_number_of_allocations\\\": 3,\\n    \\\"max_number_of_allocations\\\": 10\\n  }\\n}\\n--------------------------------------------------\\n// TEST[skip:TBD]\",\"timestamp\":1762415382154,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01Fq22sC1h8YKKKwZGgFJatB\",\"messageId\":\"3bb70396-e683-4551-aadd-50e0deeb60b8\",\"toolId\":\"\",\"isError\":false,\"value\":\"[role=\\\"xpack\\\"]\\n[[update-trained-model-deployment]]\\n= Update trained model deployment API\\n\\n[subs=\\\"attributes\\\"]\\n++++\\n<titleabbrev>Update trained model deployment</titleabbrev>\\n++++\\n\\nUpdates certain properties of a trained model deployment.\\n\\n[[update-trained-model-deployment-request]]\\n== {api-request-title}\\n\\n`POST _ml/trained_models/<deployment_id>/deployment/_update`\\n\\n\\n[[update-trained-model-deployments-prereqs]]\\n== {api-prereq-title}\\n\\nRequires the `manage_ml` cluster privilege. This privilege is included in the\\n`machine_learning_admin` built-in role.\\n\\n[[update-trained-model-deployment-desc]]\\n== {api-description-title}\\n\\nYou can update a trained model deployment whose `assignment_state` is `started`.\\nYou can enable adaptive allocations to automatically scale model allocations up\\nand down based on the actual resource requirement of the processes.\\nOr you can manually increase or decrease the number of allocations of a model\\ndeployment.\\n\\n\\n[[update-trained-model-deployments-path-parms]]\\n== {api-path-parms-title}\\n\\n`<deployment_id>`::\\n(Required, string)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=deployment-id]\\n\\n[[update-trained-model-deployment-request-body]]\\n== {api-request-body-title}\\n\\n`adaptive_allocations`::\\n(Optional, object)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation]\\n\\n`enabled`:::\\n(Optional, Boolean)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-enabled]\\n\\n`max_number_of_allocations`:::\\n(Optional, integer)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-max-number]\\n\\n`min_number_of_allocations`:::\\n(Optional, integer)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=adaptive-allocation-min-number]\\n\\n`number_of_allocations`::\\n(Optional, integer)\\nThe total number of allocations this model is assigned across {ml} nodes.\\nIncreasing this value generally increases the throughput.\\nIf `adaptive_allocations` is enabled, do not set this value, because it's automatically set.\\n\\n\\n[[update-trained-model-deployment-example]]\\n== {api-examples-title}\\n\\nThe following example updates the deployment for a\\n`elastic__distilbert-base-uncased-finetuned-conll03-english` trained model to have 4 allocations:\\n\\n[source,console]\\n--------------------------------------------------\\nPOST _ml/trained_models/elastic__distilbert-base-uncased-finetuned-conll03-english/deployment/_update\\n{\\n  \\\"number_of_allocations\\\": 4\\n}\\n--------------------------------------------------\\n// TEST[skip:TBD]\\n\\nThe API returns the following results:\\n\\n[source,console-result]\\n----\\n{\\n    \\\"assignment\\\": {\\n        \\\"task_parameters\\\": {\\n            \\\"model_id\\\": \\\"elastic__distilbert-base-uncased-finetuned-conll03-english\\\",\\n            \\\"model_bytes\\\": 265632637,\\n            \\\"threads_per_allocation\\\" : 1,\\n            \\\"number_of_allocations\\\" : 4,\\n            \\\"queue_capacity\\\" : 1024\\n        },\\n        \\\"routing_table\\\": {\\n            \\\"uckeG3R8TLe2MMNBQ6AGrw\\\": {\\n                \\\"current_allocations\\\": 1,\\n                \\\"target_allocations\\\": 4,\\n                \\\"routing_state\\\": \\\"started\\\",\\n                \\\"reason\\\": \\\"\\\"\\n            }\\n        },\\n        \\\"assignment_state\\\": \\\"started\\\",\\n        \\\"start_time\\\": \\\"2022-11-02T11:50:34.766591Z\\\"\\n    }\\n}\\n----\\n\\nThe following example updates the deployment for a\\n`elastic__distilbert-base-uncased-finetuned-conll03-english` trained model to\\nenable adaptive allocations with the minimum number of 3 allocations and the\\nmaximum number of 10:\\n\\n[source,console]\\n--------------------------------------------------\\nPOST _ml/trained_models/elastic__distilbert-base-uncased-finetuned-conll03-english/deployment/_update\\n{\\n  \\\"adaptive_allocations\\\": {\\n    \\\"enabled\\\": true,\\n    \\\"min_number_of_allocations\\\": 3,\\n    \\\"max_number_of_allocations\\\": 10\\n  }\\n}\\n--------------------------------------------------\\n// TEST[skip:TBD]\",\"timestamp\":1762415382154,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01VKn2ipwbayn5qrS6G6nCCc\",\"messageId\":\"3bb70396-e683-4551-aadd-50e0deeb60b8\",\"toolId\":\"\",\"isError\":false,\"value\":\"tag::adaptive-allocation[]\\nAdaptive allocations configuration object.\\nIf enabled, the number of allocations of the model is set based on the current load the process gets.\\nWhen the load is high, a new model allocation is automatically created (respecting the value of `max_number_of_allocations` if it's set).\\nWhen the load is low, a model allocation is automatically removed (respecting the value of `min_number_of_allocations` if it's set).\\nThe number of model allocations cannot be scaled down to less than `1` this way.\\nIf `adaptive_allocations` is enabled, do not set the number of allocations manually.\\nend::adaptive-allocation[]\\n\\ntag::adaptive-allocation-enabled[]\\nIf `true`, `adaptive_allocations` is enabled.\\nDefaults to `false`.\\nend::adaptive-allocation-enabled[]\\n\\ntag::adaptive-allocation-max-number[]\\nSpecifies the maximum number of allocations to scale to.\\nIf set, it must be greater than or equal to `min_number_of_allocations`.\\nend::adaptive-allocation-max-number[]\\n\\ntag::adaptive-allocation-min-number[]\\nSpecifies the minimum number of allocations to scale to.\\nIf set, it must be greater than or equal to `1`.\\nend::adaptive-allocation-min-number[]\\n\\ntag::aggregations[]\\nIf set, the {dfeed} performs aggregation searches. Support for aggregations is\\nlimited and should be used only with low cardinality data. For more information,\\nsee\\n{ml-docs}/ml-configuring-aggregation.html[Aggregating data for faster performance].\\nend::aggregations[]\\n\\ntag::allow-lazy-open[]\\nAdvanced configuration option. Specifies whether this job can open when there is\\ninsufficient {ml} node capacity for it to be immediately assigned to a node. The\\ndefault value is `false`; if a {ml} node with capacity to run the job cannot\\nimmediately be found, the <<ml-open-job,open {anomaly-jobs} API>> returns an\\nerror. However, this is also subject to the cluster-wide\\n`xpack.ml.max_lazy_ml_nodes` setting; see <<advanced-ml-settings>>. If this\\noption is set to `true`, the <<ml-open-job,open {anomaly-jobs} API>> does not\\nreturn an error and the job waits in the `opening` state until sufficient {ml}\\nnode capacity is available.\\nend::allow-lazy-open[]\\n\\ntag::allow-no-match-datafeeds[]\\nSpecifies what to do when the request:\\n+\\n--\\n* Contains wildcard expressions and there are no {dfeeds} that match.\\n* Contains the `_all` string or no identifiers and there are no matches.\\n* Contains wildcard expressions and there are only partial matches.\\n\\nThe default value is `true`, which returns an empty `datafeeds` array when\\nthere are no matches and the subset of results when there are partial matches.\\nIf this parameter is `false`, the request returns a `404` status code when there\\nare no matches or only partial matches.\\n--\\nend::allow-no-match-datafeeds[]\\n\\ntag::allow-no-match-deployments[]\\nSpecifies what to do when the request:\\n+\\n--\\n* Contains wildcard expressions and there are no deployments that match.\\n* Contains the `_all` string or no identifiers and there are no matches.\\n* Contains wildcard expressions and there are only partial matches.\\n\\nThe default value is `true`, which returns an empty array when there are no\\nmatches and the subset of results when there are partial matches. If this\\nparameter is `false`, the request returns a `404` status code when there are no\\nmatches or only partial matches.\\n--\\nend::allow-no-match-deployments[]\\n\\ntag::allow-no-match-dfa-jobs[]\\n Specifies what to do when the request:\\n+\\n--\\n* Contains wildcard expressions and there are no {dfanalytics-jobs} that match.\\n* Contains the `_all` string or no identifiers and there are no matches.\\n* Contains wildcard expressions and there are only partial matches.\\n\\nThe default value is `true`, which returns an empty `data_frame_analytics` array\\nwhen there are no matches and the subset of results when there are partial\\nmatches. If this parameter is `false`, the request returns a `404` status code\\nwhen there are no matches or only partial matches.\\n--\\nend::allow-no-match-dfa-jobs[]\\n\\ntag::allow-no-match-jobs[]\\nSpecifies what to do when the request:\\n+\\n--\\n* Contains wildcard expressions and there are no jobs that match.\\n* Contains the `_all` string or no identifiers and there are no matches.\\n* Contains wildcard expressions and there are only partial matches.\\n\\nThe default value is `true`, which returns an empty `jobs` array\\nwhen there are no matches and the subset of results when there are partial\\nmatches. If this parameter is `false`, the request returns a `404` status code\\nwhen there are no matches or only partial matches.\\n--\\nend::allow-no-match-jobs[]\\n\\ntag::allow-no-match-models[]\\nSpecifies what to do when the request:\\n+\\n--\\n* Contains wildcard expressions and there are no models that match.\\n* Contains the `_all` string or no identifiers and there are no matches.\\n* Contains wildcard expressions and there are only partial matches.\\n\\nThe default value is `true`, which returns an empty array when there are no\\nmatches and the subset of results when there are partial matches. If this\\nparameter is `false`, the request returns a `404` status code when there are no\\nmatches or only partial matches.\\n--\\nend::allow-no-match-models[]\\n\\ntag::analysis[]\\nDefines the type of {dfanalytics} you want to perform on your source index. For\\nexample: `outlier_detection`. See <<ml-dfa-analysis-objects>>.\\nend::analysis[]\\n\\ntag::analysis-config[]\\nThe analysis configuration, which specifies how to analyze the data. After you\\ncreate a job, you cannot change the analysis configuration; all the properties\\nare informational.\\nend::analysis-config[]\\n\\ntag::analysis-limits[]\\nLimits can be applied for the resources required to hold the mathematical models\\nin memory. These limits are approximate and can be set per job. They do not\\ncontrol the memory used by other processes, for example the {es} Java processes.\\nend::analysis-limits[]\\n\\ntag::assignment-explanation-anomaly-jobs[]\\nFor open {anomaly-jobs} only, contains messages relating to the selection\\nof a node to run the job.\\nend::assignment-explanation-anomaly-jobs[]\\n\\ntag::assignment-explanation-datafeeds[]\\nFor started {dfeeds} only, contains messages relating to the selection of a\\nnode.\\nend::assignment-explanation-datafeeds[]\\n\\ntag::assignment-explanation-dfanalytics[]\\nContains messages relating to the selection of a node.\\nend::assignment-explanation-dfanalytics[]\\n\\ntag::assignment-memory-basis[]\\nIndicates where to find the memory requirement that is used to decide where the\\njob runs. The possible values are:\\n+\\n--\\n* `model_memory_limit`: The job's memory requirement is calculated on the basis\\nthat its model memory will grow to the `model_memory_limit` specified in the\\n`analysis_limits` of its config.\\n* `current_model_bytes`: The job's memory requirement is calculated on the basis\\nthat its current model memory size is a good reflection of what it will be in\\nthe future.\\n* `peak_model_bytes`: The job's memory requirement is calculated on the basis\\nthat its peak model memory size is a good reflection of what the model size will\\nbe in the future.\\n--\\nend::assignment-memory-basis[]\\n\\ntag::background-persist-interval[]\\nAdvanced configuration option. The time between each periodic persistence of the\\nmodel. The default value is a randomized value between 3 to 4 hours, which\\navoids all jobs persisting at exactly the same time. The smallest allowed value\\nis 1 hour.\\n+\\n--\\nTIP: For very large models (several GB), persistence could take 10-20 minutes,\\nso do not set the `background_persist_interval` value too low.\\n\\n--\\nend::background-persist-interval[]\\n\\ntag::bucket-allocation-failures-count[]\\nThe number of buckets for which new entities in incoming data were not processed\\ndue to insufficient model memory. This situation is also signified by a\\n`hard_limit: memory_status` property value.\\nend::bucket-allocation-failures-count[]\\n\\ntag::bucket-count[]\\nThe number of buckets processed.\\nend::bucket-count[]\\n\\ntag::bucket-count-anomaly-jobs[]\\nThe number of bucket results produced by the job.\\nend::bucket-count-anomaly-jobs[]\\n\\ntag::bucket-span[]\\nThe size of the interval that the analysis is aggregated into, typically between\\n`5m` and `1h`. This value should be either a whole number of days or equate to a\\nwhole number of buckets in one day;\\ndeprecated:[8.1, Values that do not meet these recommendations are deprecated and will be disallowed in a future version].\\nIf the {anomaly-job} uses a {dfeed} with\\n{ml-docs}/ml-configuring-aggregation.html[aggregations], this value must also be\\ndivisible by the interval of the date histogram aggregation. The default value\\nis `5m`. For more information, see\\n{ml-docs}/ml-ad-run-jobs.html#ml-ad-bucket-span[Bucket span].\\nend::bucket-span[]\\n\\ntag::bucket-span-results[]\\nThe length of the bucket in seconds. This value matches the `bucket_span`\\nthat is specified in the job.\\nend::bucket-span-results[]\\n\\ntag::bucket-time-exponential-average[]\\nExponential moving average of all bucket processing times, in milliseconds.\\nend::bucket-time-exponential-average[]\\n\\ntag::bucket-time-exponential-average-hour[]\\nExponentially-weighted moving average of bucket processing times\\ncalculated in a 1 hour time window, in milliseconds.\\nend::bucket-time-exponential-average-hour[]\\n\\ntag::bucket-time-maximum[]\\nMaximum among all bucket processing times, in milliseconds.\\nend::bucket-time-maximum[]\\n\\ntag::bucket-time-minimum[]\\nMinimum among all bucket processing times, in milliseconds.\\nend::bucket-time-minimum[]\\n\\ntag::bucket-time-total[]\\nSum of all bucket processing times, in milliseconds.\\nend::bucket-time-total[]\\n\\ntag::by-field-name[]\\nThe field used to split the data. In particular, this property is used for\\nanalyzing the splits with respect to their own history. It is used for finding\\nunusual values in the context of the split.\\nend::by-field-name[]\\n\\ntag::calendar-id[]\\nA string that uniquely identifies a calendar.\\nend::calendar-id[]\\n\\ntag::categorization-analyzer[]\\nIf `categorization_field_name` is specified, you can also define the analyzer\\nthat is used to interpret the categorization field. This property cannot be used\\nat the same time as `categorization_filters`. The categorization analyzer\\nspecifies how the `categorization_field` is interpreted by the categorization\\nprocess. The syntax is very similar to that used to define the `analyzer` in the\\n<<indices-analyze,Analyze endpoint>>. For more information, see\\n{ml-docs}/ml-configuring-categories.html[Categorizing log messages].\\n+\\nThe `categorization_analyzer` field can be specified either as a string or as an\\nobject. If it is a string it must refer to a\\n<<analysis-analyzers,built-in analyzer>> or one added by another plugin. If it\\nis an object it has the following properties:\\n+\\n.Properties of `categorization_analyzer`\\n[%collapsible%open]\\n=====\\n`char_filter`::::\\n(array of strings or objects)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=char-filter]\\n\\n`tokenizer`::::\\n(string or object)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=tokenizer]\\n\\n`filter`::::\\n(array of strings or objects)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=filter]\\n=====\\nend::categorization-analyzer[]\\n\\ntag::categorization-examples-limit[]\\nThe maximum number of examples stored per category in memory and in the results\\ndata store. The default value is 4. If you increase this value, more examples\\nare available, however it requires that you have more storage available. If you\\nset this value to `0`, no examples are stored.\\n+\\nNOTE: The `categorization_examples_limit` only applies to analysis that uses\\ncategorization. For more information, see\\n{ml-docs}/ml-configuring-categories.html[Categorizing log messages].\\nend::categorization-examples-limit[]\\n\\ntag::categorization-field-name[]\\nIf this property is specified, the values of the specified field will be\\ncategorized. The resulting categories must be used in a detector by setting\\n`by_field_name`, `over_field_name`, or `partition_field_name` to the keyword\\n`mlcategory`. For more information, see\\n{ml-docs}/ml-configuring-categories.html[Categorizing log messages].\\nend::categorization-field-name[]\\n\\ntag::categorization-filters[]\\nIf `categorization_field_name` is specified, you can also define optional\\nfilters. This property expects an array of regular expressions. The expressions\\nare used to filter out matching sequences from the categorization field values.\\nYou can use this functionality to fine tune the categorization by excluding\\nsequences from consideration when categories are defined. For example, you can\\nexclude SQL statements that appear in your log files. For more information, see\\n{ml-docs}/ml-configuring-categories.html[Categorizing log messages]. This\\nproperty cannot be used at the same time as `categorization_analyzer`. If you\\nonly want to define simple regular expression filters that are applied prior to\\ntokenization, setting this property is the easiest method. If you also want to\\ncustomize the tokenizer or post-tokenization filtering, use the\\n`categorization_analyzer` property instead and include the filters as\\n`pattern_replace` character filters. The effect is exactly the same.\\nend::categorization-filters[]\\n\\ntag::categorization-status[]\\nThe status of categorization for the job. Contains one of the following values:\\n+\\n--\\n* `ok`: Categorization is performing acceptably well (or not being used at all).\\n* `warn`: Categorization is detecting a distribution of categories that suggests\\nthe input data is inappropriate for categorization. Problems could be that there\\nis only one category, more than 90% of categories are rare, the number of\\ncategories is greater than 50% of the number of categorized documents, there are\\nno frequently matched categories, or more than 50% of categories are dead.\\n\\n--\\nend::categorization-status[]\\n\\ntag::categorized-doc-count[]\\nThe number of documents that have had a field categorized.\\nend::categorized-doc-count[]\\n\\ntag::char-filter[]\\nOne or more <<analysis-charfilters,character filters>>. In addition to the\\nbuilt-in character filters, other plugins can provide more character filters.\\nThis property is optional. If it is not specified, no character filters are\\napplied prior to categorization. If you are customizing some other aspect of the\\nanalyzer and you need to achieve the equivalent of `categorization_filters`\\n(which are not permitted when some other aspect of the analyzer is customized),\\nadd them here as\\n<<analysis-pattern-replace-charfilter,pattern replace character filters>>.\\nend::char-filter[]\\n\\ntag::chunking-config[]\\n{dfeeds-cap} might be required to search over long time periods, for several\\nmonths or years. This search is split into time chunks in order to ensure the\\nload on {es} is managed. Chunking configuration controls how the size of these\\ntime chunks are calculated and is an advanced configuration option.\\nend::chunking-config[]\\n\\ntag::class-assignment-objective[]\\nDefines the objective to optimize when assigning class labels:\\n`maximize_accuracy` or `maximize_minimum_recall`. When maximizing accuracy,\\nclass labels are chosen to maximize the number of correct predictions. When\\nmaximizing minimum recall, labels are chosen to maximize the minimum recall for\\nany class. Defaults to `maximize_minimum_recall`.\\nend::class-assignment-objective[]\\n\\ntag::compute-feature-influence[]\\nSpecifies whether the feature influence calculation is enabled. Defaults to\\n`true`.\\nend::compute-feature-influence[]\\n\\ntag::custom-preprocessor[]\\n(Optional, Boolean)\\nBoolean value indicating if the analytics job created the preprocessor\\nor if a user provided it. This adjusts the feature importance calculation.\\nWhen `true`, the feature importance calculation returns importance for the\\nprocessed feature. When `false`, the total importance of the original field\\nis returned. Default is `false`.\\nend::custom-preprocessor[]\\n\\ntag::custom-rules[]\\nAn array of custom rule objects, which enable you to customize the way detectors\\noperate. For example, a rule may dictate to the detector conditions under which\\nresults should be skipped. {kib} refers to custom rules as _job rules_. For more\\nexamples, see\\n{ml-docs}/ml-configuring-detector-custom-rules.html[Customizing detectors with custom rules].\\nend::custom-rules[]\\n\\ntag::custom-rules-actions[]\\nThe set of actions to be triggered when the rule applies. If\\nmore than one action is specified the effects of all actions are combined. The\\navailable actions include:\\n\\n* `skip_result`: The result will not be created. This is the default value.\\nUnless you also specify `skip_model_update`, the model will be updated as usual\\nwith the corresponding series value.\\n* `skip_model_update`: The value for that series will not be used to update the\\nmodel. Unless you also specify `skip_result`, the results will be created as\\nusual. This action is suitable when certain values are expected to be\\nconsistently anomalous and they affect the model in a way that negatively\\nimpacts the rest of the results.\\n* `force_time_shift`: This action will shift the time inside the anomaly detector by a specified\\namount. This is useful, e.g. to quickly adapt to the daylight saving time events that\\nare known beforehand. This action requires a `force_time_shift` parameter\\nin the `params` object.\\nend::custom-rules-actions[]\\n\\ntag::custom-rules-scope[]\\nAn optional scope of series where the rule applies. A rule must either\\nhave a non-empty scope or at least one condition. By default, the scope includes\\nall series. Scoping is allowed for any of the fields that are also specified in\\n`by_field_name`, `over_field_name`, or `partition_field_name`. To add a scope\\nfor a field, add the field name as a key in the scope object and set its value\\nto an object with the following properties:\\nend::custom-rules-scope[]\\n\\ntag::custom-rules-scope-filter-id[]\\nThe id of the filter to be used.\\nend::custom-rules-scope-filter-id[]\\n\\ntag::custom-rules-scope-filter-type[]\\nEither `include` (the rule applies for values in the filter) or `exclude` (the\\nrule applies for values not in the filter). Defaults to `include`.\\nend::custom-rules-scope-filter-type[]\\n\\ntag::custom-rules-conditions[]\\nAn optional array of numeric conditions when the rule applies. A rule must\\neither have a non-empty scope or at least one condition. Multiple conditions are\\ncombined together with a logical `AND`. A condition has the following\\nproperties:\\nend::custom-rules-conditions[]\\n\\ntag::custom-rules-conditions-applies-to[]\\nSpecifies the result property to which the condition applies. The available\\noptions are `actual`, `typical`, `diff_from_typical`, `time`. If your detector\\nuses `lat_long`, `metric`, `rare`, or `freq_rare` functions, you can only\\nspecify conditions that apply to `time`.\\nend::custom-rules-conditions-applies-to[]\\n\\ntag::custom-rules-conditions-operator[]\\nSpecifies the condition operator. The available options are `gt` (greater than),\\n`gte` (greater than or equals), `lt` (less than) and `lte` (less than or\\nequals).\\nend::custom-rules-conditions-operator[]\\n\\ntag::custom-rules-conditions-value[]\\nThe value that is compared against the `applies_to` field using the `operator`.\\nend::custom-rules-conditions-value[]\\n\\ntag::custom-rules-params[]\\nA set of parameter objects that customize the actions defined in the custom rules\\nactions array. The available parameters (depending on the specified actions) include:\\n`force_time_shift`.\\nend::custom-rules-params[]\\n\\ntag::custom-rules-params-force-time-shift[]\\nSet `time_shift_amount` to the signed number of seconds by which you want to shift the time.\\nend::custom-rules-params-force-time-shift[]\\n\\ntag::custom-settings[]\\nAdvanced configuration option. Contains custom metadata about the job. For\\nexample, it can contain custom URL information as shown in\\n{ml-docs}/ml-configuring-url.html[Adding custom URLs to {ml} results].\\nend::custom-settings[]\\n\\ntag::daily-model-snapshot-retention-after-days[]\\nAdvanced configuration option, which affects the automatic removal of old model\\nsnapshots for this job. It specifies a period of time (in days) after which only\\nthe first snapshot per day is retained. This period is relative to the timestamp\\nof the most recent snapshot for this job. Valid values range from `0` to\\n`model_snapshot_retention_days`. For new jobs, the default value is `1`. For\\njobs created before version 7.8.0, the default value matches\\n`model_snapshot_retention_days`. For more information, refer to\\n{ml-docs}/ml-ad-run-jobs.html#ml-ad-model-snapshots[Model snapshots].\\n+\\n--\\nNOTE: From {es} 8.10.0,  a new version number is used to\\ntrack the configuration and state changes in the {ml} plugin. This new\\nversion number is decoupled from the product version and will increment\\nindependently.\\n--\\nend::daily-model-snapshot-retention-after-days[]\\n\\ntag::data-description[]\\nThe data description defines the format of the input data when you send data to\\nthe job by using the <<ml-post-data,post data>> API. Note that when using a\\n{dfeed}, only the `time_field` needs to be set, the rest of the properties are\\nautomatically set. When data is received via the <<ml-post-data,post data>> API,\\nit is not stored in {es}. Only the results for {anomaly-detect} are retained.\\n+\\n.Properties of `data_description`\\n[%collapsible%open]\\n====\\n`format`:::\\n  (string) Only `xcontent` format is supported at this time, and this is the\\n  default value.\\n\\n`time_field`:::\\n  (string) The name of the field that contains the timestamp.\\n  The default value is `time`.\\n\\n`time_format`:::\\n(string)\\ninclude::{es-ref-dir}/ml/ml-shared.asciidoc[tag=time-format]\\n====\\nend::data-description[]\\n\\ntag::datafeed-id[]\\nA numerical character string that uniquely identifies the\\n{dfeed}. This identifier can contain lowercase alphanumeric characters (a-z\\nand 0-9), hyphens, and underscores. It must start and end with alphanumeric\\ncharacters.\\nend::datafeed-id[]\\n\\ntag::datafeed-id-wildcard[]\\nIdentifier for the {dfeed}. It can be a {dfeed} identifier or a wildcard\\nexpression.\\nend::datafeed-id-wildcard[]\\n\\ntag::dead-category-count[]\\nThe number of categories created by categorization that will never be assigned\\nagain because another category's definition makes it a superset of the dead\\ncategory. (Dead categories are a side effect of the way categorization has no\\nprior training.)\\nend::dead-category-count[]\\n\\ntag::delayed-data-check-config[]\\nSpecifies whether the {dfeed} checks for missing data and the size of the\\nwindow. For example: `{\\\"enabled\\\": true, \\\"check_window\\\": \\\"1h\\\"}`.\\n+\\nThe {dfeed} can optionally search over indices that have already been read in\\nan effort to determine whether any data has subsequently been added to the\\nindex. If missing data is found, it is a good indication that the `query_delay`\\noption is set too low and the data is being indexed after the {dfeed} has passed\\nthat moment in time. See\\n{ml-docs}/ml-delayed-data-detection.html[Working with delayed data].\\n+\\nThis check runs only on real-time {dfeeds}.\\nend::delayed-data-check-config[]\\n\\ntag::delayed-data-check-config-check-window[]\\nThe window of time that is searched for late data. This window of time ends with\\nthe latest finalized bucket. It defaults to `null`, which causes an appropriate\\n`check_window` to be calculated when the real-time {dfeed} runs. In particular,\\nthe default `check_window` span calculation is based on the maximum of `2h` or\\n`8 * bucket_span`.\\nend::delayed-data-check-config-check-window[]\\n\\ntag::delayed-data-check-config-enabled[]\\nSpecifies whether the {dfeed} periodically checks for delayed data. Defaults to\\n`true`.\\nend::delayed-data-check-config-enabled[]\\n\\ntag::dependent-variable[]\\nDefines which field of the document is to be predicted.\\nThis parameter is supplied by field name and must match one of the fields in\\nthe index being used to train. If this field is missing from a document, then\\nthat document will not be used for training, but a prediction with the trained\\nmodel will be generated for it. It is also known as continuous target variable.\\nend::dependent-variable[]\\n\\ntag::deployment-id[]\\nA unique identifier for the deployment of the model.\\nend::deployment-id[]\\n\\ntag::desc-results[]\\nIf true, the results are sorted in descending order.\\nend::desc-results[]\\n\\ntag::description-dfa[]\\nA description of the job.\\nend::description-dfa[]\\n\\ntag::dest[]\\nThe destination configuration, consisting of `index` and optionally\\n`results_field` (`ml` by default).\\n+\\n.Properties of `dest`\\n[%collapsible%open]\\n====\\n`index`:::\\n(Required, string) Defines the _destination index_ to store the results of the\\n{dfanalytics-job}.\\n\\n`results_field`:::\\n(Optional, string) Defines the name of the field in which to store the results\\nof the analysis. Defaults to `ml`.\\n====\\nend::dest[]\\n\\ntag::detector-description[]\\nA description of the detector. For example, `Low event rate`.\\nend::detector-description[]\\n\\ntag::detector-field-name[]\\nThe field that the detector uses in the function. If you use an event rate\\nfunction such as `count` or `rare`, do not specify this field.\\n+\\n--\\nNOTE: The `field_name` cannot contain double quotes or backslashes.\\n\\n--\\nend::detector-field-name[]\\n\\ntag::detector-index[]\\nA unique identifier for the detector. This identifier is based on the order of\\nthe detectors in the `analysis_config`, starting at zero.\\nend::detector-index[]\\n\\ntag::dfas-alpha[]\\nAdvanced configuration option. {ml-cap} uses loss guided tree growing, which\\nmeans that the decision trees grow where the regularized loss decreases most\\nquickly. This parameter affects loss calculations by acting as a multiplier of\\nthe tree depth. Higher alpha values result in shallower trees and faster\\ntraining times. By default, this value is calculated during hyperparameter\\noptimization. It must be greater than or equal to zero.\\nend::dfas-alpha[]\\n\\ntag::dfas-downsample-factor[]\\nAdvanced configuration option. Controls the fraction of data that is used to\\ncompute the derivatives of the loss function for tree training. A small value\\nresults in the use of a small fraction of the data. If this value is set to be\\nless than 1, accuracy typically improves. However, too small a value may result\\nin poor convergence for the ensemble and so require more trees. For more\\ninformation about shrinkage, refer to\\n{wikipedia}/Gradient_boosting#Stochastic_gradient_boosting[this wiki article].\\nBy default, this value is calculated during hyperparameter optimization. It\\nmust be greater than zero and less than or equal to 1.\\nend::dfas-downsample-factor[]\\n\\ntag::dfas-early-stopping-enabled[]\\nAdvanced configuration option.\\nSpecifies whether the training process should finish if it is not finding any\\nbetter performing models. If disabled, the training process can take significantly\\nlonger and the chance of finding a better performing model is unremarkable.\\nBy default, early stoppping is enabled.\\nend::dfas-early-stopping-enabled[]\\n\\ntag::dfas-eta-growth[]\\nAdvanced configuration option. Specifies the rate at which `eta` increases for\\neach new tree that is added to the forest. For example, a rate of 1.05\\nincreases `eta` by 5% for each extra tree. By default, this value is calculated\\nduring hyperparameter optimization. It must be between 0.5 and 2.\\nend::dfas-eta-growth[]\\n\\ntag::dfas-feature-bag-fraction[]\\nThe fraction of features that is used when selecting a random bag for each\\ncandidate split.\\nend::dfas-feature-bag-fraction[]\\n\\ntag::dfas-feature-processors[]\\nAdvanced configuration option. A collection of feature preprocessors that modify\\none or more included fields. The analysis uses the resulting one or more\\nfeatures instead of the original document field. However, these features are\\nephemeral; they are not stored in the destination index. Multiple\\n`feature_processors` entries can refer to the same document fields. Automatic\\ncategorical {ml-docs}/ml-feature-encoding.html[feature encoding] still occurs\\nfor the fields that are unprocessed by a custom processor or that have\\ncategorical values. Use this property only if you want to override the automatic\\nfeature encoding of the specified fields. Refer to\\n{ml-docs}/ml-feature-processors.html[{dfanalytics} feature processors] to learn\\nmore.\\nend::dfas-feature-processors[]\\n\\ntag::dfas-feature-processors-feat-name[]\\nThe resulting feature name.\\nend::dfas-feature-processors-feat-name[]\\n\\ntag::dfas-feature-processors-field[]\\nThe name of the field to encode.\\nend::dfas-feature-processors-field[]\\n\\ntag::dfas-feature-processors-frequency[]\\nThe configuration information necessary to perform frequency encoding.\\nend::dfas-feature-processors-frequency[]\\n\\ntag::dfas-feature-processors-frequency-map[]\\nThe resulting frequency map for the field value. If the field value is missing\\nfrom the `frequency_map`, the resulting value is `0`.\\nend::dfas-feature-processors-frequency-map[]\\n\\ntag::dfas-feature-processors-multi[]\\nThe configuration information necessary to perform multi encoding. It allows\\nmultiple processors to be changed together. This way the output of a processor\\ncan then be passed to another as an input.\\nend::dfas-feature-processors-multi[]\\n\\ntag::dfas-feature-processors-multi-proc[]\\nThe ordered array of custom processors to execute. Must be more than 1.\\nend::dfas-feature-processors-multi-proc[]\\n\\ntag::dfas-feature-processors-ngram[]\\nThe configuration information necessary to perform n-gram encoding. Features\\ncreated by this encoder have the following name format:\\n`<feature_prefix>.<ngram><string position>`. For example, if the\\n`feature_prefix` is `f`, the feature name for the second unigram in a string is\\n`f.11`.\\nend::dfas-feature-processors-ngram[]\\n\\ntag::dfas-feature-processors-ngram-feat-pref[]\\nThe feature name prefix. Defaults to `ngram_<start>_<length>`.\\nend::dfas-feature-processors-ngram-feat-pref[]\\n\\ntag::dfas-feature-processors-ngram-field[]\\nThe name of the text field to encode.\\nend::dfas-feature-processors-ngram-field[]\\n\\ntag::dfas-feature-processors-ngram-length[]\\nSpecifies the length of the n-gram substring. Defaults to `50`. Must be greater\\nthan `0`.\\nend::dfas-feature-processors-ngram-length[]\\n\\ntag::dfas-feature-processors-ngram-ngrams[]\\nSpecifies which n-grams to gather. It\u2019s an array of integer values where the\\nminimum value is 1, and a maximum value is 5.\\nend::dfas-feature-processors-ngram-ngrams[]\\n\\ntag::dfas-feature-processors-ngram-start[]\\nSpecifies the zero-indexed start of the n-gram substring. Negative values are\\nallowed for encoding n-grams of string suffixes. Defaults to `0`.\\nend::dfas-feature-processors-ngram-start[]\\n\\ntag::dfas-feature-processors-one-hot[]\\nThe configuration information necessary to perform one hot encoding.\\nend::dfas-feature-processors-one-hot[]\\n\\ntag::dfas-feature-processors-one-hot-map[]\\nThe one hot map mapping the field value with the column name.\\nend::dfas-feature-processors-one-hot-map[]\\n\\ntag::dfas-feature-processors-target-mean[]\\nThe configuration information necessary to perform target mean encoding.\\nend::dfas-feature-processors-target-mean[]\\n\\ntag::dfas-feature-processors-target-mean-default[]\\nThe default value if field value is not found in the `target_map`.\\nend::dfas-feature-processors-target-mean-default[]\\n\\ntag::dfas-feature-processors-target-mean-map[]\\nThe field value to target mean transition map.\\nend::dfas-feature-processors-target-mean-map[]\\n\\ntag::dfas-iteration[]\\nThe number of iterations on the analysis.\\nend::dfas-iteration[]\\n\\ntag::dfas-max-attempts[]\\nIf the algorithm fails to determine a non-trivial tree (more than a single\\nleaf), this parameter determines how many of such consecutive failures are\\ntolerated. Once the number of attempts exceeds the threshold, the forest\\ntraining stops.\\nend::dfas-max-attempts[]\\n\\ntag::dfas-max-optimization-rounds[]\\nAdvanced configuration option.\\nA multiplier responsible for determining the maximum number of\\nhyperparameter optimization steps in the Bayesian optimization procedure.\\nThe maximum number of steps is determined based on the number of undefined\\nhyperparameters times the maximum optimization rounds per hyperparameter.\\nBy default, this value is calculated during hyperparameter optimization.\\nend::dfas-max-optimization-rounds[]\\n\\ntag::dfas-num-folds[]\\nThe maximum number of folds for the cross-validation procedure.\\nend::dfas-num-folds[]\\n\\ntag::dfas-num-splits[]\\nDetermines the maximum number of splits for every feature that can occur in a\\ndecision tree when the tree is trained.\\nend::dfas-num-splits[]\\n\\ntag::dfas-soft-limit[]\\nAdvanced configuration option. {ml-cap} uses loss guided tree growing, which\\nmeans that the decision trees grow where the regularized loss decreases most\\nquickly. This soft limit combines with the `soft_tree_depth_tolerance` to\\npenalize trees that exceed the specified depth; the regularized loss increases\\nquickly beyond this depth. By default, this value is calculated during\\nhyperparameter optimization. It must be greater than or equal to 0.\\nend::dfas-soft-limit[]\\n\\ntag::dfas-soft-tolerance[]\\nAdvanced configuration option. This option controls how quickly the regularized\\nloss increases when the tree depth exceeds `soft_tree_depth_limit`. By default,\\nthis value is calculated during hyperparameter optimization. It must be greater\\nthan or equal to 0.01.\\nend::dfas-soft-tolerance[]\\n\\ntag::dfas-timestamp[]\\nThe timestamp when the statistics were reported in milliseconds since the epoch.\\nend::dfas-timestamp[]\\n\\ntag::dfas-timing-stats[]\\nAn object containing time statistics about the {dfanalytics-job}.\\nend::dfas-timing-stats[]\\n\\ntag::dfas-timing-stats-elapsed[]\\nRuntime of the analysis in milliseconds.\\nend::dfas-timing-stats-elapsed[]\\n\\ntag::dfas-timing-stats-iteration[]\\nRuntime of the latest iteration of the analysis in milliseconds.\\nend::dfas-timing-stats-iteration[]\\n\\ntag::dfas-validation-loss[]\\nAn object containing information about validation loss.\\nend::dfas-validation-loss[]\\n\\ntag::dfas-validation-loss-fold[]\\nValidation loss values for every added decision tree during the forest growing\\nprocedure.\\nend::dfas-validation-loss-fold[]\\n\\ntag::dfas-validation-loss-type[]\\nThe type of the loss metric. For example, `binomial_logistic`.\\nend::dfas-validation-loss-type[]\\n\\ntag::earliest-record-timestamp[]\\nThe timestamp of the earliest chronologically input document.\\nend::earliest-record-timestamp[]\\n\\ntag::empty-bucket-count[]\\nThe number of buckets which did not contain any data. If your data\\ncontains many empty buckets, consider increasing your `bucket_span` or using\\nfunctions that are tolerant to gaps in data such as `mean`, `non_null_sum` or\\n`non_zero_count`.\\nend::empty-bucket-count[]\\n\\ntag::eta[]\\nAdvanced configuration option. The shrinkage applied to the weights. Smaller\\nvalues result in larger forests which have a better generalization error.\\nHowever, larger forests cause slower training. For more information about\\nshrinkage, refer to\\n{wikipedia}/Gradient_boosting#Shrinkage[this wiki article].\\nBy default, this value is calculated during hyperparameter optimization. It must\\nbe a value between 0.001 and 1.\\nend::eta[]\\n\\ntag::exclude-frequent[]\\nContains one of the following values: `all`, `none`, `by`, or `over`. If set,\\nfrequent entities are excluded from influencing the anomaly results. Entities\\ncan be considered frequent over time or frequent in a population. If you are\\nworking with both over and by fields, then you can set `exclude_frequent` to\\n`all` for both fields, or to `by` or `over` for those specific fields.\\nend::exclude-frequent[]\\n\\ntag::exclude-interim-results[]\\nIf `true`, the output excludes interim results. Defaults to `false`, which means interim results are included.\\nend::exclude-interim-results[]\\n\\ntag::failed-category-count[]\\nThe number of times that categorization wanted to create a new category but\\ncouldn't because the job had hit its `model_memory_limit`. This count does not\\ntrack which specific categories failed to be created. Therefore you cannot use\\nthis value to determine the number of unique categories that were missed.\\nend::failed-category-count[]\\n\\ntag::feature-bag-fraction[]\\nAdvanced configuration option. Defines the fraction of features that will be\\nused when selecting a random bag for each candidate split. By default, this\\nvalue is calculated during hyperparameter optimization.\\nend::feature-bag-fraction[]\\n\\ntag::feature-influence-threshold[]\\nThe minimum {olscore} that a document needs to have in order to calculate its\\n{fiscore}. Value range: 0-1 (`0.1` by default).\\nend::feature-influence-threshold[]\\n\\ntag::filter[]\\nOne or more <<analysis-tokenfilters,token filters>>. In addition to the built-in\\ntoken filters, other plugins can provide more token filters. This property is\\noptional. If it is not specified, no token filters are applied prior to\\ncategorization.\\nend::filter[]\\n\\ntag::filter-id[]\\nA string that uniquely identifies a filter.\\nend::filter-id[]\\n\\ntag::forecast-total[]\\nThe number of individual forecasts currently available for the job. A value of\\n`1` or more indicates that forecasts exist.\\nend::forecast-total[]\\n\\ntag::exclude-generated[]\\nIndicates if certain fields should be removed from the configuration on\\nretrieval. This allows the configuration to be in an acceptable format to be retrieved\\nand then added to another cluster. Default is false.\\nend::exclude-generated[]\\n\\ntag::frequency[]\\nThe interval at which scheduled queries are made while the {dfeed} runs in real\\ntime. The default value is either the bucket span for short bucket spans, or,\\nfor longer bucket spans, a sensible fraction of the bucket span. For example:\\n`150s`. When `frequency` is shorter than the bucket span, interim results for\\nthe last (partial) bucket are written then eventually overwritten by the full\\nbucket results. If the {dfeed} uses aggregations, this value must be divisible\\nby the interval of the date histogram aggregation.\\nend::frequency[]\\n\\ntag::frequent-category-count[]\\nThe number of categories that match more than 1% of categorized documents.\\nend::frequent-category-count[]\\n\\ntag::from[]\\nSkips the specified number of {dfanalytics-jobs}. The default value is `0`.\\nend::from[]\\n\\ntag::from-models[]\\nSkips the specified number of models. The default value is `0`.\\nend::from-models[]\\n\\ntag::function[]\\nThe analysis function that is used. For example, `count`, `rare`, `mean`, `min`,\\n`max`, and `sum`. For more information, see\\n{ml-docs}/ml-functions.html[Function reference].\\nend::function[]\\n\\ntag::gamma[]\\nAdvanced configuration option. Regularization parameter to prevent overfitting\\non the training data set. Multiplies a linear penalty associated with the size\\nof individual trees in the forest. A high gamma value causes training to prefer\\nsmall trees. A small gamma value results in larger individual trees and slower\\ntraining. By default, this value is calculated during hyperparameter\\noptimization. It must be a nonnegative value.\\nend::gamma[]\\n\\ntag::groups[]\\nA list of job groups. A job can belong to no groups or many.\\nend::groups[]\\n\\ntag::indices[]\\nAn array of index names. Wildcards are supported. For example:\\n`[\\\"it_ops_metrics\\\", \\\"server*\\\"]`.\\n+\\n--\\nNOTE: If any indices are in remote clusters then the {ml} nodes need to have the\\n`remote_cluster_client` role.\\n\\n--\\nend::indices[]\\n\\ntag::indices-options[]\\nSpecifies index expansion options that are used during search.\\n+\\n--\\nFor example:\\n```\\n{\\n   \\\"expand_wildcards\\\": [\\\"all\\\"],\\n   \\\"ignore_unavailable\\\": true,\\n   \\\"allow_no_indices\\\": \\\"false\\\",\\n   \\\"ignore_throttled\\\": true\\n}\\n```\\nFor more information about these options, see <<api-multi-index>>.\\n--\\nend::indices-options[]\\n\\ntag::runtime-mappings[]\\nSpecifies runtime fields for the datafeed search.\\n+\\n--\\nFor example:\\n```\\n{\\n  \\\"day_of_week\\\": {\\n    \\\"type\\\": \\\"keyword\\\",\\n    \\\"script\\\": {\\n      \\\"source\\\": \\\"emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ENGLISH))\\\"\\n    }\\n  }\\n}\\n```\\n\\n--\\nend::runtime-mappings[]\\n\\ntag::inference-config-classification-num-top-classes[]\\nSpecifies the number of top class predictions to return. Defaults to 0.\\nend::inference-config-classification-num-top-classes[]\\n\\ntag::inference-config-classification-num-top-feature-importance-values[]\\nSpecifies the maximum number of\\n{ml-docs}/ml-feature-importance.html[{feat-imp}] values per document. Defaults\\nto 0 which means no {feat-imp} calculation occurs.\\nend::inference-config-classification-num-top-feature-importance-values[]\\n\\ntag::inference-config-classification-top-classes-results-field[]\\nSpecifies the field to which the top classes are written. Defaults to\\n`top_classes`.\\nend::inference-config-classification-top-classes-results-field[]\\n\\ntag::inference-config-classification-prediction-field-type[]\\nSpecifies the type of the predicted field to write.\\nValid values are: `string`, `number`, `boolean`. When `boolean` is provided\\n`1.0` is transformed to `true` and `0.0` to `false`.\\nend::inference-config-classification-prediction-field-type[]\\n\\ntag::inference-config-nlp-tokenization[]\\nIndicates the tokenization to perform and the desired settings.\\nThe default tokenization configuration is `bert`. Valid tokenization\\nvalues are\\n+\\n--\\n* `bert`: Use for BERT-style models\\n* `deberta_v2`: Use for DeBERTa v2 and v3-style models\\n* `mpnet`: Use for MPNet-style models\\n* `roberta`: Use for RoBERTa-style and BART-style models\\n* experimental:[] `xlm_roberta`: Use for XLMRoBERTa-style models\\n* experimental:[] `bert_ja`: Use for BERT-style models trained for the Japanese\\nlanguage.\\n--\\nend::inference-config-nlp-tokenization[]\\n\\ntag::inference-config-nlp-tokenization-bert[]\\nBERT-style tokenization is to be performed with the enclosed settings.\\nend::inference-config-nlp-tokenization-bert[]\\n\\ntag::inference-config-nlp-tokenization-bert-ja[]\\nexperimental:[] BERT-style tokenization for Japanese text is to be performed\\nwith the enclosed settings.\\nend::inference-config-nlp-tokenization-bert-ja[]\\n\\ntag::inference-config-nlp-tokenization-do-lower-case[]\\nSpecifies if the tokenization lower case the text sequence when building the\\ntokens.\\nend::inference-config-nlp-tokenization-do-lower-case[]\\n\\ntag::inference-config-nlp-tokenization-span[]\\nWhen `truncate` is `none`, you can partition longer text sequences\\nfor inference. The value indicates how many tokens overlap between each\\nsubsequence.\\n+\\nThe default value is `-1`, indicating no windowing or spanning occurs.\\n+\\nNOTE: When your typical input is just slightly larger than `max_sequence_length`, it may be best to simply truncate;\\nthere will be very little information in the second subsequence.\\nend::inference-config-nlp-tokenization-span[]\\n\\ntag::inference-config-nlp-tokenization-truncate[]\\nIndicates how tokens are truncated when they exceed `max_sequence_length`.\\nThe default value is `first`.\\n+\\n--\\n* `none`: No truncation occurs; the inference request receives an error.\\n* `first`: Only the first sequence is truncated.\\n* `second`: Only the second sequence is truncated. If there is just one sequence,\\n\\t\\t\\t\\t\\t that sequence is truncated.\\n--\\n\\nNOTE: For `zero_shot_classification`, the hypothesis sequence is always the second\\nsequence. Therefore, do not use `second` in this case.\\n\\nend::inference-config-nlp-tokenization-truncate[]\\n\\ntag::inference-config-nlp-tokenization-truncate-deberta-v2[]\\nIndicates how tokens are truncated when they exceed `max_sequence_length`.\\nThe default value is `first`.\\n+\\n--\\n* `balanced`: One or both of the first and second sequences may be truncated so as to balance the tokens included from both sequences.\\n* `none`: No truncation occurs; the inference request receives an error.\\n* `first`: Only the first sequence is truncated.\\n* `second`: Only the second sequence is truncated. If there is just one sequence, that sequence is truncated.\\n--\\n\\nend::inference-config-nlp-tokenization-truncate-deberta-v2[]\\n\\ntag::inference-config-nlp-tokenization-bert-with-special-tokens[]\\nTokenize with special tokens. The tokens typically included in BERT-style tokenization are:\\n+\\n--\\n* `[CLS]`: The first token of the sequence being classified.\\n* `[SEP]`: Indicates sequence separation.\\n--\\nend::inference-config-nlp-tokenization-bert-with-special-tokens[]\\n\\ntag::inference-config-nlp-tokenization-bert-ja-with-special-tokens[]\\nTokenize with special tokens if `true`.\\nend::inference-config-nlp-tokenization-bert-ja-with-special-tokens[]\\n\\ntag::inference-config-nlp-tokenization-deberta-v2[]\\nDeBERTa-style tokenization is to be performed with the enclosed settings.\\nend::inference-config-nlp-tokenization-deberta-v2[]\\n\\ntag::inference-config-nlp-tokenization-max-sequence-length[]\\nSpecifies the maximum number of tokens allowed to be output by the tokenizer.\\nend::inference-config-nlp-tokenization-max-sequence-length[]\\n\\ntag::inference-config-nlp-tokenization-deberta-v2-with-special-tokens[]\\nTokenize with special tokens. The tokens typically included in DeBERTa-style tokenization are:\\n+\\n--\\n* `[CLS]`: The first token of the sequence being classified.\\n* `[SEP]`: Indicates sequence separation and sequence end.\\n--\\nend::inference-config-nlp-tokenization-deberta-v2-with-special-tokens[]\\n\\ntag::inference-config-nlp-tokenization-roberta[]\\nRoBERTa-style tokenization is to be performed with the enclosed settings.\\nend::inference-config-nlp-tokenization-roberta[]\\n\\ntag::inference-config-nlp-tokenization-roberta-add-prefix-space[]\\nSpecifies if the tokenization should prefix a space to the tokenized input to the model.\\nend::inference-config-nlp-tokenization-roberta-add-prefix-space[]\\n\\ntag::inference-config-nlp-tokenization-roberta-with-special-tokens[]\\nTokenize with special tokens. The tokens typically included in RoBERTa-style tokenization are:\\n+\\n--\\n* `<s>`: The first token of the sequence being classified.\\n* `</s>`: Indicates sequence separation.\\n--\\nend::inference-config-nlp-tokenization-roberta-with-special-tokens[]\\n\\ntag::inference-config-nlp-tokenization-mpnet[]\\nMPNet-style tokenization is to be performed with the enclosed settings.\\nend::inference-config-nlp-tokenization-mpnet[]\\n\\ntag::inference-config-nlp-tokenization-mpnet-with-special-tokens[]\\nTokenize with special tokens. The tokens typically included in MPNet-style tokenization are:\\n+\\n--\\n* `<s>`: The first token of the sequence being classified.\\n* `</s>`: Indicates sequence separation.\\n--\\nend::inference-config-nlp-tokenization-mpnet-with-special-tokens[]\\n\\ntag::inference-config-nlp-tokenization-xlm-roberta[]\\nexperimental:[] XLMRoBERTa-style tokenization is to be performed with the enclosed settings.\\nend::inference-config-nlp-tokenization-xlm-roberta[]\\n\\ntag::inference-config-nlp-vocabulary[]\\nThe configuration for retrieving the vocabulary of the model. The vocabulary is\\nthen used at inference time. This information is usually provided automatically\\nby storing vocabulary in a known, internally managed index.\\nend::inference-config-nlp-vocabulary[]\\n\\ntag::inference-config-nlp-fill-mask[]\\nConfiguration for a fill_mask natural language processing (NLP) task. The\\nfill_mask task works with models optimized for a fill mask action. For example,\\nfor BERT models, the following text may be provided: \\\"The capital of France is\\n[MASK].\\\". The response indicates the value most likely to replace `[MASK]`. In\\nthis instance, the most probable token is `paris`.\\nend::inference-config-nlp-fill-mask[]\\n\\ntag::inference-config-ner[]\\nConfigures a named entity recognition (NER) task. NER is a special case of token\\nclassification. Each token in the sequence is classified according to the\\nprovided classification labels. Currently, the NER task requires the\\n`classification_labels` Inside-Outside-Beginning (IOB) formatted labels. Only\\nperson, organization, location, and miscellaneous are supported.\\nend::inference-config-ner[]\\n\\ntag::inference-config-pass-through[]\\nConfigures a `pass_through` task. This task is useful for debugging as no\\npost-processing is done to the inference output and the raw pooling layer\\nresults are returned to the caller.\\nend::inference-config-pass-through[]\\n\\ntag::inference-config-nlp-question-answering[]\\nConfigures a question answering natural language processing (NLP) task. Question\\nanswering is useful for extracting answers for certain questions from a large\\ncorpus of text.\\nend::inference-config-nlp-question-answering[]\\n\\ntag::inference-config-text-classification[]\\nA text classification task. Text classification classifies a provided text\\nsequence into previously known target classes. A specific example of this is\\nsentiment analysis, which returns the likely target classes indicating text\\nsentiment, such as \\\"sad\\\", \\\"happy\\\", or \\\"angry\\\".\\nend::inference-config-text-classification[]\\n\\ntag::inference-config-text-embedding[]\\nText embedding takes an input sequence and transforms it into a vector of\\nnumbers. These embeddings capture not simply tokens, but semantic meanings and\\ncontext. These embeddings can be used in a <<dense-vector,dense vector>> field\\nfor powerful insights.\\nend::inference-config-text-embedding[]\\n\\ntag::inference-config-text-embedding-size[]\\nThe number of dimensions in the embedding vector produced by the model.\\nend::inference-config-text-embedding-size[]\\n\\ntag::inference-config-text-similarity[]\\nText similarity takes an input sequence and compares it with another input sequence. This is commonly referred to\\nas cross-encoding. This task is useful for ranking document text when comparing it to another provided text input.\\nend::inference-config-text-similarity[]\\n\\ntag::inference-config-text-similarity-span-score-func[]\\nIdentifies how to combine the resulting similarity score when a provided text passage is longer than `max_sequence_length` and must be\\nautomatically separated for multiple calls. This only is applicable when `truncate` is `none` and `span` is a non-negative\\nnumber. The default value is `max`. Available options are:\\n+\\n--\\n* `max`: The maximum score from all the spans is returned.\\n* `mean`: The mean score over all the spans is returned.\\n--\\nend::inference-config-text-similarity-span-score-func[]\\n\\ntag::inference-config-text-similarity-text[]\\nThis is the text with which to compare all document provided text inputs.\\nend::inference-config-text-similarity-text[]\\n\\ntag::inference-config-regression-num-top-feature-importance-values[]\\nSpecifies the maximum number of\\n{ml-docs}/ml-feature-importance.html[{feat-imp}] values per document.\\nBy default, it is zero and no {feat-imp} calculation occurs.\\nend::inference-config-regression-num-top-feature-importance-values[]\\n\\ntag::inference-config-results-field[]\\nThe field that is added to incoming documents to contain the inference\\nprediction. Defaults to `predicted_value`.\\nend::inference-config-results-field[]\\n\\ntag::inference-config-mask-token[]\\nThe string/token which will be removed from incoming documents and replaced with the inference prediction(s). In a response, this field contains the mask token for the specified model/tokenizer. Each model and tokenizer has a predefined mask token which cannot be changed. Thus, it is recommended not to set this value in requests. However, if this field is present in a request, its value must match the predefined value for that model/tokenizer, otherwise the request will fail.\\nend::inference-config-mask-token[]\\n\\ntag::inference-config-results-field-processor[]\\nThe field that is added to incoming documents to contain the inference\\nprediction. Defaults to the `results_field` value of the {dfanalytics-job} that was\\nused to train the model, which defaults to `<dependent_variable>_prediction`.\\nend::inference-config-results-field-processor[]\\n\\ntag::inference-config-zero-shot-classification[]\\nConfigures a zero-shot classification task. Zero-shot classification allows for\\ntext classification to occur without pre-determined labels. At inference time,\\nit is possible to adjust the labels to classify. This makes this type of model\\nand task exceptionally flexible.\\n+\\n--\\nIf consistently classifying the same labels, it may be better to use a\\nfine-tuned text classification model.\\n--\\nend::inference-config-zero-shot-classification[]\\n\\ntag::inference-config-zero-shot-classification-classification-labels[]\\nThe classification labels used during the zero-shot classification. Classification\\nlabels must not be empty or null and only set at model creation. They must be all three\\nof [\\\"entailment\\\", \\\"neutral\\\", \\\"contradiction\\\"].\\n\\nNOTE: This is NOT the same as `labels` which are the values that zero-shot is attempting to\\n      classify.\\nend::inference-config-zero-shot-classification-classification-labels[]\\n\\ntag::inference-config-zero-shot-classification-hypothesis-template[]\\nThis is the template used when tokenizing the sequences for classification.\\n+\\n--\\nThe labels replace the `{}` value in the text. The default value is:\\n`This example is {}.`\\n--\\nend::inference-config-zero-shot-classification-hypothesis-template[]\\n\\ntag::inference-config-zero-shot-classification-labels[]\\nThe labels to classify. Can be set at creation for default labels, and\\nthen updated during inference.\\nend::inference-config-zero-shot-classification-labels[]\\n\\ntag::inference-config-zero-shot-classification-multi-label[]\\nIndicates if more than one `true` label is possible given the input.\\nThis is useful when labeling text that could pertain to more than one of the\\ninput labels. Defaults to `false`.\\nend::inference-config-zero-shot-classification-multi-label[]\\n\\ntag::inference-metadata-feature-importance-feature-name[]\\nThe feature for which this importance was calculated.\\nend::inference-metadata-feature-importance-feature-name[]\\ntag::inference-metadata-feature-importance-magnitude[]\\nThe average magnitude of this feature across all the training data.\\nThis value is the average of the absolute values of the importance\\nfor this feature.\\nend::inference-metadata-feature-importance-magnitude[]\\ntag::inference-metadata-feature-importance-max[]\\nThe maximum importance value across all the training data for this\\nfeature.\\nend::inference-metadata-feature-importance-max[]\\ntag::inference-metadata-feature-importance-min[]\\nThe minimum importance value across all the training data for this\\nfeature.\\nend::inference-metadata-feature-importance-min[]\\n\\ntag::influencers[]\\nA comma separated list of influencer field names. Typically these can be the by,\\nover, or partition fields that are used in the detector configuration. You might\\nalso want to use a field name that is not specifically named in a detector, but\\nis available as part of the input data. When you use multiple detectors, the use\\nof influencers is recommended as it aggregates results for each influencer\\nentity.\\nend::influencers[]\\n\\ntag::input-bytes[]\\nThe number of bytes of input data posted to the {anomaly-job}.\\nend::input-bytes[]\\n\\ntag::input-field-count[]\\nThe total number of fields in input documents posted to the {anomaly-job}. This\\ncount includes fields that are not used in the analysis. However, be aware that\\nif you are using a {dfeed}, it extracts only the required fields from the\\ndocuments it retrieves before posting them to the job.\\nend::input-field-count[]\\n\\ntag::input-record-count[]\\nThe number of input documents posted to the {anomaly-job}.\\nend::input-record-count[]\\n\\ntag::invalid-date-count[]\\nThe number of input documents with either a missing date field or a date that\\ncould not be parsed.\\nend::invalid-date-count[]\\n\\ntag::is-interim[]\\nIf `true`, this is an interim result. In other words, the results are calculated\\nbased on partial input data.\\nend::is-interim[]\\n\\ntag::job-id-anomaly-detection[]\\nIdentifier for the {anomaly-job}.\\nend::job-id-anomaly-detection[]\\n\\ntag::job-id-data-frame-analytics[]\\nIdentifier for the {dfanalytics-job}.\\nend::job-id-data-frame-analytics[]\\n\\ntag::job-id-anomaly-detection-default[]\\nIdentifier for the {anomaly-job}. It can be a job identifier, a group name, or a\\nwildcard expression. If you do not specify one of these options, the API returns\\ninformation for all {anomaly-jobs}.\\nend::job-id-anomaly-detection-default[]\\n\\ntag::job-id-data-frame-analytics-default[]\\nIdentifier for the {dfanalytics-job}. If you do not specify this option, the API\\nreturns information for the first hundred {dfanalytics-jobs}.\\nend::job-id-data-frame-analytics-default[]\\n\\ntag::job-id-anomaly-detection-list[]\\nAn identifier for the {anomaly-jobs}. It can be a job\\nidentifier, a group name, or a comma-separated list of jobs or groups.\\nend::job-id-anomaly-detection-list[]\\n\\ntag::job-id-anomaly-detection-wildcard[]\\nIdentifier for the {anomaly-job}. It can be a job identifier, a group name, or a\\nwildcard expression.\\nend::job-id-anomaly-detection-wildcard[]\\n\\ntag::job-id-anomaly-detection-wildcard-list[]\\nIdentifier for the {anomaly-job}. It can be a job identifier, a group name, a\\ncomma-separated list of jobs or groups, or a wildcard expression.\\nend::job-id-anomaly-detection-wildcard-list[]\\n\\ntag::job-id-anomaly-detection-define[]\\nIdentifier for the {anomaly-job}. This identifier can contain lowercase\\nalphanumeric characters (a-z and 0-9), hyphens, and underscores. It must start\\nand end with alphanumeric characters.\\nend::job-id-anomaly-detection-define[]\\n\\ntag::job-id-data-frame-analytics-define[]\\nIdentifier for the {dfanalytics-job}. This identifier can contain lowercase\\nalphanumeric characters (a-z and 0-9), hyphens, and underscores. It must start\\nand end with alphanumeric characters.\\nend::job-id-data-frame-analytics-define[]\\n\\ntag::job-id-datafeed[]\\nThe unique identifier for the job to which the {dfeed} sends data.\\nend::job-id-datafeed[]\\n\\ntag::output-memory-allocator-bytes[]\\nThe amount of memory, in bytes, used to output {anomaly-job} documents.\\nend::output-memory-allocator-bytes[]\\n\\ntag::lambda[]\\nAdvanced configuration option. Regularization parameter to prevent overfitting\\non the training data set. Multiplies an L2 regularization term which applies to\\nleaf weights of the individual trees in the forest. A high lambda value causes\\ntraining to favor small leaf weights. This behavior makes the prediction\\nfunction smoother at the expense of potentially not being able to capture\\nrelevant relationships between the features and the {depvar}. A small lambda\\nvalue results in large individual trees and slower training. By default, this\\nvalue is calculated during hyperparameter optimization. It must be a nonnegative\\nvalue.\\nend::lambda[]\\n\\ntag::last-data-time[]\\nThe timestamp at which data was last analyzed, according to server time.\\nend::last-data-time[]\\n\\ntag::latency[]\\nThe size of the window in which to expect data that is out of time order. The\\ndefault value is 0 (no latency). If you specify a non-zero value, it must be\\ngreater than or equal to one second. For more information about time units, see\\n<<time-units>>.\\n+\\n--\\nNOTE: Latency is only applicable when you send data by using\\nthe <<ml-post-data,post data>> API.\\n\\n--\\nend::latency[]\\n\\ntag::latest-empty-bucket-timestamp[]\\nThe timestamp of the last bucket that did not contain any data.\\nend::latest-empty-bucket-timestamp[]\\n\\ntag::latest-record-timestamp[]\\nThe timestamp of the latest chronologically input document.\\nend::latest-record-timestamp[]\\n\\ntag::latest-sparse-record-timestamp[]\\nThe timestamp of the last bucket that was considered sparse.\\nend::latest-sparse-record-timestamp[]\\n\\ntag::max-empty-searches[]\\nIf a real-time {dfeed} has never seen any data (including during any initial\\ntraining period) then it will automatically stop itself and close its associated\\njob after this many real-time searches that return no documents. In other words,\\nit will stop after `frequency` times `max_empty_searches` of real-time\\noperation. If not set then a {dfeed} with no end time that sees no data will\\nremain started until it is explicitly stopped. By default this setting is not\\nset.\\nend::max-empty-searches[]\\n\\ntag::max-trees[]\\nAdvanced configuration option. Defines the maximum number of decision trees in\\nthe forest. The maximum value is 2000. By default, this value is calculated\\nduring hyperparameter optimization.\\nend::max-trees[]\\n\\ntag::max-trees-trained-models[]\\nThe maximum number of decision trees in the forest. The maximum value is 2000.\\nBy default, this value is calculated during hyperparameter optimization.\\nend::max-trees-trained-models[]\\n\\ntag::meta[]\\nAdvanced configuration option. Contains custom metadata about the job. For\\nexample, it can contain custom URL information.\\nend::meta[]\\n\\ntag::method[]\\nThe method that {oldetection} uses. Available methods are `lof`, `ldof`,\\n`distance_kth_nn`, `distance_knn`, and `ensemble`. The default value is\\n`ensemble`, which means that {oldetection} uses an ensemble of different methods\\nand normalises and combines their individual {olscores} to obtain the overall\\n{olscore}.\\nend::method[]\\n\\ntag::missing-field-count[]\\n\\n<system-reminder>[Showing lines 1-1432 of 1984 total lines, truncated to 60k characters]</system-reminder>\",\"timestamp\":1762415382154,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"d9b12701-07f6-4e98-91bf-27e9aee7cc4c\",\"text\":\"Let me read the key documentation files that mention adaptive allocations:\",\"timestamp\":1762415382149,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01WAiFiGEkDNmQ5Vxt1fejvR\",\"messageId\":\"d9b12701-07f6-4e98-91bf-27e9aee7cc4c\",\"toolId\":\"Read\",\"toolName\":\"Read\",\"parameters\":{\"file_path\":\"/private/var/folders/1t/tn_3vh3x0nv_snwb7xrv97th0000gn/T/tmpj1glbd_x/workspace/docs/reference/ml/trained-models/apis/start-trained-model-deployment.asciidoc\"},\"timestamp\":1762415382149,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01Fq22sC1h8YKKKwZGgFJatB\",\"messageId\":\"d9b12701-07f6-4e98-91bf-27e9aee7cc4c\",\"toolId\":\"Read\",\"toolName\":\"Read\",\"parameters\":{\"file_path\":\"/private/var/folders/1t/tn_3vh3x0nv_snwb7xrv97th0000gn/T/tmpj1glbd_x/workspace/docs/reference/ml/trained-models/apis/update-trained-model-deployment.asciidoc\"},\"timestamp\":1762415382149,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01VKn2ipwbayn5qrS6G6nCCc\",\"messageId\":\"d9b12701-07f6-4e98-91bf-27e9aee7cc4c\",\"toolId\":\"Read\",\"toolName\":\"Read\",\"parameters\":{\"file_path\":\"/private/var/folders/1t/tn_3vh3x0nv_snwb7xrv97th0000gn/T/tmpj1glbd_x/workspace/docs/reference/ml/ml-shared.asciidoc\"},\"timestamp\":1762415382149,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01Dcs172tUGCd6UJhgB3mjs1\",\"messageId\":\"1dbf959d-829d-4a1f-ad84-b343c5b9fbb9\",\"toolId\":\"\",\"isError\":false,\"value\":\"TODO List Updated\",\"timestamp\":1762415391495,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_017YzSz2qX6ue34iQj9sQnMC\",\"messageId\":\"1dbf959d-829d-4a1f-ad84-b343c5b9fbb9\",\"toolId\":\"\",\"isError\":false,\"value\":\"./docs/reference/ml/ml-shared.asciidoc-tag::adaptive-allocation[]\\n./docs/reference/ml/ml-shared.asciidoc-Adaptive allocations configuration object.\\n./docs/reference/ml/ml-shared.asciidoc-If enabled, the number of allocations of the model is set based on the current load the process gets.\\n./docs/reference/ml/ml-shared.asciidoc-When the load is high, a new model allocation is automatically created (respecting the value of `max_number_of_allocations` if it's set).\\n./docs/reference/ml/ml-shared.asciidoc-When the load is low, a model allocation is automatically removed (respecting the value of `min_number_of_allocations` if it's set).\\n./docs/reference/ml/ml-shared.asciidoc:The number of model allocations cannot be scaled down to less than `1` this way.\\n./docs/reference/ml/ml-shared.asciidoc-If `adaptive_allocations` is enabled, do not set the number of allocations manually.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc-tag::adaptive-allocation-enabled[]\\n./docs/reference/ml/ml-shared.asciidoc-If `true`, `adaptive_allocations` is enabled.\\n./docs/reference/ml/ml-shared.asciidoc-Defaults to `false`.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation-enabled[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc-tag::adaptive-allocation-max-number[]\\n./docs/reference/ml/ml-shared.asciidoc-Specifies the maximum number of allocations to scale to.\",\"timestamp\":1762415391495,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"28f01fa0-c688-486c-bab3-438fb1d4913f\",\"text\":\"Perfect! I found it. Let me read the full content around line 5 to see the exact sentence that needs to be removed:\",\"timestamp\":1762415391368,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01Dcs172tUGCd6UJhgB3mjs1\",\"messageId\":\"28f01fa0-c688-486c-bab3-438fb1d4913f\",\"toolId\":\"TodoWrite\",\"toolName\":\"TodoWrite\",\"parameters\":{\"todos\":[{\"id\":\"1\",\"content\":\"Search for adaptive allocations documentation files\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"2\",\"content\":\"Locate the sentence about not being able to scale down to 0 allocations\",\"status\":\"in_progress\",\"priority\":\"high\"},{\"id\":\"3\",\"content\":\"Remove or update the incorrect sentence\",\"status\":\"pending\",\"priority\":\"high\"},{\"id\":\"4\",\"content\":\"Verify the changes are correct\",\"status\":\"pending\",\"priority\":\"medium\"}]},\"timestamp\":1762415391368,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_017YzSz2qX6ue34iQj9sQnMC\",\"messageId\":\"28f01fa0-c688-486c-bab3-438fb1d4913f\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"cannot be scaled down to less than\",\"output_mode\":\"content\",\"context\":10,\"glob_pattern\":\"*.asciidoc\"},\"timestamp\":1762415391368,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01DeRKuoYXwWg9FMWZtNbKfZ\",\"messageId\":\"33ac59ac-b22d-4cfc-a3a5-74489a6b7997\",\"toolId\":\"\",\"isError\":false,\"value\":\"TODO List Updated\",\"timestamp\":1762415402295,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_014gk5DDf77DSGNdTUWqs17M\",\"messageId\":\"33ac59ac-b22d-4cfc-a3a5-74489a6b7997\",\"toolId\":\"\",\"isError\":false,\"value\":\"{\\\"diffLines\\\":[{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"If enabled, the number of allocations of the model is set based on the current load the process gets.\\\",\\\"lineNumber\\\":{\\\"old\\\":3,\\\"new\\\":3}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"When the load is high, a new model allocation is automatically created (respecting the value of `max_number_of_allocations` if it's set).\\\",\\\"lineNumber\\\":{\\\"old\\\":4,\\\"new\\\":4}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"When the load is low, a model allocation is automatically removed (respecting the value of `min_number_of_allocations` if it's set).\\\",\\\"lineNumber\\\":{\\\"old\\\":5,\\\"new\\\":5}},{\\\"type\\\":\\\"removed\\\",\\\"content\\\":\\\"The number of model allocations cannot be scaled down to less than `1` this way.\\\",\\\"lineNumber\\\":{\\\"old\\\":6}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"If `adaptive_allocations` is enabled, do not set the number of allocations manually.\\\",\\\"lineNumber\\\":{\\\"old\\\":7,\\\"new\\\":6}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"end::adaptive-allocation[]\\\",\\\"lineNumber\\\":{\\\"old\\\":8,\\\"new\\\":7}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"\\\",\\\"lineNumber\\\":{\\\"old\\\":9,\\\"new\\\":8}}]}\",\"timestamp\":1762415402295,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"1de03982-da23-4bba-a08f-0eaf7b77a154\",\"text\":\"Perfect! I found the exact sentence that needs to be removed. It's on line 6 within the `adaptive-allocation` tag section of the `ml-shared.asciidoc` file:\\n\\n\\\"The number of model allocations cannot be scaled down to less than `1` this way.\\\"\\n\\nSince adaptive allocations can now scale down to 0 allocations, this sentence is incorrect and needs to be removed. Let me update the documentation:\",\"timestamp\":1762415402280,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01DeRKuoYXwWg9FMWZtNbKfZ\",\"messageId\":\"1de03982-da23-4bba-a08f-0eaf7b77a154\",\"toolId\":\"TodoWrite\",\"toolName\":\"TodoWrite\",\"parameters\":{\"todos\":[{\"id\":\"1\",\"content\":\"Search for adaptive allocations documentation files\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"2\",\"content\":\"Locate the sentence about not being able to scale down to 0 allocations\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"3\",\"content\":\"Remove or update the incorrect sentence\",\"status\":\"in_progress\",\"priority\":\"high\"},{\"id\":\"4\",\"content\":\"Verify the changes are correct\",\"status\":\"pending\",\"priority\":\"medium\"}]},\"timestamp\":1762415402280,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_014gk5DDf77DSGNdTUWqs17M\",\"messageId\":\"1de03982-da23-4bba-a08f-0eaf7b77a154\",\"toolId\":\"Edit\",\"toolName\":\"Edit\",\"parameters\":{\"file_path\":\"/private/var/folders/1t/tn_3vh3x0nv_snwb7xrv97th0000gn/T/tmpj1glbd_x/workspace/docs/reference/ml/ml-shared.asciidoc\",\"old_str\":\"tag::adaptive-allocation[]\\nAdaptive allocations configuration object.\\nIf enabled, the number of allocations of the model is set based on the current load the process gets.\\nWhen the load is high, a new model allocation is automatically created (respecting the value of `max_number_of_allocations` if it's set).\\nWhen the load is low, a model allocation is automatically removed (respecting the value of `min_number_of_allocations` if it's set).\\nThe number of model allocations cannot be scaled down to less than `1` this way.\\nIf `adaptive_allocations` is enabled, do not set the number of allocations manually.\\nend::adaptive-allocation[]\",\"new_str\":\"tag::adaptive-allocation[]\\nAdaptive allocations configuration object.\\nIf enabled, the number of allocations of the model is set based on the current load the process gets.\\nWhen the load is high, a new model allocation is automatically created (respecting the value of `max_number_of_allocations` if it's set).\\nWhen the load is low, a model allocation is automatically removed (respecting the value of `min_number_of_allocations` if it's set).\\nIf `adaptive_allocations` is enabled, do not set the number of allocations manually.\\nend::adaptive-allocation[]\"},\"timestamp\":1762415402280,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01KkWWgb4E43xkkiSve3TBbn\",\"messageId\":\"e98a07f3-7b82-477d-b11a-d4ab762e6a51\",\"toolId\":\"\",\"isError\":false,\"value\":\"TODO List Updated\",\"timestamp\":1762415408065,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01SAdeRDnLVxAE1YEDsb71Hr\",\"messageId\":\"e98a07f3-7b82-477d-b11a-d4ab762e6a51\",\"toolId\":\"\",\"isError\":false,\"value\":\"./docs/reference/ml/ml-shared.asciidoc:tag::adaptive-allocation[]\\n./docs/reference/ml/ml-shared.asciidoc-Adaptive allocations configuration object.\\n./docs/reference/ml/ml-shared.asciidoc-If enabled, the number of allocations of the model is set based on the current load the process gets.\\n./docs/reference/ml/ml-shared.asciidoc-When the load is high, a new model allocation is automatically created (respecting the value of `max_number_of_allocations` if it's set).\\n./docs/reference/ml/ml-shared.asciidoc-When the load is low, a model allocation is automatically removed (respecting the value of `min_number_of_allocations` if it's set).\\n./docs/reference/ml/ml-shared.asciidoc-If `adaptive_allocations` is enabled, do not set the number of allocations manually.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc-tag::adaptive-allocation-enabled[]\\n./docs/reference/ml/ml-shared.asciidoc-If `true`, `adaptive_allocations` is enabled.\\n./docs/reference/ml/ml-shared.asciidoc-Defaults to `false`.\",\"timestamp\":1762415408065,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"7a00e219-db26-469b-b5e2-53b3d86ca256\",\"text\":\"Excellent! I've successfully removed the sentence. Now let me verify the changes by reading the updated section to make sure it looks correct:\",\"timestamp\":1762415407931,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01KkWWgb4E43xkkiSve3TBbn\",\"messageId\":\"7a00e219-db26-469b-b5e2-53b3d86ca256\",\"toolId\":\"TodoWrite\",\"toolName\":\"TodoWrite\",\"parameters\":{\"todos\":[{\"id\":\"1\",\"content\":\"Search for adaptive allocations documentation files\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"2\",\"content\":\"Locate the sentence about not being able to scale down to 0 allocations\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"3\",\"content\":\"Remove or update the incorrect sentence\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"4\",\"content\":\"Verify the changes are correct\",\"status\":\"in_progress\",\"priority\":\"medium\"}]},\"timestamp\":1762415407931,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01SAdeRDnLVxAE1YEDsb71Hr\",\"messageId\":\"7a00e219-db26-469b-b5e2-53b3d86ca256\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"tag::adaptive-allocation\\\\[\\\\]\",\"output_mode\":\"content\",\"context\":10,\"glob_pattern\":\"*.asciidoc\"},\"timestamp\":1762415407931,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01TCMX3oTJkwNSmDYqLwdFxT\",\"messageId\":\"7daece7b-9fd6-4b04-85b8-d867279a2f06\",\"toolId\":\"\",\"isError\":false,\"value\":\"./docs/reference/ml/ml-shared.asciidoc-tag::adaptive-allocation-max-number[]\\n./docs/reference/ml/ml-shared.asciidoc-Specifies the maximum number of allocations to scale to.\\n./docs/reference/ml/ml-shared.asciidoc-If set, it must be greater than or equal to `min_number_of_allocations`.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation-max-number[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc:tag::adaptive-allocation-min-number[]\\n./docs/reference/ml/ml-shared.asciidoc-Specifies the minimum number of allocations to scale to.\\n./docs/reference/ml/ml-shared.asciidoc-If set, it must be greater than or equal to `1`.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation-min-number[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc-tag::aggregations[]\",\"timestamp\":1762415413759,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"b5280c43-adb3-4dba-b44e-a19d6a63f982\",\"text\":\"Perfect! The sentence \\\"The number of model allocations cannot be scaled down to less than `1` this way.\\\" has been successfully removed. Let me also check if there's a `min_number_of_allocations` documentation that might need updating to reflect that 0 is now a valid value:\",\"timestamp\":1762415413628,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01TCMX3oTJkwNSmDYqLwdFxT\",\"messageId\":\"b5280c43-adb3-4dba-b44e-a19d6a63f982\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"tag::adaptive-allocation-min-number\\\\[\\\\]\",\"output_mode\":\"content\",\"context\":5,\"glob_pattern\":\"*.asciidoc\"},\"timestamp\":1762415413628,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"7d0cd556-fd6c-4068-b0bf-a0f9e1924412\",\"text\":\"I notice that the `min_number_of_allocations` documentation states \\\"If set, it must be greater than or equal to `1`.\\\" However, since adaptive allocations can now scale down to 0, this should be updated to allow 0 as a valid minimum value. Let me check the code to see if 0 is indeed allowed:\",\"timestamp\":1762415418476,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01XdrcQvua44fvdJhnt8KJLz\",\"messageId\":\"7d0cd556-fd6c-4068-b0bf-a0f9e1924412\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"min_number_of_allocations\",\"output_mode\":\"content\",\"context\":3,\"type\":\"java\"},\"timestamp\":1762415418476,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01XdrcQvua44fvdJhnt8KJLz\",\"messageId\":\"804be145-f480-4c22-8730-e7d1a38b73d5\",\"toolId\":\"Grep\",\"isError\":false,\"value\":\"./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-        assertThat(\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-            modelConfig.toString(),\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-            adaptiveAllocations,\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java:            Matchers.is(Map.of(\\\"enabled\\\", true, \\\"min_number_of_allocations\\\", 0, \\\"max_number_of_allocations\\\", 8))\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-        );\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-    }\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-\\n--\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-        assertThat(\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-            modelConfig.toString(),\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-            adaptiveAllocations,\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java:            Matchers.is(Map.of(\\\"enabled\\\", true, \\\"min_number_of_allocations\\\", 0, \\\"max_number_of_allocations\\\", 8))\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-        );\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-    }\\n./x-pack/plugin/inference/qa/inference-service-tests/src/javaRestTest/java/org/elasticsearch/xpack/inference/DefaultEndPointsIT.java-}\\n--\\n./x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/services/ServiceUtilsTests.java-\\n./x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/services/ServiceUtilsTests.java-    public void testRemoveAsAdaptiveAllocationsSettings() {\\n./x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/services/ServiceUtilsTests.java-        Map<String, Object> map = new HashMap<>(\\n./x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/services/ServiceUtilsTests.java:            Map.of(\\\"settings\\\", new HashMap<>(Map.of(\\\"enabled\\\", true, \\\"min_number_of_allocations\\\", 7, \\\"max_number_of_allocations\\\", 42)))\\n./x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/services/ServiceUtilsTests.java-        );\\n./x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/services/ServiceUtilsTests.java-        ValidationException validationException = new ValidationException();\\n./x-pack/plugin/inference/src/test/java/org/elasticsearch/xpack/inference/services/ServiceUtilsTests.java-        assertThat(\\n--\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/AdaptiveAllocationsSettings.java-    public static final AdaptiveAllocationsSettings RESET_PLACEHOLDER = new AdaptiveAllocationsSettings(false, -1, -1);\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/AdaptiveAllocationsSettings.java-\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/AdaptiveAllocationsSettings.java-    public static final ParseField ENABLED = new ParseField(\\\"enabled\\\");\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/AdaptiveAllocationsSettings.java:    public static final ParseField MIN_NUMBER_OF_ALLOCATIONS = new ParseField(\\\"min_number_of_allocations\\\");\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/AdaptiveAllocationsSettings.java-    public static final ParseField MAX_NUMBER_OF_ALLOCATIONS = new ParseField(\\\"max_number_of_allocations\\\");\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/AdaptiveAllocationsSettings.java-\\n./x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/AdaptiveAllocationsSettings.java-    public static final ObjectParser<AdaptiveAllocationsSettings.Builder, Void> PARSER = new ObjectParser<>(\",\"timestamp\":1762415419133,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"f7f3319d-4de4-4827-958c-33ba7b6ead2f\",\"text\":\"Great! I can see from the test code that `min_number_of_allocations` is being set to `0`, which confirms that 0 is now a valid value. Let me check the validation code to see if this is the current behavior:\",\"timestamp\":1762415425224,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01Hbc6USni82hoySR872E9Fr\",\"messageId\":\"f7f3319d-4de4-4827-958c-33ba7b6ead2f\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"MIN_NUMBER_OF_ALLOCATIONS.*greater\",\"case_insensitive\":true,\"output_mode\":\"content\",\"context\":5},\"timestamp\":1762415425224,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01VWj5RkX32ZiJXudqDkbyky\",\"messageId\":\"f7f3319d-4de4-4827-958c-33ba7b6ead2f\",\"toolId\":\"Read\",\"toolName\":\"Read\",\"parameters\":{\"file_path\":\"/private/var/folders/1t/tn_3vh3x0nv_snwb7xrv97th0000gn/T/tmpj1glbd_x/workspace/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ml/inference/assignment/AdaptiveAllocationsSettings.java\"},\"timestamp\":1762415425224,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01Hbc6USni82hoySR872E9Fr\",\"messageId\":\"e3db988d-7b43-4e5b-a94b-81491915ffad\",\"toolId\":\"Grep\",\"isError\":false,\"value\":\"No matches found\",\"timestamp\":1762415426090,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01VWj5RkX32ZiJXudqDkbyky\",\"messageId\":\"e3db988d-7b43-4e5b-a94b-81491915ffad\",\"toolId\":\"Read\",\"isError\":false,\"value\":\"/*\\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\\n * or more contributor license agreements. Licensed under the Elastic License\\n * 2.0; you may not use this file except in compliance with the Elastic License\\n * 2.0.\\n */\\n\\npackage org.elasticsearch.xpack.core.ml.inference.assignment;\\n\\nimport org.elasticsearch.action.ActionRequestValidationException;\\nimport org.elasticsearch.common.io.stream.StreamInput;\\nimport org.elasticsearch.common.io.stream.StreamOutput;\\nimport org.elasticsearch.common.io.stream.Writeable;\\nimport org.elasticsearch.xcontent.ObjectParser;\\nimport org.elasticsearch.xcontent.ParseField;\\nimport org.elasticsearch.xcontent.ToXContentObject;\\nimport org.elasticsearch.xcontent.XContentBuilder;\\nimport org.elasticsearch.xcontent.XContentParser;\\n\\nimport java.io.IOException;\\nimport java.util.Objects;\\n\\npublic class AdaptiveAllocationsSettings implements ToXContentObject, Writeable {\\n\\n    public static final AdaptiveAllocationsSettings RESET_PLACEHOLDER = new AdaptiveAllocationsSettings(false, -1, -1);\\n\\n    public static final ParseField ENABLED = new ParseField(\\\"enabled\\\");\\n    public static final ParseField MIN_NUMBER_OF_ALLOCATIONS = new ParseField(\\\"min_number_of_allocations\\\");\\n    public static final ParseField MAX_NUMBER_OF_ALLOCATIONS = new ParseField(\\\"max_number_of_allocations\\\");\\n\\n    public static final ObjectParser<AdaptiveAllocationsSettings.Builder, Void> PARSER = new ObjectParser<>(\\n        \\\"autoscaling_settings\\\",\\n        AdaptiveAllocationsSettings.Builder::new\\n    );\\n\\n    static {\\n        PARSER.declareBoolean(Builder::setEnabled, ENABLED);\\n        PARSER.declareIntOrNull(Builder::setMinNumberOfAllocations, -1, MIN_NUMBER_OF_ALLOCATIONS);\\n        PARSER.declareIntOrNull(Builder::setMaxNumberOfAllocations, -1, MAX_NUMBER_OF_ALLOCATIONS);\\n    }\\n\\n    public static AdaptiveAllocationsSettings parseRequest(XContentParser parser) {\\n        return PARSER.apply(parser, null).build();\\n    }\\n\\n    public static class Builder {\\n        private Boolean enabled;\\n        private Integer minNumberOfAllocations;\\n        private Integer maxNumberOfAllocations;\\n\\n        public Builder() {}\\n\\n        public Builder(AdaptiveAllocationsSettings settings) {\\n            enabled = settings.enabled;\\n            minNumberOfAllocations = settings.minNumberOfAllocations;\\n            maxNumberOfAllocations = settings.maxNumberOfAllocations;\\n        }\\n\\n        public void setEnabled(Boolean enabled) {\\n            this.enabled = enabled;\\n        }\\n\\n        public void setMinNumberOfAllocations(Integer minNumberOfAllocations) {\\n            this.minNumberOfAllocations = minNumberOfAllocations;\\n        }\\n\\n        public void setMaxNumberOfAllocations(Integer maxNumberOfAllocations) {\\n            this.maxNumberOfAllocations = maxNumberOfAllocations;\\n        }\\n\\n        public AdaptiveAllocationsSettings build() {\\n            return new AdaptiveAllocationsSettings(enabled, minNumberOfAllocations, maxNumberOfAllocations);\\n        }\\n    }\\n\\n    private final Boolean enabled;\\n    private final Integer minNumberOfAllocations;\\n    private final Integer maxNumberOfAllocations;\\n\\n    public AdaptiveAllocationsSettings(Boolean enabled, Integer minNumberOfAllocations, Integer maxNumberOfAllocations) {\\n        this.enabled = enabled;\\n        this.minNumberOfAllocations = minNumberOfAllocations;\\n        this.maxNumberOfAllocations = maxNumberOfAllocations;\\n    }\\n\\n    public AdaptiveAllocationsSettings(StreamInput in) throws IOException {\\n        enabled = in.readOptionalBoolean();\\n        minNumberOfAllocations = in.readOptionalInt();\\n        maxNumberOfAllocations = in.readOptionalInt();\\n    }\\n\\n    public Boolean getEnabled() {\\n        return enabled;\\n    }\\n\\n    public Integer getMinNumberOfAllocations() {\\n        return minNumberOfAllocations;\\n    }\\n\\n    public Integer getMaxNumberOfAllocations() {\\n        return maxNumberOfAllocations;\\n    }\\n\\n    @Override\\n    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\\n        builder.startObject();\\n        builder.field(ENABLED.getPreferredName(), enabled != null ? enabled : false);\\n        if (minNumberOfAllocations != null) {\\n            builder.field(MIN_NUMBER_OF_ALLOCATIONS.getPreferredName(), minNumberOfAllocations);\\n        }\\n        if (maxNumberOfAllocations != null) {\\n            builder.field(MAX_NUMBER_OF_ALLOCATIONS.getPreferredName(), maxNumberOfAllocations);\\n        }\\n        builder.endObject();\\n        return builder;\\n    }\\n\\n    @Override\\n    public void writeTo(StreamOutput out) throws IOException {\\n        out.writeOptionalBoolean(enabled);\\n        out.writeOptionalInt(minNumberOfAllocations);\\n        out.writeOptionalInt(maxNumberOfAllocations);\\n    }\\n\\n    public AdaptiveAllocationsSettings merge(AdaptiveAllocationsSettings updates) {\\n        AdaptiveAllocationsSettings.Builder builder = new Builder(this);\\n        if (updates.getEnabled() != null) {\\n            builder.setEnabled(updates.enabled);\\n        }\\n        if (updates.minNumberOfAllocations != null) {\\n            if (updates.minNumberOfAllocations == -1) {\\n                builder.setMinNumberOfAllocations(null);\\n            } else {\\n                builder.setMinNumberOfAllocations(updates.minNumberOfAllocations);\\n            }\\n        }\\n        if (updates.maxNumberOfAllocations != null) {\\n            if (updates.maxNumberOfAllocations == -1) {\\n                builder.setMaxNumberOfAllocations(null);\\n            } else {\\n                builder.setMaxNumberOfAllocations(updates.maxNumberOfAllocations);\\n            }\\n        }\\n        return builder.build();\\n    }\\n\\n    public ActionRequestValidationException validate() {\\n        ActionRequestValidationException validationException = new ActionRequestValidationException();\\n        boolean hasMinNumberOfAllocations = (minNumberOfAllocations != null && minNumberOfAllocations != -1);\\n        if (hasMinNumberOfAllocations && minNumberOfAllocations < 0) {\\n            validationException.addValidationError(\\\"[\\\" + MIN_NUMBER_OF_ALLOCATIONS + \\\"] must be a non-negative integer or null\\\");\\n        }\\n        boolean hasMaxNumberOfAllocations = (maxNumberOfAllocations != null && maxNumberOfAllocations != -1);\\n        if (hasMaxNumberOfAllocations && maxNumberOfAllocations < 1) {\\n            validationException.addValidationError(\\\"[\\\" + MAX_NUMBER_OF_ALLOCATIONS + \\\"] must be a positive integer or null\\\");\\n        }\\n        if (hasMinNumberOfAllocations && hasMaxNumberOfAllocations && minNumberOfAllocations > maxNumberOfAllocations) {\\n            validationException.addValidationError(\\n                \\\"[\\\" + MIN_NUMBER_OF_ALLOCATIONS + \\\"] must not be larger than [\\\" + MAX_NUMBER_OF_ALLOCATIONS + \\\"]\\\"\\n            );\\n        }\\n        return validationException.validationErrors().isEmpty() ? null : validationException;\\n    }\\n\\n    @Override\\n    public boolean equals(Object o) {\\n        if (this == o) return true;\\n        if (o == null || getClass() != o.getClass()) return false;\\n        AdaptiveAllocationsSettings that = (AdaptiveAllocationsSettings) o;\\n        return Objects.equals(enabled, that.enabled)\\n            && Objects.equals(minNumberOfAllocations, that.minNumberOfAllocations)\\n            && Objects.equals(maxNumberOfAllocations, that.maxNumberOfAllocations);\\n    }\\n\\n    @Override\\n    public int hashCode() {\\n        return Objects.hash(enabled, minNumberOfAllocations, maxNumberOfAllocations);\\n    }\\n}\\n\",\"timestamp\":1762415426090,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_011FAdu2yAkcA3eqJxmdatBw\",\"messageId\":\"b5ae1034-3137-45b9-a339-d7b5c909e411\",\"toolId\":\"\",\"isError\":false,\"value\":\"{\\\"diffLines\\\":[{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"\\\",\\\"lineNumber\\\":{\\\"old\\\":18,\\\"new\\\":18}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"tag::adaptive-allocation-min-number[]\\\",\\\"lineNumber\\\":{\\\"old\\\":19,\\\"new\\\":19}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"Specifies the minimum number of allocations to scale to.\\\",\\\"lineNumber\\\":{\\\"old\\\":20,\\\"new\\\":20}},{\\\"type\\\":\\\"removed\\\",\\\"content\\\":\\\"If set, it must be greater than or equal to `1`.\\\",\\\"lineNumber\\\":{\\\"old\\\":21}},{\\\"type\\\":\\\"added\\\",\\\"content\\\":\\\"If set, it must be a non-negative integer.\\\",\\\"lineNumber\\\":{\\\"new\\\":21}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"end::adaptive-allocation-min-number[]\\\",\\\"lineNumber\\\":{\\\"old\\\":22,\\\"new\\\":22}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"\\\",\\\"lineNumber\\\":{\\\"old\\\":23,\\\"new\\\":23}},{\\\"type\\\":\\\"unchanged\\\",\\\"content\\\":\\\"tag::aggregations[]\\\",\\\"lineNumber\\\":{\\\"old\\\":24,\\\"new\\\":24}}]}\",\"timestamp\":1762415435311,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"6c77888d-13e0-4690-9c03-8b42ab9742d6\",\"text\":\"Perfect! Looking at the validation code (line 151-153), I can see that `minNumberOfAllocations` validation now checks:\\n```java\\nif (hasMinNumberOfAllocations && minNumberOfAllocations < 0) {\\n    validationException.addValidationError(\\\"[\\\" + MIN_NUMBER_OF_ALLOCATIONS + \\\"] must be a non-negative integer or null\\\");\\n}\\n```\\n\\nThis means that the minimum value allowed is 0 (non-negative), not 1. So the documentation for `min_number_of_allocations` is also incorrect and should be updated. Let me fix that as well:\",\"timestamp\":1762415435305,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_011FAdu2yAkcA3eqJxmdatBw\",\"messageId\":\"6c77888d-13e0-4690-9c03-8b42ab9742d6\",\"toolId\":\"Edit\",\"toolName\":\"Edit\",\"parameters\":{\"file_path\":\"/private/var/folders/1t/tn_3vh3x0nv_snwb7xrv97th0000gn/T/tmpj1glbd_x/workspace/docs/reference/ml/ml-shared.asciidoc\",\"old_str\":\"tag::adaptive-allocation-min-number[]\\nSpecifies the minimum number of allocations to scale to.\\nIf set, it must be greater than or equal to `1`.\\nend::adaptive-allocation-min-number[]\",\"new_str\":\"tag::adaptive-allocation-min-number[]\\nSpecifies the minimum number of allocations to scale to.\\nIf set, it must be a non-negative integer.\\nend::adaptive-allocation-min-number[]\"},\"timestamp\":1762415435305,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01CtWYXFY6Ak6ZXmujDwKkya\",\"messageId\":\"1a90a5f5-b0ed-4743-9d5a-49e6a158542f\",\"toolId\":\"\",\"isError\":false,\"value\":\"TODO List Updated\",\"timestamp\":1762415442692,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_result\",\"id\":\"toolu_01LKPpqNu8RE8z7S7Xzei3Rm\",\"messageId\":\"1a90a5f5-b0ed-4743-9d5a-49e6a158542f\",\"toolId\":\"\",\"isError\":false,\"value\":\"./docs/reference/ml/ml-shared.asciidoc:tag::adaptive-allocation[]\\n./docs/reference/ml/ml-shared.asciidoc-Adaptive allocations configuration object.\\n./docs/reference/ml/ml-shared.asciidoc-If enabled, the number of allocations of the model is set based on the current load the process gets.\\n./docs/reference/ml/ml-shared.asciidoc-When the load is high, a new model allocation is automatically created (respecting the value of `max_number_of_allocations` if it's set).\\n./docs/reference/ml/ml-shared.asciidoc-When the load is low, a model allocation is automatically removed (respecting the value of `min_number_of_allocations` if it's set).\\n./docs/reference/ml/ml-shared.asciidoc-If `adaptive_allocations` is enabled, do not set the number of allocations manually.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc:tag::adaptive-allocation-enabled[]\\n./docs/reference/ml/ml-shared.asciidoc-If `true`, `adaptive_allocations` is enabled.\\n./docs/reference/ml/ml-shared.asciidoc-Defaults to `false`.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation-enabled[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc:tag::adaptive-allocation-max-number[]\\n./docs/reference/ml/ml-shared.asciidoc-Specifies the maximum number of allocations to scale to.\\n./docs/reference/ml/ml-shared.asciidoc-If set, it must be greater than or equal to `min_number_of_allocations`.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation-max-number[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc:tag::adaptive-allocation-min-number[]\\n./docs/reference/ml/ml-shared.asciidoc-Specifies the minimum number of allocations to scale to.\\n./docs/reference/ml/ml-shared.asciidoc-If set, it must be a non-negative integer.\\n./docs/reference/ml/ml-shared.asciidoc-end::adaptive-allocation-min-number[]\\n./docs/reference/ml/ml-shared.asciidoc-\\n./docs/reference/ml/ml-shared.asciidoc-tag::aggregations[]\",\"timestamp\":1762415442692,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"message\",\"role\":\"assistant\",\"id\":\"d07c441f-b1ad-47f4-beb8-5e9e9c47bb1c\",\"text\":\"Excellent! Now let me verify all the changes are correct by reviewing the updated documentation:\",\"timestamp\":1762415442568,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01CtWYXFY6Ak6ZXmujDwKkya\",\"messageId\":\"d07c441f-b1ad-47f4-beb8-5e9e9c47bb1c\",\"toolId\":\"TodoWrite\",\"toolName\":\"TodoWrite\",\"parameters\":{\"todos\":[{\"id\":\"1\",\"content\":\"Search for adaptive allocations documentation files\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"2\",\"content\":\"Locate the sentence about not being able to scale down to 0 allocations\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"3\",\"content\":\"Remove or update the incorrect sentence\",\"status\":\"completed\",\"priority\":\"high\"},{\"id\":\"4\",\"content\":\"Verify the changes are correct\",\"status\":\"completed\",\"priority\":\"medium\"}]},\"timestamp\":1762415442568,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n{\"type\":\"tool_call\",\"id\":\"toolu_01LKPpqNu8RE8z7S7Xzei3Rm\",\"messageId\":\"d07c441f-b1ad-47f4-beb8-5e9e9c47bb1c\",\"toolId\":\"Grep\",\"toolName\":\"Grep\",\"parameters\":{\"pattern\":\"tag::adaptive-allocation\",\"output_mode\":\"content\",\"context\":5,\"glob_pattern\":\"*.asciidoc\"},\"timestamp\":1762415442568,\"session_id\":\"81b91a01-1d05-431d-b53d-2d17fe5a3236\"}\n", "stderr": "", "returncode": 0}
