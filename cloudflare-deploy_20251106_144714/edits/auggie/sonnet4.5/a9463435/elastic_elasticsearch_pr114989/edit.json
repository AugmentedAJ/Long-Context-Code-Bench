{
  "repo_url": "https://github.com/elastic/elasticsearch.git",
  "pr_number": 114989,
  "base_commit": "f95d3f8d430641e2bff22cbdc05f7d6cd9616443",
  "runner": "auggie",
  "model": "sonnet4.5",
  "timeout_s": 1800,
  "status": "success",
  "elapsed_ms": 826614,
  "patch_unified": "diff --git a/modules/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3BlobContainer.java b/modules/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3BlobContainer.java\nindex 902dcb42..4eaa6b97 100644\n--- a/modules/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3BlobContainer.java\n+++ b/modules/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3BlobContainer.java\n@@ -160,23 +160,22 @@ class S3BlobContainer extends AbstractBlobContainer {\n     ) throws IOException {\n         assert purpose != OperationPurpose.SNAPSHOT_DATA && BlobContainer.assertPurposeConsistency(purpose, blobName) : purpose;\n         final String absoluteBlobKey = buildKey(blobName);\n-        try (\n-            AmazonS3Reference clientReference = blobStore.clientReference();\n-            ChunkedBlobOutputStream<PartETag> out = new ChunkedBlobOutputStream<>(blobStore.bigArrays(), blobStore.bufferSizeInBytes()) {\n+        try (ChunkedBlobOutputStream<PartETag> out = new ChunkedBlobOutputStream<>(blobStore.bigArrays(), blobStore.bufferSizeInBytes()) {\n \n-                private final SetOnce<String> uploadId = new SetOnce<>();\n+            private final SetOnce<String> uploadId = new SetOnce<>();\n \n-                @Override\n-                protected void flushBuffer() throws IOException {\n-                    flushBuffer(false);\n-                }\n+            @Override\n+            protected void flushBuffer() throws IOException {\n+                flushBuffer(false);\n+            }\n \n-                private void flushBuffer(boolean lastPart) throws IOException {\n-                    if (buffer.size() == 0) {\n-                        return;\n-                    }\n-                    if (flushedBytes == 0L) {\n-                        assert lastPart == false : \"use single part upload if there's only a single part\";\n+            private void flushBuffer(boolean lastPart) throws IOException {\n+                if (buffer.size() == 0) {\n+                    return;\n+                }\n+                if (flushedBytes == 0L) {\n+                    assert lastPart == false : \"use single part upload if there's only a single part\";\n+                    try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n                         uploadId.set(\n                             SocketAccess.doPrivileged(\n                                 () -> clientReference.client()\n@@ -184,51 +183,55 @@ class S3BlobContainer extends AbstractBlobContainer {\n                                     .getUploadId()\n                             )\n                         );\n-                        if (Strings.isEmpty(uploadId.get())) {\n-                            throw new IOException(\"Failed to initialize multipart upload \" + absoluteBlobKey);\n-                        }\n                     }\n-                    assert lastPart == false || successful : \"must only write last part if successful\";\n-                    final UploadPartRequest uploadRequest = createPartUploadRequest(\n-                        purpose,\n-                        buffer.bytes().streamInput(),\n-                        uploadId.get(),\n-                        parts.size() + 1,\n-                        absoluteBlobKey,\n-                        buffer.size(),\n-                        lastPart\n-                    );\n+                    if (Strings.isEmpty(uploadId.get())) {\n+                        throw new IOException(\"Failed to initialize multipart upload \" + absoluteBlobKey);\n+                    }\n+                }\n+                assert lastPart == false || successful : \"must only write last part if successful\";\n+                final UploadPartRequest uploadRequest = createPartUploadRequest(\n+                    purpose,\n+                    buffer.bytes().streamInput(),\n+                    uploadId.get(),\n+                    parts.size() + 1,\n+                    absoluteBlobKey,\n+                    buffer.size(),\n+                    lastPart\n+                );\n+                try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n                     final UploadPartResult uploadResponse = SocketAccess.doPrivileged(\n                         () -> clientReference.client().uploadPart(uploadRequest)\n                     );\n                     finishPart(uploadResponse.getPartETag());\n                 }\n+            }\n \n-                @Override\n-                protected void onCompletion() throws IOException {\n-                    if (flushedBytes == 0L) {\n-                        writeBlob(purpose, blobName, buffer.bytes(), failIfAlreadyExists);\n-                    } else {\n-                        flushBuffer(true);\n-                        final CompleteMultipartUploadRequest complRequest = new CompleteMultipartUploadRequest(\n-                            blobStore.bucket(),\n-                            absoluteBlobKey,\n-                            uploadId.get(),\n-                            parts\n-                        );\n-                        S3BlobStore.configureRequestForMetrics(complRequest, blobStore, Operation.PUT_MULTIPART_OBJECT, purpose);\n+            @Override\n+            protected void onCompletion() throws IOException {\n+                if (flushedBytes == 0L) {\n+                    writeBlob(purpose, blobName, buffer.bytes(), failIfAlreadyExists);\n+                } else {\n+                    flushBuffer(true);\n+                    final CompleteMultipartUploadRequest complRequest = new CompleteMultipartUploadRequest(\n+                        blobStore.bucket(),\n+                        absoluteBlobKey,\n+                        uploadId.get(),\n+                        parts\n+                    );\n+                    S3BlobStore.configureRequestForMetrics(complRequest, blobStore, Operation.PUT_MULTIPART_OBJECT, purpose);\n+                    try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n                         SocketAccess.doPrivilegedVoid(() -> clientReference.client().completeMultipartUpload(complRequest));\n                     }\n                 }\n+            }\n \n-                @Override\n-                protected void onFailure() {\n-                    if (Strings.hasText(uploadId.get())) {\n-                        abortMultiPartUpload(purpose, uploadId.get(), absoluteBlobKey);\n-                    }\n+            @Override\n+            protected void onFailure() {\n+                if (Strings.hasText(uploadId.get())) {\n+                    abortMultiPartUpload(purpose, uploadId.get(), absoluteBlobKey);\n                 }\n             }\n-        ) {\n+        }) {\n             writer.accept(out);\n             out.markSuccess();\n         }\n@@ -319,20 +322,26 @@ class S3BlobContainer extends AbstractBlobContainer {\n     public DeleteResult delete(OperationPurpose purpose) throws IOException {\n         final AtomicLong deletedBlobs = new AtomicLong();\n         final AtomicLong deletedBytes = new AtomicLong();\n-        try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n+        try {\n             ObjectListing prevListing = null;\n             while (true) {\n                 final ObjectListing list;\n                 if (prevListing != null) {\n                     final var listNextBatchOfObjectsRequest = new ListNextBatchOfObjectsRequest(prevListing);\n                     S3BlobStore.configureRequestForMetrics(listNextBatchOfObjectsRequest, blobStore, Operation.LIST_OBJECTS, purpose);\n-                    list = SocketAccess.doPrivileged(() -> clientReference.client().listNextBatchOfObjects(listNextBatchOfObjectsRequest));\n+                    try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n+                        list = SocketAccess.doPrivileged(\n+                            () -> clientReference.client().listNextBatchOfObjects(listNextBatchOfObjectsRequest)\n+                        );\n+                    }\n                 } else {\n                     final ListObjectsRequest listObjectsRequest = new ListObjectsRequest();\n                     listObjectsRequest.setBucketName(blobStore.bucket());\n                     listObjectsRequest.setPrefix(keyPath);\n                     S3BlobStore.configureRequestForMetrics(listObjectsRequest, blobStore, Operation.LIST_OBJECTS, purpose);\n-                    list = SocketAccess.doPrivileged(() -> clientReference.client().listObjects(listObjectsRequest));\n+                    try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n+                        list = SocketAccess.doPrivileged(() -> clientReference.client().listObjects(listObjectsRequest));\n+                    }\n                 }\n                 final Iterator<String> blobNameIterator = Iterators.map(list.getObjectSummaries().iterator(), summary -> {\n                     deletedBlobs.incrementAndGet();\n@@ -360,10 +369,9 @@ class S3BlobContainer extends AbstractBlobContainer {\n \n     @Override\n     public Map<String, BlobMetadata> listBlobsByPrefix(OperationPurpose purpose, @Nullable String blobNamePrefix) throws IOException {\n-        try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n+        try {\n             return executeListing(\n                 purpose,\n-                clientReference,\n                 listObjectsRequest(purpose, blobNamePrefix == null ? keyPath : buildKey(blobNamePrefix))\n             ).stream()\n                 .flatMap(listing -> listing.getObjectSummaries().stream())\n@@ -381,8 +389,8 @@ class S3BlobContainer extends AbstractBlobContainer {\n \n     @Override\n     public Map<String, BlobContainer> children(OperationPurpose purpose) throws IOException {\n-        try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n-            return executeListing(purpose, clientReference, listObjectsRequest(purpose, keyPath)).stream().flatMap(listing -> {\n+        try {\n+            return executeListing(purpose, listObjectsRequest(purpose, keyPath)).stream().flatMap(listing -> {\n                 assert listing.getObjectSummaries().stream().noneMatch(s -> {\n                     for (String commonPrefix : listing.getCommonPrefixes()) {\n                         if (s.getKey().substring(keyPath.length()).startsWith(commonPrefix)) {\n@@ -403,11 +411,7 @@ class S3BlobContainer extends AbstractBlobContainer {\n         }\n     }\n \n-    private List<ObjectListing> executeListing(\n-        OperationPurpose purpose,\n-        AmazonS3Reference clientReference,\n-        ListObjectsRequest listObjectsRequest\n-    ) {\n+    private List<ObjectListing> executeListing(OperationPurpose purpose, ListObjectsRequest listObjectsRequest) {\n         final List<ObjectListing> results = new ArrayList<>();\n         ObjectListing prevListing = null;\n         while (true) {\n@@ -415,9 +419,15 @@ class S3BlobContainer extends AbstractBlobContainer {\n             if (prevListing != null) {\n                 final var listNextBatchOfObjectsRequest = new ListNextBatchOfObjectsRequest(prevListing);\n                 S3BlobStore.configureRequestForMetrics(listNextBatchOfObjectsRequest, blobStore, Operation.LIST_OBJECTS, purpose);\n-                list = SocketAccess.doPrivileged(() -> clientReference.client().listNextBatchOfObjects(listNextBatchOfObjectsRequest));\n+                try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n+                    list = SocketAccess.doPrivileged(\n+                        () -> clientReference.client().listNextBatchOfObjects(listNextBatchOfObjectsRequest)\n+                    );\n+                }\n             } else {\n-                list = SocketAccess.doPrivileged(() -> clientReference.client().listObjects(listObjectsRequest));\n+                try (AmazonS3Reference clientReference = blobStore.clientReference()) {\n+                    list = SocketAccess.doPrivileged(() -> clientReference.client().listObjects(listObjectsRequest));\n+                }\n             }\n             results.add(list);\n             if (list.isTruncated()) {\n@@ -504,13 +514,15 @@ class S3BlobContainer extends AbstractBlobContainer {\n         final SetOnce<String> uploadId = new SetOnce<>();\n         final String bucketName = s3BlobStore.bucket();\n         boolean success = false;\n-        try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+        try {\n \n-            uploadId.set(\n-                SocketAccess.doPrivileged(\n-                    () -> clientReference.client().initiateMultipartUpload(initiateMultiPartUpload(purpose, blobName)).getUploadId()\n-                )\n-            );\n+            try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+                uploadId.set(\n+                    SocketAccess.doPrivileged(\n+                        () -> clientReference.client().initiateMultipartUpload(initiateMultiPartUpload(purpose, blobName)).getUploadId()\n+                    )\n+                );\n+            }\n             if (Strings.isEmpty(uploadId.get())) {\n                 throw new IOException(\"Failed to initialize multipart upload \" + blobName);\n             }\n@@ -531,8 +543,12 @@ class S3BlobContainer extends AbstractBlobContainer {\n                 );\n                 bytesCount += uploadRequest.getPartSize();\n \n-                final UploadPartResult uploadResponse = SocketAccess.doPrivileged(() -> clientReference.client().uploadPart(uploadRequest));\n-                parts.add(uploadResponse.getPartETag());\n+                try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+                    final UploadPartResult uploadResponse = SocketAccess.doPrivileged(\n+                        () -> clientReference.client().uploadPart(uploadRequest)\n+                    );\n+                    parts.add(uploadResponse.getPartETag());\n+                }\n             }\n \n             if (bytesCount != blobSize) {\n@@ -548,7 +564,9 @@ class S3BlobContainer extends AbstractBlobContainer {\n                 parts\n             );\n             S3BlobStore.configureRequestForMetrics(complRequest, blobStore, Operation.PUT_MULTIPART_OBJECT, purpose);\n-            SocketAccess.doPrivilegedVoid(() -> clientReference.client().completeMultipartUpload(complRequest));\n+            try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+                SocketAccess.doPrivilegedVoid(() -> clientReference.client().completeMultipartUpload(complRequest));\n+            }\n             success = true;\n \n         } catch (final AmazonClientException e) {\n@@ -605,15 +623,15 @@ class S3BlobContainer extends AbstractBlobContainer {\n     private class CompareAndExchangeOperation {\n \n         private final OperationPurpose purpose;\n-        private final AmazonS3 client;\n+        private final S3BlobStore s3BlobStore;\n         private final String bucket;\n         private final String rawKey;\n         private final String blobKey;\n         private final ThreadPool threadPool;\n \n-        CompareAndExchangeOperation(OperationPurpose purpose, AmazonS3 client, String bucket, String key, ThreadPool threadPool) {\n+        CompareAndExchangeOperation(OperationPurpose purpose, S3BlobStore s3BlobStore, String bucket, String key, ThreadPool threadPool) {\n             this.purpose = purpose;\n-            this.client = client;\n+            this.s3BlobStore = s3BlobStore;\n             this.bucket = bucket;\n             this.rawKey = key;\n             this.blobKey = buildKey(key);\n@@ -746,8 +764,8 @@ class S3BlobContainer extends AbstractBlobContainer {\n             final var listRequest = new ListMultipartUploadsRequest(bucket);\n             listRequest.setPrefix(blobKey);\n             S3BlobStore.configureRequestForMetrics(listRequest, blobStore, Operation.LIST_OBJECTS, purpose);\n-            try {\n-                return SocketAccess.doPrivileged(() -> client.listMultipartUploads(listRequest)).getMultipartUploads();\n+            try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+                return SocketAccess.doPrivileged(() -> clientReference.client().listMultipartUploads(listRequest)).getMultipartUploads();\n             } catch (AmazonS3Exception e) {\n                 if (e.getStatusCode() == 404) {\n                     return List.of();\n@@ -759,7 +777,9 @@ class S3BlobContainer extends AbstractBlobContainer {\n         private String initiateMultipartUpload() {\n             final var initiateRequest = new InitiateMultipartUploadRequest(bucket, blobKey);\n             S3BlobStore.configureRequestForMetrics(initiateRequest, blobStore, Operation.PUT_MULTIPART_OBJECT, purpose);\n-            return SocketAccess.doPrivileged(() -> client.initiateMultipartUpload(initiateRequest)).getUploadId();\n+            try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+                return SocketAccess.doPrivileged(() -> clientReference.client().initiateMultipartUpload(initiateRequest)).getUploadId();\n+            }\n         }\n \n         private PartETag uploadPart(BytesReference updated, String uploadId) throws IOException {\n@@ -772,7 +792,9 @@ class S3BlobContainer extends AbstractBlobContainer {\n             uploadPartRequest.setInputStream(updated.streamInput());\n             uploadPartRequest.setPartSize(updated.length());\n             S3BlobStore.configureRequestForMetrics(uploadPartRequest, blobStore, Operation.PUT_MULTIPART_OBJECT, purpose);\n-            return SocketAccess.doPrivileged(() -> client.uploadPart(uploadPartRequest)).getPartETag();\n+            try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+                return SocketAccess.doPrivileged(() -> clientReference.client().uploadPart(uploadPartRequest)).getPartETag();\n+            }\n         }\n \n         private int getUploadIndex(String targetUploadId, List<MultipartUpload> multipartUploads) {\n@@ -866,7 +888,9 @@ class S3BlobContainer extends AbstractBlobContainer {\n             try {\n                 final var request = new AbortMultipartUploadRequest(bucket, blobKey, uploadId);\n                 S3BlobStore.configureRequestForMetrics(request, blobStore, Operation.ABORT_MULTIPART_OBJECT, purpose);\n-                SocketAccess.doPrivilegedVoid(() -> client.abortMultipartUpload(request));\n+                try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+                    SocketAccess.doPrivilegedVoid(() -> clientReference.client().abortMultipartUpload(request));\n+                }\n             } catch (AmazonS3Exception e) {\n                 if (e.getStatusCode() != 404) {\n                     throw e;\n@@ -878,7 +902,9 @@ class S3BlobContainer extends AbstractBlobContainer {\n         private void completeMultipartUpload(String uploadId, PartETag partETag) {\n             final var completeMultipartUploadRequest = new CompleteMultipartUploadRequest(bucket, blobKey, uploadId, List.of(partETag));\n             S3BlobStore.configureRequestForMetrics(completeMultipartUploadRequest, blobStore, Operation.PUT_MULTIPART_OBJECT, purpose);\n-            SocketAccess.doPrivilegedVoid(() -> client.completeMultipartUpload(completeMultipartUploadRequest));\n+            try (AmazonS3Reference clientReference = s3BlobStore.clientReference()) {\n+                SocketAccess.doPrivilegedVoid(() -> clientReference.client().completeMultipartUpload(completeMultipartUploadRequest));\n+            }\n         }\n     }\n \n@@ -890,8 +916,7 @@ class S3BlobContainer extends AbstractBlobContainer {\n         BytesReference updated,\n         ActionListener<OptionalBytesReference> listener\n     ) {\n-        final var clientReference = blobStore.clientReference();\n-        ActionListener.run(ActionListener.releaseAfter(listener.delegateResponse((delegate, e) -> {\n+        ActionListener.run(listener.delegateResponse((delegate, e) -> {\n             logger.trace(() -> Strings.format(\"[%s]: compareAndExchangeRegister failed\", key), e);\n             if (e instanceof AmazonS3Exception amazonS3Exception && amazonS3Exception.getStatusCode() == 404) {\n                 // an uncaught 404 means that our multipart upload was aborted by a concurrent operation before we could complete it\n@@ -899,8 +924,8 @@ class S3BlobContainer extends AbstractBlobContainer {\n             } else {\n                 delegate.onFailure(e);\n             }\n-        }), clientReference),\n-            l -> new CompareAndExchangeOperation(purpose, clientReference.client(), blobStore.bucket(), key, blobStore.getThreadPool()).run(\n+        }),\n+            l -> new CompareAndExchangeOperation(purpose, blobStore, blobStore.bucket(), key, blobStore.getThreadPool()).run(\n                 expected,\n                 updated,\n                 l\n@@ -995,36 +1020,34 @@ class S3BlobContainer extends AbstractBlobContainer {\n         return new ThreadedActionListener<>(blobStore.getSnapshotExecutor(), ActionListener.releaseAfter(new ActionListener<>() {\n             @Override\n             public void onResponse(Void unused) {\n-                try (var clientReference = blobStore.clientReference()) {\n-                    for (final var abortMultipartUploadRequest : abortMultipartUploadRequests) {\n-                        abortMultipartUploadRequest.putCustomQueryParameter(\n-                            S3BlobStore.CUSTOM_QUERY_PARAMETER_PURPOSE,\n-                            OperationPurpose.SNAPSHOT_DATA.getKey()\n+                for (final var abortMultipartUploadRequest : abortMultipartUploadRequests) {\n+                    abortMultipartUploadRequest.putCustomQueryParameter(\n+                        S3BlobStore.CUSTOM_QUERY_PARAMETER_PURPOSE,\n+                        OperationPurpose.SNAPSHOT_DATA.getKey()\n+                    );\n+                    try (var clientReference = blobStore.clientReference()) {\n+                        SocketAccess.doPrivilegedVoid(() -> clientReference.client().abortMultipartUpload(abortMultipartUploadRequest));\n+                        logger.info(\n+                            \"cleaned up dangling multipart upload [{}] of blob [{}][{}][{}]\",\n+                            abortMultipartUploadRequest.getUploadId(),\n+                            blobStore.getRepositoryMetadata().name(),\n+                            abortMultipartUploadRequest.getBucketName(),\n+                            abortMultipartUploadRequest.getKey()\n                         );\n-                        try {\n-                            SocketAccess.doPrivilegedVoid(() -> clientReference.client().abortMultipartUpload(abortMultipartUploadRequest));\n-                            logger.info(\n-                                \"cleaned up dangling multipart upload [{}] of blob [{}][{}][{}]\",\n+                    } catch (Exception e) {\n+                        // Cleanup is a best-effort thing, we can't do anything better than log and carry on here. Note that any failure\n+                        // is surprising, even a 404 means that something else aborted/completed the upload at a point where there\n+                        // should be no other processes interacting with the repository.\n+                        logger.warn(\n+                            Strings.format(\n+                                \"failed to clean up multipart upload [{}] of blob [{}][{}][{}]\",\n                                 abortMultipartUploadRequest.getUploadId(),\n                                 blobStore.getRepositoryMetadata().name(),\n                                 abortMultipartUploadRequest.getBucketName(),\n                                 abortMultipartUploadRequest.getKey()\n-                            );\n-                        } catch (Exception e) {\n-                            // Cleanup is a best-effort thing, we can't do anything better than log and carry on here. Note that any failure\n-                            // is surprising, even a 404 means that something else aborted/completed the upload at a point where there\n-                            // should be no other processes interacting with the repository.\n-                            logger.warn(\n-                                Strings.format(\n-                                    \"failed to clean up multipart upload [{}] of blob [{}][{}][{}]\",\n-                                    abortMultipartUploadRequest.getUploadId(),\n-                                    blobStore.getRepositoryMetadata().name(),\n-                                    abortMultipartUploadRequest.getBucketName(),\n-                                    abortMultipartUploadRequest.getKey()\n-                                ),\n-                                e\n-                            );\n-                        }\n+                            ),\n+                            e\n+                        );\n                     }\n                 }\n             }\ndiff --git a/modules/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3BlobStoreContainerTests.java b/modules/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3BlobStoreContainerTests.java\nindex f52b3f4b..c6e8f74a 100644\n--- a/modules/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3BlobStoreContainerTests.java\n+++ b/modules/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3BlobStoreContainerTests.java\n@@ -115,8 +115,7 @@ public class S3BlobStoreContainerTests extends ESTestCase {\n         }\n \n         final AmazonS3 client = mock(AmazonS3.class);\n-        final AmazonS3Reference clientReference = new AmazonS3Reference(client);\n-        when(blobStore.clientReference()).thenReturn(clientReference);\n+        when(blobStore.clientReference()).thenAnswer(invocation -> new AmazonS3Reference(client));\n \n         final ArgumentCaptor<PutObjectRequest> argumentCaptor = ArgumentCaptor.forClass(PutObjectRequest.class);\n         when(client.putObject(argumentCaptor.capture())).thenReturn(new PutObjectResult());\n@@ -188,8 +187,7 @@ public class S3BlobStoreContainerTests extends ESTestCase {\n         }\n \n         final AmazonS3 client = mock(AmazonS3.class);\n-        final AmazonS3Reference clientReference = new AmazonS3Reference(client);\n-        when(blobStore.clientReference()).thenReturn(clientReference);\n+        when(blobStore.clientReference()).thenAnswer(invocation -> new AmazonS3Reference(client));\n \n         final ArgumentCaptor<InitiateMultipartUploadRequest> initArgCaptor = ArgumentCaptor.forClass(InitiateMultipartUploadRequest.class);\n         final InitiateMultipartUploadResult initResult = new InitiateMultipartUploadResult();\n@@ -276,11 +274,7 @@ public class S3BlobStoreContainerTests extends ESTestCase {\n         when(blobStore.getStorageClass()).thenReturn(randomFrom(StorageClass.values()));\n \n         final AmazonS3 client = mock(AmazonS3.class);\n-        final AmazonS3Reference clientReference = new AmazonS3Reference(client);\n-        doAnswer(invocation -> {\n-            clientReference.incRef();\n-            return clientReference;\n-        }).when(blobStore).clientReference();\n+        when(blobStore.clientReference()).thenAnswer(invocation -> new AmazonS3Reference(client));\n \n         final String uploadId = randomAlphaOfLength(25);\n ",
  "logs_path": "auggie/sonnet4.5/a9463435/elastic_elasticsearch_pr114989/logs.jsonl",
  "errors": [],
  "edit_run_id": "a9463435",
  "test_label": "v0"
}