# Long-Context-Bench

A benchmark for evaluating and ranking long-context code editing capabilities of CLI-based coding agents on real-world GitHub pull requests from massive codebases.

## Overview

Long-Context-Bench evaluates how well coding agents can understand, modify, and integrate changes across **massive repositories with tens of thousands of files** when given natural-language task instructions. The benchmark uses real GitHub PRs from enterprise-scale codebases, with agents tasked to **recreate the PR changes given only the PR description** (title + body).

Primary use case: **side-by-side agent comparison using test labels** for reproducible, apples-to-apples runs across enterprise-scale repositories. Leaderboards are supported as a secondary workflow.

**Key Features:**
- ğŸ† **Leaderboard Generation**: Rank multiple agents across standardized benchmarks
- ğŸ”¬ **Agent Comparison**: Label runs and generate side-by-side performance comparisons
- ğŸ“Š **Standard v0 Dataset**: 40 Elasticsearch PRs with pre-synthesized prompts (~40K files per codebase)
- ğŸ¯ **Pre-synthesized prompts**: LLM-generated natural task instructions (standard for v0 results)
- ğŸ”Œ Agent-agnostic: pluggable adapters for different CLI agents (Auggie, Claude Code, Factory, Codex, Aider, etc.)
- ğŸ“ˆ Comprehensive metrics: correctness, completeness, code reuse, best practices, and more
- âš¡ Scalable: supports sharding and concurrency for parallel execution
- ğŸ“ Traceable: complete provenance tracking for all runs

## Installation

### Prerequisites

- Python â‰¥ 3.11
- Git
- GitHub token for API access (set as `GITHUB_GIT_TOKEN`)
- **Agent authentication** (choose one or more):
  - **Auggie**: OAuth login (recommended) or API token
  - **Claude Code**: OAuth login (recommended) or API key
  - **Factory CLI**: OAuth login (run `droid` interactively first)
  - **Codex CLI**: Requires `OPENAI_API_KEY`
  - **Aider**: Requires API keys for model provider

### Install from source

```bash
git clone https://github.com/AugmentedAJ/Long-Context-Code-Bench.git
cd Long-Context-Code-Bench
pip install -e .
```
### Development environment (recommended)

For local development we recommend using a virtual environment so that all
Python dependencies are isolated from your system Python:

```bash
python -m venv .venv
source .venv/bin/activate
python -m pip install -e ".[dev]"
```

This installs Long-Context-Bench in editable mode along with all dev
dependencies (pytest, black, ruff, mypy, etc.). All subsequent commands in
this README assume you are running inside that environment.

If you see errors like `ModuleNotFoundError: No module named 'click'` or
`ModuleNotFoundError: No module named 'rich'`, make sure you have run one of
these installs in your active environment:

```bash
# minimal (runtime only)
python -m pip install -e .

# full dev environment
python -m pip install -e ".[dev]"
```


## Quick Start: Leaderboard & Comparison

The primary workflows are generating leaderboards to rank agents and comparing specific agents side-by-side using test labels.

### 1. Authenticate with Agents

**Recommended: Use OAuth (subscription mode)**

For Auggie:
```bash
auggie login  # Opens browser for OAuth authentication
```

For Claude Code:
```bash
claude setup-token  # Sets up subscription-based authentication
```

**Alternative: Use API keys/tokens**

```bash
export GITHUB_GIT_TOKEN=your_github_token

# For Auggie (if not using OAuth)
export AUGMENT_API_TOKEN=your_augment_token

# For Claude Code (if not using OAuth)
export ANTHROPIC_API_KEY=your_key

# For LLM judge (optional)
export OPENAI_API_KEY=your_key
```

### 2. Use Pre-Built Dataset

The v0 dataset is included in the repository with pre-synthesized prompts:

```bash
# Dataset is ready to use at data/samples/v0/
ls data/samples/v0/
```

**Note:** The v0 dataset includes 40 Elasticsearch PRs with pre-synthesized task instructions generated by `auggie/claude-sonnet-4.5`. This is the **standard dataset for all published v0 benchmark results**. No setup requiredâ€”the pipeline automatically uses these samples!

### 3. Run Agents with Test Label

Run each agent you want to compare, using the **same test label**:

```bash
# Run Auggie
long-context-bench edit data/samples/v0 \
  --runner auggie \
  --model claude-sonnet-4.5 \
  --test-label "sonnet-4.5-comparison"

# Run Claude Code
long-context-bench edit data/samples/v0 \
  --runner claude-code \
  --model claude-sonnet-4.5 \
  --test-label "sonnet-4.5-comparison"
```

**Note:** Agent runs are non-deterministic. Running the same agent multiple times will produce different results due to the stochastic nature of LLMs.

### 4. Evaluate Results

Judge the agent outputs against ground truth:

```bash
long-context-bench judge \
  --edit-run-ids <run_id_1>,<run_id_2> \
  --test-label "sonnet-4.5-comparison"
```

You can find the edit run IDs in the console output or in `output/edits/<runner>/<model>/` directories.

### 5. Generate Leaderboard or Comparison

**Option A: Generate Leaderboard (Ranked)**

Create a ranked leaderboard of all agents:

```bash
long-context-bench compare output/ "sonnet-4.5-comparison" \
  --format leaderboard \
  --rank-by mean_aggregate \
  --output-file leaderboard.csv
```

This displays agents ranked by performance (default: mean aggregate score).

**Option B: Generate Side-by-Side Comparison**

Create a detailed side-by-side comparison:

```bash
long-context-bench compare output/ "sonnet-4.5-comparison" \
  --format comparison \
  --output-file comparison.csv
```

Both formats support CSV and JSON output.

## Features

### Leaderboard Generation

Rank multiple agents across standardized benchmarks:

1. **Run agents with a test label**: Execute multiple agents/models with the same test label (e.g., `--test-label "v0-leaderboard"`)
2. **Generate leaderboard**: Use `compare --format leaderboard` to rank all agents by performance
### Agents-as-judge head-to-head evaluation

In addition to scalar LLM-judge scores, Long-Context-Bench supports
**agents acting as judges over each other**.

The `head-to-head-pr` command:
- Finds all agent submissions (edits) for a given PR
- Uses each agent as a judge over every other agent's patch
- Reuses scalar scores from a prior LLM judge run when available
- Does **not** make any new LLM-as-judge calls in this stage

Example: run head-to-head for a single Elasticsearch PR where Auggie,
Claude Code, and Factory have all produced edits:

```bash
long-context-bench head-to-head-pr \
  --pr-number 114860 \
  --judge-model placeholder \
  --output-dir output \
  --cache-dir .repo_cache \
  --include-codebase-context \
  --force
```

Notes:
- `--judge-model` is only used to look up existing scalar scores from the
  `cross_agent_analysis` stage; no new LLM calls are made.
- For each pairwise decision, `judge_runner` and `judge_model` are taken
  from the agent's original edit (e.g., `auggie/sonnet4.5`,
  `claude-code/claude-sonnet-4-5`).
- Claude Code runs under a PTY so that its Ink-based TTY handling works
  reliably in CI and other non-interactive environments.

3. **Customize ranking**: Use `--rank-by` to rank by different metrics (mean_aggregate, success_rate, tasks_per_hour, etc.)
4. **Export results**: Save leaderboard as CSV or JSON for sharing

Example command:
```bash
long-context-bench compare output/ "v0-leaderboard" \
  --format leaderboard \
  --rank-by mean_aggregate \
  --output-file leaderboard.csv
```

Example leaderboard output:
```
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rank â”‚ Agent               â”‚ Success Rate â”‚ Mean Agg     â”‚ Tasks/Hour   â”‚ Total Samplesâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1   â”‚ auggie/sonnet-4.5   â”‚ 88.0%        â”‚ 0.82         â”‚ 85.3         â”‚ 50           â”‚
â”‚  2   â”‚ claude-code/sonnet  â”‚ 85.0%        â”‚ 0.78         â”‚ 72.1         â”‚ 50           â”‚
â”‚  3   â”‚ auggie/opus-4       â”‚ 82.0%        â”‚ 0.75         â”‚ 45.2         â”‚ 50           â”‚
â”‚  4   â”‚ cursor/gpt-4        â”‚ 78.0%        â”‚ 0.71         â”‚ 92.5         â”‚ 50           â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ranking Metrics:**
- `mean_aggregate` (default): Overall performance score
- `success_rate`: Percentage of successful completions
- `tasks_per_hour`: Speed/throughput metric
- `mean_correctness`: Accuracy of changes
- `mean_completeness`: Coverage of required changes

### Agent Comparison with Test Labels

Compare specific agents side-by-side:

1. **Label your runs**: Add `--test-label "my-comparison"` to edit and judge commands
2. **Run multiple agents**: Execute different agents/models with the same test label
3. **Generate comparison**: Use `compare` command to see side-by-side metrics

Example comparison output:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric              â”‚ auggie       â”‚ claude-code  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Success Rate        â”‚ 85.0%        â”‚ 82.0%        â”‚
â”‚ Mean Correctness    â”‚ 0.78         â”‚ 0.75         â”‚
â”‚ Mean Completeness   â”‚ 0.82         â”‚ 0.79         â”‚
â”‚ Mean Aggregate      â”‚ 0.78         â”‚ 0.75         â”‚
â”‚ Tasks/Hour          â”‚ 80.0         â”‚ 69.2         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Built-in Dataset

The v0 dataset (40 Elasticsearch PRs with pre-synthesized prompts) is included in the repository at `data/samples/v0/`. No need to download or specify file pathsâ€”the pipeline automatically uses these samples!

### Prompt Synthesis (Optional)

Long-Context-Bench supports **two types of task instructions**:

1. **Template-based** (default): Simple concatenation of PR title and body
2. **Synthesized** (optional): LLM-generated natural instructions that mimic human requests

**Why synthesize?** Synthesized prompts are more concise, natural, and focused on the core taskâ€”similar to how developers actually use coding agents.

**Example comparison:**

Template-based (369 chars):
```
You are working on a codebase. Your task is to make the necessary code changes to accomplish the following:

[DOCS] Update local data extraction version info

This PR updates the documentation to reflect the new version of the local data extraction module...

Please make all necessary code changes to complete this task.
```

Synthesized (69 chars):
```
Update the documentation for the local data extraction module version
```

**Usage:**

```bash
# Step 1: Sample with synthesis (requires LiteLLM + API key)
pip install litellm
export ANTHROPIC_API_KEY=your_key

long-context-bench sample \
  data/elasticsearch_prs_50.json \
  --synthesize \
  --synthesis-model claude-3-7-sonnet-20250219

# Step 2: Run with synthesized prompts
long-context-bench edit \
  output/samples/v0 \
  --runner auggie \
  --model claude-3-7-sonnet-20250219 \
  --use-synthesized \
  --test-label synthesized-prompts

# Step 3: Compare with template-based prompts
long-context-bench edit \
  output/samples/v0 \
  --runner auggie \
  --model claude-3-7-sonnet-20250219 \
  --test-label template-prompts
```

**Benefits:**
- âœ… **Cached**: Synthesize once during sampling, reuse forever
- âœ… **Comparable**: Use test labels to compare synthesized vs template-based
- âœ… **Flexible**: Supports any LiteLLM model
- âœ… **Optional**: Falls back to template-based if unavailable

See [docs/PROMPT_SYNTHESIS.md](docs/PROMPT_SYNTHESIS.md) for details.

### Repository Caching & Workspace Isolation

The benchmark implements strict workspace isolation to ensure valid, unbiased evaluation:

**Workspace Materialization:**
- Each agent run gets a fresh, isolated workspace initialized at the base commit
- Shallow fetch (--depth=1) minimizes git history exposure
- The `.git` directory is hidden from agents during execution to prevent history inspection
- After agent execution, `.git` is restored to capture accurate diffs

**Cache Usage:**
- Repositories are cached in `.repo_cache/` for sample/judge stages (not for agent workspaces)
- Shallow fetches minimize bandwidth and history exposure
- Cache is used only for fetching commits needed for ground-truth diffs

**Security:**
- Agents cannot access PR fix commits or full git history
- No persistent remote configured in agent workspaces
- Git interactive prompts disabled during execution

### Selective Execution

Run specific PRs using:
- `--pr-numbers`: Filter by PR number (e.g., `--pr-numbers "115001,114998"`)
- `--pr-indices`: Filter by index in dataset (e.g., `--pr-indices "0,1,2"`)

### Resuming Interrupted Runs

**The pipeline automatically resumes interrupted runs.** If a run is stopped mid-way (e.g., due to timeout, crash, or manual interruption), simply re-run the same command and it will:

- âœ… **Skip completed PRs** - Checks for existing `sample.json`, `edit_summary.json`, and `judge.json` files
- âœ… **Continue from where it left off** - Only processes PRs that haven't been completed
- âœ… **Preserve existing results** - Does not overwrite or re-run completed work

**Example:**
```bash
# Initial run (interrupted after 10 PRs)
long-context-bench pipeline \
  --runner auggie \
  --model sonnet4.5 \
  --test-label v0-auggie-sonnet45

# Resume run (skips the 10 completed PRs, continues with remaining)
long-context-bench pipeline \
  --runner auggie \
  --model sonnet4.5 \
  --test-label v0-auggie-sonnet45
```

**Force re-run:** Use `--force` to re-run all stages regardless of existing outputs:
```bash
long-context-bench pipeline \
  --runner auggie \
  --model sonnet4.5 \
  --test-label v0-auggie-sonnet45 \
  --force
```

**Note:** The `--force` flag is also available for individual stages (`sample`, `edit`, `judge`).

## Pipeline Stages

The benchmark consists of three stages that can be run together (pipeline mode) or separately (staged mode).

### Pipeline Mode (All Stages Together)

Run all stages in one command:

```bash
long-context-bench pipeline \
  --runner auggie \
  --model claude-sonnet-4
```

### Staged Mode (Run Stages Separately)

Run stages independently for more control:

#### 1. Sample Stage

Extracts PR metadata and creates sample.json files (uses built-in dataset by default):

```bash
long-context-bench sample data/elasticsearch_prs_50.json \
  --dataset-version v0 \
  --output-dir data/samples
```

**Output:** `data/samples/v0/<pr_id>/sample.json`

**Note:** The v0 dataset is already included in the repository, so you typically don't need to run this stage.

#### 2. Edit Stage

Runs the agent on samples and captures diffs. Each run gets a unique ID:

```bash
long-context-bench edit data/samples/v0 \
  --runner auggie \
  --model claude-sonnet-4 \
  --dataset-version v0 \
  --output-dir output/edits
```

**Output:**
- `output/edits/<runner>/<model>/<edit_run_id>/edit_run_manifest.json` - Run metadata
- `output/edits/<runner>/<model>/<edit_run_id>/<pr_id>/edit.json` - Full edit data
- `output/edits/<runner>/<model>/<edit_run_id>/<pr_id>/edit_summary.json` - Clean JSON without inline patch
- `output/edits/<runner>/<model>/<edit_run_id>/<pr_id>/edit.patch` - Diff patch file
- `output/edits/<runner>/<model>/<edit_run_id>/<pr_id>/logs.jsonl` - Agent logs

**Returns:** Edit run ID (e.g., `a1b2c3d4`)

**Monitoring Agent Output:**

By default, agent output is captured to log files. To see agent output in real-time during execution, use the `--stream-output` flag:

```bash
long-context-bench edit data/samples/v0 \
  --runner auggie \
  --model claude-sonnet-4 \
  --stream-output
```

This is useful for:
- Monitoring long-running agent executions
- Debugging agent behavior in real-time
- Observing agent progress on complex tasks

The output is still saved to log files even when streaming is enabled.

#### 3. Judge Stage

Scores agent edits against ground truth. Can evaluate one or more edit runs:

```bash
# Evaluate specific edit run(s)
long-context-bench judge \
  --edit-run-ids a1b2c3d4,b2c3d4e5 \
  --judge-model anthropic/claude-3-5-sonnet-20241022 \
  --output-dir output/judges
```

**Output:** `output/judges/llm/<judge_model>/<judge_run_id>/<pr_id>/judge.json`
**Returns:** Judge run ID (e.g., `e5f6g7h8`)

#### 4. Summary

Generate aggregate statistics for specific runs:

```bash
long-context-bench summary \
  --edit-run-id a1b2c3d4 \
  --judge-run-id e5f6g7h8 \
  --output-dir output/summaries/my_run \
  output
```

### Staged Execution Use Cases

**Compare Multiple Models:**
```bash
# Run two models
long-context-bench edit --runner auggie --model claude-sonnet-4 output/samples/v0
# Returns: Edit run ID: aaaa1111

long-context-bench edit --runner auggie --model gpt-4 output/samples/v0
# Returns: Edit run ID: bbbb2222

# Evaluate both
long-context-bench judge --edit-run-ids aaaa1111,bbbb2222 --judge-model anthropic/claude-3-5-sonnet-20241022
```

**Re-evaluate with Different Judge:**
```bash
# Initial evaluation with Claude
long-context-bench judge --edit-run-ids aaaa1111 --judge-model anthropic/claude-3-5-sonnet-20241022

# Re-evaluate with GPT-4
long-context-bench judge --edit-run-ids aaaa1111 --judge-model gpt-4
```

## Evaluation Metrics

Each sample is scored on five primary metrics (range: -1.0 to 1.0):

1. **Correctness**: Does the change implement the intended behavior?
2. **Completeness**: Does it achieve all requested changes?
3. **Code Reuse**: Preference for leveraging existing code over duplication
4. **Best Practices**: Style, structure, and idiomatic usage
5. **Unsolicited Documentation**: Penalizes documentation added when not requested

**Aggregate Score**: Unweighted average of the five metrics

### LLM Judge

The benchmark uses an LLM (via LiteLLM) to evaluate diffs with detailed reasoning:
- Nuanced evaluation of code quality
- Provides rationale for scores
- Supports any model via LiteLLM (OpenAI, Anthropic, etc.)
- Consistent settings (temperature=0.0, seed=42)

```bash
# Using Claude (via Anthropic)
export ANTHROPIC_API_KEY=your_key
long-context-bench judge \
  --edit-run-ids a1b2c3d4 \
  --judge-model anthropic/claude-3-5-sonnet-20241022

# Using OpenAI
export OPENAI_API_KEY=your_key
long-context-bench judge \
  --edit-run-ids a1b2c3d4 \
  --judge-model gpt-4o-mini

# Using any LiteLLM-supported model
long-context-bench judge \
  --edit-run-ids a1b2c3d4 \
  --judge-model bedrock/anthropic.claude-v2
```

### Cross-Agent Analysis

Compare multiple agents' solutions for the same PR to understand different approaches and identify the best performer.

#### Features

- **Multi-Agent Comparison**: Analyzes all agent attempts for a specific PR
- **Individual Scoring**: Judges each agent's solution independently
- **Comparative Analysis**: LLM-generated side-by-side comparison (in comparative mode)
- **Ranking**: Automatically ranks agents by performance
- **Approach Analysis**: Identifies key differences in how agents solved the problem

#### Usage

```bash
# Basic cross-agent analysis with comparative analysis
export ANTHROPIC_API_KEY=your_key
long-context-bench analyze-pr \
  --pr-number 114869 \
  --test-label v0 \
  --judge-model anthropic/claude-3-5-sonnet-20241022 \
  --comparative

# Analyze specific PR without comparative analysis
long-context-bench analyze-pr \
  --pr-number 114869 \
  --test-label v0 \
  --judge-model anthropic/claude-3-5-sonnet-20241022 \
  --no-comparative

# Using OpenAI for comparative analysis
export OPENAI_API_KEY=your_key
long-context-bench analyze-pr \
  --pr-number 114869 \
  --test-label v0 \
  --judge-model gpt-4o \
  --comparative
```

#### Options

- `--comparative/--no-comparative`: Enable/disable cross-agent comparison and ranking (default: enabled)
- `--judge-model`: LLM model to use for evaluation (required)

#### Output

Results are saved to `output/cross_agent_analysis/pr{number}_{run_id}.json`:

```json
{
  "pr_number": 114869,
  "task_instructions": "Fix the bug in search functionality",
  "agent_results": [
    {
      "runner": "auggie",
      "model": "sonnet4.5",
      "status": "success",
      "aggregate": 0.85,
      "scores": {
        "correctness": 0.9,
        "completeness": 0.95,
        "code_reuse": 0.8,
        "best_practices": 0.85,
        "unsolicited_docs": 1.0
      }
    },
    {
      "runner": "claude-code",
      "model": "claude-sonnet-4-5",
      "aggregate": 0.72,
      ...
    }
  ],
  "comparative_analysis": {
    "summary": "Auggie produced a more complete solution...",
    "best_agent": "auggie:sonnet4.5",
    "best_agent_reasoning": "Higher scores across all metrics...",
    "approach_differences": "Auggie leveraged existing utilities...",
    "ranking": ["auggie:sonnet4.5", "claude-code:claude-sonnet-4-5"]
  }
}
```

#### Example: Analyze v0 Test Results

```bash
# Compare all agents on PR 114869 from v0 test run
export ANTHROPIC_API_KEY=your_key
long-context-bench analyze-pr \
  --pr-number 114869 \
  --test-label v0 \
  --judge-model anthropic/claude-3-5-sonnet-20241022 \
  --comparative

# Output shows:
# - Individual scores for auggie:sonnet4.5, claude-code:claude-sonnet-4-5, factory:claude-sonnet-4-5-20250929
# - Comparative analysis of different approaches
# - Ranking from best to worst
```

## Supported Runners

The benchmark supports multiple CLI coding agents through pluggable adapters:

### Auggie

Augment's CLI coding agent.

```bash
# Recommended: Use OAuth (run 'auggie login' first)
long-context-bench pipeline \
  --runner auggie \
  --model claude-sonnet-4

# Alternative: Use API token
export AUGMENT_API_TOKEN=your_token
long-context-bench pipeline \
  --runner auggie \
  --model claude-sonnet-4
```

**Authentication:**
- **Recommended:** OAuth via `auggie login` (uses subscription)
- **Alternative:** Set `AUGMENT_API_TOKEN` environment variable

**Model aliases:** Use `sonnet`, `opus`, `haiku` or full model names

### Claude Code

Anthropic's command-line coding agent.

```bash
# Recommended: Use OAuth (run 'claude setup-token' first)
long-context-bench pipeline \
  --runner claude-code \
  --model sonnet

# Alternative: Use API key
export ANTHROPIC_API_KEY=your_key
long-context-bench pipeline \
  --runner claude-code \
  --model sonnet
```

**Authentication:**
- **Recommended:** OAuth via `claude setup-token` (uses subscription)
- **Alternative:** Set `ANTHROPIC_API_KEY` environment variable

**Model aliases:** Use `sonnet`, `opus`, `haiku` or full model names like `claude-sonnet-4`

#### Claude Code authentication modes and visibility

By default, the harness selects Claude authentication automatically (uses API key if `ANTHROPIC_API_KEY` is set, otherwise uses your Claude subscription token from `claude setup-token`). You can override this with `LCB_CLAUDE_AUTH`:

- `LCB_CLAUDE_AUTH=auto` (default): prefer API key if present, else subscription
- `LCB_CLAUDE_AUTH=subscription`: force subscription; the harness strips `ANTHROPIC_*` env vars for the Claude run
- `LCB_CLAUDE_AUTH=api-key`: force API key; requires `ANTHROPIC_API_KEY`

Visibility:
- The edit stage prints a clear line to stdout, for example:
  `Claude auth: subscription (mode=auto, ANTHROPIC_API_KEY=absent)`
- Each PR's `logs.jsonl` also records an `auth_info` event with fields `auth_mode`, `used_auth`, and `anthropic_api_key_present`.

Examples:
```bash
# Force subscription (ignore any Anthropic env vars for Claude runs)
export LCB_CLAUDE_AUTH=subscription
long-context-bench edit --runner claude-code --model sonnet output/samples/v0

# Force API key
export ANTHROPIC_API_KEY=sk-...
export LCB_CLAUDE_AUTH=api-key
long-context-bench edit --runner claude-code --model sonnet output/samples/v0

# Default auto (use API key if set, else subscription)
unset LCB_CLAUDE_AUTH
long-context-bench edit --runner claude-code --model sonnet output/samples/v0
```


### Codex CLI

OpenAI's command-line coding agent.

```bash
long-context-bench pipeline \
  --runner codex \
  --model gpt-5-codex \
  --agent-binary /path/to/codex  # Optional, defaults to 'codex' in PATH
```

**Install:** `npm install -g @openai/codex`
**Environment:** Requires `OPENAI_API_KEY`

### Factory CLI

Factory AI's command-line coding agent.

```bash
long-context-bench pipeline \
  --runner factory \
  --model claude-sonnet-4-5-20250929 \
  --agent-binary /path/to/droid  # Optional, defaults to 'droid' in PATH
```

**Install:** `npm install -g @factory-ai/droid`
**Authentication:** OAuth-based (run `droid` interactively first to authenticate)
**Models:** Claude models work with Factory subscription (e.g., `claude-sonnet-4-5-20250929`). GPT models require payment.

### Aider

Open-source AI pair programming tool.

```bash
long-context-bench pipeline \
  --runner aider \
  --model claude-sonnet-4 \
  --agent-binary /path/to/aider  # Optional, defaults to 'aider' in PATH
```

**Install:** `pip install aider-chat`
**Environment:** Requires API keys for the model provider (e.g., `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`)

### Generic CLI Agent

For other CLI agents not listed above, use the generic runner:

```bash
long-context-bench pipeline \
  --runner generic \
  --model your-model \
  --agent-binary /path/to/your/agent
```

The generic runner passes task instructions via stdin.

## Configuration Options

### Required Options

- `--runner`: Agent runner name (e.g., `auggie`, `generic`)
- `--model`: Model name to use

### Optional Flags

- `--agent-binary`: Path to agent binary (defaults to runner name in PATH)
- `--timeout`: Timeout in seconds per task (default: 1800)
- `--concurrency`: Max concurrent tasks (default: 1)
- `--total-shards`: Total number of shards (default: 1)
- `--shard-index`: Current shard index, 0-based (default: 0)
- `--disable-retrieval`: Disable retrieval features
- `--disable-shell`: Disable shell access
- `--enable-mcp-codebase-qa`: Enable MCP codebase QA

### Judge Options

- `--judge-model`: LLM judge model (optional, skips judge stage if not provided)

## Viewing Results

### Web Dashboard (Recommended)

The benchmark automatically generates an interactive web dashboard for visualizing results.

#### Starting the Web Server

After running any benchmark command, start the web server:

```bash
cd output/web
npm install  # First time only
npm start
```

Then open http://localhost:3000 in your browser.

The web app provides:
- **Leaderboard**: Compare all runs with filtering and sorting
- **Run Details**: Deep dive into individual run metrics and per-PR results
- **Agent Comparison**: Side-by-side comparison with interactive charts
- **Real-time Updates**: Refresh to see latest results

The web app is automatically deployed and updated when you run:
- `long-context-bench pipeline`
- `long-context-bench summary`
- `long-context-bench stats`
- `long-context-bench compare`

### Packaging and Sharing Results

Package your results for easy sharing with an interactive web UI:

```bash
./scripts/package_results.sh
```

This creates an archive (~4-5MB compressed, ~30MB uncompressed) containing:
- Interactive web dashboard (all HTML/JS/CSS files)
- Result data (summaries, edits, judges, samples)
- **Full agent logs** (stdout/stderr for every task)
- Quick-start launchers (`start.sh` / `start.bat`)

**Sharing options:**

1. **Cloudflare Pages** - Drag & drop `web/` folder to https://pages.cloudflare.com/ (recommended)
2. **Netlify/Vercel** - Deploy the `web/` folder for instant live URL (drag & drop to netlify.com/drop)
3. **GitHub Releases** - Upload the `.tar.gz` file for downloads
4. **GitHub Pages** - Host permanently with version control
5. **Email/Drive** - Share the 4-5MB archive directly

Recipients can extract and run `./start.sh` to view results locally, or you can deploy to any static hosting service for a live URL with full logs available.

### Alternative: Full Output Archive (For Local Development)

If you need the complete `output/` directory structure for local development:

```bash
# Archive the complete output directory
./scripts/archive_full_output.sh
```

This creates a larger archive (~9-10MB compressed, ~60MB uncompressed) with the exact `output/` directory structure including symlinks. Useful for:
- Local development and testing
- Sharing with team members who will run locally
- Preserving the exact directory structure

**Note:** The standard package (`package_results.sh`) already includes all logs and is recommended for most use cases, including deployment to static hosting.

See [DISTRIBUTION.md](DISTRIBUTION.md) for detailed distribution strategies and security considerations.

### Generate Statistics (CLI)

```bash
long-context-bench stats output/
```

This displays:
- Success rate
- Mean scores for all metrics
- Per-PR breakdown (top 10)
- Latency metrics

### Output Files

Results are saved in the following structure:

```
output/
â”œâ”€â”€ web/                          # Interactive web dashboard
â”‚   â”œâ”€â”€ index.html               # Leaderboard view
â”‚   â”œâ”€â”€ summary.html             # Run details view
â”‚   â””â”€â”€ comparison.html          # Agent comparison view
â”œâ”€â”€ index.json                   # Manifest for web app
â”œâ”€â”€ samples/v0/<pr_id>/sample.json
â”œâ”€â”€ edits/<runner>/<model>/<run_id>/<pr_id>/
â”‚   â”œâ”€â”€ edit.json
â”‚   â””â”€â”€ logs.jsonl
â”œâ”€â”€ judges/llm/<judge_model>/<run_id>/<pr_id>/judge.json
â””â”€â”€ summaries/<run_id>/
    â”œâ”€â”€ summary.json
    â”œâ”€â”€ summary.csv
    â””â”€â”€ run_manifest.json
```

## Reproducibility and Provenance

All runs record complete provenance for traceability:

- Dataset version
- Harness version
- Runner and model
- OS and Python version
- All flags and configuration
- Timestamps
- Test label (for comparison runs)

**Important:** Agent runs are **non-deterministic** due to the stochastic nature of LLMs. Running the same agent with identical inputs will produce different outputs each time. This is why the benchmark uses:
- **Unique run IDs** for each execution
- **Test labels** to group related runs for comparison
- **Complete provenance tracking** to understand what produced each result

The **judge stage** (evaluation) uses LLM-based evaluation with consistent settings (temperature=0.0, seed=42) to provide nuanced scoring of agent outputs.

## Dataset

**Version:** v0 (PR Recreation Methodology)

**Size:** 40 PRs from elastic/elasticsearch with pre-synthesized prompts
**Location:** `data/samples/v0/`
**PR URLs:** `data/elasticsearch_prs_50.json` (42 validated URLs, 40 with complete samples)

### Methodology

The v0 dataset evaluates agents on **recreating a PR given its description**:

- **Task Instructions:** Synthesized natural prompts generated by `auggie/claude-sonnet-4.5`
- **Ground Truth:** The actual PR diff (what the developer actually did)
- **Evaluation:** How well can the agent recreate the solution given the task description?

This tests an agent's ability to:
1. Understand a developer's intended changes from a natural task description
2. Navigate and modify a massive codebase (~40K files)
3. Produce changes that match the actual implementation

### Dataset Curation

The v0 dataset was curated in two stages:

**Stage 1: PR URL Validation**

Removed GitHub issue numbers that would cause 404 errors when fetched as PRs:
- **Original dataset:** 50 entries (mix of PRs and issues)
- **Removed issue numbers:** 114968, 114962, 114956, 114947, 114941, 114926, 114902, 114893
- **Result:** 42 validated PR URLs in `data/elasticsearch_prs_50.json`

**Stage 2: Prompt Synthesis**

Generated natural task instructions for all valid PRs:
- **Synthesis Model:** `auggie/claude-sonnet-4.5`
- **Synthesis Date:** 2025-10-30
- **Result:** 40 complete samples in `data/samples/v0/` (2 PRs returned 404 during synthesis)

Each sample includes:
- PR metadata (base/head commits, repo URL, PR number)
- Template-based task instructions (PR title + body)
- **Synthesized task instructions** (concise, natural prompts)
- Diff statistics and context size

**This is the standard dataset that will be used for all published v0 benchmark results.**

The original uncurated dataset is preserved as `data/elasticsearch_prs_50_original.json` for reference. The curation script is available at `scripts/make_v0_valid_from_known_issues.py`.

**Note:** A future dataset version (v2) on the `feature/v2-issue-based-dataset` branch uses a different methodology: mapping issues to their closing PRs and using issue descriptions (the problem) as task instructions instead of PR descriptions (the solution).

The v0 dataset is frozen. Future versions may rotate or expand PRs with semantic versioning.

## License

- **Harness code:** Apache-2.0
- **Documentation:** CC BY 4.0
- **Dataset metadata:** Compliant with GitHub ToS (URLs and metadata only, no source code redistribution)

## Contributing

Contributions are welcome! Please see the PRD (`prd.md`) for detailed requirements and design specifications.

## Citation

If you use Long-Context-Bench in your research, please cite:

```bibtex
@software{long_context_bench,
  title = {Long-Context-Bench: Benchmark for Long-Context Code Editing},
  author = {Augment Code},
  year = {2025},
  version = {0.1.0},
  url = {https://github.com/AugmentedAJ/Long-Context-Code-Bench}
}
```

## Support

For issues and questions, please open an issue on GitHub.

